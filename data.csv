abstract,authorship,topic_id,topic_score
"Socio-technical networks represent emerging cyber-physical infrastructures
that are tightly interwoven with human networks. The coupling between human and
technical networks presents significant challenges in managing, controlling,
and securing these complex, interdependent systems. This paper investigates
game-theoretic frameworks for the design and control of socio-technical
networks, with a focus on critical applications such as misinformation
management, infrastructure optimization, and resilience in socio-cyber-physical
systems (SCPS). Core methodologies, including Stackelberg games, mechanism
design, and dynamic game theory, are examined as powerful tools for modeling
interactions in hierarchical, multi-agent environments. Key challenges
addressed include mitigating human-driven vulnerabilities, managing large-scale
system dynamics, and countering adversarial threats. By bridging individual
agent behaviors with overarching system goals, this work illustrates how the
integration of game theory and control theory can lead to robust, resilient,
and adaptive socio-technical networks. This paper highlights the potential of
these frameworks to dynamically align decentralized agent actions with
system-wide objectives of stability, security, and efficiency.","['Quanyan Zhu', 'Tamer Ba≈üar']",2,0.5982904
"With the advent of social media networks and the vast amount of information
circulating through them, automatic fact verification is an essential component
to prevent the spread of misinformation. It is even more useful to have fact
verification systems that provide explanations along with their classifications
to ensure accurate predictions. To address both of these requirements, we
implement AMREx, an Abstract Meaning Representation (AMR)-based veracity
prediction and explanation system for fact verification using a combination of
Smatch, an AMR evaluation metric to measure meaning containment and textual
similarity, and demonstrate its effectiveness in producing partially
explainable justifications using two community standard fact verification
datasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy
showing the effectiveness of our approach for real-world claim verification. It
follows an interpretable pipeline and returns an explainable AMR node mapping
to clarify the system's veracity predictions when applicable. We further
demonstrate that AMREx output can be used to prompt LLMs to generate
natural-language explanations using the AMR mappings as a guide to lessen the
probability of hallucinations.","['Chathuri Jayaweera', 'Sangpil Youm', 'Bonnie Dorr']",1,0.7115773
"Narratives are key interpretative devices by which humans make sense of
political reality. As the significance of narratives for understanding current
societal issues such as polarization and misinformation becomes increasingly
evident, there is a growing demand for methods that support their empirical
analysis. To this end, we propose a graph-based formalism and machine-guided
method for extracting, representing, and analyzing selected narrative signals
from digital textual corpora, based on Abstract Meaning Representation (AMR).
The formalism and method introduced here specifically cater to the study of
political narratives that figure in texts from digital media such as archived
political speeches, social media posts, political manifestos and transcripts of
parliamentary debates. We conceptualize these political narratives as a type of
ontological narratives: stories by which actors position themselves as
political beings, and which are akin to political worldviews in which actors
present their normative vision of the world, or aspects thereof. We approach
the study of such political narratives as a problem of information retrieval:
starting from a textual corpus, we first extract a graph-like representation of
the meaning of each sentence in the corpus using AMR. Drawing on transferable
concepts from narratology, we then apply a set of heuristics to filter these
graphs for representations of 1) actors, 2) the events in which these actors
figure, and 3) traces of the perspectivization of these events. We approach
these references to actors, events, and instances of perspectivization as core
narrative signals that initiate a further analysis by alluding to larger
political narratives. By means of a case study of State of the European Union
addresses, we demonstrate how the formalism can be used to inductively surface
signals of political narratives from public discourse.","['Armin Pournaki', 'Tom Willaert']",0,0.58086175
"Retrieval-augmented generation methods often neglect the quality of content
retrieved from external knowledge bases, resulting in irrelevant information or
potential misinformation that negatively affects the generation results of
large language models. In this paper, we propose an end-to-end model with
adaptive filtering for retrieval-augmented generation (E2E-AFG), which
integrates answer existence judgment and text generation into a single
end-to-end framework. This enables the model to focus more effectively on
relevant content while reducing the influence of irrelevant information and
generating accurate answers. We evaluate E2E-AFG on six representative
knowledge-intensive language datasets, and the results show that it
consistently outperforms baseline models across all tasks, demonstrating the
effectiveness and robustness of the proposed approach.","['Yun Jiang', 'Zilong Xie', 'Wei Zhang', 'Yun Fang', 'Shuai Pan']",1,0.7031169
"In this paper, we introduce the first release of a large-scale dataset
capturing discourse on $\mathbb{X}$ (a.k.a., Twitter) related to the upcoming
2024 U.S. Presidential Election. Our dataset comprises 22 million publicly
available posts on X.com, collected from May 1, 2024, to July 31, 2024, using a
custom-built scraper, which we describe in detail. By employing targeted
keywords linked to key political figures, events, and emerging issues, we
aligned data collection with the election cycle to capture evolving public
sentiment and the dynamics of political engagement on social media. This
dataset offers researchers a robust foundation to investigate critical
questions about the influence of social media in shaping political discourse,
the propagation of election-related narratives, and the spread of
misinformation. We also present a preliminary analysis that highlights
prominent hashtags and keywords within the dataset, offering initial insights
into the dominant themes and conversations occurring in the lead-up to the
election. Our dataset is available at:
url{https://github.com/sinking8/usc-x-24-us-election","['Ashwin Balasubramanian', 'Vito Zou', 'Hitesh Narayana', 'Christina You', 'Luca Luceri', 'Emilio Ferrara']",10,0.6384562
"Recently, there has been an explosion of large language models created
through fine-tuning with data from larger models. These small models able to
produce outputs that appear qualitatively similar to significantly larger
models. However, one of the key limitations that have been observed with these
models is their propensity to hallucinate significantly more often than larger
models. In particular, they have been observed to generate coherent outputs
that involve factually incorrect information and spread misinformation,
toxicity, and stereotypes. There are many potential causes of hallucination, of
which, one hypothesis is that fine-tuning a model on data produced by a larger
model leads to a knowledge mismatch which contributes to hallucination. In
particular, it is hypothesized that there is a mismatch between the knowledge
that is fed to the model to fine-tune it and the knowledge that is already
present in the graph. Fine-tuning the model on data that has such mismatch
could contribute to an increased propensity to hallucinate. We show that on an
unseen test set, a smaller model fine-tuned on data generated from a larger
model produced more wrong answers when compared to models fine-tuned on data
created by the small model, which confirms the hypothesis.","['Phil Wee', 'Riyadh Baghdadi']",1,0.5473321
"Retrieval-augmented generation (RAG) addresses key limitations of large
language models (LLMs), such as hallucinations and outdated knowledge, by
incorporating external databases. These databases typically consult multiple
sources to encompass up-to-date and various information. However, standard RAG
methods often overlook the heterogeneous source reliability in the multi-source
database and retrieve documents solely based on relevance, making them prone to
propagating misinformation. To address this, we propose Reliability-Aware RAG
(RA-RAG) which estimates the reliability of multiple sources and incorporates
this information into both retrieval and aggregation processes. Specifically,
it iteratively estimates source reliability and true answers for a set of
queries with no labelling. Then, it selectively retrieves relevant documents
from a few of reliable sources and aggregates them using weighted majority
voting, where the selective retrieval ensures scalability while not
compromising the performance. We also introduce a benchmark designed to reflect
real-world scenarios with heterogeneous source reliability and demonstrate the
effectiveness of RA-RAG compared to a set of baselines.","['Jeongyeon Hwang', 'Junyoung Park', 'Hyejin Park', 'Sangdon Park', 'Jungseul Ok']",1,0.6805981
"Developing algorithms to differentiate between machine-generated texts and
human-written texts has garnered substantial attention in recent years.
Existing methods in this direction typically concern an offline setting where a
dataset containing a mix of real and machine-generated texts is given upfront,
and the task is to determine whether each sample in the dataset is from a large
language model (LLM) or a human. However, in many practical scenarios, sources
such as news websites, social media accounts, or on other forums publish
content in a streaming fashion. Therefore, in this online scenario, how to
quickly and accurately determine whether the source is an LLM with strong
statistical guarantees is crucial for these media or platforms to function
effectively and prevent the spread of misinformation and other potential misuse
of LLMs. To tackle the problem of online detection, we develop an algorithm
based on the techniques of sequential hypothesis testing by betting that not
only builds upon and complements existing offline detection techniques but also
enjoys statistical guarantees, which include a controlled false positive rate
and the expected time to correctly identify a source as an LLM. Experiments
were conducted to demonstrate the effectiveness of our method.","['Can Chen', 'Jun-Kun Wang']",1,0.7144151
"Social network platforms (SNP), such as X and TikTok, rely heavily on
user-generated content to attract users and advertisers, yet they have limited
control over content provision, which leads to the proliferation of
misinformation across platforms. As countermeasures, SNPs have implemented
various policies, such as tweet labeling, to notify users about potentially
misleading information, influencing users' responses, either favorably or
unfavorably, to the tagged contents. The population-level response creates a
social nudge to the content provider that encourages it to supply more
authentic content without exerting direct control over the provider. Yet, when
designing such tagging policies to leverage social nudges, SNP must be cautious
about the potential misdetection of misinformation (wrongly detecting factual
content as misinformation and vice versa), which impairs its credibility to
generic users and, hence, its ability to create social nudges. This work
establishes a Bayesian persuaded branching process to study SNP's tagging
policy design under misdetection. Misinformation circulation is modeled by a
multi-type branching process, where users are persuaded through tagging to give
positive and negative comments that influence the spread of misinformation.
When translated into posterior belief space, the SNP's problem is reduced to an
equality-constrained convex optimization, the optimal condition of which is
given by the Lagrangian characterization. The key finding is that SNP's optimal
policy is simply transparent tagging, i.e., revealing the content's
authenticity to the user, albeit midsection, which nudges the provider not to
generate misinformation. We corroborate our findings using numerical
simulations.","['Ya-Ting Yang', 'Tao Li', 'Quanyan Zhu']",1,0.6069323
"Social media platforms have evolved rapidly in modernity without strong
regulation. One clear obstacle faced by current users is that of toxicity.
Toxicity on social media manifests through a number of forms, including
harassment, negativity, misinformation or other means of divisiveness. In this
paper, we characterize literature surrounding toxicity, formalize a definition
of toxicity, propose a novel cycle of internet extremism, list current
approaches to toxicity detection, outline future directions to minimize
toxicity in future social media endeavors, and identify current gaps in
research space. We present a novel perspective of the negative impacts of
social media platforms and fill a gap in literature to help improve the future
of social media platforms.","['Rhett Hanscom', 'Tamara Silbergleit Lehman', 'Qin Lv', 'Shivakant Mishra']",3,0.5942201
"Information-seeking dialogues span a wide range of questions, from simple
factoid to complex queries that require exploring multiple facets and
viewpoints. When performing exploratory searches in unfamiliar domains, users
may lack background knowledge and struggle to verify the system-provided
information, making them vulnerable to misinformation. We investigate the
limitations of response generation in conversational information-seeking
systems, highlighting potential inaccuracies, pitfalls, and biases in the
responses. The study addresses the problem of query answerability and the
challenge of response incompleteness. Our user studies explore how these issues
impact user experience, focusing on users' ability to identify biased,
incorrect, or incomplete responses. We design two crowdsourcing tasks to assess
user experience with different system response variants, highlighting critical
issues to be addressed in future conversational information-seeking research.
Our analysis reveals that it is easier for users to detect response
incompleteness than query answerability and user satisfaction is mostly
associated with response diversity, not factual correctness.","['Weronika ≈Åajewska', 'Krisztian Balog', 'Damiano Spina', 'Johanne Trippas']",0,0.65335727
"This study systematically analyzes the vulnerability of 36 large language
models (LLMs) to various prompt injection attacks, a technique that leverages
carefully crafted prompts to elicit malicious LLM behavior. Across 144 prompt
injection tests, we observed a strong correlation between model parameters and
vulnerability, with statistical analyses, such as logistic regression and
random forest feature analysis, indicating that parameter size and architecture
significantly influence susceptibility. Results revealed that 56 percent of
tests led to successful prompt injections, emphasizing widespread vulnerability
across various parameter sizes, with clustering analysis identifying distinct
vulnerability profiles associated with specific model configurations.
Additionally, our analysis uncovered correlations between certain prompt
injection techniques, suggesting potential overlaps in vulnerabilities. These
findings underscore the urgent need for robust, multi-layered defenses in LLMs
deployed across critical infrastructure and sensitive industries. Successful
prompt injection attacks could result in severe consequences, including data
breaches, unauthorized access, or misinformation. Future research should
explore multilingual and multi-step defenses alongside adaptive mitigation
strategies to strengthen LLM security in diverse, real-world environments.","['Victoria Benjamin', 'Emily Braca', 'Israel Carter', 'Hafsa Kanchwala', 'Nava Khojasteh', 'Charly Landow', 'Yi Luo', 'Caroline Ma', 'Anna Magarelli', 'Rachel Mirin', 'Avery Moyer', 'Kayla Simpson', 'Amelia Skawinski', 'Thomas Heverin']",6,0.71632385
"With the intensification of climate change discussion, social media has
become prominent in disseminating reliable and unreliable content. In this
study, we present a cross-platform analysis on Youtube and Twitter, and examine
the polarization and echo chambers in social media discussions in four datasets
related to climate change: COP27, IPCC, Climate Refugees, and Do\~{n}ana. We
have identified communities of users spreading misinformation on Twitter,
although they remain relatively isolated from the rest of the network. The
analysis by interaction type reveals that climate change sceptics use mentions
to draw the attention of other communities. The YouTube posts referenced on
Twitter reveal a strong correlation in the community organisation of social
media, suggesting a platform alignment. Moreover, we report the presence of
echo chambers in YouTube post-sharing related to mainstream and sceptical
content.","['Aleix Bassolas', 'Joan Massachs', 'Emanuele Cozzo', 'Julian Vicens']",3,0.7467592
"The widespread use of AI-generated content from diffusion models has raised
significant concerns regarding misinformation and copyright infringement.
Watermarking is a crucial technique for identifying these AI-generated images
and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new
watermarking technique that embeds robust and invisible watermarks into
diffusion model outputs. Unlike existing approaches that integrate watermarking
throughout the entire diffusion sampling process, Shallow Diffuse decouples
these steps by leveraging the presence of a low-dimensional subspace in the
image generation process. This method ensures that a substantial portion of the
watermark lies in the null space of this subspace, effectively separating it
from the image generation process. Our theoretical and empirical analyses show
that this decoupling strategy greatly enhances the consistency of data
generation and the detectability of the watermark. Extensive experiments
further validate that our Shallow Diffuse outperforms existing watermarking
methods in terms of robustness and consistency. The codes will be released at
https://github.com/liwd190019/Shallow-Diffuse.","['Wenda Li', 'Huijie Zhang', 'Qing Qu']",7,0.7401267
"We investigate the challenge of generating adversarial examples to test the
robustness of text classification algorithms detecting low-credibility content,
including propaganda, false claims, rumours and hyperpartisan news. We focus on
simulation of content moderation by setting realistic limits on the number of
queries an attacker is allowed to attempt. Within our solution (TREPAT),
initial rephrasings are generated by large language models with prompts
inspired by meaning-preserving NLP tasks, e.g. text simplification and style
transfer. Subsequently, these modifications are decomposed into small changes,
applied through beam search procedure until the victim classifier changes its
decision. The evaluation confirms the superiority of our approach in the
constrained scenario, especially in case of long input text (news articles),
where exhaustive search is not feasible.",['Piotr Przyby≈Ça'],1,0.68748164
"Fact-checking is extensively studied in the context of misinformation and
disinformation, addressing objective inaccuracies. However, a softer form of
misinformation involves responses that are factually correct but lack certain
features such as clarity and relevance. This challenge is prevalent in formal
Question-Answer (QA) settings such as press conferences in finance, politics,
sports, and other domains, where subjective answers can obscure transparency.
Despite this, there is a lack of manually annotated datasets for subjective
features across multiple dimensions. To address this gap, we introduce
SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs)
QA sessions as the answers given by company representatives are often open to
subjective interpretations and scrutiny. The dataset includes 49,446
annotations for long-form QA pairs across six features: Assertive, Cautious,
Optimistic, Specific, Clear, and Relevant. These features are carefully
selected to encompass the key attributes that reflect the tone of the answers
provided during QA sessions across different domain. Our findings are that the
best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar
weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity,
such as Relevant and Clear, with a mean difference of 2.17% in their weighted
F1 scores. The models perform significantly better on features with higher
subjectivity, such as Specific and Assertive, with a mean difference of 10.01%
in their weighted F1 scores. Furthermore, testing SubjECTive-QA's
generalizability using QAs from White House Press Briefings and Gaggles yields
an average weighted F1 score of 65.97% using our best models for each feature,
demonstrating broader applicability beyond the financial domain. SubjECTive-QA
is publicly available under the CC BY 4.0 license","['Huzaifa Pardawala', 'Siddhant Sukhani', 'Agam Shah', 'Veer Kejriwal', 'Abhishek Pillai', 'Rohan Bhasin', 'Andrew DiBiasio', 'Tarun Mandapati', 'Dhruv Adha', 'Sudheer Chava']",1,0.6648165
"Misinformation presents threats to societal mental well-being, public health
initiatives, as well as satisfaction in democracy. Those who spread
misinformation can leverage cognitive biases to make others more likely to
believe and share their misinformation unquestioningly. For example, by sharing
misinformation whilst claiming to be someone from a highly respectable
profession, a propagandist may seek to increase the effectiveness of their
campaign using authority bias. Using retweet data from the spread of
misinformation about two former UK Prime Ministers (Boris Johnson and Theresa
May), we find that 3.1% of those who retweeted such misinformation claimed to
be teachers or lecturers (20.7% of those who claimed to have a profession in
their Twitter bio field in our sample), despite such professions representing
under 1.15% of the UK population. Whilst polling data shows teachers and
healthcare workers are amongst the most trusted professions in society, these
were amongst the most popular professions that those in our sample claimed to
have.",['Junade Ali'],10,0.6393118
"Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4
have shown remarkable abilities in a wide range of tasks such as summarizing
texts and assisting in coding. Scientific research has demonstrated that these
models can also play text-adventure games. This study aims to explore whether
LLMs can autonomously create text-based games based on anthropological
classics, evaluating also their effectiveness in communicating knowledge. To
achieve this, the study engaged anthropologists in discussions to gather their
expectations and design inputs for an anthropologically themed game. Through
iterative processes following the established HCI principle of 'design
thinking', the prompts and the conceptual framework for crafting these games
were refined. Leveraging GPT3.5, the study created three prototypes of games
centered around the seminal anthropological work of the social anthropologist's
Bronislaw Malinowski's ""Argonauts of the Western Pacific"" (1922). Subsequently,
evaluations were conducted by inviting senior anthropologists to playtest these
games, and based on their inputs, the game designs were refined. The tests
revealed promising outcomes but also highlighted key challenges: the models
encountered difficulties in providing in-depth thematic understandings, showed
suspectibility to misinformation, tended towards monotonic responses after an
extended period of play, and struggled to offer detailed biographical
information. Despite these limitations, the study's findings open up new
research avenues at the crossroads of artificial intelligence, machine
learning, LLMs, ethnography, anthropology and human-computer interaction.","['Michael Peter Hoffmann', 'Jan Fillies', 'Adrian Paschke']",0,0.524304
"The retrieval-augmented generation (RAG) approach is used to reduce the
confabulation of large language models (LLMs) for question answering by
retrieving and providing additional context coming from external knowledge
sources (e.g., by adding the context to the prompt). However, injecting
incorrect information can mislead the LLM to generate an incorrect answer.
  In this paper, we evaluate the effectiveness and robustness of four LLMs
against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in
answering biomedical questions. We assess the answer accuracy on yes-no and
free-form questions in three scenarios: vanilla LLM answers (no context is
provided), ""perfect"" augmented generation (correct context is provided), and
prompt-injection attacks (incorrect context is provided). Our results show that
Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla
(0.651) and ""perfect"" RAG (0.802) scenarios. However, the accuracy gap between
the models almost disappears with ""perfect"" RAG, suggesting its potential to
mitigate the LLM's size-related effectiveness differences.
  We further evaluate the ability of the LLMs to generate malicious context on
one hand and the LLM's robustness against prompt-injection attacks on the other
hand, using metrics such as attack success rate (ASR), accuracy under attack,
and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2,
GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is
injected in the target model's prompt. Interestingly, Llama is shown to be the
most effective adversary, causing accuracy drops of up to 0.48 for vanilla
answers and 0.63 for ""perfect"" RAG across target models. Our analysis reveals
that robustness rankings vary depending on the evaluation measure, highlighting
the complexity of assessing LLM resilience to adversarial attacks.","['Alexander Bondarenko', 'Adrian Viehweger']",6,0.7379272
"Social media platforms like Twitter, Facebook, and Instagram have facilitated
the spread of misinformation, necessitating automated detection systems. This
systematic review evaluates 36 studies that apply machine learning (ML) and
deep learning (DL) models to detect fake news, spam, and fake accounts on
social media. Using the Prediction model Risk Of Bias ASsessment Tool
(PROBAST), the review identified key biases across the ML lifecycle: selection
bias due to non-representative sampling, inadequate handling of class
imbalance, insufficient linguistic preprocessing (e.g., negations), and
inconsistent hyperparameter tuning. Although models such as Support Vector
Machines (SVM), Random Forests, and Long Short-Term Memory (LSTM) networks
showed strong potential, over-reliance on accuracy as an evaluation metric in
imbalanced data settings was a common flaw. The review highlights the need for
improved data preprocessing (e.g., resampling techniques), consistent
hyperparameter tuning, and the use of appropriate metrics like precision,
recall, F1 score, and AUROC. Addressing these limitations can lead to more
reliable and generalizable ML/DL models for detecting deceptive content,
ultimately contributing to the reduction of misinformation on social media.","['Yunchong Liu', 'Xiaorui Shen', 'Yeyubei Zhang', 'Zhongyan Wang', 'Yexin Tian', 'Jianglai Dai', 'Yuchen Cao']",1,0.7348387
"One of the most challenging forms of misinformation involves the
out-of-context (OOC) use of images paired with misleading text, creating false
narratives. Existing AI-driven detection systems lack explainability and
require expensive fine-tuning. We address these issues with MAD-Sherlock: a
Multi-Agent Debate system for OOC Misinformation Detection. MAD-Sherlock
introduces a novel multi-agent debate framework where multimodal agents
collaborate to assess contextual consistency and request external information
to enhance cross-context reasoning and decision-making. Our framework enables
explainable detection with state-of-the-art accuracy even without
domain-specific fine-tuning. Extensive ablation studies confirm that external
retrieval significantly improves detection accuracy, and user studies
demonstrate that MAD-Sherlock boosts performance for both experts and
non-experts. These results position MAD-Sherlock as a powerful tool for
autonomous and citizen intelligence applications.","['Kumud Lakara', 'Juil Sock', 'Christian Rupprecht', 'Philip Torr', 'John Collomosse', 'Christian Schroeder de Witt']",1,0.7065695
"The rise of social media has amplified the spread of fake news, now further
complicated by large language models (LLMs) like ChatGPT, which ease the
generation of highly convincing, error-free misinformation, making it
increasingly challenging for the public to discern truth from falsehood.
Traditional fake news detection methods relying on linguistic cues also becomes
less effective. Moreover, current detectors primarily focus on binary
classification and English texts, often overlooking the distinction between
machine-generated true vs. fake news and the detection in low-resource
languages. To this end, we updated detection schema to include
machine-generated news with focus on the Urdu language. We further propose a
hierarchical detection strategy to improve the accuracy and robustness.
Experiments show its effectiveness across four datasets in various settings.","['Muhammad Zain Ali', 'Yuxia Wang', 'Bernhard Pfahringer', 'Tony Smith']",4,0.78999406
"Large language models (LLMs) have achieved a degree of success in generating
coherent and contextually relevant text, yet they remain prone to a significant
challenge known as hallucination: producing information that is not
substantiated by the input or external knowledge. Previous efforts to mitigate
hallucinations have focused on techniques such as fine-tuning models on
high-quality datasets, incorporating fact-checking mechanisms, and developing
adversarial training methods. While these approaches have shown some promise,
they often address the issue at the level of individual model outputs, leaving
unexplored the effects of inter-model interactions on hallucination. This study
investigates the phenomenon of hallucination in LLMs through a novel
experimental framework where multiple instances of GPT-4o-Mini models engage in
a debate-like interaction prompted with questions from the TruthfulQA dataset.
One model is deliberately instructed to generate plausible but false answers
while the other models are asked to respond truthfully. The experiment is
designed to assess whether the introduction of misinformation by one model can
challenge the truthful majority to better justify their reasoning, improving
performance on the TruthfulQA benchmark. The findings suggest that inter-model
interactions can offer valuable insights into improving the accuracy and
robustness of LLM outputs, complementing existing mitigation strategies.","['Ray Li', 'Tanishka Bagade', 'Kevin Martinez', 'Flora Yasmin', 'Grant Ayala', 'Michael Lam', 'Kevin Zhu']",6,0.78819984
"Large Language Models (LLMs) can assist in the prebunking of election
misinformation. Using results from a preregistered two-wave experimental study
of 4,293 U.S. registered voters conducted in August 2024, we show that
LLM-assisted prebunking significantly reduced belief in specific election
myths,with these effects persisting for at least one week. Confidence in
election integrity was also increased post-treatment. Notably, the effect was
consistent across partisan lines, even when controlling for demographic and
attitudinal factors like conspiratorial thinking. LLM-assisted prebunking is a
promising tool for rapidly responding to changing election misinformation
narratives.","['Mitchell Linegar', 'Betsy Sinclair', 'Sander van der Linden', 'R. Michael Alvarez']",10,0.6017859
"The widely adopted and powerful generative large language models (LLMs) have
raised concerns about intellectual property rights violations and the spread of
machine-generated misinformation. Watermarking serves as a promising approch to
establish ownership, prevent unauthorized use, and trace the origins of
LLM-generated content. This paper summarizes and shares the challenges and
opportunities we found when watermarking LLMs. We begin by introducing
techniques for watermarking LLMs themselves under different threat models and
scenarios. Next, we investigate watermarking methods designed for the content
generated by LLMs, assessing their effectiveness and resilience against various
attacks. We also highlight the importance of watermarking domain-specific
models and data, such as those used in code generation, chip design, and
medical applications. Furthermore, we explore methods like hardware
acceleration to improve the efficiency of the watermarking process. Finally, we
discuss the limitations of current approaches and outline future research
directions for the responsible use and protection of these generative AI tools.","['Ruisi Zhang', 'Farinaz Koushanfar']",6,0.66396546
"With the growing spread of misinformation online, research has increasingly
focused on detecting and tracking fake news. However, an overlooked issue is
that fake news does not naturally exist in social networks -- it often
originates from distorted facts or deliberate fabrication by malicious actors.
Understanding how true news gradually evolves into fake news is critical for
early detection and prevention, reducing its spread and impact. Hence, in this
paper, we take the first step toward simulating and revealing this evolution,
proposing a Fake News evolUtion Simulation framEwork (FUSE) based on large
language models (LLMs). Specifically, we employ LLM as agents to represent
individuals in a simulated social network. We define four types of agents
commonly observed in daily interactions: spreaders, who propagate information;
commentators, who provide opinions and interpretations; verifiers, who check
the accuracy of information; and bystanders, who passively observe without
engaging. For simulated environments, we model various social network
structures, such as high-clustering networks and scale-free networks, to mirror
real-world network dynamics. Each day, the agents engage in belief exchanges,
reflect on their thought processes, and reintroduce the news accordingly. Given
the lack of prior work in this area, we developed a FUSE-EVAL evaluation
framework to measure the deviation from true news during the fake news
evolution process. The results show that FUSE successfully captures the
underlying patterns of how true news transforms into fake news and accurately
reproduces previously discovered instances of fake news, aligning closely with
human evaluations. Moreover, our work provides insights into the fact that
combating fake news should not be delayed until it has fully evolved; instead,
prevention in advance is key to achieving better outcomes.","['Yuhan Liu', 'Zirui Song', 'Xiaoqing Zhang', 'Xiuying Chen', 'Rui Yan']",4,0.8088968
"The emergence of diffusion models has transformed synthetic media generation,
offering unmatched realism and control over content creation. These
advancements have driven innovation across fields such as art, design, and
scientific visualization. However, they also introduce significant ethical and
societal challenges, particularly through the creation of hyper-realistic
images that can facilitate deepfakes, misinformation, and unauthorized
reproduction of copyrighted material. In response, the need for effective
detection mechanisms has become increasingly urgent. This review examines the
evolving adversarial relationship between diffusion model development and the
advancement of detection methods. We present a thorough analysis of
contemporary detection strategies, including frequency and spatial domain
techniques, deep learning-based approaches, and hybrid models that combine
multiple methodologies. We also highlight the importance of diverse datasets
and standardized evaluation metrics in improving detection accuracy and
generalizability. Our discussion explores the practical applications of these
detection systems in copyright protection, misinformation prevention, and
forensic analysis, while also addressing the ethical implications of synthetic
media. Finally, we identify key research gaps and propose future directions to
enhance the robustness and adaptability of detection methods in line with the
rapid advancements of diffusion models. This review emphasizes the necessity of
a comprehensive approach to mitigating the risks associated with AI-generated
content in an increasingly digital world.","['Linda Laurier', 'Ave Giulietta', 'Arlo Octavia', 'Meade Cleti']",11,0.70276535
"In this paper, we present a comprehensive survey on the pervasive issue of
medical misinformation in social networks from the perspective of information
technology. The survey aims at providing a systematic review of related
research and helping researchers and practitioners navigate through this
fast-changing field. Specifically, we first present manual and automatic
approaches for fact-checking. We then explore fake news detection methods,
using content, propagation features, or source features, as well as mitigation
approaches for countering the spread of misinformation. We also provide a
detailed list of several datasets on health misinformation and of publicly
available tools. We conclude the survey with a discussion on the open
challenges and future research directions in the battle against health
misinformation.","['Vasiliki Papanikou', 'Panagiotis Papadakos', 'Theodora Karamanidou', 'Thanos G. Stavropoulos', 'Evaggelia Pitoura', 'Panayiotis Tsaparas']",0,0.76376855
"Social media is often the first place where communities discuss the latest
societal trends. Prior works have utilized this platform to extract
epidemic-related information (e.g. infections, preventive measures) to provide
early warnings for epidemic prediction. However, these works only focused on
English posts, while epidemics can occur anywhere in the world, and early
discussions are often in the local, non-English languages. In this work, we
introduce the first multilingual Event Extraction (EE) framework SPEED++ for
extracting epidemic event information for a wide range of diseases and
languages. To this end, we extend a previous epidemic ontology with 20 argument
roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in
four languages for four diseases. Annotating data in every language is
infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e.,
training only on English COVID data) utilizing multilingual pre-training and
show their efficacy in extracting epidemic-related events for 65 diverse
languages across different diseases. Experiments demonstrate that our framework
can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019
(3 weeks before global discussions) from Chinese Weibo posts without any
training in Chinese. Furthermore, we exploit our framework's argument
extraction capabilities to aggregate community epidemic discussions like
symptoms and cure measures, aiding misinformation detection and public
attention monitoring. Overall, we lay a strong foundation for multilingual
epidemic preparedness.","['Tanmay Parekh', 'Jeffrey Kwan', 'Jiarui Yu', 'Sparsh Johri', 'Hyosang Ahn', 'Sreya Muppalla', 'Kai-Wei Chang', 'Wei Wang', 'Nanyun Peng']",5,0.6475849
"In today's global digital landscape, misinformation transcends linguistic
boundaries, posing a significant challenge for moderation systems. While
significant advances have been made in misinformation detection, the focus
remains largely on monolingual high-resource contexts, with low-resource
languages often overlooked. This survey aims to bridge that gap by providing a
comprehensive overview of the current research on low-resource language
misinformation detection in both monolingual and multilingual settings. We
review the existing datasets, methodologies, and tools used in these domains,
identifying key challenges related to: data resources, model development,
cultural and linguistic context, real-world applications, and research efforts.
We also examine emerging approaches, such as language-agnostic models and
multi-modal techniques, while emphasizing the need for improved data collection
practices, interdisciplinary collaboration, and stronger incentives for
socially responsible AI research. Our findings underscore the need for robust,
inclusive systems capable of addressing misinformation across diverse
linguistic and cultural contexts.","['Xinyu Wang', 'Wenbo Zhang', 'Sarah Rajtmajer']",8,0.78447497
"Influence maximization (IM) is a classic problem that aims to identify a
small group of critical individuals, known as seeds, who can influence the
largest number of users in a social network through word-of-mouth. This problem
finds important applications including viral marketing, infection detection,
and misinformation containment. The conventional IM problem is typically
studied with the oversimplified goal of selecting a single seed set. Many
real-world scenarios call for multiple sets of seeds, particularly on social
media platforms where various viral marketing campaigns need different sets of
seeds to propagate effectively. To this end, previous works have formulated
various IM variants, central to which is the requirement of multiple seed sets,
naturally modeled as a matroid constraint. However, the current best-known
solutions for these variants either offer a weak
$(1/2-\epsilon)$-approximation, or offer a $(1-1/e-\epsilon)$-approximation
algorithm that is very expensive. We propose an efficient seed selection method
called AMP, an algorithm with a $(1-1/e-\epsilon)$-approximation guarantee for
this family of IM variants. To further improve efficiency, we also devise a
fast implementation, called RAMP. We extensively evaluate the performance of
our proposal against 6 competitors across 4 IM variants and on 7 real-world
networks, demonstrating that our proposal outperforms all competitors in terms
of result quality, running time, and memory usage. We have also deployed RAMP
in a real industry strength application involving online gaming, where we show
that our deployed solution significantly improves upon the baselines.","['Yiqian Huang', 'Shiqi Zhang', 'Laks V. S. Lakshmanan', 'Wenqing Lin', 'Xiaokui Xiao', 'Bo Tang']",2,0.6725736
"The rise of social media and short-form video (SFV) has facilitated a
breeding ground for misinformation. With the emergence of large language
models, significant research has gone into curbing this misinformation problem
with automatic false claim detection for text. Unfortunately, the automatic
detection of misinformation in SFV is a more complex problem that remains
largely unstudied. While text samples are monomodal (only containing words),
SFVs comprise three different modalities: words, visuals, and non-linguistic
audio. In this work, we introduce Video Masked Autoencoders for Misinformation
Guarding (ViMGuard), the first deep-learning architecture capable of
fact-checking an SFV through analysis of all three of its constituent
modalities. ViMGuard leverages a dual-component system. First, Video and Audio
Masked Autoencoders analyze the visual and non-linguistic audio elements of a
video to discern its intention; specifically whether it intends to make an
informative claim. If it is deemed that the SFV has informative intent, it is
passed through our second component: a Retrieval Augmented Generation system
that validates the factual accuracy of spoken words. In evaluation, ViMGuard
outperformed three cutting-edge fact-checkers, thus setting a new standard for
SFV fact-checking and marking a significant stride toward trustworthy news on
social platforms. To promote further testing and iteration, VimGuard was
deployed into a Chrome extension and all code was open-sourced on GitHub.","['Andrew Kan', 'Christopher Kan', 'Zaid Nabulsi']",11,0.5965907
"Large language models (LLMs) have empowered nodes within multi-agent networks
with intelligence, showing growing applications in both academia and industry.
However, how to prevent these networks from generating malicious information
remains unexplored with previous research on single LLM's safety be challenging
to transfer. In this paper, we focus on the safety of multi-agent networks from
a topological perspective, investigating which topological properties
contribute to safer networks. To this end, we propose a general framework,
NetSafe along with an iterative RelCom interaction to unify existing diverse
LLM-based agent frameworks, laying the foundation for generalized topological
safety research. We identify several critical phenomena when multi-agent
networks are exposed to attacks involving misinformation, bias, and harmful
information, termed as Agent Hallucination and Aggregation Safety. Furthermore,
we find that highly connected networks are more susceptible to the spread of
adversarial attacks, with task performance in a Star Graph Topology decreasing
by 29.7%. Besides, our proposed static metrics aligned more closely with
real-world dynamic evaluations than traditional graph-theoretic metrics,
indicating that networks with greater average distances from attackers exhibit
enhanced safety. In conclusion, our work introduces a new topological
perspective on the safety of LLM-based multi-agent networks and discovers
several unreported phenomena, paving the way for future research to explore the
safety of such networks.","['Miao Yu', 'Shilong Wang', 'Guibin Zhang', 'Junyuan Mao', 'Chenlong Yin', 'Qijiong Liu', 'Qingsong Wen', 'Kun Wang', 'Yang Wang']",2,0.6693149
"In an era increasingly dominated by digital platforms, the spread of
misinformation poses a significant challenge, highlighting the need for
solutions capable of assessing information veracity. Our research contributes
to the field of Explainable Artificial Antelligence (XAI) by developing
transformer-based fact-checking models that contextualise and justify their
decisions by generating human-accessible explanations. Importantly, we also
develop models for automatic evaluation of explanations for fact-checking
verdicts across different dimensions such as \texttt{(self)-contradiction},
\texttt{hallucination}, \texttt{convincingness} and \texttt{overall quality}.
By introducing human-centred evaluation methods and developing specialised
datasets, we emphasise the need for aligning Artificial Intelligence
(AI)-generated explanations with human judgements. This approach not only
advances theoretical knowledge in XAI but also holds practical implications by
enhancing the transparency, reliability and users' trust in AI-driven
fact-checking systems. Furthermore, the development of our metric learning
models is a first step towards potentially increasing efficiency and reducing
reliance on extensive manual assessment. Based on experimental results, our
best performing generative model \textsc{ROUGE-1} score of 47.77, demonstrating
superior performance in generating fact-checking explanations, particularly
when provided with high-quality evidence. Additionally, the best performing
metric learning model showed a moderately strong correlation with human
judgements on objective dimensions such as \texttt{(self)-contradiction and
\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of
around 0.7.}","['Darius Feher', 'Abdullah Khered', 'Hao Zhang', 'Riza Batista-Navarro', 'Viktor Schlegel']",1,0.74636465
"Misinformation undermines individual knowledge and affects broader societal
narratives. Despite growing interest in the research community in multi-modal
misinformation detection, existing methods exhibit limitations in capturing
semantic cues, key regions, and cross-modal similarities within multi-modal
datasets. We propose SceneGraMMi, a Scene Graph-boosted Hybrid-fusion approach
for Multi-modal Misinformation veracity prediction, which integrates scene
graphs across different modalities to improve detection performance.
Experimental results across four benchmark datasets show that SceneGraMMi
consistently outperforms state-of-the-art methods. In a comprehensive ablation
study, we highlight the contribution of each component, while Shapley values
are employed to examine the explainability of the model's decision-making
process.","['Swarang Joshi', 'Siddharth Mavani', 'Joel Alex', 'Arnav Negi', 'Rahul Mishra', 'Ponnurangam Kumaraguru']",2,0.66893137
"Despite significant research on online harm, polarization, public
deliberation, and justice, CSCW still lacks a comprehensive understanding of
the experiences of religious minorities, particularly in relation to fear, as
prominently evident in our study. Gaining faith-sensitive insights into the
expression, participation, and inter-religious interactions on social media can
contribute to CSCW's literature on online safety and interfaith communication.
In pursuit of this goal, we conducted a six-month-long, interview-based study
with the Hindu, Buddhist, and Indigenous communities in Bangladesh. Our study
draws on an extensive body of research encompassing the spiral of silence, the
cultural politics of fear, and communication accommodation to examine how
social media use by religious minorities is influenced by fear, which is
associated with social conformity, misinformation, stigma, stereotypes, and
South Asian postcolonial memory. Moreover, we engage with scholarly
perspectives from religious studies, justice, and South Asian violence and
offer important critical insights and design lessons for the CSCW literature on
public deliberation, justice, and interfaith communication.","['Mohammad Rashidujjaman Rifat', 'Dipto Das', 'Arpon Podder', 'Mahiratul Jannat', 'Robert Soden', 'Bryan Semaan', 'Syed Ishtiaque Ahmed']",0,0.4940131
"The ability for individuals to constructively engage with one another across
lines of difference is a critical feature of a healthy pluralistic society.
This is also true in online discussion spaces like social media platforms. To
date, much social media research has focused on preventing ills -- like
political polarization and the spread of misinformation. While this is
important, enhancing the quality of online public discourse requires not just
reducing ills but also promoting foundational human virtues. In this study, we
focus on one particular virtue: ``intellectual humility'' (IH), or
acknowledging the potential limitations in one's own beliefs. Specifically, we
explore the development of computational methods for measuring IH at scale. We
manually curate and validate an IH codebook on 350 posts about religion drawn
from subreddits and use them to develop LLM-based models for automating this
measurement. Our best model achieves a Macro-F1 score of 0.64 across labels
(and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an
expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human
annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results
both highlight the challenging nature of detecting IH online -- opening the
door to new directions in NLP research -- and also lay a foundation for
computational social science researchers interested in analyzing and fostering
more IH in online public discourse.","['Xiaobo Guo', 'Neil Potnis', 'Melody Yu', 'Nabeel Gillani', 'Soroush Vosoughi']",9,0.5974486
"Fallacies are defective arguments with faulty reasoning. Detecting and
classifying them is a crucial NLP task to prevent misinformation, manipulative
claims, and biased decisions. However, existing fallacy classifiers are limited
by the requirement for sufficient labeled data for training, which hinders
their out-of-distribution (OOD) generalization abilities. In this paper, we
focus on leveraging Large Language Models (LLMs) for zero-shot fallacy
classification. To elicit fallacy-related knowledge and reasoning abilities of
LLMs, we propose diverse single-round and multi-round prompting schemes,
applying different task-specific instructions such as extraction,
summarization, and Chain-of-Thought reasoning. With comprehensive experiments
on benchmark datasets, we suggest that LLMs could be potential zero-shot
fallacy classifiers. In general, LLMs under single-round prompting schemes have
achieved acceptable zero-shot performances compared to the best full-shot
baselines and can outperform them in all OOD inference scenarios and some
open-domain tasks. Our novel multi-round prompting schemes can effectively
bring about more improvements, especially for small LLMs. Our analysis further
underlines the future research on zero-shot fallacy classification. Codes and
data are available at: https://github.com/panFJCharlotte98/Fallacy_Detection.","['Fengjun Pan', 'Xiaobao Wu', 'Zongrui Li', 'Anh Tuan Luu']",6,0.7983826
"Large language models (LLMs) are susceptible to persuasion, which can pose
risks when models are faced with an adversarial interlocutor. We take a first
step towards defending models against persuasion while also arguing that
defense against adversarial (i.e. negative) persuasion is only half of the
equation: models should also be able to accept beneficial (i.e. positive)
persuasion to improve their answers. We show that optimizing models for only
one side results in poor performance on the other. In order to balance positive
and negative persuasion, we introduce Persuasion-Balanced Training (or PBT),
which leverages multi-agent recursive dialogue trees to create data and trains
models via preference optimization to accept persuasion when appropriate. PBT
consistently improves resistance to misinformation and resilience to being
challenged while also resulting in the best overall performance on holistic
data containing both positive and negative persuasion. Crucially, we show that
PBT models are better teammates in multi-agent debates. We find that without
PBT, pairs of stronger and weaker models have unstable performance, with the
order in which the models present their answers determining whether the team
obtains the stronger or weaker model's performance. PBT leads to better and
more stable results and less order dependence, with the stronger model
consistently pulling the weaker one up.","['Elias Stengel-Eskin', 'Peter Hase', 'Mohit Bansal']",1,0.5105003
"Misinformation spreads rapidly on social media, confusing the truth and
targetting potentially vulnerable people. To effectively mitigate the negative
impact of misinformation, it must first be accurately detected before applying
a mitigation strategy, such as X's community notes, which is currently a manual
process. This study takes a knowledge-based approach to misinformation
detection, modelling the problem similarly to one of natural language
inference. The EffiARA annotation framework is introduced, aiming to utilise
inter- and intra-annotator agreement to understand the reliability of each
annotator and influence the training of large language models for
classification based on annotator reliability. In assessing the EffiARA
annotation framework, the Russo-Ukrainian Conflict Knowledge-Based
Misinformation Classification Dataset (RUC-MCD) was developed and made publicly
available. This study finds that sample weighting using annotator reliability
performs the best, utilising both inter- and intra-annotator agreement and
soft-label training. The highest classification performance achieved using
Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.","['Owen Cook', 'Charlie Grimshaw', 'Ben Wu', 'Sophie Dillon', 'Jack Hicks', 'Luke Jones', 'Thomas Smith', 'Matyas Szert', 'Xingyi Song']",1,0.6786802
"Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating coherent text but remain limited by the static nature of their
training data. Retrieval Augmented Generation (RAG) addresses this issue by
combining LLMs with up-to-date information retrieval, but also expand the
attack surface of the system. This paper investigates prompt injection attacks
on RAG, focusing on malicious objectives beyond misinformation, such as
inserting harmful links, promoting unauthorized services, and initiating
denial-of-service behaviors. We build upon existing corpus poisoning techniques
and propose a novel backdoor attack aimed at the fine-tuning process of the
dense retriever component. Our experiments reveal that corpus poisoning can
achieve significant attack success rates through the injection of a small
number of compromised documents into the retriever corpus. In contrast,
backdoor attacks demonstrate even higher success rates but necessitate a more
complex setup, as the victim must fine-tune the retriever using the attacker
poisoned dataset.","['Cody Clop', 'Yannick Teglia']",1,0.638044
"The development of Large Language Models (LLMs) has brought impressive
performances on mitigation strategies against misinformation, such as
counterargument generation. However, LLMs are still seriously hindered by
outdated knowledge and by their tendency to generate hallucinated content. In
order to circumvent these issues, we propose a new task, namely, Critical
Questions Generation, consisting of processing an argumentative text to
generate the critical questions (CQs) raised by it. In argumentation theory CQs
are tools designed to lay bare the blind spots of an argument by pointing at
the information it could be missing. Thus, instead of trying to deploy LLMs to
produce knowledgeable and relevant counterarguments, we use them to question
arguments, without requiring any external knowledge. Research on CQs Generation
using LLMs requires a reference dataset for large scale experimentation. Thus,
in this work we investigate two complementary methods to create such a
resource: (i) instantiating CQs templates as defined by Walton's argumentation
theory and (ii), using LLMs as CQs generators. By doing so, we contribute with
a procedure to establish what is a valid CQ and conclude that, while LLMs are
reasonable CQ generators, they still have a wide margin for improvement in this
task.","['Blanca Calvo Figueras', 'Rodrigo Agerri']",6,0.74248356
"The rapid development of large language models (LLMs), like ChatGPT, has
resulted in the widespread presence of LLM-generated content on social media
platforms, raising concerns about misinformation, data biases, and privacy
violations, which can undermine trust in online discourse. While detecting
LLM-generated content is crucial for mitigating these risks, current methods
often focus on binary classification, failing to address the complexities of
real-world scenarios like human-AI collaboration. To move beyond binary
classification and address these challenges, we propose a new paradigm for
detecting LLM-generated content. This approach introduces two novel tasks: LLM
Role Recognition (LLM-RR), a multi-class classification task that identifies
specific roles of LLM in content generation, and LLM Influence Measurement
(LLM-IM), a regression task that quantifies the extent of LLM involvement in
content creation. To support these tasks, we propose LLMDetect, a benchmark
designed to evaluate detectors' performance on these new tasks. LLMDetect
includes the Hybrid News Detection Corpus (HNDC) for training detectors, as
well as DetectEval, a comprehensive evaluation suite that considers five
distinct cross-context variations and multi-intensity variations within the
same LLM role. This allows for a thorough assessment of detectors'
generalization and robustness across diverse contexts. Our empirical validation
of 10 baseline detection methods demonstrates that fine-tuned PLM-based models
consistently outperform others on both tasks, while advanced LLMs face
challenges in accurately detecting their own generated content. Our
experimental results and analysis offer insights for developing more effective
detection models for LLM-generated content. This research enhances the
understanding of LLM-generated content and establishes a foundation for more
nuanced detection methodologies.","['Zihao Cheng', 'Li Zhou', 'Feng Jiang', 'Benyou Wang', 'Haizhou Li']",6,0.84585494
"In this study, we investigate the under-explored intervention planning aimed
at disseminating accurate information within dynamic opinion networks by
leveraging learning strategies. Intervention planning involves identifying key
nodes (search) and exerting control (e.g., disseminating accurate or official
information through the nodes) to mitigate the influence of misinformation.
However, as the network size increases, the problem becomes computationally
intractable. To address this, we first introduce a ranking algorithm to
identify key nodes for disseminating accurate information, which facilitates
the training of neural network classifiers that provide generalized solutions
for the search and planning problems. Second, we mitigate the complexity of
label generation, which becomes challenging as the network grows, by developing
a reinforcement learning-based centralized dynamic planning framework. We
analyze these NN-based planners for opinion networks governed by two dynamic
propagation models. Each model incorporates both binary and continuous opinion
and trust representations. Our experimental results demonstrate that the
ranking algorithm-based classifiers provide plans that enhance infection rate
control, especially with increased action budgets for small networks. Further,
we observe that the reward strategies focusing on key metrics, such as the
number of susceptible nodes and infection rates, outperform those prioritizing
faster blocking strategies. Additionally, our findings reveal that graph
convolutional network-based planners facilitate scalable centralized plans that
achieve lower infection rates (higher control) across various network
configurations, including Watts-Strogatz topology, varying action budgets,
varying initial infected nodes, and varying degrees of infected nodes.","['Bharath Muppasani', 'Protik Nag', 'Vignesh Narayanan', 'Biplav Srivastava', 'Michael N. Huhns']",2,0.7706679
"Algorithmic intermediaries govern the digital public sphere through their
architectures, amplification algorithms, and moderation practices. In doing so,
they shape public communication and distribute attention in ways that were
previously infeasible with such subtlety, speed and scale. From misinformation
and affective polarisation to hate speech and radicalisation, the many
pathologies of the digital public sphere attest that they could do so better.
But what ideals should they aim at? Political philosophy should be able to
help, but existing theories typically assume that a healthy public sphere will
spontaneously emerge if only we get the boundaries of free expression right.
They offer little guidance on how to intentionally constitute the digital
public sphere. In addition to these theories focused on expression, we need a
further theory of communicative justice, targeted specifically at the
algorithmic intermediaries that shape communication and distribute attention.
This lecture argues that political philosophy urgently owes an account of how
to govern communication in the digital public sphere, and introduces and
defends a democratic egalitarian theory of communicative justice.",['Seth Lazar'],3,0.58756375
"Large language models (LLMs) inherit biases from their training data and
alignment processes, influencing their responses in subtle ways. While many
studies have examined these biases, little work has explored their robustness
during interactions. In this paper, we introduce a novel approach where two
instances of an LLM engage in self-debate, arguing opposing viewpoints to
persuade a neutral version of the model. Through this, we evaluate how firmly
biases hold and whether models are susceptible to reinforcing misinformation or
shifting to harmful viewpoints. Our experiments span multiple LLMs of varying
sizes, origins, and languages, providing deeper insights into bias persistence
and flexibility across linguistic and cultural contexts.","['Virgile Rennard', 'Christos Xypolopoulos', 'Michalis Vazirgiannis']",6,0.75147307
"The proliferation of fake news in the digital age has raised critical
concerns, particularly regarding its impact on societal trust and democratic
processes. Diverging from conventional agent-based simulation approaches, this
work introduces an innovative approach by employing a large language model
(LLM)-driven multi-agent simulation to replicate complex interactions within
information ecosystems. We investigate key factors that facilitate news
propagation, such as agent personalities and network structures, while also
evaluating strategies to combat misinformation. Through simulations across
varying network structures, we demonstrate the potential of LLM-based agents in
modeling the dynamics of misinformation spread, validating the influence of
agent traits on the diffusion process. Our findings emphasize the advantages of
LLM-based simulations over traditional techniques, as they uncover underlying
causes of information spread -- such as agents promoting discussions -- beyond
the predefined rules typically employed in existing agent-based models.
Additionally, we evaluate three countermeasure strategies, discovering that
brute-force blocking influential agents in the network or announcing news
accuracy can effectively mitigate misinformation. However, their effectiveness
is influenced by the network structure, highlighting the importance of
considering network structure in the development of future misinformation
countermeasures.","['Xinyi Li', 'Yu Xu', 'Yongfeng Zhang', 'Edward C. Malthouse']",2,0.71108246
"Social media has a misinformation problem, and counterspeech -- fighting bad
speech with more speech -- has been an ineffective solution. Here, we argue
that bridging-based ranking -- an algorithmic approach to promoting content
favored by users of diverse viewpoints -- is a promising approach to helping
counterspeech combat misinformation. By identifying counterspeech that is
favored both by users who are inclined to agree and by users who are inclined
to disagree with a piece of misinformation, bridging promotes counterspeech
that persuades the users most likely to believe the misinformation.
Furthermore, this algorithmic approach leverages crowd-sourced votes, shifting
discretion from platforms back to users and enabling counterspeech at the speed
and scale required to combat misinformation online. Bridging is respectful of
users' autonomy and encourages broad participation in healthy exchanges; it
offers a way for the free speech tradition to persist in modern speech
environments.","['Kenny Peng', 'James Grimmelmann']",3,0.6108297
"In-context knowledge editing (IKE) enables efficient modification of large
language model (LLM) outputs without parameter changes and at zero-cost.
However, it can be misused to manipulate responses opaquely, e.g., insert
misinformation or offensive content. Such malicious interventions could be
incorporated into high-level wrapped APIs where the final input prompt is not
shown to end-users. To address this issue, we investigate the detection and
reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected
with high accuracy (F1 > 80\%) using only the top-10 output probabilities of
the next token, even in a black-box setting, e.g. proprietary LLMs with limited
output information. Further, we introduce the novel task of reversing IKE-edits
using specially tuned reversal tokens. We explore using both continuous and
discrete reversal tokens, achieving over 80\% accuracy in recovering original,
unedited outputs across multiple LLMs. Our continuous reversal tokens prove
particularly effective, with minimal impact on unedited prompts. Through
analysis of output distributions, attention patterns, and token rankings, we
provide insights into IKE's effects on LLMs and how reversal tokens mitigate
them. This work represents a significant step towards enhancing LLM resilience
against potential misuse of in-context editing, improving their transparency
and trustworthiness.","['Paul Youssef', 'Zhixue Zhao', 'J√∂rg Schl√∂tterer', 'Christin Seifert']",6,0.68224263
"Fake news threatens democracy and exacerbates the polarization and divisions
in society; therefore, accurately detecting online misinformation is the
foundation of addressing this issue. We present CrediRAG, the first fake news
detection model that combines language models with access to a rich external
political knowledge base with a dense social network to detect fake news across
social media at scale. CrediRAG uses a news retriever to initially assign a
misinformation score to each post based on the source credibility of similar
news articles to the post title content. CrediRAG then improves the initial
retrieval estimations through a novel weighted post-to-post network connected
based on shared commenters and weighted by the average stance of all shared
commenters across every pair of posts. We achieve 11% increase in the F1-score
in detecting misinformative posts over state-of-the-art methods. Extensive
experiments conducted on curated real-world Reddit data of over 200,000 posts
demonstrate the superior performance of CrediRAG on existing baselines. Thus,
our approach offers a more accurate and scalable solution to combat the spread
of fake news across social media platforms.","['Ashwin Ram', 'Yigit Ege Bayiz', 'Arash Amini', 'Mustafa Munir', 'Radu Marculescu']",4,0.8062186
"Illusions of causality occur when people develop the belief that there is a
causal connection between two variables with no supporting evidence. This
cognitive bias has been proposed to underlie many societal problems including
social prejudice, stereotype formation, misinformation and superstitious
thinking. In this research we investigate whether large language models develop
the illusion of causality in real-world settings. We evaluated and compared
news headlines generated by GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro
to determine whether the models incorrectly framed correlations as causal
relationships. In order to also measure sycophantic behavior, which occurs when
a model aligns with a user's beliefs in order to look favorable even if it is
not objectively correct, we additionally incorporated the bias into the
prompts, observing if this manipulation increases the likelihood of the models
exhibiting the illusion of causality. We found that Claude-3.5-Sonnet is the
model that presents the lowest degree of causal illusion aligned with
experiments on Correlation-to-Causation Exaggeration in human-written press
releases. On the other hand, our findings suggest that while mimicry sycophancy
increases the likelihood of causal illusions in these models, especially in
GPT-4o-Mini, Claude-3.5-Sonnet remains the most robust against this cognitive
bias.","['Mar√≠a Victoria Carro', 'Francisca Gauna Selasco', 'Denise Alejandra Mester', 'Mario Alejandro Leiva']",3,0.59750724
"Over the past few years, the European Commission has made significant steps
to reduce disinformation in cyberspace. One of those steps has been the
introduction of the 2022 ""Strengthened Code of Practice on Disinformation"".
Signed by leading online platforms, this Strengthened Code of Practice on
Disinformation is an attempt to combat disinformation on the Web. The Code of
Practice includes a variety of measures including the demonetization of
disinformation, urging, for example, advertisers ""to avoid the placement of
advertising next to Disinformation content"".
  In this work, we set out to explore what was the impact of the Code of
Practice and especially to explore to what extent ad networks continue to
advertise on dis-/mis-information sites. We perform a historical analysis and
find that, although at a hasty glance things may seem to be improving, there is
really no significant reduction in the amount of advertising relationships
among popular misinformation websites and major ad networks. In fact, we show
that ad networks have withdrawn mostly from unpopular misinformation websites
with very few visitors, but still form relationships with highly unreliable
websites that account for the majority of misinformation traffic. To make
matters worse, we show that ad networks continue to place advertisements of
legitimate companies next to misinformation content. In fact, major ad networks
place ads in almost 400 misinformation websites of our dataset.","['Emmanouil Papadogiannakis', 'Panagiotis Papadopoulos', 'Nicolas Kourtellis', 'Evangelos P. Markatos']",0,0.5923482
"The rise of digital misinformation has heightened interest in using
multilingual Large Language Models (LLMs) for fact-checking. This study
systematically evaluates translation bias and the effectiveness of LLMs for
cross-lingual claim verification across 15 languages from five language
families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT
dataset to assess their impact on accuracy and bias, we investigate two
distinct translation methods: pre-translation and self-translation. We use
mBERT's performance on the English dataset as a baseline to compare
language-specific accuracies. Our findings reveal that low-resource languages
exhibit significantly lower accuracy in direct inference due to
underrepresentation in the training data. Furthermore, larger models
demonstrate superior performance in self-translation, improving translation
accuracy and reducing bias. These results highlight the need for balanced
multilingual training, especially in low-resource languages, to promote
equitable access to reliable fact-checking tools and minimize the risk of
spreading misinformation in different linguistic contexts.","['Aryan Singhal', 'Veronica Shao', 'Gary Sun', 'Ryan Ding', 'Jonathan Lu', 'Kevin Zhu']",8,0.85882145
"The spread of misinformation on social media platforms threatens democratic
processes, contributes to massive economic losses, and endangers public health.
Many efforts to address misinformation focus on a knowledge deficit model and
propose interventions for improving users' critical thinking through access to
facts. Such efforts are often hampered by challenges with scalability, and by
platform users' personal biases. The emergence of generative AI presents
promising opportunities for countering misinformation at scale across
ideological barriers.
  In this paper, we introduce a framework (MisinfoEval) for generating and
comprehensively evaluating large language model (LLM) based misinformation
interventions. We present (1) an experiment with a simulated social media
environment to measure effectiveness of misinformation interventions, and (2) a
second experiment with personalized explanations tailored to the demographics
and beliefs of users with the goal of countering misinformation by appealing to
their pre-existing values. Our findings confirm that LLM-based interventions
are highly effective at correcting user behavior (improving overall user
accuracy at reliability labeling by up to 41.72%). Furthermore, we find that
users favor more personalized interventions when making decisions about news
reliability and users shown personalized interventions have significantly
higher accuracy at identifying misinformation.","['Saadia Gabriel', 'Liang Lyu', 'James Siderius', 'Marzyeh Ghassemi', 'Jacob Andreas', 'Asu Ozdaglar']",0,0.73211205
"This paper introduces misinfo-general, a benchmark dataset for evaluating
misinformation models' ability to perform out-of-distribution generalisation.
Misinformation changes rapidly, much quicker than moderators can annotate at
scale, resulting in a shift between the training and inference data
distributions. As a result, misinformation models need to be able to perform
out-of-distribution generalisation, an understudied problem in existing
datasets. We identify 6 axes of generalisation-time, event, topic, publisher,
political bias, misinformation type-and design evaluation procedures for each.
We also analyse some baseline models, highlighting how these fail important
desiderata.","['Ivo Verhoeven', 'Pushkar Mishra', 'Ekaterina Shutova']",1,0.6578349
"This study investigates who should bear the responsibility of combating the
spread of misinformation in social networks. Should that be the online
platforms or their users? Should that be done by debunking the ""fake news""
already in circulation or by investing in preemptive efforts to prevent their
diffusion altogether? We seek to answer such questions in a stylized opinion
dynamics framework, where agents in a network aggregate the information they
receive from peers and/or from influential external sources, with the aim of
learning a ground truth among a set of competing hypotheses. In most cases, we
find centralized sources to be more effective at combating misinformation than
distributed ones, suggesting that online platforms should play an active role
in the fight against fake news. In line with literature on the ""backfire
effect"", we find that debunking in certain circumstances can be a
counterproductive strategy, whereas some targeted strategies (akin to
""deplatforming"") and/or preemptive campaigns turn out to be quite effective.
Despite its simplicity, our model provides useful guidelines that could inform
the ongoing debate on online disinformation and the best ways to limit its
damaging effects.","['Diana Riazi', 'Giacomo Livan']",4,0.7168596
"Language models (LMs) are known to suffer from hallucinations and
misinformation. Retrieval augmented generation (RAG) that retrieves verifiable
information from an external knowledge corpus to complement the parametric
knowledge in LMs provides a tangible solution to these problems. However, the
generation quality of RAG is highly dependent on the relevance between a user's
query and the retrieved documents. Inaccurate responses may be generated when
the query is outside of the scope of knowledge represented in the external
knowledge corpus or if the information in the corpus is out-of-date. In this
work, we establish a statistical framework that assesses how well a query can
be answered by an RAG system by capturing the relevance of knowledge. We
introduce an online testing procedure that employs goodness-of-fit (GoF) tests
to inspect the relevance of each user query to detect out-of-knowledge queries
with low knowledge relevance. Additionally, we develop an offline testing
framework that examines a collection of user queries, aiming to detect
significant shifts in the query distribution which indicates the knowledge
corpus is no longer sufficiently capable of supporting the interests of the
users. We demonstrate the capabilities of these strategies through a systematic
evaluation on eight question-answering (QA) datasets, the results of which
indicate that the new testing framework is an efficient solution to enhance the
reliability of existing RAG systems.","['Zhuohang Li', 'Jiaxin Zhang', 'Chao Yan', 'Kamalika Das', 'Sricharan Kumar', 'Murat Kantarcioglu', 'Bradley A. Malin']",1,0.75272787
"Multimodal encoders like CLIP excel in tasks such as zero-shot image
classification and cross-modal retrieval. However, they require excessive
training data. We propose canonical similarity analysis (CSA), which uses two
unimodal encoders to replicate multimodal encoders using limited data. CSA maps
unimodal features into a multimodal space, using a new similarity score to
retain only the multimodal information. CSA only involves the inference of
unimodal encoders and a cubic-complexity matrix decomposition, eliminating the
need for extensive GPU-based model training. Experiments show that CSA
outperforms CLIP while requiring $300,000\times$ fewer multimodal data pairs
and $6\times$ fewer unimodal data for ImageNet classification and
misinformative news captions detection. CSA surpasses the state-of-the-art
method to map unimodal features to multimodal features. We also demonstrate the
ability of CSA with modalities beyond image and text, paving the way for future
modality pairs with limited paired multimodal data but abundant unpaired
unimodal data, such as lidar and text.","['Po-han Li', 'Sandeep P. Chinchali', 'Ufuk Topcu']",2,0.55542916
"As Large Language Models (LLMs) grow increasingly powerful, multi-agent
systems are becoming more prevalent in modern AI applications. Most safety
research, however, has focused on vulnerabilities in single-agent LLMs. These
include prompt injection attacks, where malicious prompts embedded in external
content trick the LLM into executing unintended or harmful actions,
compromising the victim's application. In this paper, we reveal a more
dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We
introduce Prompt Infection, a novel attack where malicious prompts
self-replicate across interconnected agents, behaving much like a computer
virus. This attack poses severe threats, including data theft, scams,
misinformation, and system-wide disruption, all while propagating silently
through the system. Our extensive experiments demonstrate that multi-agent
systems are highly susceptible, even when agents do not publicly share all
communications. To address this, we propose LLM Tagging, a defense mechanism
that, when combined with existing safeguards, significantly mitigates infection
spread. This work underscores the urgent need for advanced security measures as
multi-agent LLM systems become more widely adopted.","['Donghyun Lee', 'Mo Tiwari']",6,0.6448767
"Recent advances in text-to-speech (TTS) systems, particularly those with
voice cloning capabilities, have made voice impersonation readily accessible,
raising ethical and legal concerns due to potential misuse for malicious
activities like misinformation campaigns and fraud. While synthetic speech
detectors (SSDs) exist to combat this, they are vulnerable to ``test domain
shift"", exhibiting decreased performance when audio is altered through
transcoding, playback, or background noise. This vulnerability is further
exacerbated by deliberate manipulation of synthetic speech aimed at deceiving
detectors. This work presents the first systematic study of such active
malicious attacks against state-of-the-art open-source SSDs. White-box attacks,
black-box attacks, and their transferability are studied from both attack
effectiveness and stealthiness, using both hardcoded metrics and human ratings.
The results highlight the urgent need for more robust detection methods in the
face of evolving adversarial threats.","['Hongbin Liu', 'Youzheng Chen', 'Arun Narayanan', 'Athula Balachandran', 'Pedro J. Moreno', 'Lun Wang']",11,0.6986153
"Mis/disinformation is a common and dangerous occurrence on social media.
Misattribution is a form of mis/disinformation that deals with a false claim of
authorship, which means a user is claiming someone said (posted) something they
never did. We discuss the difference between misinformation and disinformation
and how screenshots are used to spread author misattribution on social media
platforms. It is important to be able to find the original post of a screenshot
to determine if the screenshot is being correctly attributed. To do this we
have built several tools to aid in automating this search process. The first is
a Python script that aims to categorize Twitter posts based on their structure,
extract the metadata from a screenshot, and use this data to group all the
posts within a screenshot together. We tested this process on 75 Twitter posts
containing screenshots collected by hand to determine how well the script
extracted metadata and grouped the individual posts, F1 = 0.80. The second is a
series of scrapers being used to collect a dataset that can train and test a
model to differentiate between various social media platforms. We collected
16,620 screenshots have been collected from Facebook, Instagram, Truth Social,
and Twitter. Screenshots were taken by the scrapers of the web version and
mobile version of each platform in both light and dark mode.","['Ashlyn M. Farris', 'Michael L. Nelson']",13,0.5878994
"This study evaluates the effectiveness of machine learning (ML) and deep
learning (DL) models in detecting COVID-19-related misinformation on online
social networks (OSNs), aiming to develop more effective tools for countering
the spread of health misinformation during the pan-demic. The study trained and
tested various ML classifiers (Naive Bayes, SVM, Random Forest, etc.), DL
models (CNN, LSTM, hybrid CNN+LSTM), and pretrained language models
(DistilBERT, RoBERTa) on the ""COVID19-FNIR DATASET"". These models were
evaluated for accuracy, F1 score, recall, precision, and ROC, and used
preprocessing techniques like stemming and lemmatization. The results showed
SVM performed well, achieving a 94.41% F1-score. DL models with Word2Vec
embeddings exceeded 98% in all performance metrics (accuracy, F1 score, recall,
precision & ROC). The CNN+LSTM hybrid models also exceeded 98% across
performance metrics, outperforming pretrained models like DistilBERT and
RoBERTa. Our study concludes that DL and hybrid DL models are more effective
than conventional ML algorithms for detecting COVID-19 misinformation on OSNs.
The findings highlight the importance of advanced neural network approaches and
large-scale pretraining in misinformation detection. Future research should
optimize these models for various misinformation types and adapt to changing
OSNs, aiding in combating health misinformation.","['Mkululi Sikosana', 'Oluwaseun Ajao', 'Sean Maudsley-Barton']",6,0.63348997
"As Canada prepares for the 2025 federal election, ensuring the integrity and
security of the electoral process against cyber threats is crucial. Recent
foreign interference in elections globally highlight the increasing
sophistication of adversaries in exploiting technical and human
vulnerabilities. Such vulnerabilities also exist in Canada's electoral system
that relies on a complex network of IT systems, vendors, and personnel. To
mitigate these vulnerabilities, a threat assessment is crucial to identify
emerging threats, develop incident response capabilities, and build public
trust and resilience against cyber threats. Therefore, this paper presents a
comprehensive national cyber threat assessment, following the NIST Special
Publication 800-30 framework, focusing on identifying and mitigating
cybersecurity risks to the upcoming 2025 Canadian federal election. The
research identifies three major threats: misinformation, disinformation, and
malinformation (MDM) campaigns; attacks on critical infrastructure and election
support systems; and espionage by malicious actors. Through detailed analysis,
the assessment offers insights into the capabilities, intent, and potential
impact of these threats. The paper also discusses emerging technologies and
their influence on election security and proposes a multi-faceted approach to
risk mitigation ahead of the election.","['Nazmul Islam', 'Soomin Kim', 'Mohammad Pirooz', 'Sasha Shvetsov']",9,0.5974336
"When the physics is wrong, physics-informed machine learning becomes
physics-misinformed machine learning. A powerful black-box model should not be
able to conceal misconceived physics. We propose two criteria that can be used
to assert integrity that a hybrid (physics plus black-box) model: 0) the
black-box model should be unable to replicate the physical model, and 1) any
best-fit hybrid model has the same physical parameter as a best-fit standalone
physics model. We demonstrate them for a sample nonlinear mechanical system
approximated by its small-signal linearization.","['Simon Kuang', 'Xinfan Lin']",1,0.37619644
"Human fact-checkers have specialized domain knowledge that allows them to
formulate precise questions to verify information accuracy. However, this
expert-driven approach is labor-intensive and is not scalable, especially when
dealing with complex multimodal misinformation. In this paper, we propose a
fully-automated framework, LRQ-Fact, for multimodal fact-checking. Firstly, the
framework leverages Vision-Language Models (VLMs) and Large Language Models
(LLMs) to generate comprehensive questions and answers for probing multimodal
content. Next, a rule-based decision-maker module evaluates both the original
content and the generated questions and answers to assess the overall veracity.
Extensive experiments on two benchmarks show that LRQ-Fact improves detection
accuracy for multimodal misinformation. Moreover, we evaluate its
generalizability across different model backbones, offering valuable insights
for further refinement.","['Alimohammad Beigi', 'Bohan Jiang', 'Dawei Li', 'Tharindu Kumarage', 'Zhen Tan', 'Pouya Shaeri', 'Huan Liu']",1,0.72054833
"In this work, we address the real-world, challenging task of out-of-context
misinformation detection, where a real image is paired with an incorrect
caption for creating fake news. Existing approaches for this task assume the
availability of large amounts of labeled data, which is often impractical in
real-world, since it requires extensive manual intervention and domain
expertise. In contrast, since obtaining a large corpus of unlabeled image-text
pairs is much easier, here, we propose a semi-supervised protocol, where the
model has access to a limited number of labeled image-text pairs and a large
corpus of unlabeled pairs. Additionally, the occurrence of fake news being much
lesser compared to the real ones, the datasets tend to be highly imbalanced,
thus making the task even more challenging. Towards this goal, we propose a
novel framework, Consensus from Vision-Language Models (CoVLM), which generates
robust pseudo-labels for unlabeled pairs using thresholds derived from the
labeled data. This approach can automatically determine the right threshold
parameters of the model for selecting the confident pseudo-labels. Experimental
results on benchmark datasets across challenging conditions and comparisons
with state-of-the-art approaches demonstrate the effectiveness of our
framework.","['Devank', 'Jayateja Kalla', 'Soma Biswas']",7,0.70342267
"We investigate and observe the behaviour and performance of Large Language
Model (LLM)-backed chatbots in addressing misinformed prompts and questions
with demographic information within the domains of Climate Change and Mental
Health. Through a combination of quantitative and qualitative methods, we
assess the chatbots' ability to discern the veracity of statements, their
adherence to facts, and the presence of bias or misinformation in their
responses. Our quantitative analysis using True/False questions reveals that
these chatbots can be relied on to give the right answers to these close-ended
questions. However, the qualitative insights, gathered from domain experts,
shows that there are still concerns regarding privacy, ethical implications,
and the necessity for chatbots to direct users to professional services. We
conclude that while these chatbots hold significant promise, their deployment
in sensitive areas necessitates careful consideration, ethical oversight, and
rigorous refinement to ensure they serve as a beneficial augmentation to human
expertise rather than an autonomous solution.","['Toluwani Aremu', 'Oluwakemi Akinwehinmi', 'Chukwuemeka Nwagu', 'Syed Ishtiaque Ahmed', 'Rita Orji', 'Pedro Arnau Del Amo', 'Abdulmotaleb El Saddik']",0,0.6477835
"Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using
generative Artificial Intelligence (AI) technology have made it possible to
generate high-quality and realistic human-like audio. This introduces
significant challenges to distinguishing AI-synthesized speech from the
authentic human voice and could raise potential issues of misuse for malicious
purposes such as impersonation and fraud, spreading misinformation, deepfakes,
and scams. However, existing detection techniques for AI-synthesized audio have
not kept pace and often exhibit poor generalization across diverse datasets. In
this paper, we introduce SONAR, a synthetic AI-Audio Detection Framework and
Benchmark, aiming to provide a comprehensive evaluation for distinguishing
cutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation
dataset sourced from 9 diverse audio synthesis platforms, including leading TTS
providers and state-of-the-art TTS models. It is the first framework to
uniformly benchmark AI-audio detection across both traditional and foundation
model-based deepfake detection systems. Through extensive experiments, we
reveal the generalization limitations of existing detection methods and
demonstrate that foundation models exhibit stronger generalization
capabilities, which can be attributed to their model size and the scale and
quality of pretraining data. Additionally, we explore the effectiveness and
efficiency of few-shot fine-tuning in improving generalization, highlighting
its potential for tailored applications, such as personalized detection systems
for specific entities or individuals. Code and dataset are available at
https://github.com/Jessegator/SONAR.","['Xiang Li', 'Pin-Yu Chen', 'Wenqi Wei']",11,0.7105814
"The rise of large language models (LLMs) has significantly influenced the
quality of information in decision-making systems, leading to the prevalence of
AI-generated content and challenges in detecting misinformation and managing
conflicting information, or ""inter-evidence conflicts."" This study introduces a
method for generating diverse, validated evidence conflicts to simulate
real-world misinformation scenarios. We evaluate conflict detection methods,
including Natural Language Inference (NLI) models, factual consistency (FC)
models, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict
resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models
exhibit high precision in detecting answer conflicts, though weaker models
suffer from low recall; (2) FC models struggle with lexically similar answer
conflicts, while NLI and LLM models handle these better; and (3) stronger
models like GPT-4 show robust performance, especially with nuanced conflicts.
For conflict resolution, LLMs often favor one piece of conflicting evidence
without justification and rely on internal knowledge if they have prior
beliefs.","['Cheng Jiayang', 'Chunkit Chan', 'Qianqian Zhuang', 'Lin Qiu', 'Tianhang Zhang', 'Tengxiao Liu', 'Yangqiu Song', 'Yue Zhang', 'Pengfei Liu', 'Zheng Zhang']",6,0.7300081
"Computational methods to aid journalists in the task often require adapting a
model to specific domains and generating explanations. However, most automated
fact-checking methods rely on three-class datasets, which do not accurately
reflect real-world misinformation. Moreover, fact-checking explanations are
often generated based on text summarization of evidence, failing to address the
relationship between the claim and the evidence. To address these issues, we
extend the self-rationalization method--typically used in natural language
inference (NLI) tasks--to fact verification. We propose a label-adaptive
learning approach: first, we fine-tune a model to learn veracity prediction
with annotated labels (step-1 model). Then, we fine-tune the step-1 model again
to learn self-rationalization, using the same data and additional annotated
explanations. Our results show that our label-adaptive approach improves
veracity prediction by more than ten percentage points (Macro F1) on both the
PubHealth and AVeriTec datasets, outperforming the GPT-4 model. Furthermore, to
address the high cost of explanation annotation, we generated 64 synthetic
explanations from three large language models: GPT-4-turbo, GPT-3.5-turbo, and
Llama-3-8B and few-shot fine-tune our step-1 model. The few-shot synthetic
explanation fine-tuned model performed comparably to the fully fine-tuned
self-rationalization model, demonstrating the potential of low-budget learning
with synthetic data. Our label-adaptive self-rationalization approach presents
a promising direction for future research on real-world explainable
fact-checking with different labeling schemes.","['Jing Yang', 'Anderson Rocha']",1,0.73298174
"Misinformation, defined as false or inaccurate information, can result in
significant societal harm when it is spread with malicious or even innocuous
intent. The rapid online information exchange necessitates advanced detection
mechanisms to mitigate misinformation-induced harm. Existing research, however,
has predominantly focused on assessing veracity, overlooking the legal
implications and social consequences of misinformation. In this work, we take a
novel angle to consolidate the definition of misinformation detection using
legal issues as a measurement of societal ramifications, aiming to bring
interdisciplinary efforts to tackle misinformation and its consequence. We
introduce a new task: Misinformation with Legal Consequence (MisLC), which
leverages definitions from a wide range of legal domains covering 4 broader
legal topics and 11 fine-grained legal issues, including hate speech, election
laws, and privacy regulations. For this task, we advocate a two-step dataset
curation approach that utilizes crowd-sourced checkworthiness and expert
evaluations of misinformation. We provide insights about the MisLC task through
empirical evidence, from the problem definition to experiments and expert
involvement. While the latest large language models and retrieval-augmented
generation are effective baselines for the task, we find they are still far
from replicating expert performance.","['Chu Fei Luo', 'Radin Shayanfar', 'Rohan Bhambhoria', 'Samuel Dahan', 'Xiaodan Zhu']",0,0.7726462
"The rapid advancement of deepfake technology poses a significant threat to
digital media integrity. Deepfakes, synthetic media created using AI, can
convincingly alter videos and audio to misrepresent reality. This creates risks
of misinformation, fraud, and severe implications for personal privacy and
security. Our research addresses the critical issue of deepfakes through an
innovative multimodal approach, targeting both visual and auditory elements.
This comprehensive strategy recognizes that human perception integrates
multiple sensory inputs, particularly visual and auditory information, to form
a complete understanding of media content. For visual analysis, a model that
employs advanced feature extraction techniques was developed, extracting nine
distinct facial characteristics and then applying various machine learning and
deep learning models. For auditory analysis, our model leverages
mel-spectrogram analysis for feature extraction and then applies various
machine learning and deep learningmodels. To achieve a combined analysis, real
and deepfake audio in the original dataset were swapped for testing purposes
and ensured balanced samples. Using our proposed models for video and audio
classification i.e. Artificial Neural Network and VGG19, the overall sample is
classified as deepfake if either component is identified as such. Our
multimodal framework combines visual and auditory analyses, yielding an
accuracy of 94%.","['Kashish Gandhi', 'Prutha Kulkarni', 'Taran Shah', 'Piyush Chaudhari', 'Meera Narvekar', 'Kranti Ghag']",11,0.7891402
"The widespread use of AI technologies to generate digital content has led to
increased misinformation and online harm. Deep fake technologies, a type of AI,
make it easier to create convincing but fake content on social media, leading
to various cyber threats. Malicious actors exploit AI capabilities, posing
digital, physical, and psychological harm to individuals. While social media
platforms have safety measures such as content rating and feedback systems,
these are often used by people with higher digital literacy. There is a lack of
preventive measures and a need for user-friendly tools that can be used by
people with lower digital literacy. Our goal is to create a user-friendly
multilingual AI-based personal assistant, Malak, to reduce online harm and
promote safe online interactions, benefiting users with lower literacy levels.","['Farnaz Farid', 'Farhad Ahamed']",9,0.69565415
"Despite large language models (LLMs) increasingly becoming important
components of news recommender systems, employing LLMs in such systems
introduces new risks, such as the influence of cognitive biases in LLMs.
Cognitive biases refer to systematic patterns of deviation from norms or
rationality in the judgment process, which can result in inaccurate outputs
from LLMs, thus threatening the reliability of news recommender systems.
Specifically, LLM-based news recommender systems affected by cognitive biases
could lead to the propagation of misinformation, reinforcement of stereotypes,
and the formation of echo chambers. In this paper, we explore the potential
impact of multiple cognitive biases on LLM-based news recommender systems,
including anchoring bias, framing bias, status quo bias and group attribution
bias. Furthermore, to facilitate future research at improving the reliability
of LLM-based news recommender systems, we discuss strategies to mitigate these
biases through data augmentation, prompt engineering and learning algorithms
aspects.","['Yougang Lyu', 'Xiaoyu Zhang', 'Zhaochun Ren', 'Maarten de Rijke']",6,0.7459258
"LLMs are increasingly being used in workflows involving generating content to
be consumed by humans (e.g., marketing) and also in directly interacting with
humans (e.g., through chatbots). The development of such systems that are
capable of generating verifiably persuasive messages presents both
opportunities and challenges for society. On the one hand, such systems could
positively impact domains like advertising and social good, such as addressing
drug addiction, and on the other, they could be misused for spreading
misinformation and shaping political opinions. To channel LLMs' impact on
society, we need to develop systems to measure and benchmark their
persuasiveness. With this motivation, we introduce PersuasionBench and
PersuasionArena, the first large-scale benchmark and arena containing a battery
of tasks to measure the persuasion ability of generative models automatically.
We investigate to what extent LLMs know and leverage linguistic patterns that
can help them generate more persuasive language. Our findings indicate that the
persuasiveness of LLMs correlates positively with model size, but smaller
models can also be made to have a higher persuasiveness than much larger
models. Notably, targeted training using synthetic and natural datasets
significantly enhances smaller models' persuasive capabilities, challenging
scale-dependent assumptions. Our findings carry key implications for both model
developers and policymakers. For instance, while the EU AI Act and California's
SB-1047 aim to regulate AI models based on the number of floating point
operations, we demonstrate that simple metrics like this alone fail to capture
the full scope of AI's societal impact. We invite the community to explore and
contribute to PersuasionArena and PersuasionBench, available at
https://bit.ly/measure-persuasion, to advance our understanding of AI-driven
persuasion and its societal implications.","['Somesh Singh', 'Yaman K Singla', 'Harini SI', 'Balaji Krishnamurthy']",9,0.7369905
"The proliferation of fake news has emerged as a significant threat to the
integrity of information dissemination, particularly on social media platforms.
Misinformation can spread quickly due to the ease of creating and disseminating
content, affecting public opinion and sociopolitical events. Identifying false
information is therefore essential to reducing its negative consequences and
maintaining the reliability of online news sources. Traditional approaches to
fake news detection often rely solely on content-based features, overlooking
the crucial role of social context in shaping the perception and propagation of
news articles. In this paper, we propose a comprehensive approach that
integrates social context-based features with news content features to enhance
the accuracy of fake news detection in under-resourced languages. We perform
several experiments utilizing a variety of methodologies, including traditional
machine learning, neural networks, ensemble learning, and transfer learning.
Assessment of the outcomes of the experiments shows that the ensemble learning
approach has the highest accuracy, achieving a 0.99 F1 score. Additionally,
when compared with monolingual models, the fine-tuned model with the target
language outperformed others, achieving a 0.94 F1 score. We analyze the
functioning of the models, considering the important features that contribute
to model performance, using explainable AI techniques.","['Mesay Gemeda Yigezu', 'Melkamu Abay Mersha', 'Girma Yohannis Bade', 'Jugal Kalita', 'Olga Kolesnikova', 'Alexander Gelbukh']",4,0.78140223
"Large Language Models (LLMs) can be \emph{misused} to spread online spam and
misinformation. Content watermarking deters misuse by hiding a message in
model-generated outputs, enabling their detection using a secret watermarking
key. Robustness is a core security property, stating that evading detection
requires (significant) degradation of the content's quality. Many LLM
watermarking methods have been proposed, but robustness is tested only against
\emph{non-adaptive} attackers who lack knowledge of the watermarking method and
can find only suboptimal attacks. We formulate the robustness of LLM
watermarking as an objective function and propose preference-based optimization
to tune \emph{adaptive} attacks against the specific watermarking method. Our
evaluation shows that (i) adaptive attacks substantially outperform
non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks
optimized against a few known watermarks remain highly effective when tested
against other unseen watermarks, and (iii) optimization-based attacks are
practical and require less than seven GPU hours. Our findings underscore the
need to test robustness against adaptive attackers.","['Abdulrahman Diaa', 'Toluwani Aremu', 'Nils Lukas']",7,0.59162223
"In an era of ""moving fast and breaking things"", regulators have moved slowly
to pick up the safety, bias, and legal pieces left in the wake of broken
Artificial Intelligence (AI) deployment. Since AI models, such as large
language models, are able to push misinformation and stoke division within our
society, it is imperative for regulators to employ a framework that mitigates
these dangers and ensures user safety. While there is much-warranted discussion
about how to address the safety, bias, and legal woes of state-of-the-art AI
models, the number of rigorous and realistic mathematical frameworks to
regulate AI safety is lacking. We take on this challenge, proposing an
auction-based regulatory mechanism that provably incentivizes model-building
agents (i) to deploy safer models and (ii) to participate in the regulation
process. We provably guarantee, via derived Nash Equilibria, that each
participating agent's best strategy is to submit a model safer than a
prescribed minimum-safety threshold. Empirical results show that our regulatory
auction boosts safety and participation rates by 20% and 15% respectively,
outperforming simple regulatory frameworks that merely enforce minimum safety
standards.","['Marco Bornstein', 'Zora Che', 'Suhas Julapalli', 'Abdirisak Mohamed', 'Amrit Singh Bedi', 'Furong Huang']",9,0.6103051
"We introduce Loki, an open-source tool designed to address the growing
problem of misinformation. Loki adopts a human-centered approach, striking a
balance between the quality of fact-checking and the cost of human involvement.
It decomposes the fact-checking task into a five-step pipeline: breaking down
long texts into individual claims, assessing their check-worthiness, generating
queries, retrieving evidence, and verifying the claims. Instead of fully
automating the claim verification process, Loki provides essential information
at each step to assist human judgment, especially for general users such as
journalists and content moderators. Moreover, it has been optimized for
latency, robustness, and cost efficiency at a commercially usable level. Loki
is released under an MIT license and is available on GitHub. We also provide a
video presenting the system and its capabilities.","['Haonan Li', 'Xudong Han', 'Hao Wang', 'Yuxia Wang', 'Minghan Wang', 'Rui Xing', 'Yilin Geng', 'Zenan Zhai', 'Preslav Nakov', 'Timothy Baldwin']",1,0.56285805
"While generative AI (GenAI) offers countless possibilities for creative and
productive tasks, artificially generated media can be misused for fraud,
manipulation, scams, misinformation campaigns, and more. To mitigate the risks
associated with maliciously generated media, forensic classifiers are employed
to identify AI-generated content. However, current forensic classifiers are
often not evaluated in practically relevant scenarios, such as the presence of
an attacker or when real-world artifacts like social media degradations affect
images. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)
detectors under different attack scenarios. We demonstrate that forensic
classifiers can be effectively attacked in realistic settings, even when the
attacker does not have access to the target model and post-processing occurs
after the adversarial examples are created, which is standard on social media
platforms. These attacks can significantly reduce detection accuracy to the
extent that the risks of relying on detectors outweigh their benefits. Finally,
we propose a simple defense mechanism to make CLIP-based detectors, which are
currently the best-performing detectors, robust against these attacks.","['Sina Mavali', 'Jonas Ricker', 'David Pape', 'Yash Sharma', 'Asja Fischer', 'Lea Sch√∂nherr']",7,0.73728573
"The spread of misinformation through online social media platforms has had
substantial societal consequences. As a result, platforms have introduced
measures to alert users of news content that may be misleading or contain
inaccuracies as a means to discourage them from sharing it. These interventions
sometimes cite external sources, such as fact-checking organizations and news
outlets, for providing assessments related to the accuracy of the content.
However, it is unclear whether users trust the assessments provided by these
entities and whether perceptions vary across different topics of news. We
conducted an online study with 655 US participants to explore user perceptions
of eight categories of fact-checking entities across two misinformation topics,
as well as factors that may impact users' perceptions. We found that
participants' opinions regarding the trustworthiness and bias of the entities
varied greatly, aligning largely with their political preference. However, just
the presence of a fact-checking label appeared to discourage participants from
sharing the headlines studied. Our results hint at the need for further
exploring fact-checking entities that may be perceived as neutral, as well as
the potential for incorporating multiple assessments in such labels.","['Hana Habib', 'Sara Elsharawy', 'Rifat Rahman']",3,0.7416729
"Background: Large language models (LLMs) are trained to follow directions,
but this introduces a vulnerability to blindly comply with user requests even
if they generate wrong information. In medicine, this could accelerate the
generation of misinformation that impacts human well-being.
  Objectives/Methods: We analyzed compliance to requests to generate misleading
content about medications in settings where models know the request is
illogical. We investigated whether in-context directions and instruction-tuning
of LLMs to prioritize logical reasoning over compliance reduced misinformation
risk.
  Results: While all frontier LLMs complied with misinformation requests, both
prompt-based and parameter-based approaches can improve the detection of logic
flaws in requests and prevent the dissemination of medical misinformation.
  Conclusion: Shifting LLMs to prioritize logic over compliance could reduce
risks of exploitation for medical misinformation.","['Shan Chen', 'Mingye Gao', 'Kuleen Sasse', 'Thomas Hartvigsen', 'Brian Anthony', 'Lizhou Fan', 'Hugo Aerts', 'Jack Gallifant', 'Danielle Bitterman']",6,0.7320219
"Detecting multimodal misinformation, especially in the form of image-text
pairs, is crucial. Obtaining large-scale, high-quality real-world fact-checking
datasets for training detectors is costly, leading researchers to use synthetic
datasets generated by AI technologies. However, the generalizability of
detectors trained on synthetic data to real-world scenarios remains unclear due
to the distribution gap. To address this, we propose learning from synthetic
data for detecting real-world multimodal misinformation through two
model-agnostic data selection methods that match synthetic and real-world data
distributions. Experiments show that our method enhances the performance of a
small MLLM (13B) on real-world fact-checking datasets, enabling it to even
surpass GPT-4V~\cite{GPT-4V}.","['Fengzhu Zeng', 'Wenqian Li', 'Wei Gao', 'Yan Pang']",1,0.6455377
"The emergence of Multimodal Foundation Models (MFMs) holds significant
promise for transforming social media platforms. However, this advancement also
introduces substantial security and ethical concerns, as it may facilitate
malicious actors in the exploitation of online users. We aim to evaluate the
strength of security protocols on prominent social media platforms in
mitigating the deployment of MFM bots. We examined the bot and content policies
of eight popular social media platforms: X (formerly Twitter), Instagram,
Facebook, Threads, TikTok, Mastodon, Reddit, and LinkedIn. Using Selenium, we
developed a web bot to test bot deployment and AI-generated content policies
and their enforcement mechanisms. Our findings indicate significant
vulnerabilities within the current enforcement mechanisms of these platforms.
Despite having explicit policies against bot activity, all platforms failed to
detect and prevent the operation of our MFM bots. This finding reveals a
critical gap in the security measures employed by these social media platforms,
underscoring the potential for malicious actors to exploit these weaknesses to
disseminate misinformation, commit fraud, or manipulate users.","['Kristina Radivojevic', 'Christopher McAleer', 'Catrell Conley', 'Cormac Kennedy', 'Paul Brenner']",13,0.754858
"Recent advancements in artificial intelligence have enabled generative models
to produce synthetic scientific images that are indistinguishable from pristine
ones, posing a challenge even for expert scientists habituated to working with
such content. When exploited by organizations known as paper mills, which
systematically generate fraudulent articles, these technologies can
significantly contribute to the spread of misinformation about ungrounded
science, potentially undermining trust in scientific research. While previous
studies have explored black-box solutions, such as Convolutional Neural
Networks, for identifying synthetic content, only some have addressed the
challenge of generalizing across different models and providing insight into
the artifacts in synthetic images that inform the detection process. This study
aims to identify explainable artifacts generated by state-of-the-art generative
models (e.g., Generative Adversarial Networks and Diffusion Models) and
leverage them for open-set identification and source attribution (i.e.,
pointing to the model that created the image).","['Jo√£o Phillipe Cardenuto', 'Sara Mandelli', 'Daniel Moreira', 'Paolo Bestagini', 'Edward Delp', 'Anderson Rocha']",7,0.71644044
"A recent article in $\textit{Science}$ by Guess et al. estimated the effect
of Facebook's news feed algorithm on exposure to misinformation and political
information among Facebook users. However, its reporting and conclusions did
not account for a series of temporary emergency changes to Facebook's news feed
algorithm in the wake of the 2020 U.S. presidential election that were designed
to diminish the spread of voter-fraud misinformation. Here, we demonstrate that
these emergency measures systematically reduced the amount of misinformation in
the control group of the study, which was using the news feed algorithm. This
issue may have led readers to misinterpret the results of the study and to
conclude that the Facebook news feed algorithm used outside of the study period
mitigates political misinformation as compared to reverse chronological feed.","['Chhandak Bagchi', 'Filippo Menczer', 'Jennifer Lundquist', 'Monideepa Tarafdar', 'Anthony Paik', 'Przemyslaw A. Grabowicz']",10,0.65685016
"Widely distributed misinformation shared across social media channels is a
pressing issue that poses a significant threat to many aspects of society's
well-being. Inaccurate shared information causes confusion, can adversely
affect mental health, and can lead to mis-informed decision-making. Therefore,
it is important to implement proactive measures to intervene and curb the
spread of misinformation where possible. This has prompted scholars to
investigate a variety of intervention strategies for misinformation sharing on
social media. This study explores the typology of intervention strategies for
addressing misinformation sharing on social media, identifying 4 important
clusters - cognition-based, automated-based, information-based, and
hybrid-based. The literature selection process utilized the PRISMA method to
ensure a systematic and comprehensive analysis of relevant literature while
maintaining transparency and reproducibility. A total of 139 articles published
from 2013-2023 were then analyzed. Meanwhile, bibliometric analyses were
conducted using performance analysis and science mapping techniques for the
typology development. A comparative analysis of the typology was conducted to
reveal patterns and evolution in the field. This provides valuable insights for
both theory and practical applications. Overall, the study concludes that
scholarly contributions to scientific research and publication help to address
research gaps and expand knowledge in this field. Understanding the evolution
of intervention strategies for misinformation sharing on social media can
support future research that contributes to the development of more effective
and sustainable solutions to this persistent problem.","['Juanita Zainudin', 'Nazlena Mohamad Ali', 'Alan F. Smeaton', 'Mohamad Taha Ijab']",0,0.77607524
"Advances in generative models have created Artificial Intelligence-Generated
Images (AIGIs) nearly indistinguishable from real photographs. Leveraging a
large corpus of 30,824 AIGIs collected from Instagram and Twitter, and
combining quantitative content analysis with qualitative analysis, this study
unpacks AI photorealism of AIGIs from four key dimensions, content, human,
aesthetic, and production features. We find that photorealistic AIGIs often
depict human figures, especially celebrities and politicians, with a high
degree of surrealism and aesthetic professionalism, alongside a low degree of
overt signals of AI production. This study is the first to empirically
investigate photorealistic AIGIs across multiple platforms using a
mixed-methods approach. Our findings provide important implications and
insights for understanding visual misinformation and mitigating potential risks
associated with photorealistic AIGIs. We also propose design recommendations to
enhance the responsible use of AIGIs.","['Qiyao Peng', 'Yingdan Lu', 'Yilang Peng', 'Sijia Qian', 'Xinyi Liu', 'Cuihua Shen']",7,0.6657334
"In an increasingly interconnected world, a key scientific challenge is to
examine mechanisms that lead to the widespread propagation of contagions, such
as misinformation and pathogens, and identify risk factors that can trigger
large-scale outbreaks. Underlying both the spread of disease and misinformation
epidemics is the evolution of the contagion as it propagates, leading to the
emergence of different strains, e.g., through genetic mutations in pathogens
and alterations in the information content. Recent studies have revealed that
models that do not account for heterogeneity in transmission risks associated
with different strains of the circulating contagion can lead to inaccurate
predictions. However, existing results on multi-strain spreading assume that
the network has a vanishingly small clustering coefficient, whereas clustering
is widely known to be a fundamental property of real-world social networks. In
this work, we investigate spreading processes that entail evolutionary
adaptations on random graphs with tunable clustering and arbitrary degree
distributions. We derive a mathematical framework to quantify the epidemic
characteristics of a contagion that evolves as it spreads, with the structure
of the underlying network as given via arbitrary {\em joint} degree
distributions of single-edges and triangles. To the best of our knowledge, our
work is the first to jointly analyze the impact of clustering and evolution on
the emergence of epidemic outbreaks. We supplement our theoretical finding with
numerical simulations and case studies, shedding light on the impact of
clustering on contagion spread.","['Mansi Sood', 'Hejin Gu', 'Rashad Eletreby', 'Swarun Kumar', 'Chai Wah Wu', 'Osman Yagan']",2,0.7256391
"Generative AI holds immense promise in addressing global healthcare access
challenges, with numerous innovative applications now ready for use across
various healthcare domains. However, a significant barrier to the widespread
adoption of these domain-specific AI solutions is the lack of robust safety
mechanisms to effectively manage issues such as hallucination, misinformation,
and ensuring truthfulness. Left unchecked, these risks can compromise patient
safety and erode trust in healthcare AI systems. While general-purpose
frameworks like Llama Guard are useful for filtering toxicity and harmful
content, they do not fully address the stringent requirements for truthfulness
and safety in healthcare contexts. This paper examines the unique safety and
security challenges inherent to healthcare AI, particularly the risk of
hallucinations, the spread of misinformation, and the need for factual accuracy
in clinical settings. I propose enhancements to existing guardrails frameworks,
such as Nvidia NeMo Guardrails, to better suit healthcare-specific needs. By
strengthening these safeguards, I aim to ensure the secure, reliable, and
accurate use of AI in healthcare, mitigating misinformation risks and improving
patient safety.",['Ananya Gangavarapu'],9,0.62743545
"The emergence of social media has made the spread of misinformation easier.
In the financial domain, the accuracy of information is crucial for various
aspects of financial market, which has made financial misinformation detection
(FMD) an urgent problem that needs to be addressed. Large language models
(LLMs) have demonstrated outstanding performance in various fields. However,
current studies mostly rely on traditional methods and have not explored the
application of LLMs in the field of FMD. The main reason is the lack of FMD
instruction tuning datasets and evaluation benchmarks. In this paper, we
propose FMDLlama, the first open-sourced instruction-following LLMs for FMD
task based on fine-tuning Llama3.1 with instruction data, the first multi-task
FMD instruction dataset (FMDID) to support LLM instruction tuning, and a
comprehensive FMD evaluation benchmark (FMD-B) with classification and
explanation generation tasks to test the FMD ability of LLMs. We compare our
models with a variety of LLMs on FMD-B, where our model outperforms all other
open-sourced LLMs as well as ChatGPT.","['Zhiwei Liu', 'Xin Zhang', 'Kailai Yang', 'Qianqian Xie', 'Jimin Huang', 'Sophia Ananiadou']",6,0.73658454
"Recent advancements have showcased the capabilities of Large Language Models
like GPT4 and Llama2 in tasks such as summarization, translation, and content
review. However, their widespread use raises concerns, particularly around the
potential for LLMs to spread persuasive, humanlike misinformation at scale,
which could significantly influence public opinion. This study examines these
risks, focusing on LLMs ability to propagate misinformation as factual. To
investigate this, we built the LLM Echo Chamber, a controlled digital
environment simulating social media chatrooms, where misinformation often
spreads. Echo chambers, where individuals only interact with like minded
people, further entrench beliefs. By studying malicious bots spreading
misinformation in this environment, we can better understand this phenomenon.
We reviewed current LLMs, explored misinformation risks, and applied sota
finetuning techniques. Using Microsoft phi2 model, finetuned with our custom
dataset, we generated harmful content to create the Echo Chamber. This setup,
evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the
ethical concerns surrounding LLMs and emphasizes the need for stronger
safeguards against misinformation.",['Tony Ma'],6,0.7545711
"With the rapid development of large language models (LLMs), assessing their
performance on health-related inquiries has become increasingly essential. It
is critical that these models provide accurate and trustworthy health
information, as their application in real-world contexts--where misinformation
can have serious consequences for individuals seeking medical advice and
support--depends on their reliability. In this work, we present CHBench, the
first comprehensive Chinese Health-related Benchmark designed to evaluate LLMs'
capabilities in understanding physical and mental health across diverse
scenarios. CHBench includes 6,493 entries related to mental health and 2,999
entries focused on physical health, covering a broad spectrum of topics. This
dataset serves as a foundation for evaluating Chinese LLMs' capacity to
comprehend and generate accurate health-related information. Our extensive
evaluations of four popular Chinese LLMs demonstrate that there remains
considerable room for improvement in their understanding of health-related
information. The code is available at https://github.com/TracyGuo2001/CHBench.","['Chenlu Guo', 'Nuo Xu', 'Yi Chang', 'Yuan Wu']",6,0.56448364
"Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing (NLP) tasks, capturing the attention of
both practitioners and the broader public. A key question that now preoccupies
the AI community concerns the capabilities and limitations of these models,
with trustworthiness emerging as a central issue, particularly as LLMs are
increasingly applied in sensitive fields like healthcare and finance, where
errors can have serious consequences. However, most previous studies on the
trustworthiness of LLMs have been limited to a single language, typically the
predominant one in the dataset, such as English. In response to the growing
global deployment of LLMs, we introduce XTRUST, the first comprehensive
multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of
topics, including illegal activities, hallucination, out-of-distribution (OOD)
robustness, physical and mental health, toxicity, fairness, misinformation,
privacy, and machine ethics, across 10 different languages. Using XTRUST, we
conduct an empirical evaluation of the multilingual trustworthiness of five
widely used LLMs, offering an in-depth analysis of their performance across
languages and tasks. Our results indicate that many LLMs struggle with certain
low-resource languages, such as Arabic and Russian, highlighting the
considerable room for improvement in the multilingual trustworthiness of
current language models. The code is available at
https://github.com/LluckyYH/XTRUST.","['Yahan Li', 'Yi Wang', 'Yi Chang', 'Yuan Wu']",6,0.7395092
"Currently, the rapid development of computer vision and deep learning has
enabled the creation or manipulation of high-fidelity facial images and videos
via deep generative approaches. This technology, also known as deepfake, has
achieved dramatic progress and become increasingly popular in social media.
However, the technology can generate threats to personal privacy and national
security by spreading misinformation. To diminish the risks of deepfake, it is
desirable to develop powerful forgery detection methods to distinguish fake
faces from real faces. This paper presents a comprehensive survey of recent
deep learning-based approaches for facial forgery detection. We attempt to
provide the reader with a deeper understanding of the current advances as well
as the major challenges for deepfake detection based on deep learning. We
present an overview of deepfake techniques and analyse the characteristics of
various deepfake datasets. We then provide a systematic review of different
categories of deepfake detection and state-of-the-art deepfake detection
methods. The drawbacks of existing detection methods are analyzed, and future
research directions are discussed to address the challenges in improving both
the performance and generalization of deepfake detection.","['Lixia Ma', 'Puning Yang', 'Yuting Xu', 'Ziming Yang', 'Peipei Li', 'Huaibo Huang']",11,0.84560984
"While large language models (LLMs) exhibit significant utility across various
domains, they simultaneously are susceptible to exploitation for unethical
purposes, including academic misconduct and dissemination of misinformation.
Consequently, AI-generated text detection systems have emerged as a
countermeasure. However, these detection mechanisms demonstrate vulnerability
to evasion techniques and lack robustness against textual manipulations. This
paper introduces back-translation as a novel technique for evading detection,
underscoring the need to enhance the robustness of current detection systems.
The proposed method involves translating AI-generated text through multiple
languages before back-translating to English. We present a model that combines
these back-translated texts to produce a manipulated version of the original
AI-generated text. Our findings demonstrate that the manipulated text retains
the original semantics while significantly reducing the true positive rate
(TPR) of existing detection methods. We evaluate this technique on nine AI
detectors, including six open-source and three proprietary systems, revealing
their susceptibility to back-translation manipulation. In response to the
identified shortcomings of existing AI text detectors, we present a
countermeasure to improve the robustness against this form of manipulation. Our
results indicate that the TPR of the proposed method declines by only 1.85%
after back-translation manipulation. Furthermore, we build a large dataset of
720k texts using eight different LLMs. Our dataset contains both human-authored
and LLM-generated texts in various domains and writing styles to assess the
performance of our method and existing detectors. This dataset is publicly
shared for the benefit of the research community.","['Navid Ayoobi', 'Lily Knab', 'Wen Cheng', 'David Pantoja', 'Hamidreza Alikhani', 'Sylvain Flamant', 'Jin Kim', 'Arjun Mukherjee']",8,0.7058229
"Infectious diseases, transmitted directly or indirectly, are among the
leading causes of epidemics and pandemics. Consequently, several open
challenges exist in predicting epidemic outbreaks, detecting variants, tracing
contacts, discovering new drugs, and fighting misinformation. Artificial
Intelligence (AI) can provide tools to deal with these scenarios, demonstrating
promising results in the fight against the COVID-19 pandemic. AI is becoming
increasingly integrated into various aspects of society. However, ensuring that
AI benefits are distributed equitably and that they are used responsibly is
crucial. Multiple countries are creating regulations to address these concerns,
but the borderless nature of AI requires global cooperation to define
regulatory and guideline consensus. Considering this, The Global South AI for
Pandemic & Epidemic Preparedness & Response Network (AI4PEP) has developed an
initiative comprising 16 projects across 16 countries in the Global South,
seeking to strengthen equitable and responsive public health systems that
leverage Southern-led responsible AI solutions to improve prevention,
preparedness, and response to emerging and re-emerging infectious disease
outbreaks. This opinion introduces our branches in Latin American and Caribbean
(LAC) countries and discusses AI governance in LAC in the light of
biotechnology. Our network in LAC has high potential to help fight infectious
diseases, particularly in low- and middle-income countries, generating
opportunities for the widespread use of AI techniques to improve the health and
well-being of their communities.","['Andre de Carvalho', 'Robson Bonidia', 'Jude Dzevela Kong', 'Mariana Dauhajre', 'Claudio Struchiner', 'Guilherme Goedert', 'Peter F. Stadler', 'Maria Emilia Walter', 'Danilo Sanches', 'Troy Day', 'Marcia Castro', 'John Edmunds', 'Manuel Colome-Hidalgo', 'Demian Arturo Herrera Morban', 'Edian F. Franco', 'Cesar Ugarte-Gil', 'Patricia Espinoza-Lopez', 'Gabriel Carrasco-Escobar', 'Ulisses Rocha']",9,0.6883294
"Unpacking the relationship between the ideology of social media users and
their online news consumption offers critical insight into the feedback loop
between users' engagement behavior and the recommender systems' content
provision. However, disentangling inherent user behavior from platform-induced
influences poses significant challenges, particularly when working with
datasets covering limited time periods. In this study, we conduct both
aggregate and longitudinal analyses using the Facebook Privacy-Protected Full
URLs Dataset, examining user engagement metrics related to news URLs in the
U.S. from January 2017 to December 2020. By incorporating the ideological
alignment and quality of news sources, along with users' political preferences,
we construct weighted averages of ideology and quality of news consumption for
liberal, conservative, and moderate audiences. This allows us to track the
evolution of (i) the ideological gap between liberals and conservatives and
(ii) the average quality of each group's news consumption. These metrics are
linked to broader phenomena such as polarization and misinformation. We
identify two significant shifts in trends for both metrics, each coinciding
with changes in user engagement. Interestingly, during both inflection points,
the ideological gap widens and news quality declines; however, engagement
increases after the first one and decreases after the second. Finally, we
contextualize these changes by discussing their potential relation to two major
updates to Facebook's News Feed algorithm.","['Emma Fraxanet', 'Fabrizio Germano', 'Andreas Kaltenbrunner', 'Vicen√ß G√≥mez']",3,0.7473902
"Data poisoning and leakage risks impede the massive deployment of federated
learning in the real world. This chapter reveals the truths and pitfalls of
understanding two dominating threats: {\em training data privacy intrusion} and
{\em training data poisoning}. We first investigate training data privacy
threat and present our observations on when and how training data may be leaked
during the course of federated training. One promising defense strategy is to
perturb the raw gradient update by adding some controlled randomized noise
prior to sharing during each round of federated learning. We discuss the
importance of determining the proper amount of randomized noise and the proper
location to add such noise for effective mitigation of gradient leakage threats
against training data privacy. Then we will review and compare different
training data poisoning threats and analyze why and when such data poisoning
induced model Trojan attacks may lead to detrimental damage on the performance
of the global model. We will categorize and compare representative poisoning
attacks and the effectiveness of their mitigation techniques, delivering an
in-depth understanding of the negative impact of data poisoning. Finally, we
demonstrate the potential of dynamic model perturbation in simultaneously
ensuring privacy protection, poisoning resilience, and model performance. The
chapter concludes with a discussion on additional risk factors in federated
learning, including the negative impact of skewness, data and algorithmic
biases, as well as misinformation in training data. Powered by empirical
evidence, our analytical study offers some transformative insights into
effective privacy protection and security assurance strategies in
attack-resilient federated learning.","['Wenqi Wei', 'Tiansheng Huang', 'Zachary Yahn', 'Anoop Singhal', 'Margaret Loper', 'Ling Liu']",2,0.65028965
"Across various applications, humans increasingly use black-box artificial
intelligence (AI) systems without insight into these systems' reasoning. To
counter this opacity, explainable AI (XAI) methods promise enhanced
transparency and interpretability. While recent studies have explored how XAI
affects human-AI collaboration, few have examined the potential pitfalls caused
by incorrect explanations. The implications for humans can be far-reaching but
have not been explored extensively. To investigate this, we ran a study (n=160)
on AI-assisted decision-making in which humans were supported by XAI. Our
findings reveal a misinformation effect when incorrect explanations accompany
correct AI advice with implications post-collaboration. This effect causes
humans to infer flawed reasoning strategies, hindering task execution and
demonstrating impaired procedural knowledge. Additionally, incorrect
explanations compromise human-AI team-performance during collaboration. With
our work, we contribute to HCI by providing empirical evidence for the negative
consequences of incorrect explanations on humans post-collaboration and
outlining guidelines for designers of AI.","['Philipp Spitzer', 'Joshua Holstein', 'Katelyn Morrison', 'Kenneth Holstein', 'Gerhard Satzger', 'Niklas K√ºhl']",9,0.7533575
"The use of Wikipedia citations in scholarly research has been the topic of
much inquiry over the past decade. A cross-publisher study (Taylor & Francis
and University of Michigan Press) convened by Digital Science was established
in late 2022 to explore author sentiment towards Wikipedia as a trusted source
of information. A short survey was designed to poll published authors about
views and uses of Wikipedia and explore how the increased addition of research
citations in Wikipedia might help combat misinformation in the context of
increasing public engagement with and access to validated research sources.
With 21,854 surveys sent, targeting 40,402 papers mentioned in Wikipedia, a
total of 750 complete surveys from 60 countries were included in this analysis.
In general, responses revealed a positive sentiment towards research citation
in Wikipedia and the researcher engagement practices. However, our sub analysis
revealed statistically significant differences when comparison articles vs
books and across disciplines, but not open vs closed access. This study will
open the door to further research and deepen our understanding of authors
perceived trustworthiness of the representation of their research in Wikipedia.","['Michael Taylor', 'Carlos Areia', 'Kath Burton', 'Charles Watkinson']",0,0.52981645
"In recent years, speech generation technology has advanced rapidly, fueled by
generative models and large-scale training techniques. While these developments
have enabled the production of high-quality synthetic speech, they have also
raised concerns about the misuse of this technology, particularly for
generating synthetic misinformation. Current research primarily focuses on
distinguishing machine-generated speech from human-produced speech, but the
more urgent challenge is detecting misinformation within spoken content. This
task requires a thorough analysis of factors such as speaker identity, topic,
and synthesis. To address this need, we conduct an initial investigation into
synthetic spoken misinformation detection by introducing an open-source
dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers
across five common topics, utilizing state-of-the-art text-to-speech systems.
Although our results show promising detection capabilities, they also reveal
substantial challenges for practical implementation, underscoring the
importance of ongoing research in this critical area.","['Peizhuo Liu', 'Li Wang', 'Renqiang He', 'Haorui He', 'Lei Wang', 'Huadi Zheng', 'Jie Shi', 'Tong Xiao', 'Zhizheng Wu']",11,0.6807031
"Real-time deepfake, a type of generative AI, is capable of ""creating""
non-existing contents (e.g., swapping one's face with another) in a video. It
has been, very unfortunately, misused to produce deepfake videos (during web
conferences, video calls, and identity authentication) for malicious purposes,
including financial scams and political misinformation. Deepfake detection, as
the countermeasure against deepfake, has attracted considerable attention from
the academic community, yet existing works typically rely on learning passive
features that may perform poorly beyond seen datasets. In this paper, we
propose SFake, a new real-time deepfake detection method that innovatively
exploits deepfake models' inability to adapt to physical interference.
Specifically, SFake actively sends probes to trigger mechanical vibrations on
the smartphone, resulting in the controllable feature on the footage.
Consequently, SFake determines whether the face is swapped by deepfake based on
the consistency of the facial area with the probe pattern. We implement SFake,
evaluate its effectiveness on a self-built dataset, and compare it with six
other detection methods. The results show that SFake outperforms other
detection methods with higher detection accuracy, faster process speed, and
lower memory consumption.","['Zhixin Xie', 'Jun Luo']",11,0.8200859
"Despite being an integral tool for finding health-related information online,
YouTube has faced criticism for disseminating COVID-19 misinformation globally
to its users. Yet, prior audit studies have predominantly investigated YouTube
within the Global North contexts, often overlooking the Global South. To
address this gap, we conducted a comprehensive 10-day geolocation-based audit
on YouTube to compare the prevalence of COVID-19 misinformation in search
results between the United States (US) and South Africa (SA), the countries
heavily affected by the pandemic in the Global North and the Global South,
respectively. For each country, we selected 3 geolocations and placed
sock-puppets, or bots emulating ""real"" users, that collected search results for
48 search queries sorted by 4 search filters for 10 days, yielding a dataset of
915K results. We found that 31.55% of the top-10 search results contained
COVID-19 misinformation. Among the top-10 search results, bots in SA faced
significantly more misinformative search results than their US counterparts.
Overall, our study highlights the contrasting algorithmic behaviors of YouTube
search between two countries, underscoring the need for the platform to
regulate algorithmic behavior consistently across different regions of the
Globe.","['Hayoung Jung', 'Prerna Juneja', 'Tanushree Mitra']",5,0.5571615
"This paper addresses the critical challenge of building consumer trust in
AI-powered customer engagement by emphasising the necessity for transparency
and accountability. Despite the potential of AI to revolutionise business
operations and enhance customer experiences, widespread concerns about
misinformation and the opacity of AI decision-making processes hinder trust.
Surveys highlight a significant lack of awareness among consumers regarding
their interactions with AI, alongside apprehensions about bias and fairness in
AI algorithms. The paper advocates for the development of explainable AI models
that are transparent and understandable to both consumers and organisational
leaders, thereby mitigating potential biases and ensuring ethical use. It
underscores the importance of organisational commitment to transparency
practices beyond mere regulatory compliance, including fostering a culture of
accountability, prioritising clear data policies and maintaining active
engagement with stakeholders. By adopting a holistic approach to transparency
and explainability, businesses can cultivate trust in AI technologies, bridging
the gap between technological innovation and consumer acceptance, and paving
the way for more ethical and effective AI-powered customer engagements.
KEYWORDS: artificial intelligence (AI), transparency",['Tara DeZao'],9,0.7126843
"Displaying community fact-checks is a promising approach to reduce engagement
with misinformation on social media. However, how users respond to misleading
content emotionally after community fact-checks are displayed on posts is
unclear. Here, we employ quasi-experimental methods to causally analyze changes
in sentiments and (moral) emotions in replies to misleading posts following the
display of community fact-checks. Our evaluation is based on a large-scale
panel dataset comprising N=2,225,260 replies across 1841 source posts from X's
Community Notes platform. We find that informing users about falsehoods through
community fact-checks significantly increases negativity (by 7.3%), anger (by
13.2%), disgust (by 4.7%), and moral outrage (by 16.0%) in the corresponding
replies. These results indicate that users perceive spreading misinformation as
a violation of social norms and that those who spread misinformation should
expect negative reactions once their content is debunked. We derive important
implications for the design of community-based fact-checking systems.","['Yuwei Chuai', 'Anastasia Sergeeva', 'Gabriele Lenzini', 'Nicolas Pr√∂llochs']",3,0.724583
"Community-based fact-checking is a promising approach to verify social media
content and correct misleading posts at scale. Yet, causal evidence regarding
its effectiveness in reducing the spread of misinformation on social media is
missing. Here, we performed a large-scale empirical study to analyze whether
community notes reduce the spread of misleading posts on X. Using a
Difference-in-Differences design and repost time series data for N=237,677
(community fact-checked) cascades that had been reposted more than 431 million
times, we found that exposing users to community notes reduced the spread of
misleading posts by, on average, 62.0%. Furthermore, community notes increased
the odds that users delete their misleading posts by 103.4%. However, our
findings also suggest that community notes might be too slow to intervene in
the early (and most viral) stage of the diffusion. Our work offers important
implications to enhance the effectiveness of community-based fact-checking
approaches on social media.","['Yuwei Chuai', 'Moritz Pilarski', 'Thomas Renault', 'David Restrepo-Amariles', 'Aurore Troussel-Cl√©ment', 'Gabriele Lenzini', 'Nicolas Pr√∂llochs']",3,0.66878414
"The COVID-19 pandemic accelerated the use of preprints, aiding rapid research
dissemination but also facilitating the spread of misinformation. This study
analyzes media coverage of preprints from 2014 to 2023, revealing a significant
post-pandemic decline. Our findings suggest that heightened awareness of the
risks associated with preprints has led to more cautious media practices. While
the decline in preprint coverage may mitigate concerns about premature media
exposure, it also raises questions about the future role of preprints in
science communication, especially during emergencies. Balanced policies based
on up-to-date evidence are needed to address this shift.","['Juan Pablo Alperin', 'Kenneth Shores', 'Alice Fleerackers', 'Natascha Chtena']",5,0.6430807
"Large Language Models (LLMs) demonstrate impressive capabilities across
various fields, yet their increasing use raises critical security concerns.
This article reviews recent literature addressing key issues in LLM security,
with a focus on accuracy, bias, content detection, and vulnerability to
attacks. Issues related to inaccurate or misleading outputs from LLMs is
discussed, with emphasis on the implementation from fact-checking methodologies
to enhance response reliability. Inherent biases within LLMs are critically
examined through diverse evaluation techniques, including controlled input
studies and red teaming exercises. A comprehensive analysis of bias mitigation
strategies is presented, including approaches from pre-processing interventions
to in-training adjustments and post-processing refinements. The article also
probes the complexity of distinguishing LLM-generated content from
human-produced text, introducing detection mechanisms like DetectGPT and
watermarking techniques while noting the limitations of machine learning
enabled classifiers under intricate circumstances. Moreover, LLM
vulnerabilities, including jailbreak attacks and prompt injection exploits, are
analyzed by looking into different case studies and large-scale competitions
like HackAPrompt. This review is concluded by retrospecting defense mechanisms
to safeguard LLMs, accentuating the need for more extensive research into the
LLM security field.","['Benji Peng', 'Keyu Chen', 'Ming Li', 'Pohsun Feng', 'Ziqian Bi', 'Junyu Liu', 'Qian Niu']",6,0.81282985
"In the past decade, social media platforms have been used for information
dissemination and consumption. While a major portion of the content is posted
to promote citizen journalism and public awareness, some content is posted to
mislead users. Among different content types such as text, images, and videos,
memes (text overlaid on images) are particularly prevalent and can serve as
powerful vehicles for propaganda, hate, and humor. In the current literature,
there have been efforts to individually detect such content in memes. However,
the study of their intersection is very limited. In this study, we explore the
intersection between propaganda and hate in memes using a multi-agent LLM-based
approach. We extend the propagandistic meme dataset with coarse and
fine-grained hate labels. Our finding suggests that there is an association
between propaganda and hate in memes. We provide detailed experimental results
that can serve as a baseline for future studies. We will make the experimental
resources publicly available to the community
(https://github.com/firojalam/propaganda-and-hateful-memes).","['Firoj Alam', 'Md. Rafiul Biswas', 'Uzair Shah', 'Wajdi Zaghouani', 'Georgios Mikros']",3,0.5991181
"Large Language Models (LLMs) have revolutionized numerous applications,
making them an integral part of our digital ecosystem. However, their
reliability becomes critical, especially when these models are exposed to
misinformation. We primarily analyze the susceptibility of state-of-the-art
LLMs to factual inaccuracies when they encounter false information in a QnA
scenario, an issue that can lead to a phenomenon we refer to as *knowledge
drift*, which significantly undermines the trustworthiness of these models. We
evaluate the factuality and the uncertainty of the models' responses relying on
Entropy, Perplexity, and Token Probability metrics. Our experiments reveal that
an LLM's uncertainty can increase up to 56.6% when the question is answered
incorrectly due to the exposure to false information. At the same time,
repeated exposure to the same false information can decrease the models
uncertainty again (-52.8% w.r.t. the answers on the untainted prompts),
potentially manipulating the underlying model's beliefs and introducing a drift
from its original knowledge. These findings provide insights into LLMs'
robustness and vulnerability to adversarial inputs, paving the way for
developing more reliable LLM applications across various domains. The code is
available at https://github.com/afastowski/knowledge_drift.","['Alina Fastowski', 'Gjergji Kasneci']",6,0.7990188
"In recent years there have been a growing interest in online auditing of
information flow over social networks with the goal of monitoring undesirable
effects, such as, misinformation and fake news. Most previous work on the
subject, focus on the binary classification problem of classifying information
as fake or genuine. Nonetheless, in many practical scenarios, the
multi-class/label setting is of particular importance. For example, it could be
the case that a social media platform may want to distinguish between ``true"",
``partly-true"", and ``false"" information. Accordingly, in this paper, we
consider the problem of online multiclass classification of information flow.
To that end, driven by empirical studies on information flow over real-world
social media networks, we propose a probabilistic information flow model over
graphs. Then, the learning task is to detect the label of the information flow,
with the goal of minimizing a combination of the classification error and the
detection time. For this problem, we propose two detection algorithms; the
first is based on the well-known multiple sequential probability ratio test,
while the second is a novel graph neural network based sequential decision
algorithm. For both algorithms, we prove several strong statistical guarantees.
We also construct a data driven algorithm for learning the proposed
probabilistic model. Finally, we test our algorithms over two real-world
datasets, and show that they outperform other state-of-the-art misinformation
detection algorithms, in terms of detection time and classification error.","['Daniel Toma', 'Wasim Huleihel']",2,0.7634803
"We study interactions between agents in multi-agent systems, in which the
agents are misinformed with regards to the game that they play, essentially
having a subjective and incorrect understanding of the setting, without being
aware of it. For that, we introduce a new game-theoretic concept, called
misinformation games, that provides the necessary toolkit to study this
situation. Subsequently, we enhance this framework by developing a
time-discrete procedure (called the Adaptation Procedure) that captures
iterative interactions in the above context. During the Adaptation Procedure,
the agents update their information and reassess their behaviour in each step.
We demonstrate our ideas through an implementation, which is used to study the
efficiency and characteristics of the Adaptation Procedure.","['Konstantinos Varsos', 'Merkouris Papamichail', 'Giorgos Flouris', 'Marina Bitsaki']",0,0.5705245
"The rapid advancements in computer graphics have greatly enhanced the quality
of computer-generated images (CGI), making them increasingly indistinguishable
from authentic images captured by digital cameras (ADI). This
indistinguishability poses significant challenges, especially in an era of
widespread misinformation and digitally fabricated content. This research
proposes a novel approach to classify CGI and ADI using Swin Transformers and
preprocessing techniques involving RGB and CbCrY color frame analysis. By
harnessing the capabilities of Swin Transformers, our method foregoes
handcrafted features instead of relying on raw pixel data for model training.
This approach achieves state-of-the-art accuracy while offering substantial
improvements in processing speed and robustness against joint image
manipulations such as noise addition, blurring, and JPEG compression. Our
findings highlight the potential of Swin Transformers combined with advanced
color frame analysis for effective and efficient image authenticity detection.","['Preeti Mehta', 'Aman Sagar', 'Suchi Kumari']",7,0.72203445
"Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui","['Larissa Pusch', 'Tim O. F. Conrad']",1,0.75680983
"This article presents the Political Deepfakes Incidents Database (PDID), a
collection of politically-salient deepfakes, encompassing synthetically-created
videos, images, and less-sophisticated `cheapfakes.' The project is driven by
the rise of generative AI in politics, ongoing policy efforts to address harms,
and the need to connect AI incidents and political communication research. The
database contains political deepfake content, metadata, and researcher-coded
descriptors drawn from political science, public policy, communication, and
misinformation studies. It aims to help reveal the prevalence, trends, and
impact of political deepfakes, such as those featuring major political figures
or events. The PDID can benefit policymakers, researchers, journalists,
fact-checkers, and the public by providing insights into deepfake usage, aiding
in regulation, enabling in-depth analyses, supporting fact-checking and
trust-building efforts, and raising awareness of political deepfakes. It is
suitable for research and application on media effects, political discourse, AI
ethics, technology governance, media literacy, and countermeasures.","['Christina P. Walker', 'Daniel S. Schiff', 'Kaylyn Jackson Schiff']",0,0.6128508
"Fake news detection remains a critical challenge in today's rapidly evolving
digital landscape, where misinformation can spread faster than ever before.
Traditional fake news detection models often rely on static datasets and
auxiliary information, such as metadata or social media interactions, which
limits their adaptability to real-time scenarios. Recent advancements in Large
Language Models (LLMs) have demonstrated significant potential in addressing
these challenges due to their extensive pre-trained knowledge and ability to
analyze textual content without relying on auxiliary data. However, many of
these LLM-based approaches are still rooted in static datasets, with limited
exploration into their real-time processing capabilities. This paper presents a
systematic evaluation of both traditional offline models and state-of-the-art
LLMs for real-time fake news detection. We demonstrate the limitations of
existing offline models, including their inability to adapt to dynamic
misinformation patterns. Furthermore, we show that newer LLM models with online
capabilities, such as GPT-4, Claude, and Gemini, are better suited for
detecting emerging fake news in real-time contexts. Our findings emphasize the
importance of transitioning from offline to online LLM models for real-time
fake news detection. Additionally, the public accessibility of LLMs enhances
their scalability and democratizes the tools needed to combat misinformation.
By leveraging real-time data, our work marks a significant step toward more
adaptive, effective, and scalable fake news detection systems.","['Ruoyu Xu', 'Gaoxiang Li']",6,0.75917965
"This paper presents the experiments and results for the CheckThat! Lab at
CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial
Examples (InCrediblAE). The primary objective of this task was to generate
adversarial examples in five problem domains in order to evaluate the
robustness of widely used text classification methods (fine-tuned BERT, BiLSTM,
and RoBERTa) when applied to credibility assessment issues.
  This study explores the application of ensemble learning to enhance
adversarial attacks on natural language processing (NLP) models. We
systematically tested and refined several adversarial attack methods, including
BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across
various misinformation tasks. By developing modified versions of BERT-Attack
and hybrid methods, we achieved significant improvements in attack
effectiveness. Our results demonstrate the potential of modification and
combining multiple methods to create more sophisticated and effective
adversarial attack strategies, contributing to the development of more robust
and secure systems.","['W≈Çodzimierz Lewoniewski', 'Piotr Stolarski', 'Milena Str√≥≈ºyna', 'Elzbieta Lewa≈Ñska', 'Aleksandra Wojewoda', 'Ewelina Ksiƒô≈ºniak', 'Marcin Sawi≈Ñski']",1,0.6444379
"Open Source Intelligence (OSINT) refers to intelligence efforts based on
freely available data. It has become a frequent topic of conversation on social
media, where private users or networks can share their findings. Such data is
highly valuable in conflicts, both for gaining a new understanding of the
situation as well as for tracking the spread of misinformation. In this paper,
we present a method for collecting such data as well as a novel OSINT dataset
for the Russo-Ukrainian war drawn from Twitter between January 2022 and July
2023. It is based on an initial search of users posting OSINT and a subsequent
snowballing approach to detect more. The final dataset contains almost 2
million Tweets posted by 1040 users. We also provide some first analyses and
experiments on the data, and make suggestions for its future usage.","['Johannes Niu', 'Mila Stillman', 'Philipp Seeberger', 'Anna Kruspe']",13,0.59678113
"Leveraging recent advances in wireless communication, IoT, and AI,
intelligent transportation systems (ITS) played an important role in reducing
traffic congestion and enhancing user experience. Within ITS, navigational
recommendation systems (NRS) are essential for helping users simplify route
choices in urban environments. However, NRS are vulnerable to information-based
attacks that can manipulate both the NRS and users to achieve the objectives of
the malicious entities. This study aims to assess the risks of misinformed
demand attacks, where attackers use techniques like Sybil-based attacks to
manipulate the demands of certain origins and destinations considered by the
NRS. We propose a game-theoretic framework for proactive risk assessment of
demand attacks (PRADA) and treat the interaction between attackers and the NRS
as a Stackelberg game. The attacker is the leader who conveys misinformed
demands, while the NRS is the follower responding to the provided information.
Specifically, we consider the case of local-targeted attacks, in which the
attacker aims to make the NRS recommend the authentic users towards a specific
road that favors certain groups. Our analysis unveils the equivalence between
users' incentive compatibility and Wardrop equilibrium recommendations and
shows that the NRS and its users are at high risk when encountering intelligent
attackers who can significantly alter user routes by strategically fabricating
non-existent demands. To mitigate these risks, we introduce a trust mechanism
that leverages users' confidence in the integrity of the NRS, and show that it
can effectively reduce the impact of misinformed demand attacks. Numerical
experiments are used to corroborate the results and demonstrate a Resilience
Paradox, where locally targeted attacks can sometimes benefit the overall
traffic conditions.","['Ya-Ting Yang', 'Haozhe Lei', 'Quanyan Zhu']",9,0.58510673
"The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,
leading to widespread discrimination against individuals of Chinese descent.
Large language models (LLMs) are pre-trained deep learning models used for
natural language processing (NLP) tasks. The ability of LLMs to understand and
generate human-like text makes them particularly useful for analysing social
media data to detect and evaluate sentiments. We present a sentiment analysis
framework utilising LLMs for longitudinal sentiment analysis of the Sinophobic
sentiments expressed in X (Twitter) during the COVID-19 pandemic. The results
show a significant correlation between the spikes in Sinophobic tweets,
Sinophobic sentiments and surges in COVID-19 cases, revealing that the
evolution of the pandemic influenced public sentiment and the prevalence of
Sinophobic discourse. Furthermore, the sentiment analysis revealed a
predominant presence of negative sentiments, such as annoyance and denial,
which underscores the impact of political narratives and misinformation shaping
public opinion. The lack of empathetic sentiment which was present in previous
studies related to COVID-19 highlights the way the political narratives in
media viewed the pandemic and how it blamed the Chinese community. Our study
highlights the importance of transparent communication in mitigating xenophobic
sentiments during global crises.","['Chen Wang', 'Rohitash Chandra']",5,0.68090963
"Misinformation poses a significant challenge studied extensively by
researchers, yet acquiring data to identify primary sharers is costly and
challenging. To address this, we propose a low-barrier approach to
differentiate social media users who are more likely to share misinformation
from those who are less likely. Leveraging insights from previous studies, we
demonstrate that easy-access online social network metrics -- average daily
tweet count, and account age -- can be leveraged to help identify potential low
factuality content spreaders on X (previously known as Twitter). We find that
higher tweet frequency is positively associated with low factuality in shared
content, while account age is negatively associated with it. We also find that
some of the effects, namely the effect of the number of accounts followed and
the number of tweets produced, differ depending on the number of followers a
user has. Our findings show that relying on these easy-access social network
metrics could serve as a low-barrier approach for initial identification of
users who are more likely to spread misinformation, and therefore contribute to
combating misinformation effectively on social media platforms.","['J√∫lia Sz√°mely', 'Alessandro Galeazzi', 'J√∫lia Koltai', 'Elisa Omodei']",3,0.7320049
"Recent advances in AI-powered image editing tools have significantly lowered
the barrier to image modification, raising pressing security concerns those
related to spreading misinformation and disinformation on social platforms.
Image provenance analysis is crucial in this context, as it identifies relevant
images within a database and constructs a relationship graph by mining hidden
manipulation and transformation cues, thereby providing concrete evidence
chains. This paper introduces a novel end-to-end deep learning framework
designed to explore the structural information of provenance graphs. Our
proposed method distinguishes from previous approaches in two main ways. First,
unlike earlier methods that rely on prior knowledge and have limited
generalizability, our framework relies upon a patch attention mechanism to
capture image provenance clues for local manipulations and global
transformations, thereby enhancing graph construction performance. Second,
while previous methods primarily focus on identifying tampering traces only
between image pairs, they often overlook the hidden information embedded in the
topology of the provenance graph. Our approach aligns the model training
objectives with the final graph construction task, incorporating the overall
structural information of the graph into the training process. We integrate
graph structure information with the attention mechanism, enabling precise
determination of the direction of transformation. Experimental results show the
superiority of the proposed method over previous approaches, underscoring its
effectiveness in addressing the challenges of image provenance analysis.","['Keyang Zhang', 'Chenqi Kong', 'Shiqi Wang', 'Anderson Rocha', 'Haoliang Li']",7,0.68517923
"The rapid dissemination of misinformation on the internet complicates the
decision-making process for individuals seeking reliable information,
particularly parents researching child development topics. This misinformation
can lead to adverse consequences, such as inappropriate treatment of children
based on myths. While previous research has utilized text-mining techniques to
predict child abuse cases, there has been a gap in the analysis of child
development myths and facts. This study addresses this gap by applying text
mining techniques and classification models to distinguish between myths and
facts about child development, leveraging newly gathered data from publicly
available websites. The research methodology involved several stages. First,
text mining techniques were employed to pre-process the data, ensuring enhanced
accuracy. Subsequently, the structured data was analysed using six robust
Machine Learning (ML) classifiers and one Deep Learning (DL) model, with two
feature extraction techniques applied to assess their performance across three
different training-testing splits. To ensure the reliability of the results,
cross-validation was performed using both k-fold and leave-one-out methods.
Among the classification models tested, Logistic Regression (LR) demonstrated
the highest accuracy, achieving a 90% accuracy with the Bag-of-Words (BoW)
feature extraction technique. LR stands out for its exceptional speed and
efficiency, maintaining low testing time per statement (0.97 microseconds).
These findings suggest that LR, when combined with BoW, is effective in
accurately classifying child development information, thus providing a valuable
tool for combating misinformation and assisting parents in making informed
decisions.","['Mehedi Tajrian', 'Azizur Rahman', 'Muhammad Ashad Kabir', 'Md Rafiqul Islam']",1,0.67891765
"Health-related misinformation claims often falsely cite a credible biomedical
publication as evidence, which superficially appears to support the false
claim. The publication does not really support the claim, but a reader could
believe it thanks to the use of logical fallacies. Here, we aim to detect and
to highlight such fallacies, which requires carefully assessing the exact
content of the misrepresented publications. To achieve this, we introduce
MissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus
builds on Missci by grounding the applied fallacies in real-world passages from
misrepresented studies. This creates a realistic test-bed for detecting and
verbalizing these fallacies under real-world input conditions, and enables
novel passage-retrieval tasks. MissciPlus is the first logical fallacy dataset
which pairs the real-world misrepresented evidence with incorrect claims,
identical to the input to evidence-based fact-checking models. With MissciPlus,
we i) benchmark retrieval models in identifying passages that support claims
only when fallacies are applied, ii) evaluate how well LLMs articulate
fallacious reasoning from misrepresented scientific passages, and iii) assess
the effectiveness of fact-checking models in refuting claims that misrepresent
biomedical research. Our findings show that current fact-checking models
struggle to use relevant passages from misrepresented publications to refute
misinformation. Moreover, these passages can mislead LLMs into accepting false
claims as true.","['Max Glockner', 'Yufang Hou', 'Preslav Nakov', 'Iryna Gurevych']",1,0.6405897
"With the rapid development of deepfake technology, especially the deep audio
fake technology, misinformation detection on the social media scene meets a
great challenge. Social media data often contains multimodal information which
includes audio, video, text, and images. However, existing multimodal
misinformation detection methods tend to focus only on some of these
modalities, failing to comprehensively address information from all modalities.
To comprehensively address the various modal information that may appear on
social media, this paper constructs a comprehensive multimodal misinformation
detection framework. By employing corresponding neural network encoders for
each modality, the framework can fuse different modality information and
support the multimodal misinformation detection task. Based on the constructed
framework, this paper explores the importance of the audio modality in
multimodal misinformation detection tasks on social media. By adjusting the
architecture of the acoustic encoder, the effectiveness of different acoustic
feature encoders in the multimodal misinformation detection tasks is
investigated. Furthermore, this paper discovers that audio and video
information must be carefully aligned, otherwise the misalignment across
different audio and video modalities can severely impair the model performance.","['Moyang Liu', 'Yukun Liu', 'Ruibo Fu', 'Zhengqi Wen', 'Jianhua Tao', 'Xuefei Liu', 'Guanjun Li']",11,0.6633221
"Automated fact-checking is a key strategy to overcome the spread of COVID-19
misinformation on the internet. These systems typically leverage deep learning
approaches through Natural Language Inference (NLI) to verify the truthfulness
of information based on supporting evidence. However, one challenge that arises
in deep learning is performance stagnation due to a lack of knowledge during
training. This study proposes using a Knowledge Graph (KG) as external
knowledge to enhance NLI performance for automated COVID-19 fact-checking in
the Indonesian language. The proposed model architecture comprises three
modules: a fact module, an NLI module, and a classifier module. The fact module
processes information from the KG, while the NLI module handles semantic
relationships between the given premise and hypothesis. The representation
vectors from both modules are concatenated and fed into the classifier module
to produce the final result. The model was trained using the generated
Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.
Our study demonstrates that incorporating KGs can significantly improve NLI
performance in fact-checking, achieving the best accuracy of 0,8616. This
suggests that KGs are a valuable component for enhancing NLI performance in
automated fact-checking.","['Arief Purnama Muharram', 'Ayu Purwarianti']",1,0.7093652
"Large language models (LLMs) have achieved impressive advancements across
numerous disciplines, yet the critical issue of knowledge conflicts, a major
source of hallucinations, has rarely been studied. Only a few research explored
the conflicts between the inherent knowledge of LLMs and the retrieved
contextual knowledge. However, a thorough assessment of knowledge conflict in
LLMs is still missing. Motivated by this research gap, we present ConflictBank,
the first comprehensive benchmark developed to systematically evaluate
knowledge conflicts from three aspects: (i) conflicts encountered in retrieved
knowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the
interplay between these conflict forms. Our investigation delves into four
model families and twelve LLM instances, meticulously analyzing conflicts
stemming from misinformation, temporal discrepancies, and semantic divergences.
Based on our proposed novel construction framework, we create 7,453,853
claim-evidence pairs and 553,117 QA pairs. We present numerous findings on
model scale, conflict causes, and conflict types. We hope our ConflictBank
benchmark will help the community better understand model behavior in conflicts
and develop more reliable LLMs.","['Zhaochen Su', 'Jun Zhang', 'Xiaoye Qu', 'Tong Zhu', 'Yanshu Li', 'Jiashuo Sun', 'Juntao Li', 'Min Zhang', 'Yu Cheng']",6,0.6206932
"Given the widespread dissemination of misinformation on social media,
implementing fact-checking mechanisms for online claims is essential. Manually
verifying every claim is very challenging, underscoring the need for an
automated fact-checking system. This paper presents our system designed to
address this issue. We utilize the Averitec dataset (Schlichtkrull et al.,
2023) to assess the performance of our fact-checking system. In addition to
veracity prediction, our system provides supporting evidence, which is
extracted from the dataset. We develop a Retrieve and Generate (RAG) pipeline
to extract relevant evidence sentences from a knowledge base, which are then
inputted along with the claim into a large language model (LLM) for
classification. We also evaluate the few-shot In-Context Learning (ICL)
capabilities of multiple LLMs. Our system achieves an 'Averitec' score of 0.33,
which is a 22% absolute improvement over the baseline. Our Code is publicly
available on
https://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms.","['Ronit Singhal', 'Pransh Patwa', 'Parth Patwa', 'Aman Chadha', 'Amitava Das']",1,0.8008039
"Background: Online toxicity, encompassing behaviors such as harassment,
bullying, hate speech, and the dissemination of misinformation, has become a
pressing social concern in the digital age. The 2022 Mpox outbreak, initially
termed ""Monkeypox"" but subsequently renamed to mitigate associated stigmas and
societal concerns, serves as a poignant backdrop to this issue. Objective: In
this research, we undertake a comprehensive analysis of the toxic online
discourse surrounding the 2022 Mpox outbreak. Our objective is to dissect its
origins, characterize its nature and content, trace its dissemination patterns,
and assess its broader societal implications, with the goal of providing
insights that can inform strategies to mitigate such toxicity in future crises.
Methods: We collected more than 1.6 million unique tweets and analyzed them
from five dimensions, including context, extent, content, speaker, and intent.
Utilizing BERT-based topic modeling and social network community clustering, we
delineated the toxic dynamics on Twitter. Results: We identified five
high-level topic categories in the toxic online discourse on Twitter, including
disease (46.6%), health policy and healthcare (19.3%), homophobia (23.9%),
politics (6.0%), and racism (4.1%). Through the toxicity diffusion networks of
mentions, retweets, and the top users, we found that retweets of toxic content
were widespread, while influential users rarely engaged with or countered this
toxicity through retweets. Conclusions: By tracking topical dynamics, we can
track the changing popularity of toxic content online, providing a better
understanding of societal challenges. Network dynamics spotlight key social
media influencers and their intents, indicating that addressing these central
figures in toxic discourse can enhance crisis communication and inform
policy-making.","['Lizhou Fan', 'Lingyao Li', 'Libby Hemphill']",5,0.7084168
"This paper investigates how generative AI can potentially undermine the
integrity of collective knowledge and the processes we rely on to acquire,
assess, and trust information, posing a significant threat to our knowledge
ecosystem and democratic discourse. Grounded in social and political
philosophy, we introduce the concept of \emph{generative algorithmic epistemic
injustice}. We identify four key dimensions of this phenomenon: amplified and
manipulative testimonial injustice, along with hermeneutical ignorance and
access injustice. We illustrate each dimension with real-world examples that
reveal how generative AI can produce or amplify misinformation, perpetuate
representational harm, and create epistemic inequities, particularly in
multilingual contexts. By highlighting these injustices, we aim to inform the
development of epistemically just generative AI systems, proposing strategies
for resistance, system design principles, and two approaches that leverage
generative AI to foster a more equitable information ecosystem, thereby
safeguarding democratic values and the integrity of knowledge production.","['Jackie Kay', 'Atoosa Kasirzadeh', 'Shakir Mohamed']",9,0.752614
"In the era dominated by information overload and its facilitation with Large
Language Models (LLMs), the prevalence of misinformation poses a significant
threat to public discourse and societal well-being. A critical concern at
present involves the identification of machine-generated news. In this work, we
take a significant step by introducing a benchmark dataset designed for neural
news detection in four languages: English, Turkish, Hungarian, and Persian. The
dataset incorporates outputs from multiple multilingual generators (in both,
zero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and
GPT-4. Next, we experiment with a variety of classifiers, ranging from those
based on linguistic features to advanced Transformer-based models and LLMs
prompting. We present the detection results aiming to delve into the
interpretablity and robustness of machine-generated texts detectors across all
target languages.","['Cem √úy√ºk', 'Danica Rov√≥', 'Shaghayegh Kolli', 'Rabia Varol', 'Georg Groh', 'Daryna Dementieva']",8,0.84746265
"To assist human fact-checkers, researchers have developed automated
approaches for visual misinformation detection. These methods assign veracity
scores by identifying inconsistencies between the image and its caption, or by
detecting forgeries in the image. However, they neglect a crucial point of the
human fact-checking process: identifying the original meta-context of the
image. By explaining what is actually true about the image, fact-checkers can
better detect misinformation, focus their efforts on check-worthy visual
content, engage in counter-messaging before misinformation spreads widely, and
make their explanation more convincing. Here, we fill this gap by introducing
the task of automated image contextualization. We create 5Pils, a dataset of
1,676 fact-checked images with question-answer pairs about their original
meta-context. Annotations are based on the 5 Pillars fact-checking framework.
We implement a first baseline that grounds the image in its original
meta-context using the content of the image and textual evidence retrieved from
the open web. Our experiments show promising results while highlighting several
open challenges in retrieval and reasoning. We make our code and data publicly
available.","['Jonathan Tonglet', 'Marie-Francine Moens', 'Iryna Gurevych']",7,0.638146
"Information spreads faster through social media platforms than traditional
media, thus becoming an ideal medium to spread misinformation. Meanwhile,
automated accounts, known as social bots, contribute more to the misinformation
dissemination. In this paper, we explore the interplay between social bots and
misinformation on the Sina Weibo platform. We propose a comprehensive and
large-scale misinformation dataset, containing 11,393 misinformation and 16,416
unbiased real information with multiple modality information, with 952,955
related users. We propose a scalable weak-surprised method to annotate social
bots, obtaining 68,040 social bots and 411,635 genuine accounts. To the best of
our knowledge, this dataset is the largest dataset containing misinformation
and social bots. We conduct comprehensive experiments and analysis on this
dataset. Results show that social bots play a central role in misinformation
dissemination, participating in news discussions to amplify echo chambers,
manipulate public sentiment, and reverse public stances.","['Herun Wan', 'Minnan Luo', 'Zihan Ma', 'Guang Dai', 'Xiang Zhao']",13,0.7512821
"Accurate attribution of authorship is crucial for maintaining the integrity
of digital content, improving forensic investigations, and mitigating the risks
of misinformation and plagiarism. Addressing the imperative need for proper
authorship attribution is essential to uphold the credibility and
accountability of authentic authorship. The rapid advancements of Large
Language Models (LLMs) have blurred the lines between human and machine
authorship, posing significant challenges for traditional methods. We presents
a comprehensive literature review that examines the latest research on
authorship attribution in the era of LLMs. This survey systematically explores
the landscape of this field by categorizing four representative problems: (1)
Human-written Text Attribution; (2) LLM-generated Text Detection; (3)
LLM-generated Text Attribution; and (4) Human-LLM Co-authored Text Attribution.
We also discuss the challenges related to ensuring the generalization and
explainability of authorship attribution methods. Generalization requires the
ability to generalize across various domains, while explainability emphasizes
providing transparent and understandable insights into the decisions made by
these models. By evaluating the strengths and limitations of existing methods
and benchmarks, we identify key open problems and future research directions in
this field. This literature review serves a roadmap for researchers and
practitioners interested in understanding the state of the art in this rapidly
evolving field. Additional resources and a curated list of papers are available
and regularly updated at https://llm-authorship.github.io","['Baixiang Huang', 'Canyu Chen', 'Kai Shu']",1,0.68729806
"The landscape of social media content has evolved significantly, extending
from text to multimodal formats. This evolution presents a significant
challenge in combating misinformation. Previous research has primarily focused
on single modalities or text-image combinations, leaving a gap in detecting
multimodal misinformation. While the concept of entity consistency holds
promise in detecting multimodal misinformation, simplifying the representation
to a scalar value overlooks the inherent complexities of high-dimensional
representations across different modalities. To address these limitations, we
propose a Multimedia Misinformation Detection (MultiMD) framework for detecting
misinformation from video content by leveraging cross-modal entity consistency.
The proposed dual learning approach allows for not only enhancing
misinformation detection performance but also improving representation learning
of entity consistency across different modalities. Our results demonstrate that
MultiMD outperforms state-of-the-art baseline models and underscore the
importance of each modality in misinformation detection. Our research provides
novel methodological and technical insights into multimodal misinformation
detection.","['Zhe Fu', 'Kanlun Wang', 'Wangjiaxuan Xin', 'Lina Zhou', 'Shi Chen', 'Yaorong Ge', 'Daniel Janies', 'Dongsong Zhang']",2,0.6659258
"This paper develops an agent-based automated fact-checking approach for
detecting misinformation. We demonstrate that combining a powerful LLM agent,
which does not have access to the internet for searches, with an online web
search agent yields better results than when each tool is used independently.
Our approach is robust across multiple models, outperforming alternatives and
increasing the macro F1 of misinformation detection by as much as 20 percent
compared to LLMs without search. We also conduct extensive analyses on the
sources our system leverages and their biases, decisions in the construction of
the system like the search tool and the knowledge base, the type of evidence
needed and its impact on the results, and other parts of the overall process.
By combining strong performance with in-depth understanding, we hope to provide
building blocks for future search-enabled misinformation mitigation systems.","['Jacob-Junqi Tian', 'Hao Yu', 'Yury Orlovskiy', 'Tyler Vergho', 'Mauricio Rivera', 'Mayank Goel', 'Zachary Yang', 'Jean-Francois Godbout', 'Reihaneh Rabbany', 'Kellin Pelrine']",1,0.7161573
"Disaster events often unfold rapidly, necessitating a swift and effective
response. Developing action plans, resource allocation, and resolution of help
requests in disaster scenarios is time-consuming and complex since
disaster-relevant information is often uncertain. Leveraging real-time data can
significantly deal with data uncertainty and enhance disaster response efforts.
To deal with real-time data uncertainty, social media appeared as an
alternative effective source of real-time data as there has been extensive use
of social media during and after the disasters. However, it also brings forth
challenges regarding trustworthiness and bias in these data. To fully leverage
social media data for disaster management, it becomes crucial to mitigate
biases that may arise due to specific disaster types or regional contexts.
Additionally, the presence of misinformation within social media data raises
concerns about the reliability of data sources, potentially impeding actionable
insights and leading to improper resource utilization. To overcome these
challenges, our research aimed to investigate how to ensure trustworthiness and
address biases in social media data. We aim to investigate and identify the
factors that can be used to enhance trustworthiness and minimize bias to make
an efficient and scalable disaster management system utilizing real-time social
media posts, identify disaster-related keywords, and assess the severity of the
disaster. By doing so, the integration of real-time social data can improve the
speed and accuracy of disaster management systems","['Samia Abid', 'Bhupesh Kumar Mishra', 'Dhavalkumar Thakker', 'Nishikant Mishra']",0,0.6050154
"Misinformation - false or misleading information - is considered a
significant societal concern due to its associated ""misinformation effects,""
such as political polarization, erosion of trust in institutions, problematic
behavior, and public health challenges. However, the prevailing concept is
misaligned with what is studied. While misinformation focuses on instances of
information about factual matters, the broad spectrum of effects often
manifests at a societal level and is shaped by a wide range of interdependent
factors such as identity, values, opinions, epistemologies, and disagreements.
Unsurprisingly, misinformation effects can occur without the prevalence of
misinformation, and misinformation does not necessarily increase the effects
studied. Here, we propose using disagreement - conflicting attitudes and
beliefs between individuals and communities - as a way to study misinformation
effects because it addresses the identified conceptual limitations of
misinformation. Furthermore, unlike misinformation, disagreement does not
require researchers to determine whether a given information is false or
misleading. Thus, it can be studied and, more importantly, measured without the
need to make a normative judgment about a given information, even when the
specific topic is entirely removed, as we show in a longitudinal disagreement
measurement. We demonstrate that disagreement, as a holistic concept, provides
better explanations for the occurrence of misinformation effects, enhances
precision in developing appropriate interventions, and offers a promising
approach for evaluating them through quantification. Finally, we show how
disagreement addresses current misinformation research questions and conclude
with recommendations for research practice.","['Damian Hodel', 'Jevin West']",0,0.77806544
"Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.",['Tobias A. Opsahl'],1,0.8313203
"The advances in the digital era have led to rapid dissemination of
information. This has also aggravated the spread of misinformation and
disinformation. This has potentially serious consequences, such as civil
unrest. While fact-checking aims to combat this, manual fact-checking is
cumbersome and not scalable. While automated fact-checking approaches exist,
they do not operate in real-time and do not always account for spread of
misinformation through different modalities. This is particularly important as
proactive fact-checking on live streams in real-time can help people be
informed of false narratives and prevent catastrophic consequences that may
cause civil unrest. This is particularly relevant with the rapid dissemination
of information through video on social media platforms or other streams like
political rallies and debates. Hence, in this work we develop a platform named
LiveFC, that can aid in fact-checking live audio streams in real-time. LiveFC
has a user-friendly interface that displays the claims detected along with
their veracity and evidence for live streams with associated speakers for
claims from respective segments. The app can be accessed at
http://livefc.factiverse.ai and a screen recording of the demo can be found at
https://bit.ly/3WVAoIw.","['Venktesh V', 'Vinay Setty']",3,0.59154665
"The risks posed by Artificial Intelligence (AI) are of considerable concern
to academics, auditors, policymakers, AI companies, and the public. However, a
lack of shared understanding of AI risks can impede our ability to
comprehensively discuss, research, and react to them. This paper addresses this
gap by creating an AI Risk Repository to serve as a common frame of reference.
This comprises a living database of 777 risks extracted from 43 taxonomies,
which can be filtered based on two overarching taxonomies and easily accessed,
modified, and updated via our website and online spreadsheets. We construct our
Repository with a systematic review of taxonomies and other structured
classifications of AI risk followed by an expert consultation. We develop our
taxonomies of AI risk using a best-fit framework synthesis. Our high-level
Causal Taxonomy of AI Risks classifies each risk by its causal factors (1)
Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)
Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI
Risks classifies risks into seven AI risk domains: (1) Discrimination &
toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &
misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and
(7) AI system safety, failures, & limitations. These are further divided into
23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt
to rigorously curate, analyze, and extract AI risk frameworks into a publicly
accessible, comprehensive, extensible, and categorized risk database. This
creates a foundation for a more coordinated, coherent, and complete approach to
defining, auditing, and managing the risks posed by AI systems.","['Peter Slattery', 'Alexander K. Saeri', 'Emily A. C. Grundy', 'Jess Graham', 'Michael Noetel', 'Risto Uuk', 'James Dao', 'Soroush Pour', 'Stephen Casper', 'Neil Thompson']",9,0.7695716
"Training data sets intended for unsupervised anomaly detection, typically
presumed to be anomaly-free, often contain anomalies (or contamination), a
challenge that significantly undermines model performance. Most robust
unsupervised anomaly detection models rely on contamination ratio information
to tackle contamination. However, in reality, contamination ratio may be
inaccurate. We investigate on the impact of inaccurate contamination ratio
information in robust unsupervised anomaly detection. We verify whether they
are resilient to misinformed contamination ratios. Our investigation on 6
benchmark data sets reveals that such models are not adversely affected by
exposure to misinformation. In fact, they can exhibit improved performance when
provided with such inaccurate contamination ratios.","['Jordan F. Masakuna', 'DJeff Kanda Nkashama', 'Arian Soltani', 'Marc Frappier', 'Pierre-Martin Tardif', 'Froduald Kabanza']",1,0.5113372
"Social bots-automated accounts that generate and spread content on social
media-are exploiting vulnerabilities in these platforms to manipulate public
perception and disseminate disinformation. This has prompted the development of
public bot detection services; however, most of these services focus primarily
on Twitter, leaving niche platforms vulnerable. Fringe social media platforms
such as Parler, Gab, and Gettr often have minimal moderation, which facilitates
the spread of hate speech and misinformation. To address this gap, we introduce
Entendre, an open-access, scalable, and platform-agnostic bot detection
framework. Entendre can process a labeled dataset from any social platform to
produce a tailored bot detection model using a random forest classification
approach, ensuring robust social bot detection. We exploit the idea that most
social platforms share a generic template, where users can post content,
approve content, and provide a bio (common data features). By emphasizing
general data features over platform-specific ones, Entendre offers rapid
extensibility at the expense of some accuracy. To demonstrate Entendre's
effectiveness, we used it to explore the presence of bots among accounts
posting racist content on the now-defunct right-wing platform Parler. We
examined 233,000 posts from 38,379 unique users and found that 1,916 unique
users (4.99%) exhibited bot-like behavior. Visualization techniques further
revealed that these bots significantly impacted the network, amplifying
influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These
preliminary findings underscore the need for tools like Entendre to monitor and
assess bot activity across diverse platforms.","['Pranav Venkatesh', 'Kami Vinton', 'Dhiraj Murthy', 'Kellen Sharp', 'Akaash Kolluri']",13,0.8519844
"Retrieval augmented generation (RAG) is a process where a large language
model (LLM) retrieves useful information from a database and then generates the
responses. It is becoming popular in enterprise settings for daily business
operations. For example, Copilot for Microsoft 365 has accumulated millions of
businesses. However, the security implications of adopting such RAG-based
systems are unclear.
  In this paper, we introduce ConfusedPilot, a class of security
vulnerabilities of RAG systems that confuse Copilot and cause integrity and
confidentiality violations in its responses. First, we investigate a
vulnerability that embeds malicious text in the modified prompt in RAG,
corrupting the responses generated by the LLM. Second, we demonstrate a
vulnerability that leaks secret data, which leverages the caching mechanism
during retrieval. Third, we investigate how both vulnerabilities can be
exploited to propagate misinformation within the enterprise and ultimately
impact its operations, such as sales and manufacturing. We also discuss the
root cause of these attacks by investigating the architecture of a RAG-based
system. This study highlights the security vulnerabilities in today's RAG-based
systems and proposes design guidelines to secure future RAG-based systems.","['Ayush RoyChowdhury', 'Mulong Luo', 'Prateek Sahu', 'Sarbartha Banerjee', 'Mohit Tiwari']",1,0.58224297
"The proliferation of misleading visualizations online, particularly during
critical events like public health crises and elections, poses a significant
risk. This study investigates the capability of GPT-4 models (4V, 4o, and 4o
mini) to detect misleading visualizations. Utilizing a dataset of
tweet-visualization pairs containing various visual misleaders, we test these
models under four experimental conditions with different levels of guidance. We
show that GPT-4 models can detect misleading visualizations with moderate
accuracy without prior training (naive zero-shot) and that performance notably
improves when provided with definitions of misleaders (guided zero-shot).
However, a single prompt engineering technique does not yield the best results
for all misleader types. Specifically, providing the models with misleader
definitions and examples (guided few-shot) proves more effective for reasoning
misleaders, while guided zero-shot performs better for design misleaders. This
study underscores the feasibility of using large vision-language models to
detect visual misinformation and the importance of prompt engineering for
optimized detection accuracy.","['Jason Alexander', 'Priyal Nanda', 'Kai-Cheng Yang', 'Ali Sarvghad']",7,0.56324244
"Misinformation spreads rapidly on social media, causing serious damage by
influencing public opinion, promoting dangerous behavior, or eroding trust in
reliable sources. It spreads too fast for traditional fact-checking, stressing
the need for predictive methods. We introduce CROWDSHIELD, a crowd
intelligence-based method for early misinformation prediction. We hypothesize
that the crowd's reactions to misinformation reveal its accuracy. Furthermore,
we hinge upon exaggerated assertions/claims and replies with particular
positions/stances on the source post within a conversation thread. We employ
Q-learning to capture the two dimensions -- stances and claims. We utilize deep
Q-learning due to its proficiency in navigating complex decision spaces and
effectively learning network properties. Additionally, we use a
transformer-based encoder to develop a comprehensive understanding of both
content and context. This multifaceted approach helps ensure the model pays
attention to user interaction and stays anchored in the communication's
content. We propose MIST, a manually annotated misinformation detection Twitter
corpus comprising nearly 200 conversation threads with more than 14K replies.
In experiments, CROWDSHIELD outperformed ten baseline systems, achieving an
improvement of ~4% macro-F1 score. We conduct an ablation study and error
analysis to validate our proposed model's performance. The source code and
dataset are available at https://github.com/LCS2-IIITD/CrowdShield.git.","['Megha Sundriyal', 'Harshit Choudhary', 'Tanmoy Chakraborty', 'Md Shad Akhtar']",1,0.7607221
"Recently, federated learning (FL) has achieved wide successes for diverse
privacy-sensitive applications without sacrificing the sensitive private
information of clients. However, the data quality of client datasets can not be
guaranteed since corresponding annotations of different clients often contain
complex label noise of varying degrees, which inevitably causes the performance
degradation. Intuitively, the performance degradation is dominated by clients
with higher noise rates since their trained models contain more misinformation
from data, thus it is necessary to devise an effective optimization scheme to
mitigate the negative impacts of these noisy clients. In this work, we propose
a two-stage framework FedELC to tackle this complicated label noise issue. The
first stage aims to guide the detection of noisy clients with higher label
noise, while the second stage aims to correct the labels of noisy clients' data
via an end-to-end label correction framework which is achieved by learning
possible ground-truth labels of noisy clients' datasets via back propagation.
We implement sixteen related methods and evaluate five datasets with three
types of complicated label noise scenarios for a comprehensive comparison.
Extensive experimental results demonstrate our proposed framework achieves
superior performance than its counterparts for different scenarios.
Additionally, we effectively improve the data quality of detected noisy
clients' local datasets with our label correction framework. The code is
available at https://github.com/Sprinter1999/FedELC.","['Xuefeng Jiang', 'Sheng Sun', 'Jia Li', 'Jingjing Xue', 'Runhan Li', 'Zhiyuan Wu', 'Gang Xu', 'Yuwei Wang', 'Min Liu']",1,0.5539964
"Despite the remarkable performance of Large Language Models (LLMs) in natural
language processing tasks, they still struggle with generating logically sound
arguments, resulting in potential risks such as spreading misinformation. To
address this issue, we introduce FIPO, a fallacy-informed framework that
leverages preference optimization methods to steer LLMs toward logically sound
arguments. FIPO includes a classification loss, to capture the fine-grained
information on fallacy types. Our results on argumentation datasets show that
our method reduces the fallacy errors by up to 17.5%. Furthermore, our human
evaluation results indicate that the quality of the generated arguments by our
method significantly outperforms the fine-tuned baselines, as well as other
preference optimization methods, such as DPO. These findings highlight the
importance of ensuring models are aware of logical fallacies for effective
argument generation. Our code is available at
github.com/lucamouchel/Logical-Fallacies.","['Luca Mouchel', 'Debjit Paul', 'Shaobo Cui', 'Robert West', 'Antoine Bosselut', 'Boi Faltings']",6,0.6769242
"Calls to use open generative language models in academic research have
highlighted the need for reproducibility and transparency in scientific
research. However, the impact of generative AI extends well beyond academia, as
corporations and public interest organizations have begun integrating these
models into their data science pipelines. We expand this lens to include the
impact of open models on organizations, focusing specifically on fact-checking
organizations, which use AI to observe and analyze large volumes of circulating
misinformation, yet must also ensure the reproducibility and impartiality of
their work. We wanted to understand where fact-checking organizations use open
models in their data science pipelines; what motivates their use of open models
or proprietary models; and how their use of open or proprietary models can
inform research on the societal impact of generative AI. To answer these
questions, we conducted an interview study with N=24 professionals at 20
fact-checking organizations on six continents. Based on these interviews, we
offer a five-component conceptual model of where fact-checking organizations
employ generative AI to support or automate parts of their data science
pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data
Delivery, and Data Sharing. We then provide taxonomies of fact-checking
organizations' motivations for using open models and the limitations that
prevent them for further adopting open models, finding that they prefer open
models for Organizational Autonomy, Data Privacy and Ownership, Application
Specificity, and Capability Transparency. However, they nonetheless use
proprietary models due to perceived advantages in Performance, Usability, and
Safety, as well as Opportunity Costs related to participation in emerging
generative AI ecosystems. Our work provides novel perspective on open models in
data-driven organizations.","['Robert Wolfe', 'Tanushree Mitra']",1,0.64627635
"With the explosive growth of the Coronavirus Pandemic (COVID-19),
misinformation on social media has developed into a global phenomenon with
widespread and detrimental societal effects. Despite recent progress and
efforts in detecting COVID-19 misinformation on social media networks, this
task remains challenging due to the complexity, diversity, multi-modality, and
high costs of fact-checking or annotation. In this research, we introduce a
systematic and multidisciplinary agent-based modeling approach to limit the
spread of COVID-19 misinformation and interpret the dynamic actions of users
and communities in evolutionary online (or offline) social media networks. Our
model was applied to a Twitter network associated with an armed protest
demonstration against the COVID-19 lockdown in Michigan state in May, 2020. We
implemented a one-median problem to categorize the Twitter network into six key
communities (nodes) and identified information exchange (links) within the
network. We measured the response time to COVID-19 misinformation spread in the
network and employed a cybernetic organizational method to monitor the Twitter
network. The overall misinformation mitigation strategy was evaluated, and
agents were allocated to interact with the network based on the measured
response time and feedback. The proposed model prioritized the communities
based on the agents response times at the operational level. It then optimized
agent allocation to limit the spread of COVID19 related misinformation from
different communities, improved the information diffusion delay threshold to up
to 3 minutes, and ultimately enhanced the mitigation process to reduce
misinformation spread across the entire network.","['Mustafa Alassad', 'Nitin Agarwal']",5,0.70539093
"Large Language Models (LLMs) have made significant advances in natural
language processing, but their underlying mechanisms are often misunderstood.
Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely
on statistical patterns in word embeddings rather than true cognitive
processes. This leads to vulnerabilities such as ""hallucination"" and
misinformation. The paper argues that current LLM architectures are inherently
untrustworthy due to their reliance on correlations of sequential patterns of
word embedding vectors. However, ongoing research into combining generative
transformer-based models with fact bases and logic programming languages may
lead to the development of trustworthy LLMs capable of generating statements
based on given truth and explaining their self-reasoning process.","['Bo Zhou', 'Daniel Gei√üler', 'Paul Lukowicz']",6,0.768987
"Introduction: This article introduces DisTrack, a methodology and a tool
developed for tracking and analyzing misinformation within Online Social
Networks (OSNs). DisTrack is designed to combat the spread of misinformation
through a combination of Natural Language Processing (NLP) Social Network
Analysis (SNA) and graph visualization. The primary goal is to detect
misinformation, track its propagation, identify its sources, and assess the
influence of various actors within the network.
  Methods: DisTrack's architecture incorporates a variety of methodologies
including keyword search, semantic similarity assessments, and graph generation
techniques. These methods collectively facilitate the monitoring of
misinformation, the categorization of content based on alignment with known
false claims, and the visualization of dissemination cascades through detailed
graphs. The tool is tailored to capture and analyze the dynamic nature of
misinformation spread in digital environments.
  Results: The effectiveness of DisTrack is demonstrated through three case
studies focused on different themes: discredit/hate speech, anti-vaccine
misinformation, and false narratives about the Russia-Ukraine conflict. These
studies show DisTrack's capabilities in distinguishing posts that propagate
falsehoods from those that counteract them, and tracing the evolution of
misinformation from its inception.
  Conclusions: The research confirms that DisTrack is a valuable tool in the
field of misinformation analysis. It effectively distinguishes between
different types of misinformation and traces their development over time. By
providing a comprehensive approach to understanding and combating
misinformation in digital spaces, DisTrack proves to be an essential asset for
researchers and practitioners working to mitigate the impact of false
information in online social environments.","['Guillermo Villar-Rodr√≠guez', '√Ålvaro Huertas-Garc√≠a', 'Alejandro Mart√≠n', 'Javier Huertas-Tato', 'David Camacho']",0,0.7452061
"Advances in generative AI, the proliferation of large multimodal models
(LMMs), and democratized open access to these technologies have direct
implications for the production and diffusion of misinformation. In this
prequel, we address tackling misinformation in the unique and increasingly
popular context of podcasts. The rise of podcasts as a popular medium for
disseminating information across diverse topics necessitates a proactive
strategy to combat the spread of misinformation. Inspired by the proven
effectiveness of \textit{auditory alerts} in contexts like collision alerts for
drivers and error pings in mobile phones, our work envisions the application of
auditory alerts as an effective tool to tackle misinformation in podcasts. We
propose the integration of suitable auditory alerts to notify listeners of
potential misinformation within the podcasts they are listening to, in
real-time and without hampering listening experiences. We identify several
opportunities and challenges in this path and aim to provoke novel
conversations around instruments, methods, and measures to tackle
misinformation in podcasts.","['Sachin Pathiyan Cherumanal', 'Ujwal Gadiraju', 'Damiano Spina']",0,0.64227396
"This research investigates the extent of misinformation in certain
journalistic articles by introducing a novel measurement tool to assess the
degrees of falsity. It aims to measure misinformation using two metrics
(concealment and overstatement) to explore how information is interpreted as
false. This should help examine how articles containing partly true and partly
false information can potentially harm readers, as they are more challenging to
identify than completely fabricated information. In this study, the full story
provided by the fact-checking website serves as a standardized source of
information for comparing differences between fake and real news. The result
suggests that false news has greater concealment and overstatement, due to
longer and more complex new stories being shortened and ambiguously phrased.
While there are no major distinctions among categories of politics science and
civics, it demonstrates that misinformation lacks crucial details while
simultaneously containing more redundant words. Hence, news articles containing
partial falsity, categorized as misinformation, can deceive inattentive readers
who lack background knowledge. Hopefully, this approach instigates future
fact-checkers, journalists, and the readers to secure high quality articles for
a resilient information environment.","['Jiyoung Lee', 'Keeheon Lee']",4,0.79284716
"The rapid advancements of generative AI have fueled the potential of
generative text image editing while simultaneously escalating the threat of
misinformation spreading. However, existing forensics methods struggle to
detect unseen forgery types that they have not been trained on, leaving the
development of a model capable of generalized detection of tampered scene text
as an unresolved issue. To tackle this, we propose a novel task: open-set
tampered scene text detection, which evaluates forensics models on their
ability to identify both seen and previously unseen forgery types. We have
curated a comprehensive, high-quality dataset, featuring the texts tampered by
eight text editing models, to thoroughly assess the open-set generalization
capabilities. Further, we introduce a novel and effective pre-training paradigm
that subtly alters the texture of selected texts within an image and trains the
model to identify these regions. This approach not only mitigates the scarcity
of high-quality training data but also enhances models' fine-grained perception
and open-set generalization abilities. Additionally, we present DAF, a novel
framework that improves open-set generalization by distinguishing between the
features of authentic and tampered text, rather than focusing solely on the
tampered text's features. Our extensive experiments validate the remarkable
efficacy of our methods. For example, our zero-shot performance can even beat
the previous state-of-the-art full-shot model by a large margin. Our dataset
and code will be open-source.","['Chenfan Qu', 'Yiwu Zhong', 'Fengjun Guo', 'Lianwen Jin']",7,0.76326907
"Advanced Artificial Intelligence (AI) systems, specifically large language
models (LLMs), have the capability to generate not just misinformation, but
also deceptive explanations that can justify and propagate false information
and erode trust in the truth. We examined the impact of deceptive AI generated
explanations on individuals' beliefs in a pre-registered online experiment with
23,840 observations from 1,192 participants. We found that in addition to being
more persuasive than accurate and honest explanations, AI-generated deceptive
explanations can significantly amplify belief in false news headlines and
undermine true ones as compared to AI systems that simply classify the headline
incorrectly as being true/false. Moreover, our results show that personal
factors such as cognitive reflection and trust in AI do not necessarily protect
individuals from these effects caused by deceptive AI generated explanations.
Instead, our results show that the logical validity of AI generated deceptive
explanations, that is whether the explanation has a causal effect on the
truthfulness of the AI's classification, plays a critical role in countering
their persuasiveness - with logically invalid explanations being deemed less
credible. This underscores the importance of teaching logical reasoning and
critical thinking skills to identify logically invalid arguments, fostering
greater resilience against advanced AI-driven misinformation.","['Valdemar Danry', 'Pat Pataranutaporn', 'Matthew Groh', 'Ziv Epstein', 'Pattie Maes']",9,0.69423115
"In the rapidly evolving landscape of artificial intelligence, generative
models such as Generative Adversarial Networks (GANs) and Diffusion Models have
become cornerstone technologies, driving innovation in diverse fields from art
creation to healthcare. Despite their potential, these models face the
significant challenge of data memorization, which poses risks to privacy and
the integrity of generated content. Among various metrics of memorization
detection, our study delves into the memorization scores calculated from
encoder layer embeddings, which involves measuring distances between samples in
the embedding spaces. Particularly, we find that the memorization scores
calculated from layer embeddings of Vision Transformers (ViTs) show an notable
trend - the latter (deeper) the layer, the less the memorization measured. It
has been found that the memorization scores from the early layers' embeddings
are more sensitive to low-level memorization (e.g. colors and simple patterns
for an image), while those from the latter layers are more sensitive to
high-level memorization (e.g. semantic meaning of an image). We also observe
that, for a specific model architecture, its degree of memorization on
different levels of information is unique. It can be viewed as an inherent
property of the architecture. Building upon this insight, we introduce a unique
fingerprinting methodology. This method capitalizes on the unique distributions
of the memorization score across different layers of ViTs, providing a novel
approach to identifying models involved in generating deepfakes and malicious
content. Our approach demonstrates a marked 30% enhancement in identification
accuracy over existing baseline methods, offering a more effective tool for
combating digital misinformation.","['Jack He', 'Jianxing Zhao', 'Andrew Bai', 'Cho-Jui Hsieh']",7,0.73658323
"Images are a powerful and immediate vehicle to carry misleading or outright
false messages, yet identifying image-based misinformation at scale poses
unique challenges. In this paper, we present PIXELMOD, a system that leverages
perceptual hashes, vector databases, and optical character recognition (OCR) to
efficiently identify images that are candidates to receive soft moderation
labels on Twitter. We show that PIXELMOD outperforms existing image similarity
approaches when applied to soft moderation, with negligible performance
overhead. We then test PIXELMOD on a dataset of tweets surrounding the 2020 US
Presidential Election, and find that it is able to identify visually misleading
images that are candidates for soft moderation with 0.99% false detection and
2.06% false negatives.","['Pujan Paudel', 'Chen Ling', 'Jeremy Blackburn', 'Gianluca Stringhini']",7,0.6487982
"Knowledge editing has been increasingly adopted to correct the false or
outdated knowledge in Large Language Models (LLMs). Meanwhile, one critical but
under-explored question is: can knowledge editing be used to inject harm into
LLMs? In this paper, we propose to reformulate knowledge editing as a new type
of safety threat for LLMs, namely Editing Attack, and conduct a systematic
investigation with a newly constructed dataset EditAttack. Specifically, we
focus on two typical safety risks of Editing Attack including Misinformation
Injection and Bias Injection. For the risk of misinformation injection, we
first categorize it into commonsense misinformation injection and long-tail
misinformation injection. Then, we find that editing attacks can inject both
types of misinformation into LLMs, and the effectiveness is particularly high
for commonsense misinformation injection. For the risk of bias injection, we
discover that not only can biased sentences be injected into LLMs with high
effectiveness, but also one single biased sentence injection can cause a bias
increase in general outputs of LLMs, which are even highly irrelevant to the
injected sentence, indicating a catastrophic impact on the overall fairness of
LLMs. Then, we further illustrate the high stealthiness of editing attacks,
measured by their impact on the general knowledge and reasoning capacities of
LLMs, and show the hardness of defending editing attacks with empirical
evidence. Our discoveries demonstrate the emerging misuse risks of knowledge
editing techniques on compromising the safety alignment of LLMs and the
feasibility of disseminating misinformation or bias with LLMs as new channels.","['Canyu Chen', 'Baixiang Huang', 'Zekun Li', 'Zhaorun Chen', 'Shiyang Lai', 'Xiongxiao Xu', 'Jia-Chen Gu', 'Jindong Gu', 'Huaxiu Yao', 'Chaowei Xiao', 'Xifeng Yan', 'William Yang Wang', 'Philip Torr', 'Dawn Song', 'Kai Shu']",6,0.73746884
"Generative models, such as diffusion models (DMs), variational autoencoders
(VAEs), and generative adversarial networks (GANs), produce images with a level
of authenticity that makes them nearly indistinguishable from real photos and
artwork. While this capability is beneficial for many industries, the
difficulty of identifying synthetic images leaves online media platforms
vulnerable to impersonation and misinformation attempts. To support the
development of defensive methods, we introduce ImagiNet, a high-resolution and
balanced dataset for synthetic image detection, designed to mitigate potential
biases in existing resources. It contains 200K examples, spanning four content
categories: photos, paintings, faces, and uncategorized. Synthetic images are
produced with open-source and proprietary generators, whereas real counterparts
of the same content type are collected from public datasets. The structure of
ImagiNet allows for a two-track evaluation system: i) classification as real or
synthetic and ii) identification of the generative model. To establish a
baseline, we train a ResNet-50 model using a self-supervised contrastive
objective (SelfCon) for each track. The model demonstrates state-of-the-art
performance and high inference speed across established benchmarks, achieving
an AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%, even under
social network conditions that involve compression and resizing. Our data and
code are available at https://github.com/delyan-boychev/imaginet.","['Delyan Boychev', 'Radostin Cholakov']",7,0.8045262
"Various social media platforms, e.g., Twitter and Reddit, allow people to
disseminate a plethora of information more efficiently and conveniently.
However, they are inevitably full of misinformation, causing damage to diverse
aspects of our daily lives. To reduce the negative impact, timely
identification of misinformation, namely Misinformation Detection (MD), has
become an active research topic receiving widespread attention. As a complex
phenomenon, the veracity of an article is influenced by various aspects. In
this paper, we are inspired by the opposition of intents between misinformation
and real information. Accordingly, we propose to reason the intent of articles
and form the corresponding intent features to promote the veracity
discrimination of article features. To achieve this, we build a hierarchy of a
set of intents for both misinformation and real information by referring to the
existing psychological theories, and we apply it to reason the intent of
articles by progressively generating binary answers with an encoder-decoder
structure. We form the corresponding intent features and integrate it with the
token features to achieve more discriminative article features for MD. Upon
these ideas, we suggest a novel MD method, namely Detecting Misinformation by
Integrating Intent featuRes (DM-INTER). To evaluate the performance of
DM-INTER, we conduct extensive experiments on benchmark MD datasets. The
experimental results validate that DM-INTER can outperform the existing
baseline MD methods.","['Bing Wang', 'Ximing Li', 'Changchun Li', 'Bo Fu', 'Songwen Pei', 'Shengsheng Wang']",0,0.72043324
"Nowadays, misinformation is widely spreading over various social media
platforms and causes extremely negative impacts on society. To combat this
issue, automatically identifying misinformation, especially those containing
multimodal content, has attracted growing attention from the academic and
industrial communities, and induced an active research topic named Multimodal
Misinformation Detection (MMD). Typically, existing MMD methods capture the
semantic correlation and inconsistency between multiple modalities, but neglect
some potential clues in multimodal content. Recent studies suggest that
manipulated traces of the images in articles are non-trivial clues for
detecting misinformation. Meanwhile, we find that the underlying intentions
behind the manipulation, e.g., harmful and harmless, also matter in MMD.
Accordingly, in this work, we propose to detect misinformation by learning
manipulation features that indicate whether the image has been manipulated, as
well as intention features regarding the harmful and harmless intentions of the
manipulation. Unfortunately, the manipulation and intention labels that make
these features discriminative are unknown. To overcome the problem, we propose
two weakly supervised signals as alternatives by introducing additional
datasets on image manipulation detection and formulating two classification
tasks as positive and unlabeled learning problems. Based on these ideas, we
propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD
(HAMI-M3D). Extensive experiments across three benchmark datasets can
demonstrate that HAMI-M3D can consistently improve the performance of any MMD
baselines.","['Bing Wang', 'Shengsheng Wang', 'Changchun Li', 'Renchu Guan', 'Ximing Li']",7,0.6731556
"Understanding the spread of online rumors is a pressing societal challenge
and an active area of research across domains. In the context of the 2022 U.S.
midterm elections, one influential social media platform for sharing
information -- including rumors that may be false, misleading, or
unsubstantiated -- was Twitter (now renamed X). To increase understanding of
the dynamics of online rumors about elections, we present and analyze a dataset
of 1.81 million Twitter posts corresponding to 135 distinct rumors which spread
online during the midterm election season (September 5 to December 1, 2022). We
describe how this data was collected, compiled, and supplemented, and provide a
series of exploratory analyses along with comparisons to a previously-published
dataset on 2020 election rumors. We also conduct a mixed-methods analysis of
three distinct rumors about the election in Arizona, a particularly prominent
focus of 2022 election rumoring. Finally, we provide a set of potential future
directions for how this dataset could be used to facilitate future research
into online rumors, misinformation, and disinformation.","['Joseph S Schafer', 'Kayla Duskin', 'Stephen Prochaska', 'Morgan Wack', 'Anna Beers', 'Lia Bozarth', 'Taylor Agajanian', 'Mike Caulfield', 'Emma S Spiro', 'Kate Starbird']",10,0.6588825
"The following contribution introduces a concept that employs Large Language
Models (LLMs) and a chatbot interface to enhance SPARQL query generation for
ontologies, thereby facilitating intuitive access to formalized knowledge.
Utilizing natural language inputs, the system converts user inquiries into
accurate SPARQL queries that strictly query the factual content of the
ontology, effectively preventing misinformation or fabrication by the LLM. To
enhance the quality and precision of outcomes, additional textual information
from established domain-specific standards is integrated into the ontology for
precise descriptions of its concepts and relationships. An experimental study
assesses the accuracy of generated SPARQL queries, revealing significant
benefits of using LLMs for querying ontologies and highlighting areas for
future research.","['Jonathan Reif', 'Tom Jeleniewski', 'Milapji Singh Gill', 'Felix Gehlhoff', 'Alexander Fay']",6,0.6735185
"Deception plays a crucial role in strategic interactions with incomplete
information. Motivated by security applications, we study a class of two-player
turn-based deterministic games with one-sided incomplete information, in which
player 1 (P1) aims to prevent player 2 (P2) from reaching a set of target
states. In addition to actions, P1 can place two kinds of deception resources:
""traps"" and ""fake targets"" to disinform P2 about the transition dynamics and
payoff of the game. Traps ""hide the real"" by making trap states appear normal,
while fake targets ""reveal the fiction"" by advertising non-target states as
targets. We are interested in jointly synthesizing optimal decoy placement and
deceptive defense strategies for P1 that exploits P2's misinformation. We
introduce a novel hypergame on graph model and two solution concepts: stealthy
deceptive sure winning and stealthy deceptive almost-sure winning. These
identify states from which P1 can prevent P2 from reaching the target in a
finite number of steps or with probability one without allowing P2 to become
aware that it is being deceived. Consequently, determining the optimal decoy
placement corresponds to maximizing the size of P1's deceptive winning region.
Considering the combinatorial complexity of exploring all decoy allocations, we
utilize compositional synthesis concepts to show that the objective function
for decoy placement is monotone, non-decreasing, and, in certain cases, sub- or
super-modular. This leads to a greedy algorithm for decoy placement, achieving
a $(1 - 1/e)$-approximation when the objective function is sub- or
super-modular. The proposed hypergame model and solution concepts contribute to
understanding the optimal deception resource allocation and deception
strategies in various security applications.","['Abhishek N. Kulkarni', 'Matthew S. Cohen', 'Charles A. Kamhoua', 'Jie Fu']",2,0.57000005
"The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for misinformation
detection and fact checking. Recent advances on large language models (LLMs)
have shown remarkable performance in various tasks, but whether and how LLMs
could help with misinformation detection remains relatively underexplored. Most
of existing state-of-the-art approaches either do not consider evidence and
solely focus on claim related features or assume the evidence to be provided.
Few approaches consider evidence retrieval as part of the misinformation
detection but rely on fine-tuning models. In this paper, we investigate the
potential of LLMs for misinformation detection in a zero-shot setting. We
incorporate an evidence retrieval component into the process as it is crucial
to gather pertinent information from various sources to detect the veracity of
claims. To this end, we propose a novel re-ranking approach for multimodal
evidence retrieval using both LLMs and large vision-language models (LVLM). The
retrieved evidence samples (images and texts) serve as the input for an
LVLM-based approach for multimodal fact verification (LVLM4FV). To enable a
fair evaluation, we address the issue of incomplete ground truth for evidence
samples in an existing evidence retrieval dataset by annotating a more complete
set of evidence samples for both image and text retrieval. Our experimental
results on two datasets demonstrate the superiority of the proposed approach in
both evidence retrieval and fact verification tasks and also better
generalization capability across dataset compared to the supervised baseline.","['Sahar Tahmasebi', 'Eric M√ºller-Budack', 'Ralph Ewerth']",1,0.80977
"The increasing reliance on Large Language Models (LLMs) for health
information seeking can pose severe risks due to the potential for
misinformation and the complexity of these topics. This paper introduces
KNOWNET a visualization system that integrates LLMs with Knowledge Graphs (KG)
to provide enhanced accuracy and structured exploration. Specifically, for
enhanced accuracy, KNOWNET extracts triples (e.g., entities and their
relations) from LLM outputs and maps them into the validated information and
supported evidence in external KGs. For structured exploration, KNOWNET
provides next-step recommendations based on the neighborhood of the currently
explored entities in KGs, aiming to guide a comprehensive understanding without
overlooking critical aspects. To enable reasoning with both the structured data
in KGs and the unstructured outputs from LLMs, KNOWNET conceptualizes the
understanding of a subject as the gradual construction of graph visualization.
A progressive graph visualization is introduced to monitor past inquiries, and
bridge the current query with the exploration history and next-step
recommendations. We demonstrate the effectiveness of our system via use cases
and expert interviews.","['Youfu Yan', 'Yu Hou', 'Yongkang Xiao', 'Rui Zhang', 'Qianwen Wang']",6,0.64273095
"Out-of-context (OOC) misinformation poses a significant challenge in
multimodal fact-checking, where images are paired with texts that misrepresent
their original context to support false narratives. Recent research in
evidence-based OOC detection has seen a trend towards increasingly complex
architectures, incorporating Transformers, foundation models, and large
language models. In this study, we introduce a simple yet robust baseline,
which assesses MUltimodal SimilaritiEs (MUSE), specifically the similarity
between image-text pairs and external image and text evidence. Our results
demonstrate that MUSE, when used with conventional classifiers like Decision
Tree, Random Forest, and Multilayer Perceptron, can compete with and even
surpass the state-of-the-art on the NewsCLIPpings and VERITE datasets.
Furthermore, integrating MUSE in our proposed ""Attentive Intermediate
Transformer Representations"" (AITR) significantly improved performance, by 3.3%
and 7.5% on NewsCLIPpings and VERITE, respectively. Nevertheless, the success
of MUSE, relying on surface-level patterns and shortcuts, without examining
factuality and logical inconsistencies, raises critical questions about how we
define the task, construct datasets, collect external evidence and overall, how
we assess progress in the field. We release our code at:
https://github.com/stevejpapad/outcontext-misinfo-progress","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Symeon Papadopoulos', 'Panagiotis C. Petrantonakis']",1,0.7009106
"The framing of events in various media and discourse spaces is crucial in the
era of misinformation and polarization. Many studies, however, are limited to
specific media or networks, disregarding the importance of cross-platform
diffusion. This study overcomes that limitation by conducting a multi-platform
framing analysis on Twitter, YouTube, and traditional media analyzing the 2019
Koran burning in Kristiansand, Norway. It examines media and policy frames and
uncovers network connections through shared URLs. The findings show that online
news emphasizes the incident's legality, while social media focuses on its
morality, with harsh hate speech prevalent in YouTube comments. Additionally,
YouTube is identified as the most self-contained community, whereas Twitter is
the most open to external inputs.","['Anna-Katharina Jung', 'Gautam Kishore Shahi', 'Jennifer Fromm', 'Kari Anne R√∏ysland', 'Kim Henrik Gronert']",10,0.70926106
"Understanding the dynamics of language toxicity on social media is important
for us to investigate the propagation of misinformation and the development of
echo chambers for political scenarios such as U.S. presidential elections.
Recent research has used large-scale data to investigate the dynamics across
social media platforms. However, research on the toxicity dynamics is not
enough. This study aims to provide a first exploration of the potential
language toxicity flow among Left, Right and Center users. Specifically, we aim
to examine whether Left users were easier to be attacked by language toxicity.
In this study, more than 500M Twitter posts were examined. It was discovered
that Left users received much more toxic replies than Right and Center users.",['Wentao Xu'],3,0.7082853
"The proliferation of misinformation and disinformation on social media
networks has become increasingly concerning. With a significant portion of the
population using social media on a regular basis, there are growing efforts by
malicious organizations to manipulate public opinion through coordinated
campaigns. Current methods for identifying coordinated user accounts typically
rely on either similarities in user behaviour, latent coordination in activity
traces, or classification techniques. In our study, we propose a framework
based on the hypothesis that coordinated users will demonstrate abnormal growth
in their behavioural patterns over time relative to the wider population.
Specifically, we utilize the EPClose algorithm to extract contrasting patterns
of user behaviour during a time window of malicious activity, which we then
compare to a historical time window. We evaluated the effectiveness of our
approach using real-world data, and our results show a minimum increase of 10%
in the F1 score compared to existing approaches.","['Isura Manchanayaka', 'Zainab Zaidi', 'Shanika Karunasekera', 'Christopher Leckie']",2,0.6826602
"The rise of social media has been accompanied by a dark side with the ease of
creating fake accounts and disseminating misinformation through coordinated
attacks. Existing methods to identify such attacks often rely on thematic
similarities or network-based approaches, overlooking the intricate causal
relationships that underlie coordinated actions. This work introduces a novel
approach for detecting coordinated attacks using Convergent Cross Mapping
(CCM), a technique that infers causality from temporal relationships between
user activity. We build on the theoretical framework of CCM by incorporating
topic modelling as a basis for further optimizing its performance. We apply CCM
to real-world data from the infamous IRA attack on US elections, achieving F1
scores up to 75.3% in identifying coordinated accounts. Furthermore, we analyse
the output of our model to identify the most influential users in a community.
We apply our model to a case study involving COVID-19 anti-vax related
discussions on Twitter. Our results demonstrate the effectiveness of our model
in uncovering causal structures of coordinated behaviour, offering a promising
avenue for mitigating the threat of malicious campaigns on social media
platforms.","['Isura Manchanayaka', 'Zainab Razia Zaidi', 'Shanika Karunasekera', 'Christopher Leckie']",2,0.6883059
"In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.","['Adela Ljajiƒá', 'Milo≈° Ko≈°prdiƒá', 'Bojana Ba≈°aragin', 'Darija Medvecki', 'Lorenzo Cassano', 'Nikola Milo≈°eviƒá']",1,0.82241565
"The interplay between humans and Generative AI (Gen AI) draws an insightful
parallel with the dynamic relationship between giraffes and acacias on the
African Savannah. Just as giraffes navigate the acacia's thorny defenses to
gain nourishment, humans engage with Gen AI, maneuvering through ethical and
operational challenges to harness its benefits. This paper explores how, like
young giraffes that are still mastering their environment, humans are in the
early stages of adapting to and shaping Gen AI. It delves into the strategies
humans are developing and refining to help mitigate risks such as bias,
misinformation, and privacy breaches, that influence and shape Gen AI's
evolution. While the giraffe-acacia analogy aptly frames human-AI relations, it
contrasts nature's evolutionary perfection with the inherent flaws of
human-made technology and the tendency of humans to misuse it, giving rise to
many ethical dilemmas. Through the HHH framework we identify pathways to embed
values of helpfulness, honesty, and harmlessness in AI development, fostering
safety-aligned agents that resonate with human values. This narrative presents
a cautiously optimistic view of human resilience and adaptability, illustrating
our capacity to harness technologies and implement safeguards effectively,
without succumbing to their perils. It emphasises a symbiotic relationship
where humans and AI continually shape each other for mutual benefit.",['Waqar Hussain'],9,0.7267848
"Online platforms like Reddit, Wikipedia, and Facebook are integral to modern
life, enabling content creation and sharing through posts, comments, and
discussions. Despite their virtual and often anonymous nature, these platforms
need rules and oversight to maintain a safe and productive environment. As
these communities grow, a key question arises: how does the need for regulatory
functions scale? Do larger groups require more regulatory actions and oversight
per person, or can they manage with less? Our analysis of Reddit's regulatory
functions reveals robust scaling relationships across different subreddits,
suggesting universal patterns between community size and the amount of
regulation needed. We found that the number of comments and moderator actions,
such as comment removals, grew faster than the community size, with superlinear
exponents of 1.12 and 1.18, respectively. However, bot-based rule enforcement
did not keep pace with community growth, exhibiting a slightly sublinear
exponent of 0.95. Further analysis of the residuals from these scaling
behaviors identified a 'trade-off axis,' where one-way coordination mechanisms
(bots and moderators) counteract two-way interactions (comments) and vice
versa. Our findings suggest that a more proactive moderation approach,
characterized by increased bot activity and moderator comment removals, tends
to result in less user engagement under the scaling framework. Understanding
these natural scaling patterns and interactions can help platform
administrators and policymakers foster healthy online communities while
mitigating harmful behaviors such as harassment, doxxing, and misinformation.
Without proper regulation, these negative behaviors can proliferate and cause
significant damage. Targeted interventions based on these insights are key to
ensuring online platforms remain safe and beneficial spaces.","['Shambhobi Bhattacharya', 'Jisung Yoon', 'Hyejin Youn']",3,0.6094111
"There has been rampant propagation of fake news and misinformation videos on
many platforms lately, and moderation of such content faces many challenges
that must be overcome. Recent research has shown the feasibility of identifying
video titles from encrypted network traffic within a single platform, for
example, within YouTube or Facebook. However, there are no existing methods for
cross-platform video recognition, a crucial gap that this works aims to
address. Encrypted video traffic classification within a single platform, that
is, classifying the video title of a traffic trace of a video on one platform
by training on traffic traces of videos on the same platform, has significant
limitations due to the large number of video platforms available to users to
upload harmful content to. To attempt to address this limitation, we conduct a
feasibility analysis into and attempt to solve the challenge of recognizing
videos across multiple platforms by using the traffic traces of videos on one
platform only. We propose TripletViNet, a framework that encompasses i)
platform-wise pre-processing, ii) an encoder trained utilizing triplet learning
for improved accuracy and iii) multiclass classifier for classifying the video
title of a traffic trace. To evaluate the performance of TripletViNet, a
comprehensive dataset with traffic traces for 100 videos on six major platforms
with the potential for spreading misinformation such as YouTube, X, Instagram,
Facebook, Rumble, and Tumblr was collected and used to test TripletViNet in
both closed-set and open-set scenarios. TripletViNet achieves significant
improvements in accuracy due to the correlation between video traffic and the
video's VBR, with impressive final accuracies exceeding 90% in certain
scenarios.","['Petar Smolovic', 'Thilini Dahanayaka', 'Kanchana Thilakarathna']",11,0.5407574
"Deep generative models have demonstrated impressive performance in various
computer vision applications, including image synthesis, video generation, and
medical analysis. Despite their significant advancements, these models may be
used for malicious purposes, such as misinformation, deception, and copyright
violation. In this paper, we provide a systematic and timely review of research
efforts on defenses against AI-generated visual media, covering detection,
disruption, and authentication. We review existing methods and summarize the
mainstream defense-related tasks within a unified passive and proactive
framework. Moreover, we survey the derivative tasks concerning the
trustworthiness of defenses, such as their robustness and fairness. For each
task, we formulate its general pipeline and propose a taxonomy based on
methodological strategies that are uniformly applicable to the primary
subtasks. Additionally, we summarize the commonly used evaluation datasets,
criteria, and metrics. Finally, by analyzing the reviewed studies, we provide
insights into current research challenges and suggest possible directions for
future research.","['Jingyi Deng', 'Chenhao Lin', 'Zhengyu Zhao', 'Shuai Liu', 'Qian Wang', 'Chao Shen']",1,0.6980468
"The fabrication of visual misinformation on the web and social media has
increased exponentially with the advent of foundational text-to-image diffusion
models. Namely, Stable Diffusion inpainters allow the synthesis of maliciously
inpainted images of personal and private figures, and copyrighted contents,
also known as deepfakes. To combat such generations, a disruption framework,
namely Photoguard, has been proposed, where it adds adversarial noise to the
context image to disrupt their inpainting synthesis. While their framework
suggested a diffusion-friendly approach, the disruption is not sufficiently
strong and it requires a significant amount of GPU and time to immunize the
context image. In our work, we re-examine both the minimal and favorable
conditions for a successful inpainting disruption, proposing DDD, a ""Digression
guided Diffusion Disruption"" framework. First, we identify the most
adversarially vulnerable diffusion timestep range with respect to the hidden
space. Within this scope of noised manifold, we pose the problem as a semantic
digression optimization. We maximize the distance between the inpainting
instance's hidden states and a semantic-aware hidden state centroid, calibrated
both by Monte Carlo sampling of hidden states and a discretely projected
optimization in the token space. Effectively, our approach achieves stronger
disruption and a higher success rate than Photoguard while lowering the GPU
memory requirement, and speeding the optimization up to three times faster.","['Geonho Son', 'Juhun Lee', 'Simon S. Woo']",7,0.7013312
"We introduce a new type of indirect injection attacks against language models
that operate on images: hidden ''meta-instructions'' that influence how the
model interprets the image and steer the model's outputs to express an
adversary-chosen style, sentiment, or point of view.
  We explain how to create meta-instructions by generating images that act as
soft prompts. In contrast to jailbreaking attacks and adversarial examples,
outputs produced in response to these images are plausible and based on the
visual content of the image, yet also satisfy the adversary's (meta-)objective.
  We evaluate the efficacy of meta-instructions for multiple visual language
models and adversarial meta-objectives, and demonstrate how they can ''unlock''
capabilities of the underlying language models that are unavailable via
explicit text instructions. We describe how meta-instruction attacks could
cause harm by enabling creation of malicious, self-interpreting content that
carries spam, misinformation, and spin. Finally, we discuss defenses.","['Tingwei Zhang', 'Collin Zhang', 'John X. Morris', 'Eugene Bagdasarian', 'Vitaly Shmatikov']",7,0.60288626
"The battle against the spread of misinformation on the Internet is a daunting
task faced by modern society. Fake news content is primarily distributed
through digital platforms, with websites dedicated to producing and
disseminating such content playing a pivotal role in this complex ecosystem.
Therefore, these websites are of great interest to misinformation researchers.
However, obtaining a comprehensive list of websites labeled as producers and/or
spreaders of misinformation can be challenging, particularly in developing
countries. In this study, we propose a novel methodology for identifying
websites responsible for creating and disseminating misinformation content,
which are closely linked to users who share confirmed instances of fake news on
social media. We validate our approach on Twitter by examining various
execution modes and contexts. Our findings demonstrate the effectiveness of the
proposed methodology in identifying misinformation websites, which can aid in
gaining a better understanding of this phenomenon and enabling competent
entities to tackle the problem in various areas of society.","['Leandro Araujo', 'Joao M. M. Couto', 'Luiz Felipe Nery', 'Isadora C. Rodrigues', 'Jussara M. Almeida', 'Julio C. S. Reis', 'Fabricio Benevenuto']",0,0.8122277
"The prevalence of AI-generated imagery has raised concerns about the
authenticity of astronomical images, especially with advanced text-to-image
models like Stable Diffusion producing highly realistic synthetic samples.
Existing detection methods, primarily based on convolutional neural networks
(CNNs) or spectral analysis, have limitations when used independently. We
present AstroSpy, a hybrid model that integrates both spectral and image
features to distinguish real from synthetic astronomical images. Trained on a
unique dataset of real NASA images and AI-generated fakes (approximately 18k
samples), AstroSpy utilizes a dual-pathway architecture to fuse spatial and
spectral information. This approach enables AstroSpy to achieve superior
performance in identifying authentic astronomical images. Extensive evaluations
demonstrate AstroSpy's effectiveness and robustness, significantly
outperforming baseline models in both in-domain and cross-domain tasks,
highlighting its potential to combat misinformation in astronomy.","['Mohammed Talha Alam', 'Raza Imam', 'Mohsen Guizani', 'Fakhri Karray']",7,0.7032795
"Automated Fact-Checking (AFC) is the automated verification of claim
accuracy. AFC is crucial in discerning truth from misinformation, especially
given the huge amounts of content are generated online daily. Current research
focuses on predicting claim veracity through metadata analysis and language
scrutiny, with an emphasis on justifying verdicts. This paper surveys recent
methodologies, proposing a comprehensive taxonomy and presenting the evolution
of research in that landscape. A comparative analysis of methodologies and
future directions for improving fact-checking explainability are also
discussed.","['Islam Eldifrawi', 'Shengrui Wang', 'Amine Trabelsi']",1,0.7561114
"Harnessing the potential of large language models (LLMs) like ChatGPT can
help address social challenges through inclusive, ethical, and sustainable
means. In this paper, we investigate the extent to which ChatGPT can annotate
data for social computing tasks, aiming to reduce the complexity and cost of
undertaking web research. To evaluate ChatGPT's potential, we re-annotate seven
datasets using ChatGPT, covering topics related to pressing social issues like
COVID-19 misinformation, social bot deception, cyberbully, clickbait news, and
the Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise
in handling these data annotation tasks, albeit with some challenges. Across
the seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.
Its performance excels in clickbait news annotation, correctly labeling 89.66%
of the data. However, we also observe significant variations in performance
across individual labels. Our study reveals predictable patterns in ChatGPT's
annotation performance. Thus, we propose GPT-Rater, a tool to predict if
ChatGPT can correctly label data for a given annotation task. Researchers can
use this to identify where ChatGPT might be suitable for their annotation
requirements. We show that GPT-Rater effectively predicts ChatGPT's
performance. It performs best on a clickbait headlines dataset by achieving an
average F1-score of 95.00%. We believe that this research opens new avenues for
analysis and can reduce barriers to engaging in social computing research.","['Yiming Zhu', 'Peixian Zhang', 'Ehsan-Ul Haq', 'Pan Hui', 'Gareth Tyson']",8,0.5492758
"Multimodal generative models are rapidly evolving, leading to a surge in the
generation of realistic video and audio that offers exciting possibilities but
also serious risks. Deepfake videos, which can convincingly impersonate
individuals, have particularly garnered attention due to their potential misuse
in spreading misinformation and creating fraudulent content. This survey paper
examines the dual landscape of deepfake video generation and detection,
emphasizing the need for effective countermeasures against potential abuses. We
provide a comprehensive overview of current deepfake generation techniques,
including face swapping, reenactment, and audio-driven animation, which
leverage cutting-edge technologies like GANs and diffusion models to produce
highly realistic fake videos. Additionally, we analyze various detection
approaches designed to differentiate authentic from altered videos, from
detecting visual artifacts to deploying advanced algorithms that pinpoint
inconsistencies across video and audio signals.
  The effectiveness of these detection methods heavily relies on the diversity
and quality of datasets used for training and evaluation. We discuss the
evolution of deepfake datasets, highlighting the importance of robust, diverse,
and frequently updated collections to enhance the detection accuracy and
generalizability. As deepfakes become increasingly indistinguishable from
authentic content, developing advanced detection techniques that can keep pace
with generation technologies is crucial. We advocate for a proactive approach
in the ""tug-of-war"" between deepfake creators and detectors, emphasizing the
need for continuous research collaboration, standardization of evaluation
metrics, and the creation of comprehensive benchmarks.","['Hannah Lee', 'Changyeon Lee', 'Kevin Farhat', 'Lin Qiu', 'Steve Geluso', 'Aerin Kim', 'Oren Etzioni']",11,0.8598777
"Misinformation about climate change causes numerous negative impacts,
necessitating corrective responses. Psychological research has offered various
strategies for reducing the influence of climate misinformation, such as the
fact-myth-fallacy-fact-structure. However, practically implementing corrective
interventions at scale represents a challenge. Automatic detection and
correction of misinformation offers a solution to the misinformation problem.
This study documents the development of large language models that accept as
input a climate myth and produce a debunking that adheres to the
fact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating
contrarian claim classification and fallacy detection into an LLM prompting
framework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with
prompting strategies of varying complexity. Experiments reveal promising
performance of GPT-4 and Mixtral if combined with structured prompts. We
identify specific challenges of debunking generation and human evaluation, and
map out avenues for future work. We release a dataset of high-quality
truth-sandwich debunkings, source code and a demo of the debunking system.","['Francisco Zanartu', 'Yulia Otmakhova', 'John Cook', 'Lea Frermann']",1,0.6935731
"This article examines public exposure to and perceptions of deepfakes based
on insights from a nationally representative survey of 1403 UK adults. The
survey is one of the first of its kind since recent improvements in deepfake
technology and widespread adoption of political deepfakes. The findings reveal
three key insights. First, on average, 15% of people report exposure to harmful
deepfakes, including deepfake pornography, deepfake frauds/scams and other
potentially harmful deepfakes such as those that spread health/religious
misinformation/propaganda. In terms of common targets, exposure to deepfakes
featuring celebrities was 50.2%, whereas those featuring politicians was 34.1%.
And 5.7% of respondents recall exposure to a selection of high profile
political deepfakes in the UK. Second, while exposure to harmful deepfakes was
relatively low, awareness of and fears about deepfakes were high (and women
were significantly more likely to report experiencing such fears than men). As
with fears, general concerns about the spread of deepfakes were also high;
90.4% of the respondents were either very concerned or somewhat concerned about
this issue. Most respondents (at least 91.8%) were concerned that deepfakes
could add to online child sexual abuse material, increase distrust in
information and manipulate public opinion. Third, while awareness about
deepfakes was high, usage of deepfake tools was relatively low (8%). Most
respondents were not confident about their detection abilities and were
trustful of audiovisual content online. Our work highlights how the problem of
deepfakes has become embedded in public consciousness in just a few years; it
also highlights the need for media literacy programmes and other policy
interventions to address the spread of harmful deepfakes.","['Tvesha Sippy', 'Florence Enock', 'Jonathan Bright', 'Helen Z. Margetts']",11,0.6454379
"Misinformation is still a major societal problem and the arrival of Large
Language Models (LLMs) only added to it. This paper analyzes synthetic, false,
and genuine information in the form of text from spectral analysis,
visualization, and explainability perspectives to find the answer to why the
problem is still unsolved despite multiple years of research and a plethora of
solutions in the literature. Various embedding techniques on multiple datasets
are used to represent information for the purpose. The diverse spectral and
non-spectral methods used on these embeddings include t-distributed Stochastic
Neighbor Embedding (t-SNE), Principal Component Analysis (PCA), and Variational
Autoencoders (VAEs). Classification is done using multiple machine learning
algorithms. Local Interpretable Model-Agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Integrated Gradients are used for the
explanation of the classification. The analysis and the explanations generated
show that misinformation is quite closely intertwined with genuine information
and the machine learning algorithms are not as effective in separating the two
despite the claims in the literature.","['Vishnu S. Pendyala', 'Madhulika Dutta']",1,0.68306357
"Information diffusion across various new media platforms gradually influences
perceptions, decisions, and social behaviors of individual users. In
communication studies, the famous Five W's of Communication model (5W Model)
has displayed the process of information diffusion clearly. At present,
although plenty of studies and corresponding datasets about information
diffusion have emerged, a systematic categorization of tasks and an integration
of datasets are still lacking. To address this gap, we survey a systematic
taxonomy of information diffusion tasks and datasets based on the ""5W Model""
framework. We first categorize the information diffusion tasks into ten
subtasks with definitions and datasets analysis, from three main tasks of
information diffusion prediction, social bot detection, and misinformation
detection. We also collect the publicly available dataset repository of
information diffusion tasks with the available links and compare them based on
six attributes affiliated to users and content: user information, social
network, bot label, propagation content, propagation network, and veracity
label. In addition, we discuss the limitations and future directions of current
datasets and research topics to advance the future development of information
diffusion. The dataset repository can be accessed at our website
https://github.com/fuxiaG/Information-Diffusion-Datasets.","['Fuxia Guo', 'Xiaowen Wang', 'Yanwei Xie', 'Zehao Wang', 'Jingqiu Li', 'Lanjun Wang']",0,0.6616783
"In recent years, we have witnessed the proliferation of large amounts of
online content generated directly by users with virtually no form of external
control, leading to the possible spread of misinformation. The search for
effective solutions to this problem is still ongoing, and covers different
areas of application, from opinion spam to fake news detection. A more recently
investigated scenario, despite the serious risks that incurring disinformation
could entail, is that of the online dissemination of health information. Early
approaches in this area focused primarily on user-based studies applied to Web
page content. More recently, automated approaches have been developed for both
Web pages and social media content, particularly with the advent of the
COVID-19 pandemic. These approaches are primarily based on handcrafted features
extracted from online content in association with Machine Learning. In this
scenario, we focus on Web page content, where there is still room for research
to study structural-, content- and context-based features to assess the
credibility of Web pages. Therefore, this work aims to study the effectiveness
of such features in association with a deep learning model, starting from an
embedded representation of Web pages that has been recently proposed in the
context of phishing Web page detection, i.e., Web2Vec.","['Rishabh Upadhyay', 'Gabriella Pasi', 'Marco Viviani']",0,0.6704197
"We present an overview of the second edition of the ArAIEval shared task,
organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In
this edition, ArAIEval offers two tasks: (i) detection of propagandistic
textual spans with persuasion techniques identification in tweets and news
articles, and (ii) distinguishing between propagandistic and non-propagandistic
memes. A total of 14 teams participated in the final evaluation phase, with 6
and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams
submitted system description papers. Across both tasks, we observed that
fine-tuning transformer models such as AraBERT was at the core of the majority
of the participating systems. We provide a description of the task setup,
including a description of the dataset construction and the evaluation setup.
We further provide a brief overview of the participating systems. All datasets
and evaluation scripts are released to the research community
(https://araieval.gitlab.io/). We hope this will enable further research on
these important tasks in Arabic.","['Maram Hasanain', 'Md. Arid Hasan', 'Fatema Ahmed', 'Reem Suwaileh', 'Md. Rafiul Biswas', 'Wajdi Zaghouani', 'Firoj Alam']",1,0.52650243
"Misinformation poses a threat to democracy and to people's health.
Reliability criteria for news websites can help people identify misinformation.
But despite their importance, there has been no empirically substantiated list
of criteria for distinguishing reliable from unreliable news websites. We
identify reliability criteria, describe how they are applied in practice, and
compare them to prior work. Based on our analysis, we distinguish between
manipulable and less manipulable criteria and compare politically diverse
laypeople as end users and journalists as expert users. We discuss 11 widely
recognized criteria, including the following 6 criteria that are difficult to
manipulate: content, political alignment, authors, professional standards, what
sources are used, and a website's reputation. Finally, we describe how
technology may be able to support people in applying these criteria in practice
to assess the reliability of websites.","['Hendrik Heuer', 'Elena Leah Glassman']",4,0.6188028
"The digital revolution and the advent of the world wide web have transformed
human communication, notably through the emergence of memes. While memes are a
popular and straightforward form of expression, they can also be used to spread
misinformation and hate due to their anonymity and ease of use. In response to
these challenges, this paper introduces a solution developed by team 'Baseline'
for the AI Singapore Online Safety Prize Challenge. Focusing on computational
efficiency and feature engineering, the solution achieved an AUROC of 0.76 and
an accuracy of 0.69 on the test dataset. As key features, the solution
leverages the inherent probabilistic capabilities of large Vision-Language
Models (VLMs) to generate task-adapted feature encodings from text, and applies
a distilled quantization tailored to the specific cultural nuances present in
Singapore. This type of processing and fine-tuning can be adapted to various
visual and textual understanding and classification tasks, and even applied on
private VLMs such as OpenAI's GPT. Finally it can eliminate the need for
extensive model training on large GPUs for resource constrained applications,
also offering a solution when little or no data is available.",['Peter Gr√∂nquist'],2,0.60860085
"Social computing scholars have long known that people do not interact with
knowledge in straightforward ways, especially in digital environments. While
policies around knowledge are essential for targeting misinformation, they are
value-laden; in choosing how to present information, we undermine
non-traditional -- often non-Western -- ways of knowing. Epistemic injustice is
the systemic exclusion of certain people and methods from the knowledge canon.
Epistemic injustice chips away at one's testimony and vocabulary until they are
stripped of their due right to know and understand. In this paper, we
articulate how epistemic injustice in sociotechnical applications leads to
material harm. Inspired by a hybrid collaborative autoethnography of 14 CSCW
practitioners, we present three cases of epistemic injustice in sociotechnical
applications: online transgender healthcare, identity sensemaking on
r/bisexual, and Indigenous ways of knowing on r/AskHistorians. We further
explore signature tensions across our autoethnographic materials and relate
them to previous CSCW research areas and personal non-technological
experiences. We argue that epistemic injustice can serve as a unifying and
intersectional lens for CSCW research by surfacing dimensions of epistemic
community and power. Finally, we present a call to action of three changes the
CSCW community should make to move toward its own goals of research justice. We
call for CSCW researchers to center individual experiences, bolster
communities, and remediate issues of epistemic power as a means towards
epistemic justice. In sum, we recount, synthesize, and propose solutions for
the various forms of epistemic injustice that CSCW sites of study -- including
CSCW itself -- propagate.","['Leah Hope Ajmani', 'Jasmine C Foriest', 'Jordan Taylor', 'Kyle Pittman', 'Sarah Gilbert', 'Michael Ann Devito']",0,0.65685964
"Social media platforms enhance the propagation of online misinformation by
providing large user bases with a quick means to share content. One way to
disrupt the rapid dissemination of misinformation at scale is through warning
tags, which label content as potentially false or misleading. Past warning tag
mitigation studies yield mixed results for diverse audiences, however. We
hypothesize that personalizing warning tags to the individual characteristics
of their diverse users may enhance mitigation effectiveness. To reach the goal
of personalization, we need to understand how people differ and how those
differences predict a person's attitudes and self-described behaviors toward
tags and tagged content. In this study, we leverage Amazon Mechanical Turk (n =
132) and undergraduate students (n = 112) to provide this foundational
understanding. Specifically, we find attitudes towards warning tags and
self-described behaviors are positively influenced by factors such as
Personality Openness and Agreeableness, Need for Cognitive Closure (NFCC),
Cognitive Reflection Test (CRT) score, and Trust in Medical Scientists.
Conversely, Trust in Religious Leaders, Conscientiousness, and political
conservatism were negatively correlated with these attitudes and behaviors. We
synthesize our results into design insights and a future research agenda for
more effective and personalized misinformation warning tags and misinformation
mitigation strategies more generally.","['Robert Kaufman', 'Aaron Broukhim', 'Michael Haupt']",3,0.7230594
"Over one in five adults in the US lives with a mental illness. In the face of
a shortage of mental health professionals and offline resources, online
short-form video content has grown to serve as a crucial conduit for
disseminating mental health help and resources. However, the ease of content
creation and access also contributes to the spread of misinformation, posing
risks to accurate diagnosis and treatment. Detecting and understanding
engagement with such content is crucial to mitigating their harmful effects on
public health. We perform the first quantitative study of the phenomenon using
YouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo,
a novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos
(639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an
expert-driven annotation schema. We first found that few-shot in-context
learning with large language models (LLMs) are effective in detecting MHMisinfo
videos. Next, we discover distinct and potentially alarming linguistic patterns
in how audiences engage with MHMisinfo videos through commentary on both
video-sharing platforms. Across the two platforms, comments could exacerbate
prevailing stigma with some groups showing heightened susceptibility to and
alignment with MHMisinfo. We discuss technical and public health-driven
adaptive solutions to tackling the ""epidemic"" of mental health misinformation
online.","['Viet Cuong Nguyen', 'Mini Jain', 'Abhijat Chauhan', 'Heather Jaime Soled', 'Santiago Alvarez Lesmes', 'Zihang Li', 'Michael L. Birnbaum', 'Sunny X. Tang', 'Srijan Kumar', 'Munmun De Choudhury']",5,0.6509168
"Advancements in DeepFake (DF) audio models pose a significant threat to voice
authentication systems, leading to unauthorized access and the spread of
misinformation. We introduce a defense mechanism, SecureSpectra, addressing DF
threats by embedding orthogonal, irreversible signatures within audio.
SecureSpectra leverages the inability of DF models to replicate high-frequency
content, which we empirically identify across diverse datasets and DF models.
Integrating differential privacy into the pipeline protects signatures from
reverse engineering and strikes a delicate balance between enhanced security
and minimal performance compromises. Our evaluations on Mozilla Common Voice,
LibriSpeech, and VoxCeleb datasets showcase SecureSpectra's superior
performance, outperforming recent works by up to 71% in detection accuracy. We
open-source SecureSpectra to benefit the research community.","['Oguzhan Baser', 'Kaan Kale', 'Sandeep P. Chinchali']",11,0.6271577
"Given the growing influx of misinformation across news and social media,
there is a critical need for systems that can provide effective real-time
verification of news claims. Large language or multimodal model based
verification has been proposed to scale up online policing mechanisms for
mitigating spread of false and harmful content. While these can potentially
reduce burden on human fact-checkers, such efforts may be hampered by
foundation model training data becoming outdated. In this work, we test the
limits of improving foundation model performance without continual updating
through an initial study of knowledge transfer using either existing intra- and
inter- domain benchmarks or explanations generated from large language models
(LLMs). We evaluate on 12 public benchmarks for fact-checking and
misinformation detection as well as two other tasks relevant to content
moderation -- toxicity and stance detection. Our results on two recent
multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that
knowledge transfer strategies can improve Fakeddit performance over the
state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.","['Jaeyoung Lee', 'Ximing Lu', 'Jack Hessel', 'Faeze Brahman', 'Youngjae Yu', 'Yonatan Bisk', 'Yejin Choi', 'Saadia Gabriel']",1,0.7381696
"This research studies the nature and spread of WhatsApp content among
everyday users in a rural Indian village. Leveraging a dataset of hundreds of
private WhatsApp groups collected with consent from participants, our study
uncovers the kinds of WhatsApp groups users are part of, marking the first such
categorization. The dataset comprises tens of thousands of messages, out of
which we manually classified 604 pieces of content designated as 'forwarded
many times'-indicating their viral status.
  Our key findings indicate a high prevalence of content focused on national
politics, with the viral messages overwhelmingly supporting a specific
political party and disparaging the opposition. Significantly, these messages
were fraught with misinformation, engendering hate against Muslims and
promoting a narrative of Hindus being under threat. This trend was particularly
noticeable within caste-based groups, which were dominated by misinformation,
pro-BJP rhetoric, anti-Congress content, and Hindutva propaganda. Remarkably,
much of the misinformation circulating had previously been discredited by
established fact-checking organizations. This suggests not only a recurring
cycle of debunked information reappearing but also that fact-checks are failing
to penetrate these specific groups.
  As the first quantitative analysis of everyday WhatsApp use in a rural
context, this research has far-reaching implications for understanding the
unique challenges posed by end-to-end encrypted platforms. It serves as a
crucial baseline for designing more effective moderation policies aimed at
combating misinformation and fostering a more responsible use of encrypted
communication channels.","['Kiran Garimella', 'Bharat Nayak', 'Simon Chauchard', 'Aditya Vashistha']",3,0.7105395
"We study security threats to Markov games due to information asymmetry and
misinformation. We consider an attacker player who can spread misinformation
about its reward function to influence the robust victim player's behavior.
Given a fixed fake reward function, we derive the victim's policy under
worst-case rationality and present polynomial-time algorithms to compute the
attacker's optimal worst-case policy based on linear programming and backward
induction. Then, we provide an efficient inception (""planting an idea in
someone's mind"") attack algorithm to find the optimal fake reward function
within a restricted set of reward functions with dominant strategies.
Importantly, our methods exploit the universal assumption of rationality to
compute attacks efficiently. Thus, our work exposes a security vulnerability
arising from standard game assumptions under misinformation.","['Jeremy McMahan', 'Young Wu', 'Yudong Chen', 'Xiaojin Zhu', 'Qiaomin Xie']",2,0.5692278
"This study addresses the critical challenge of detecting DeepFake tweets by
leveraging advanced natural language processing (NLP) techniques to distinguish
between genuine and AI-generated texts. Given the increasing prevalence of
misinformation, our research utilizes the TweepFake dataset to train and
evaluate various machine learning models. The objective is to identify
effective strategies for recognizing DeepFake content, thereby enhancing the
integrity of digital communications. By developing reliable methods for
detecting AI-generated misinformation, this work contributes to a more
trustworthy online information environment.","['Adam Frej', 'Adrian Kaminski', 'Piotr Marciniak', 'Szymon Szmajdzinski', 'Soveatin Kuntur', 'Anna Wroblewska']",11,0.76582456
"Large language models (LLMs) have advanced to a point that even humans have
difficulty discerning whether a text was generated by another human, or by a
computer. However, knowing whether a text was produced by human or artificial
intelligence (AI) is important to determining its trustworthiness, and has
applications in many domains including detecting fraud and academic dishonesty,
as well as combating the spread of misinformation and political propaganda. The
task of AI-generated text (AIGT) detection is therefore both very challenging,
and highly critical. In this survey, we summarize state-of-the art approaches
to AIGT detection, including watermarking, statistical and stylistic analysis,
and machine learning classification. We also provide information about existing
datasets for this task. Synthesizing the research findings, we aim to provide
insight into the salient factors that combine to determine how ""detectable""
AIGT text is under different scenarios, and to make practical recommendations
for future work towards this significant technical and societal challenge.","['Kathleen C. Fraser', 'Hillary Dawkins', 'Svetlana Kiritchenko']",9,0.7489899
"The recent success of Large Language Models (LLMs) has sparked concerns about
their potential to spread misinformation. As a result, there is a pressing need
for tools to identify ``fake arguments'' generated by such models. To create
these tools, examples of texts generated by LLMs are needed. This paper
introduces a methodology to obtain good, bad and ugly arguments from
argumentative essays produced by ChatGPT, OpenAI's LLM. We then describe a
novel dataset containing a set of diverse arguments, ArGPT. We assess the
effectiveness of our dataset and establish baselines for several
argumentation-related tasks. Finally, we show that the artificially generated
data relates well to human argumentation and thus is useful as a tool to train
and test systems for the defined tasks.","['Victor Hugo Nascimento Rocha', 'Igor Cataneo Silveira', 'Paulo Pirozelli', 'Denis Deratani Mau√°', 'Fabio Gagliardi Cozman']",1,0.70807713
"Large language models (LLMs) excel in various capabilities but also pose
safety risks such as generating harmful content and misinformation, even after
safety alignment. In this paper, we explore the inner mechanisms of safety
alignment from the perspective of mechanistic interpretability, focusing on
identifying and analyzing safety neurons within LLMs that are responsible for
safety behaviors. We propose generation-time activation contrasting to locate
these neurons and dynamic activation patching to evaluate their causal effects.
Experiments on multiple recent LLMs show that: (1) Safety neurons are sparse
and effective. We can restore $90$% safety performance with intervention only
on about $5$% of all the neurons. (2) Safety neurons encode transferrable
mechanisms. They exhibit consistent effectiveness on different red-teaming
datasets. The finding of safety neurons also interprets ""alignment tax"". We
observe that the identified key neurons for safety and helpfulness
significantly overlap, but they require different activation patterns of the
shared neurons. Furthermore, we demonstrate an application of safety neurons in
detecting unsafe outputs before generation. Our findings may promote further
research on understanding LLM alignment. The source codes will be publicly
released to facilitate future research.","['Jianhui Chen', 'Xiaozhi Wang', 'Zijun Yao', 'Yushi Bai', 'Lei Hou', 'Juanzi Li']",6,0.57597506
"Over the past years, images generated by artificial intelligence have become
more prevalent and more realistic. Their advent raises ethical questions
relating to misinformation, artistic expression, and identity theft, among
others. The crux of many of these moral questions is the difficulty in
distinguishing between real and fake images. It is important to develop tools
that are able to detect AI-generated images, especially when these images are
too realistic-looking for the human eye to identify as fake. This paper
proposes a dual-branch neural network architecture that takes both images and
their Fourier frequency decomposition as inputs. We use standard CNN-based
methods for both branches as described in Stuchi et al. [7], followed by
fully-connected layers. Our proposed model achieves an accuracy of 94% on the
CIFAKE dataset, which significantly outperforms classic ML methods and CNNs,
achieving performance comparable to some state-of-the-art architectures, such
as ResNet.","['Jonathan Gallagher', 'William Pugsley']",7,0.82098925
"Previous works on Large Language Models (LLMs) have mainly focused on
evaluating their helpfulness or harmlessness. However, honesty, another crucial
alignment criterion, has received relatively less attention. Dishonest
behaviors in LLMs, such as spreading misinformation and defrauding users,
present severe risks that intensify as these models approach superintelligent
levels. Enhancing honesty in LLMs addresses critical limitations and helps
uncover latent capabilities that are not readily expressed. This underscores
the urgent need for reliable methods and benchmarks to effectively ensure and
evaluate the honesty of LLMs.
  In this paper, we introduce BeHonest, a pioneering benchmark specifically
designed to assess honesty in LLMs comprehensively. BeHonest evaluates three
essential aspects of honesty: awareness of knowledge boundaries, avoidance of
deceit, and consistency in responses. Building on this foundation, we designed
10 scenarios to evaluate and analyze 9 popular LLMs on the market, including
both closed-source and open-source models from different model families with
varied model sizes. Our findings indicate that there is still significant room
for improvement in the honesty of LLMs. We encourage the AI community to
prioritize honesty alignment in these models, which can harness their full
potential to benefit society while preventing them from causing harm through
deception or inconsistency. Our benchmark and code can be found at:
\url{https://github.com/GAIR-NLP/BeHonest}.","['Steffi Chern', 'Zhulin Hu', 'Yuqing Yang', 'Ethan Chern', 'Yuan Guo', 'Jiahe Jin', 'Binjie Wang', 'Pengfei Liu']",6,0.8055084
"In this paper, we explore the feasibility of leveraging large language models
(LLMs) to automate or otherwise assist human raters with identifying harmful
content including hate speech, harassment, violent extremism, and election
misinformation. Using a dataset of 50,000 comments, we demonstrate that LLMs
can achieve 90% accuracy when compared to human verdicts. We explore how to
best leverage these capabilities, proposing five design patterns that integrate
LLMs with human rating, such as pre-filtering non-violative content, detecting
potential errors in human rating, or surfacing critical context to support
human rating. We outline how to support all of these design patterns using a
single, optimized prompt. Beyond these synthetic experiments, we share how
piloting our proposed techniques in a real-world review queue yielded a 41.5%
improvement in optimizing available human rater capacity, and a 9--11% increase
(absolute) in precision and recall for detecting violative content.","['Kurt Thomas', 'Patrick Gage Kelley', 'David Tao', 'Sarah Meiklejohn', 'Owen Vallis', 'Shunwen Tan', 'Bla≈æ Brataniƒç', 'Felipe Tiengo Ferreira', 'Vijay Kumar Eranti', 'Elie Bursztein']",6,0.64936304
"This study introduces the leveled-text generation task, aiming to rewrite
educational materials to specific readability levels while preserving meaning.
We assess the capability of GPT-3.5, LLaMA-2 70B, and Mixtral 8x7B, to generate
content at various readability levels through zero-shot and few-shot prompting.
Evaluating 100 processed educational materials reveals that few-shot prompting
significantly improves performance in readability manipulation and information
preservation. LLaMA-2 70B performs better in achieving the desired difficulty
range, while GPT-3.5 maintains original meaning. However, manual inspection
highlights concerns such as misinformation introduction and inconsistent edit
distribution. These findings emphasize the need for further research to ensure
the quality of generated educational content.","['Chieh-Yang Huang', 'Jing Wei', ""Ting-Hao 'Kenneth' Huang""]",1,0.5420688
"Stance detection holds great potential for enhancing the quality of online
political discussions, as it has shown to be useful for summarizing
discussions, detecting misinformation, and evaluating opinion distributions.
Usually, transformer-based models are used directly for stance detection, which
require large amounts of data. However, the broad range of debate questions in
online political discussion creates a variety of possible scenarios that the
model is faced with and thus makes data acquisition for model training
difficult. In this work, we show how to leverage LLM-generated synthetic data
to train and improve stance detection agents for online political
discussions:(i) We generate synthetic data for specific debate questions by
prompting a Mistral-7B model and show that fine-tuning with the generated
synthetic data can substantially improve the performance of stance detection.
(ii) We examine the impact of combining synthetic data with the most
informative samples from an unlabelled dataset. First, we use the synthetic
data to select the most informative samples, second, we combine both these
samples and the synthetic data for fine-tuning. This approach reduces labelling
effort and consistently surpasses the performance of the baseline model that is
trained with fully labeled data. Overall, we show in comprehensive experiments
that LLM-generated data greatly improves stance detection performance for
online political discussions.","['Stefan Sylvius Wagner', 'Maike Behrendt', 'Marc Ziegele', 'Stefan Harmeling']",1,0.6397717
"In recent years, the proliferation of misinformation on social media
platforms has become a significant concern. Initially designed for sharing
information and fostering social connections, platforms like Twitter (now
rebranded as X) have also unfortunately become conduits for spreading
misinformation. To mitigate this, these platforms have implemented various
mechanisms, including the recent suggestion to use crowd-sourced non-expert
fact-checkers to enhance the scalability and efficiency of content vetting. An
example of this is the introduction of Community Notes on Twitter.
  While previous research has extensively explored various aspects of Twitter
tweets, such as information diffusion, sentiment analytics and opinion
summarization, there has been a limited focus on the specific feature of
Twitter Community Notes, despite its potential role in crowd-sourced
fact-checking. Prior research on Twitter Community Notes has involved empirical
analysis of the feature's dataset and comparative studies that also include
other methods like expert fact-checking. Distinguishing itself from prior
works, our study covers a multi-faceted analysis of sources and audience
perception within Community Notes. We find that the majority of cited sources
are news outlets that are left-leaning and are of high factuality, pointing to
a potential bias in the platform's community fact-checking. Left biased and low
factuality sources validate tweets more, while Center sources are used more
often to refute tweet content. Additionally, source factuality significantly
influences public agreement and helpfulness of the notes, highlighting the
effectiveness of the Community Notes Ranking algorithm. These findings showcase
the impact and biases inherent in community-based fact-checking initiatives.","['Uku Kangur', 'Roshni Chakraborty', 'Rajesh Sharma']",3,0.78692096
"Understanding susceptibility to online influence is crucial for mitigating
the spread of misinformation and protecting vulnerable audiences. This paper
investigates susceptibility to influence within social networks, focusing on
the differential effects of influence-driven versus spontaneous behaviors on
user content adoption. Our analysis reveals that influence-driven adoption
exhibits high homophily, indicating that individuals prone to influence often
connect with similarly susceptible peers, thereby reinforcing peer influence
dynamics, whereas spontaneous adoption shows significant but lower homophily.
Additionally, we extend the Generalized Friendship Paradox to influence-driven
behaviors, demonstrating that users' friends are generally more susceptible to
influence than the users themselves, de facto establishing the notion of
Susceptibility Paradox in online social influence. This pattern does not hold
for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions.
We find that susceptibility to influence can be predicted using friends'
susceptibility alone, while predicting spontaneous adoption requires additional
features, such as user metadata. These findings highlight the complex interplay
between user engagement and characteristics in spontaneous content adoption.
Our results provide new insights into social influence mechanisms and offer
implications for designing more effective moderation strategies to protect
vulnerable audiences.","['Luca Luceri', 'Jinyi Ye', 'Julie Jiang', 'Emilio Ferrara']",3,0.5956247
"Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of ""credibility-aware RAG"", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention weights based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.","['Boyi Deng', 'Wenjie Wang', 'Fengbin Zhu', 'Qifan Wang', 'Fuli Feng']",6,0.72946405
"Understanding the impact of digital platforms on user behavior presents
foundational challenges, including issues related to polarization,
misinformation dynamics, and variation in news consumption. Comparative
analyses across platforms and over different years can provide critical
insights into these phenomena. This study investigates the linguistic
characteristics of user comments over 34 years, focusing on their complexity
and temporal shifts. Utilizing a dataset of approximately 300 million English
comments from eight diverse platforms and topics, we examine the vocabulary
size and linguistic richness of user communications and their evolution over
time. Our findings reveal consistent patterns of complexity across social media
platforms and topics, characterized by a nearly universal reduction in text
length, diminished lexical richness, but decreased repetitiveness. Despite
these trends, users consistently introduce new words into their comments at a
nearly constant rate. This analysis underscores that platforms only partially
influence the complexity of user comments. Instead, it reflects a broader,
universal pattern of human behaviour, suggesting intrinsic linguistic
tendencies of users when interacting online.","['Niccol√≤ Di Marco', 'Edoardo Loru', 'Anita Bonetti', 'Alessandra Olga Grazia Serra', 'Matteo Cinelli', 'Walter Quattrociocchi']",3,0.6515108
"Large language models (LLMs) have demonstrated remarkable performance on
various natural language processing tasks. However, they are prone to
generating fluent yet untruthful responses, known as ""hallucinations"".
Hallucinations can lead to the spread of misinformation and cause harm in
critical applications. Mitigating hallucinations is challenging as they arise
from factors such as noisy data, model overconfidence, lack of knowledge, and
the generation process itself. Recent efforts have attempted to address this
issue through representation editing and decoding algorithms, reducing
hallucinations without major structural changes or retraining. However, these
approaches either implicitly edit LLMs' behavior in latent space or suppress
the tendency to output unfaithful results during decoding instead of explicitly
modeling on hallucination. In this work, we introduce Faithful Finetuning (F2),
a novel method that explicitly models the process of faithful question
answering through carefully designed loss functions during fine-tuning. We
conduct extensive experiments on popular datasets and demonstrate that F2
achieves significant improvements over vanilla models and baselines.","['Minda Hu', 'Bowei He', 'Yufei Wang', 'Liangyou Li', 'Chen Ma', 'Irwin King']",6,0.64230037
"Misinformation is prevalent in various fields such as education, politics,
health, etc., causing significant harm to society. However, current methods for
cross-domain misinformation detection rely on time and resources consuming
fine-tuning and complex model structures. With the outstanding performance of
LLMs, many studies have employed them for misinformation detection.
Unfortunately, they focus on in-domain tasks and do not incorporate significant
sentiment and emotion features (which we jointly call affect). In this paper,
we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to
address cross-domain misinformation detection using in-context learning based
on affective information. It accomplishes this by applying an emotion-aware LLM
to construct a retrieval database of affective embeddings. This database is
used by our retrieval module to obtain source-domain samples, which are
subsequently used for the inference module's in-context few-shot learning to
detect target domain misinformation. We evaluate our framework on three
misinformation benchmarks. Results show that RAEmoLLM achieves significant
improvements compared to the zero-shot method on three datasets, with the
highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be
released on https://github.com/lzw108/RAEmoLLM.","['Zhiwei Liu', 'Kailai Yang', 'Qianqian Xie', 'Christine de Kock', 'Sophia Ananiadou', 'Eduard Hovy']",6,0.71339285
"Despite the striking advances in recent language generation performance,
model-generated responses have suffered from the chronic problem of
hallucinations that are either untrue or unfaithful to a given source.
Especially in the task of knowledge grounded conversation, the models are
required to generate informative responses, but hallucinated utterances lead to
miscommunication. In particular, entity-level hallucination that causes
critical misinformation and undesirable conversation is one of the major
concerns. To address this issue, we propose a post-hoc refinement method called
REM. It aims to enhance the quality and faithfulness of hallucinated utterances
by refining them based on the source knowledge. If the generated utterance has
a low source-faithfulness score with the given knowledge, REM mines the key
entities in the knowledge and implicitly uses them for refining the utterances.
We verify that our method reduces entity hallucination in the utterance. Also,
we show the adaptability and efficacy of REM with extensive experiments and
generative results. Our code is available at https://github.com/YOONNAJANG/REM.","['Yoonna Jang', 'Suhyune Son', 'Jeongwoo Lee', 'Junyoung Son', 'Yuna Hur', 'Jungwoo Lim', 'Hyeonseok Moon', 'Kisu Yang', 'Heuiseok Lim']",6,0.5944716
"Artificial intelligence (AI) as a disruptive technology is not new. However,
its recent evolution, engineered by technological transformation, big data
analytics, and quantum computing, produces conversational and generative AI
(CGAI/GenAI) and human-like chatbots that disrupt conventional operations and
methods in different fields. This study investigates the scientific landscape
of CGAI and human-chatbot interaction/collaboration and evaluates use cases,
benefits, challenges, and policy implications for multidisciplinary education
and allied industry operations. The publications trend showed that just 4%
(n=75) occurred during 2006-2018, while 2019-2023 experienced astronomical
growth (n=1763 or 96%). The prominent use cases of CGAI (e.g., ChatGPT) for
teaching, learning, and research activities occurred in computer science
[multidisciplinary and AI] (32%), medical/healthcare (17%), engineering (7%),
and business fields (6%). The intellectual structure shows strong collaboration
among eminent multidisciplinary sources in business, Information Systems, and
other areas. The thematic structure of SLP highlights prominent CGAI use cases,
including improved user experience in human-computer interaction, computer
programs/code generation, and systems creation. Widespread CGAI usefulness for
teachers, researchers, and learners includes syllabi/course content generation,
testing aids, and academic writing. The concerns about abuse and misuse
(plagiarism, academic integrity, privacy violations) and issues about
misinformation, danger of self-diagnoses, and patient privacy in
medical/healthcare applications are prominent. Formulating strategies and
policies to address potential CGAI challenges in teaching/learning and practice
are priorities. Developing discipline-based automatic detection of GenAI
contents to check abuse is proposed.","['Ikpe Justice Akpan', 'Yawo M. Kobara', 'Josiah Owolabi', 'Asuama Akpam', 'Onyebuchi Felix Offodile']",9,0.7554544
"The COVID-19 pandemic exposed significant weaknesses in the healthcare
information system. The overwhelming volume of misinformation on social media
and other socioeconomic factors created extraordinary challenges to motivate
people to take proper precautions and get vaccinated. In this context, our work
explored a novel direction by analyzing an extensive dataset collected over two
years, identifying the topics de/motivating the public about COVID-19
vaccination. We analyzed these topics based on time, geographic location, and
political orientation. We noticed that while the motivating topics remain the
same over time and geographic location, the demotivating topics change rapidly.
We also identified that intrinsic motivation, rather than external mandate, is
more advantageous to inspire the public. This study addresses scientific
communication and public motivation in social media. It can help public health
officials, policymakers, and social media platforms develop more effective
messaging strategies to cut through the noise of misinformation and educate the
public about scientific findings.","['Ashiqur Rahman', 'Ehsan Mohammadi', 'Hamed Alhoori']",5,0.7802765
"The widespread use of Generative Artificial Intelligence (GAI) among
teenagers has led to significant misuse and safety concerns. To identify risks
and understand parental controls challenges, we conducted a content analysis on
Reddit and interviewed 20 participants (seven teenagers and 13 parents). Our
study reveals a significant gap in parental awareness of the extensive ways
children use GAI, such as interacting with character-based chatbots for
emotional support or engaging in virtual relationships. Parents and children
report differing perceptions of risks associated with GAI. Parents primarily
express concerns about data collection, misinformation, and exposure to
inappropriate content. In contrast, teenagers are more concerned about becoming
addicted to virtual relationships with GAI, the potential misuse of GAI to
spread harmful content in social groups, and the invasion of privacy due to
unauthorized use of their personal data in GAI applications. The absence of
parental control features on GAI platforms forces parents to rely on
system-built controls, manually check histories, share accounts, and engage in
active mediation. Despite these efforts, parents struggle to grasp the full
spectrum of GAI-related risks and to perform effective real-time monitoring,
mediation, and education. We provide design recommendations to improve
parent-child communication and enhance the safety of GAI use.","['Yaman Yu', 'Tanusree Sharma', 'Melinda Hu', 'Justin Wang', 'Yang Wang']",9,0.57268035
"This study aims to acquire more insights into the continuous pre-training
phase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case
study. Since the pandemic emerged after the last update of BERT's pre-training
data, the model has little to no entity knowledge about COVID-19. Using
continuous pre-training, we control what entity knowledge is available to the
model. We compare the baseline BERT model with the further pre-trained variants
on the fact-checking benchmark Check-COVID. To test the robustness of
continuous pre-training, we experiment with several adversarial methods to
manipulate the input data, such as training on misinformation and shuffling the
word order until the input becomes nonsensical. Surprisingly, our findings
reveal that these methods do not degrade, and sometimes even improve, the
model's downstream performance. This suggests that continuous pre-training of
BERT is robust against misinformation. Furthermore, we are releasing a new
dataset, consisting of original texts from academic publications in the
LitCovid repository and their AI-generated false counterparts.","['Ine Gevers', 'Walter Daelemans']",1,0.64705575
"The rapid propagation of misinformation poses substantial risks to public
interest. To combat misinformation, large language models (LLMs) are adapted to
automatically verify claim credibility. Nevertheless, existing methods heavily
rely on the embedded knowledge within LLMs and / or black-box APIs for evidence
collection, leading to subpar performance with smaller LLMs or upon unreliable
context. In this paper, we propose retrieval augmented fact verification
through the synthesis of contrasting arguments (RAFTS). Upon input claims,
RAFTS starts with evidence retrieval, where we design a retrieval pipeline to
collect and re-rank relevant documents from verifiable sources. Then, RAFTS
forms contrastive arguments (i.e., supporting or refuting) conditioned on the
retrieved evidence. In addition, RAFTS leverages an embedding model to identify
informative demonstrations, followed by in-context prompting to generate the
prediction and explanation. Our method effectively retrieves relevant documents
as evidence and evaluates arguments from varying perspectives, incorporating
nuanced information for fine-grained decision-making. Combined with informative
in-context examples as prior, RAFTS achieves significant improvements to
supervised and LLM baselines without complex prompts. We demonstrate the
effectiveness of our method through extensive experiments, where RAFTS can
outperform GPT-based methods with a significantly smaller 7B LLM.","['Zhenrui Yue', 'Huimin Zeng', 'Lanyu Shang', 'Yifan Liu', 'Yang Zhang', 'Dong Wang']",1,0.7976721
"The prevalence of misinformation and disinformation poses a significant
challenge in today's digital landscape. That is why several methods and tools
are proposed to analyze and understand these phenomena from a scientific
perspective. To assess how the mis/disinformation is being conceptualized and
evaluated in the literature, this paper surveys the existing frameworks, models
and simulations of mis/disinformation dynamics by performing a systematic
literature review up to 2023. After applying the PRISMA methodology, 57
research papers are inspected to determine (1) the terminology and definitions
of mis/disinformation, (2) the methods used to represent mis/disinformation,
(3) the primary purpose beyond modeling and simulating mis/disinformation, (4)
the context where the mis/disinformation is studied, and (5) the validation of
the proposed methods for understanding mis/disinformation.
  The main findings reveal a consistent essence definition of misinformation
and disinformation across studies, with intent as the key distinguishing
factor. Research predominantly uses social frameworks, epidemiological models,
and belief updating simulations. These studies aim to estimate the
effectiveness of mis/disinformation, primarily in health and politics. The
preferred validation strategy is to compare methods with real-world data and
statistics. Finally, this paper identifies current trends and open challenges
in the mis/disinformation research field, providing recommendations for future
work agenda.","['Alejandro Buitrago L√≥pez', 'Javier Pastor-Galindo', 'Jos√© A. Ruip√©rez-Valiente']",0,0.72128856
"Vaccines were critical in reducing hospitalizations and mortality during the
COVID-19 pandemic. Despite their wide availability in the United States, 62% of
Americans chose not to be vaccinated during 2021. While online misinformation
about COVID-19 is correlated to vaccine hesitancy, little prior work has
explored a causal link between real-world exposure to antivaccine content and
vaccine uptake. Here we present a compartmental epidemic model that includes
vaccination, vaccine hesitancy, and exposure to antivaccine content. We fit the
model to observational data to determine that a geographical pattern of
exposure to online antivaccine content across US counties is responsible for a
pattern of reduced vaccine uptake in the same counties. We find that exposure
to antivaccine content on Twitter caused about 750,000 people to refuse
vaccination between February and August 2021 in the US, resulting in at least
29,000 additional cases and 430 additional deaths. This work provides a
methodology for linking online speech to offline epidemic outcomes. Our
findings should inform social media moderation policy as well as public health
interventions.","['John Bollenbacher', 'Filippo Menczer', 'John Bryden']",12,0.84033
"Current multimodal misinformation detection (MMD) methods often assume a
single source and type of forgery for each sample, which is insufficient for
real-world scenarios where multiple forgery sources coexist. The lack of a
benchmark for mixed-source misinformation has hindered progress in this field.
To address this, we introduce MMFakeBench, the first comprehensive benchmark
for mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity
distortion, visual veracity distortion, and cross-modal consistency distortion,
along with 12 sub-categories of misinformation forgery types. We further
conduct an extensive evaluation of 6 prevalent detection methods and 15 large
vision-language models (LVLMs) on MMFakeBench under a zero-shot setting. The
results indicate that current methods struggle under this challenging and
realistic mixed-source MMD setting. Additionally, we propose an innovative
unified framework, which integrates rationales, actions, and tool-use
capabilities of LVLM agents, significantly enhancing accuracy and
generalization. We believe this study will catalyze future research into more
realistic mixed-source multimodal misinformation and provide a fair evaluation
of misinformation detection methods.","['Xuannan Liu', 'Zekun Li', 'Peipei Li', 'Shuhan Xia', 'Xing Cui', 'Linzhi Huang', 'Huaibo Huang', 'Weihong Deng', 'Zhaofeng He']",6,0.68994105
"Malicious social bots achieve their malicious purposes by spreading
misinformation and inciting social public opinion, seriously endangering social
security, making their detection a critical concern. Recently, graph-based bot
detection methods have achieved state-of-the-art (SOTA) performance. However,
our research finds many isolated and poorly linked nodes in social networks, as
shown in Fig.1, which graph-based methods cannot effectively detect. To address
this problem, our research focuses on effectively utilizing node semantics and
network structure to jointly detect sparsely linked nodes. Given the excellent
performance of language models (LMs) in natural language understanding (NLU),
we propose a novel social bot detection framework LGB, which consists of two
main components: language model (LM) and graph neural network (GNN).
Specifically, the social account information is first extracted into unified
user textual sequences, which is then used to perform supervised fine-tuning
(SFT) of the language model to improve its ability to understand social account
semantics. Next, the semantically enriched node representation is fed into the
pre-trained GNN to further enhance the node representation by aggregating
information from neighbors. Finally, LGB fuses the information from both
modalities to improve the detection performance of sparsely linked nodes.
Extensive experiments on two real-world datasets demonstrate that LGB
consistently outperforms state-of-the-art baseline models by up to 10.95%. LGB
is already online: https://botdetection.aminer.cn/robotmain.","['Ming Zhou', 'Dan Zhang', 'Yuandong Wang', 'Yangli-ao Geng', 'Yuxiao Dong', 'Jie Tang']",2,0.7254064
"Out-of-context news is a common type of misinformation on online media
platforms. This involves posting a caption, alongside a mismatched news image.
Existing out-of-context news detection models only consider the scenario where
pre-labeled data is available for each domain, failing to address the
out-of-context news detection on unlabeled domains (e.g. news topics or
agencies). In this work, we therefore focus on domain adaptive out-of-context
news detection. In order to effectively adapt the detection model to unlabeled
news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation
with Test-Time Adaptation) which applies contrastive learning and maximum mean
discrepancy (MMD) to learn domain-invariant features. In addition, we leverage
test-time target domain statistics to further assist domain adaptation.
Experimental results show that our approach outperforms baselines in most
domain adaptation settings on two public datasets, by as much as 2.93% in F1
and 2.08% in accuracy.","['Yimeng Gu', 'Mengqi Zhang', 'Ignacio Castro', 'Shu Wu', 'Gareth Tyson']",4,0.6395613
"During the COVID-19 pandemic, the proliferation of misinformation on social
media has been rapidly increasing. Automated Bot authors are believed to be
significant contributors of this surge. It is hypothesized that Bot authors
deliberately craft online misinformation aimed at triggering and exploiting
human cognitive biases, thereby enhancing tweet engagement and persuasive
influence. This study investigates this hypothesis by studying triggers of
biases embedded in Bot-authored misinformation and comparing them with their
counterparts, Human-authored misinformation. We complied a Misinfo Dataset that
contains COVID-19 vaccine-related misinformation tweets annotated by author
identities, Bots vs Humans, from Twitter during the vaccination period from
July 2020 to July 2021. We developed an algorithm to computationally automate
the extraction of triggers for eight cognitive biase. Our analysis revealed
that the Availability Bias, Cognitive Dissonance, and Confirmation Bias were
most commonly present in misinformation, with Bot-authored tweets exhibiting a
greater prevalence, with distinct patterns in utilizing bias triggers between
Humans and Bots. We further linked these bias triggers with engagement metrics,
inferring their potential influence on tweet engagement and persuasiveness.
Overall, our findings indicate that bias-triggering tactics have been more
influential on Bot-authored tweets than Human-authored tweets. While certain
bias triggers boosted engagement for Bot-authored tweets, some other bias
triggers unexpectedly decreased it. Conversely, triggers of most biases
appeared to be unrelated to the engagement of Human-authored tweets. Our work
sheds light on the differential utilization and effect of persuasion strategies
between Bot-authored and Human-authored misinformation from the lens of human
biases, offering insights for the development of effective counter-measures.","['Lynnette Hui Xian Ng', 'Wenqi Zhou', 'Kathleen M. Carley']",13,0.75885236
"This survey addresses the critical challenge of deepfake detection amidst the
rapid advancements in artificial intelligence. As AI-generated media, including
video, audio and text, become more realistic, the risk of misuse to spread
misinformation and commit identity fraud increases. Focused on face-centric
deepfakes, this work traces the evolution from traditional single-modality
methods to sophisticated multi-modal approaches that handle audio-visual and
text-visual scenarios. We provide comprehensive taxonomies of detection
techniques, discuss the evolution of generative methods from auto-encoders and
GANs to diffusion models, and categorize these technologies by their unique
attributes. To our knowledge, this is the first survey of its kind. We also
explore the challenges of adapting detection methods to new generative models
and enhancing the reliability and robustness of deepfake detectors, proposing
directions for future research. This survey offers a detailed roadmap for
researchers, supporting the development of technologies to counter the
deceptive use of AI in media creation, particularly facial forgery. A curated
list of all related papers can be found at
\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.","['Ping Liu', 'Qiqi Tao', 'Joey Tianyi Zhou']",11,0.8623109
"Social media platforms, particularly Telegram, play a pivotal role in shaping
public perceptions and opinions on global and national issues. Unlike
traditional news media, Telegram allows for the proliferation of user-generated
content with minimal oversight, making it a significant venue for the spread of
controversial and misinformative content. During the COVID-19 pandemic,
Telegram's popularity surged in Singapore, a country with one of the highest
rates of social media use globally. We leverage Singapore-based Telegram data
to analyze information flows within groups focused on COVID-19 and climate
change. Using k-means clustering, we identified distinct user archetypes,
including Strategic Disruptor, Empirical Enthusiast, Inquisitive Moderate, and
Critical Examiner, each contributing uniquely to the discourse. We developed a
model to classify users into these clusters (Precision: Climate change: 0.99;
COVID-19: 0.95).","['Val Alvern Cueco Ligo', 'Lam Yin Cheung', 'Roy Ka-Wei Lee', 'Koustuv Saha', 'Edson C. Tandoc Jr.', 'Navin Kumar']",5,0.65851676
"The integration of Large Language Models (LLMs) in social robotics presents a
unique set of ethical challenges and social impacts. This research is set out
to identify ethical considerations that arise in the design and development of
these two technologies in combination. Using LLMs for social robotics may
provide benefits, such as enabling natural language open-domain dialogues.
However, the intersection of these two technologies also gives rise to ethical
concerns related to misinformation, non-verbal cues, emotional disruption, and
biases. The robot's physical social embodiment adds complexity, as ethical
hazards associated with LLM-based Social AI, such as hallucinations and
misinformation, can be exacerbated due to the effects of physical embodiment on
social perception and communication. To address these challenges, this study
employs an empirical design justice-based methodology, focusing on identifying
socio-technical ethical considerations through a qualitative co-design and
interaction study. The purpose of the study is to identify ethical
considerations relevant to the process of co-design of, and interaction with a
humanoid social robot as the interface of a LLM, and to evaluate how a design
justice methodology can be used in the context of designing LLMs-based social
robotics. The findings reveal a mapping of ethical considerations arising in
four conceptual dimensions: interaction, co-design, terms of service and
relationship and evaluates how a design justice approach can be used
empirically in the intersection of LLMs and social robotics.",['Alva Markelius'],9,0.62899673
"Amid growing concerns over AI's societal risks--ranging from civilizational
collapse to misinformation and systemic bias--this study explores the
perceptions of AI experts and the general US registered voters on the
likelihood and impact of 18 specific AI risks, alongside their policy
preferences for managing these risks. While both groups favor international
oversight over national or corporate governance, our survey reveals a
discrepancy: voters perceive AI risks as both more likely and more impactful
than experts, and also advocate for slower AI development. Specifically, our
findings indicate that policy interventions may best assuage collective
concerns if they attempt to more carefully balance mitigation efforts across
all classes of societal-scale risks, effectively nullifying the
near-vs-long-term debate over AI risks. More broadly, our results will serve
not only to enable more substantive policy discussions for preventing and
mitigating AI risks, but also to underscore the challenge of consensus building
for effective policy implementation.","['Ross Gruetzemacher', 'Toby D. Pilditch', 'Huigang Liang', 'Christy Manning', 'Vael Gates', 'David Moss', 'James W. B. Elsey', 'Willem W. A. Sleegers', 'Kyle Kilian']",9,0.71197414
"Detecting subjectivity in news sentences is crucial for identifying media
bias, enhancing credibility, and combating misinformation by flagging
opinion-based content. It provides insights into public sentiment, empowers
readers to make informed decisions, and encourages critical thinking. While
research has developed methods and systems for this purpose, most efforts have
focused on English and other high-resourced languages. In this study, we
present the first large dataset for subjectivity detection in Arabic,
consisting of ~3.6K manually annotated sentences, and GPT-4o based explanation.
In addition, we included instructions (both in English and Arabic) to
facilitate LLM based fine-tuning. We provide an in-depth analysis of the
dataset, annotation process, and extensive benchmark results, including PLMs
and LLMs. Our analysis of the annotation process highlights that annotators
were strongly influenced by their political, cultural, and religious
backgrounds, especially at the beginning of the annotation process. The
experimental results suggest that LLMs with in-context learning provide better
performance. We aim to release the dataset and resources for the community.","['Reem Suwaileh', 'Maram Hasanain', 'Fatema Hubail', 'Wajdi Zaghouani', 'Firoj Alam']",8,0.7269708
"The rapid spread of information through mobile devices and media has led to
the widespread of false or deceptive news, causing significant concerns in
society. Among different types of misinformation, image repurposing, also known
as out-of-context misinformation, remains highly prevalent and effective.
However, current approaches for detecting out-of-context misinformation often
lack interpretability and offer limited explanations. In this study, we propose
a logic regularization approach for out-of-context detection called LOGRAN
(LOGic Regularization for out-of-context ANalysis). The primary objective of
LOGRAN is to decompose the out-of-context detection at the phrase level. By
employing latent variables for phrase-level predictions, the final prediction
of the image-caption pair can be aggregated using logical rules. The latent
variables also provide an explanation for how the final result is derived,
making this fine-grained detection method inherently explanatory. We evaluate
the performance of LOGRAN on the NewsCLIPpings dataset, showcasing competitive
overall results. Visualized examples also reveal faithful phrase-level
predictions of out-of-context images, accompanied by explanations. This
highlights the effectiveness of our approach in addressing out-of-context
detection and enhancing interpretability.","['Huanhuan Ma', 'Jinghao Zhang', 'Qiang Liu', 'Shu Wu', 'Liang Wang']",2,0.6717874
"With the rise of digital communication, memes have become a significant
medium for cultural and political expression that is often used to mislead
audiences. Identification of such misleading and persuasive multimodal content
has become more important among various stakeholders, including social media
platforms, policymakers, and the broader society as they often cause harm to
individuals, organizations, and/or society. While there has been effort to
develop AI-based automatic systems for resource-rich languages (e.g., English),
it is relatively little to none for medium to low resource languages. In this
study, we focused on developing an Arabic memes dataset with manual annotations
of propagandistic content. We annotated ~6K Arabic memes collected from various
social media platforms, which is a first resource for Arabic multimodal
research. We provide a comprehensive analysis aiming to develop computational
tools for their detection. We will make them publicly available for the
community.","['Firoj Alam', 'Abul Hasnat', 'Fatema Ahmed', 'Md Arid Hasan', 'Maram Hasanain']",8,0.63180256
"This study explores the sycophantic tendencies of Large Language Models
(LLMs), where these models tend to provide answers that match what users want
to hear, even if they are not entirely correct. The motivation behind this
exploration stems from the common behavior observed in individuals searching
the internet for facts with partial or misleading knowledge. Similar to using
web search engines, users may recall fragments of misleading keywords and
submit them to an LLM, hoping for a comprehensive response. Our empirical
analysis of several LLMs shows the potential danger of these models amplifying
misinformation when presented with misleading keywords. Additionally, we
thoroughly assess four existing hallucination mitigation strategies to reduce
LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of
these strategies for generating factually correct statements. Furthermore, our
analyses delve into knowledge-probing experiments on factual keywords and
different categories of sycophancy mitigation.","['Aswin RRV', 'Nemika Tyagi', 'Md Nayem Uddin', 'Neeraj Varshney', 'Chitta Baral']",6,0.7923515
"The spread of propaganda, misinformation, and biased narratives from
autocratic regimes, especially on social media, is a growing concern in many
democracies. Can censorship be an effective tool to curb the spread of such
slanted narratives? In this paper, we study the European Union's ban on Russian
state-led news outlets after the 2022 Russian invasion of Ukraine. We analyze
775,616 tweets from 133,276 users on Twitter/X, employing a
difference-in-differences strategy. We show that the ban reduced pro-Russian
slant among users who had previously directly interacted with banned outlets.
The impact is most pronounced among users with the highest pre-ban slant
levels. However, this effect was short-lived, with slant returning to its
pre-ban levels within two weeks post-enforcement. Additionally, we find a
detectable albeit less pronounced indirect effect on users who had not directly
interacted with the outlets before the ban. We provide evidence that other
suppliers of propaganda may have actively sought to mitigate the ban's
influence by intensifying their activity, effectively counteracting the
persistence and reach of the ban.","['Marcel Caesmann', 'Janis Goldzycher', 'Matteo Grigoletto', 'Lorenz Gschwent']",10,0.71628207
"Health-related misinformation on social networks can lead to poor
decision-making and real-world dangers. Such misinformation often misrepresents
scientific publications and cites them as ""proof"" to gain perceived
credibility. To effectively counter such claims automatically, a system must
explain how the claim was falsely derived from the cited publication. Current
methods for automated fact-checking or fallacy detection neglect to assess the
(mis)used evidence in relation to misinformation claims, which is required to
detect the mismatch between them. To address this gap, we introduce Missci, a
novel argumentation theoretical model for fallacious reasoning together with a
new dataset for real-world misinformation detection that misrepresents
biomedical publications. Unlike previous fallacy detection datasets, Missci (i)
focuses on implicit fallacies between the relevant content of the cited
publication and the inaccurate claim, and (ii) requires models to verbalize the
fallacious reasoning in addition to classifying it. We present Missci as a
dataset to test the critical reasoning abilities of large language models
(LLMs), that are required to reconstruct real-world fallacious arguments, in a
zero-shot setting. We evaluate two representative LLMs and the impact of
different levels of detail about the fallacy classes provided to the LLM via
prompts. Our experiments and human evaluation show promising results for GPT 4,
while also demonstrating the difficulty of this task.","['Max Glockner', 'Yufang Hou', 'Preslav Nakov', 'Iryna Gurevych']",1,0.78457576
"In an era increasingly influenced by artificial intelligence, the detection
of fake news is crucial, especially in contexts like election seasons where
misinformation can have significant societal impacts. This study evaluates the
effectiveness of various LLMs in identifying and filtering fake news content.
Utilizing a comparative analysis approach, we tested four large LLMs -- GPT-4,
Claude 3 Sonnet, Gemini Pro 1.0, and Mistral Large -- and two smaller LLMs --
Gemma 7B and Mistral 7B. By using fake news dataset samples from Kaggle, this
research not only sheds light on the current capabilities and limitations of
LLMs in fake news detection but also discusses the implications for developers
and policymakers in enhancing AI-driven informational integrity.","['Sahas Koka', 'Anthony Vuong', 'Anish Kataria']",6,0.7564926
"Diffusion-based re-ranking is a common method used for retrieving instances
by performing similarity propagation in a nearest neighbor graph. However,
existing techniques that construct the affinity graph based on pairwise
instances can lead to the propagation of misinformation from outliers and other
manifolds, resulting in inaccurate results. To overcome this issue, we propose
a novel Cluster-Aware Similarity (CAS) diffusion for instance retrieval. The
primary concept of CAS is to conduct similarity diffusion within local
clusters, which can reduce the influence from other manifolds explicitly. To
obtain a symmetrical and smooth similarity matrix, our Bidirectional Similarity
Diffusion strategy introduces an inverse constraint term to the optimization
objective of local cluster diffusion. Additionally, we have optimized a
Neighbor-guided Similarity Smoothing approach to ensure similarity consistency
among the local neighbors of each instance. Evaluations in instance retrieval
and object re-identification validate the effectiveness of the proposed CAS,
our code is publicly available.","['Jifei Luo', 'Hantao Yao', 'Changsheng Xu']",2,0.71277094
"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
pose significant risks, particularly in the realm of online election
interference. This paper explores the nefarious applications of GenAI,
highlighting their potential to disrupt democratic processes through deepfakes,
botnets, targeted misinformation campaigns, and synthetic identities. By
examining recent case studies and public incidents, we illustrate how malicious
actors exploit these technologies to try influencing voter behavior, spread
disinformation, and undermine public trust in electoral systems. The paper also
discusses the societal implications of these threats, emphasizing the urgent
need for robust mitigation strategies and international cooperation to
safeguard democratic integrity.",['Emilio Ferrara'],9,0.7517475
"We consider the problem of recovering the ground truth ordering (ranking,
top-$k$, or others) over a large number of alternatives. The wisdom of crowd is
a heuristic approach based on Condorcet's Jury theorem to address this problem
through collective opinions. This approach fails to recover the ground truth
when the majority of the crowd is misinformed. The surprisingly popular (SP)
algorithm cite{prelec2017solution} is an alternative approach that is able to
recover the ground truth even when experts are in minority. The SP algorithm
requires the voters to predict other voters' report in the form of a full
probability distribution over all rankings of alternatives. However, when the
number of alternatives, $m$, is large, eliciting the prediction report or even
the vote over $m$ alternatives might be too costly. In this paper, we design a
scalable alternative of the SP algorithm which only requires eliciting partial
preferences from the voters, and propose new variants of the SP algorithm. In
particular, we propose two versions -- Aggregated-SP and Partial-SP -- that ask
voters to report vote and prediction on a subset of size $k$ ($\ll m$) in terms
of top alternative, partial rank, or an approval set. Through a large-scale
crowdsourcing experiment on MTurk, we show that both of our approaches
outperform conventional preference aggregation algorithms for the recovery of
ground truth rankings, when measured in terms of Kendall-Tau distance and
Spearman's $\rho$. We further analyze the collected data and demonstrate that
voters' behavior in the experiment, including the minority of the experts, and
the SP phenomenon, can be correctly simulated by a concentric mixtures of
Mallows model. Finally, we provide theoretical bounds on the sample complexity
of SP algorithms with partial rankings to demonstrate the theoretical
guarantees of the proposed methods.","['Hadi Hosseini', 'Debmalya Mandal', 'Amrit Puhan']",2,0.6509434
"An infodemic refers to an enormous amount of true information and
misinformation disseminated during a disease outbreak. Detecting misinformation
at the early stage of an infodemic is key to manage it and reduce its harm to
public health. An early stage infodemic is characterized by a large volume of
unlabeled information concerning a disease. As a result, conventional
misinformation detection methods are not suitable for this misinformation
detection task because they rely on labeled information in the infodemic domain
to train their models. To address the limitation of conventional methods,
state-of-the-art methods learn their models using labeled information in other
domains to detect misinformation in the infodemic domain. The efficacy of these
methods depends on their ability to mitigate both covariate shift and concept
shift between the infodemic domain and the domains from which they leverage
labeled information. These methods focus on mitigating covariate shift but
overlook concept shift, rendering them less effective for the task. In
response, we theoretically show the necessity of tackling both covariate shift
and concept shift as well as how to operationalize each of them. Built on the
theoretical analysis, we develop a novel misinformation detection method that
addresses both covariate shift and concept shift. Using two real-world
datasets, we conduct extensive empirical evaluations to demonstrate the
superior performance of our method over state-of-the-art misinformation
detection methods as well as prevalent domain adaptation methods that can be
tailored to solve the misinformation detection task.","['Minjia Mao', 'Xiaohang Zhao', 'Xiao Fang']",0,0.65436375
"The rapid advancement of Large Language Models (LLMs) has ushered in an era
where AI-generated text is increasingly indistinguishable from human-generated
content. Detecting AI-generated text has become imperative to combat
misinformation, ensure content authenticity, and safeguard against malicious
uses of AI. In this paper, we propose a novel hybrid approach that combines
traditional TF-IDF techniques with advanced machine learning models, including
Bayesian classifiers, Stochastic Gradient Descent (SGD), Categorical Gradient
Boosting (CatBoost), and 12 instances of Deberta-v3-large models. Our approach
aims to address the challenges associated with detecting AI-generated text by
leveraging the strengths of both traditional feature extraction methods and
state-of-the-art deep learning models. Through extensive experiments on a
comprehensive dataset, we demonstrate the effectiveness of our proposed method
in accurately distinguishing between human and AI-generated text. Our approach
achieves superior performance compared to existing methods. This research
contributes to the advancement of AI-generated text detection techniques and
lays the foundation for developing robust solutions to mitigate the challenges
posed by AI-generated content.","['Ye Zhang', 'Qian Leng', 'Mengran Zhu', 'Rui Ding', 'Yue Wu', 'Jintong Song', 'Yulu Gong']",9,0.69834864
"An analysis drawing on Signal Detection Theory suggests that people may fall
for misinformation because they are unable to discern true from false
information (truth insensitivity) or because they tend to accept information
with a particular slant regardless of whether it is true or false (belief
bias). Three preregistered experiments with participants from the United States
and the United Kingdom (N = 961) revealed that (i) truth insensitivity in
responses to (mis)information about COVID-19 vaccines differed as a function of
prior attitudes toward COVID-19 vaccines; (ii) participants exhibited a strong
belief bias favoring attitude-congruent information; (iii) truth insensitivity
and belief bias jointly predicted acceptance of false information about
COVID-19 vaccines, but belief bias was a much stronger predictor; (iv)
cognitive elaboration increased truth sensitivity without reducing belief bias;
and (v) higher levels of confidence in one's beliefs were associated with
greater belief bias. The findings provide insights into why people fall for
misinformation, which is essential for individual-level interventions to reduce
susceptibility to misinformation.","['Lea S. Nahon', 'Nyx L. Ng', 'Bertram Gawronski']",12,0.66754127
"The wide acceptance of large language models (LLMs) has unlocked new
applications and social risks. Popular countermeasures aim at detecting
misinformation, usually involve domain specific models trained to recognize the
relevance of any information. Instead of evaluating the validity of the
information, we propose to investigate LLM generated text from the perspective
of trust. In this study, we define trust as the ability to know if an input
text was generated by a LLM or a human. To do so, we design SPOT, an efficient
method, that classifies the source of any, standalone, text input based on
originality score. This score is derived from the prediction of a given LLM to
detect other LLMs. We empirically demonstrate the robustness of the method to
the architecture, training data, evaluation data, task and compression of
modern LLMs.","['Edouard Yvinec', 'Gabriel Kasser']",6,0.8356365
"WhatsApp groups have become a hotbed for the propagation of harmful content
including misinformation, hate speech, polarizing content, and rumors,
especially in Global South countries. Given the platform's end-to-end
encryption, moderation responsibilities lie on group admins and members, who
rarely contest such content. Another approach is fact-checking, which is
unscalable, and can only contest factual content (e.g., misinformation) but not
subjective content (e.g., hate speech). Drawing on recent literature, we
explore deliberation -- open and inclusive discussion -- as an alternative. We
investigate the role of a conversational agent in facilitating deliberation on
harmful content in WhatsApp groups. We conducted semi-structured interviews
with 21 Indian WhatsApp users, employing a design probe to showcase an example
agent. Participants expressed the need for anonymity and recommended AI
assistance to reduce the effort required in deliberation. They appreciated the
agent's neutrality but pointed out the futility of deliberation in echo chamber
groups. Our findings highlight design tensions for such an agent, including
privacy versus group dynamics and freedom of speech in private spaces. We
discuss the efficacy of deliberation using deliberative theory as a lens,
compare deliberation with moderation and fact-checking, and provide design
recommendations for future such systems. Ultimately, this work advances CSCW by
offering insights into designing deliberative systems for combating harmful
content in private group chats on social media.","['Dhruv Agarwal', 'Farhana Shahid', 'Aditya Vashistha']",3,0.70338184
"Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.","['Michael Fore', 'Simranjit Singh', 'Chaehong Lee', 'Amritanshu Pandey', 'Antonios Anastasopoulos', 'Dimitrios Stamoulis']",6,0.7359128
"Online social media platforms such as YouTube have a wide, global reach.
However, little is known about the experience of low-resourced language
speakers on such platforms; especially in how they experience and navigate
harmful content. To better understand this, we (1) conducted semi-structured
interviews (n=15) and (2) analyzed search results (n=9313), recommendations
(n=3336), channels (n=120) and comments (n=406) of policy-violating sexual
content on YouTube focusing on the Amharic language. Our findings reveal that
-- although Amharic-speaking YouTube users find the platform crucial for
several aspects of their lives -- participants reported unplanned exposure to
policy-violating sexual content when searching for benign, popular queries.
Furthermore, malicious content creators seem to exploit under-performing
language technologies and content moderation to further target vulnerable
groups of speakers, including migrant domestic workers, diaspora, and local
Ethiopians. Overall, our study sheds light on how failures in low-resourced
language technology may lead to exposure to harmful content and suggests
implications for stakeholders in minimizing harm. Content Warning: This paper
includes discussions of NSFW topics and harmful content (hate, abuse, sexual
harassment, self-harm, misinformation). The authors do not support the creation
or distribution of harmful content.","['Hellina Hailu Nigatu', 'Inioluwa Deborah Raji']",3,0.6129869
"The integration of Generative Artificial Intelligence (GAI) and Large
Language Models (LLMs) in academia has spurred a global discourse on their
potential pedagogical benefits and ethical considerations. Positive reactions
highlight some potential, such as collaborative creativity, increased access to
education, and empowerment of trainers and trainees. However, negative
reactions raise concerns about ethical complexities, balancing innovation and
academic integrity, unequal access, and misinformation risks. Through a
systematic survey and text-mining-based analysis of global and national
directives, insights from independent research, and eighty university-level
guidelines, this study provides a nuanced understanding of the opportunities
and challenges posed by GAI and LLMs in education. It emphasizes the importance
of balanced approaches that harness the benefits of these technologies while
addressing ethical considerations and ensuring equitable access and educational
outcomes. The paper concludes with recommendations for fostering responsible
innovation and ethical practices to guide the integration of GAI and LLMs in
academia.","['Junfeng Jiao', 'Saleh Afroogh', 'Kevin Chen', 'David Atkinson', 'Amit Dhurandhar']",9,0.58031523
"With the widespread application of Large Language Models (LLMs) to various
domains, concerns regarding the trustworthiness of LLMs in safety-critical
scenarios have been raised, due to their unpredictable tendency to hallucinate
and generate misinformation. Existing LLMs do not have an inherent
functionality to provide the users with an uncertainty/confidence metric for
each response it generates, making it difficult to evaluate trustworthiness.
Although several studies aim to develop uncertainty quantification methods for
LLMs, they have fundamental limitations, such as being restricted to
classification tasks, requiring additional training and data, considering only
lexical instead of semantic information, and being prompt-wise but not
response-wise. A new framework is proposed in this paper to address these
issues. Semantic density extracts uncertainty/confidence information for each
response from a probability distribution perspective in semantic space. It has
no restriction on task types and is ""off-the-shelf"" for new models and tasks.
Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and
Mixtral-8x22B models, on four free-form question-answering benchmarks
demonstrate the superior performance and robustness of semantic density
compared to prior approaches.","['Xin Qiu', 'Risto Miikkulainen']",6,0.82529914
"Advances in generative AI (GenAI) have raised concerns about detecting and
discerning AI-generated content from human-generated content. Most existing
literature assumes a paradigm where 'expert' organized disinformation creators
and flawed AI models deceive 'ordinary' users. Based on longitudinal
ethnographic research with misinformation creators and consumers between
2022-2023, we instead find that GenAI supports bricolage work, where
non-experts increasingly use GenAI to remix, repackage, and (re)produce content
to meet their personal needs and desires. This research yielded four key
findings: First, participants primarily used GenAI for creation, rather than
truth-seeking. Second, a spreading 'influencer millionaire' narrative drove
participants to become content creators, using GenAI as a productivity tool to
generate a volume of (often misinformative) content. Third, GenAI lowered the
barrier to entry for content creation across modalities, enticing consumers to
become creators and significantly increasing existing creators' output.
Finally, participants used Gen AI to learn and deploy marketing tactics to
expand engagement and monetize their content. We argue for shifting analysis
from the public as consumers of AI content to bricoleurs who use GenAI
creatively, often without a detailed understanding of its underlying
technology. We analyze how these understudied emergent uses of GenAI produce
new or accelerated misinformation harms, and their implications for AI
products, platforms and policies.","['Amelia Hassoun', 'Ariel Abonizio', 'Katy Osborn', 'Cameron Wu', 'Beth Goldberg']",9,0.7014042
"Given a graph G, a budget k and a misinformation seed set S, Influence
Minimization (IMIN) via node blocking aims to find a set of k nodes to be
blocked such that the expected spread of S is minimized. This problem finds
important applications in suppressing the spread of misinformation and has been
extensively studied in the literature. However, existing solutions for IMIN
still incur significant computation overhead, especially when k becomes large.
In addition, there is still no approximation solution with non-trivial
theoretical guarantee for IMIN via node blocking prior to our work. In this
paper, we conduct the first attempt to propose algorithms that yield
data-dependent approximation guarantees. Based on the Sandwich framework, we
first develop submodular and monotonic lower and upper bounds for our
non-submodular objective function and prove the computation of proposed bounds
is \#P-hard. In addition, two advanced sampling methods are proposed to
estimate the value of bounding functions. Moreover, we develop two novel
martingale-based concentration bounds to reduce the sample complexity and
design two non-trivial algorithms that provide (1-1/e-\epsilon)-approximate
solutions to our bounding functions. Comprehensive experiments on 9 real-world
datasets are conducted to validate the efficiency and effectiveness of the
proposed techniques. Compared with the state-of-the-art methods, our solutions
can achieve up to two orders of magnitude speedup and provide theoretical
guarantees for the quality of returned results.","['Jinghao Wang', 'Yanping Wu', 'Xiaoyang Wang', 'Ying Zhang', 'Lu Qin', 'Wenjie Zhang', 'Xuemin Lin']",2,0.7299858
"The persistence of lying by some consumers in their online posts of
experiences with businesses is problematic, and taints the global pool of
information that is used for decision making by people that assume they are
true accounts of experiences. This study is based on data from my dissertation
about fake online Google reviews of restaurants (Berry, 2024), and leverages an
instrument that quantifies the trust of people. The findings are based on a
sample of n=351, and provide a general proxy for lying in online reviews, and
sketch out the characteristics of a typical person that has the propensity to
be untruthful. A predictive model of posting untrue online reviews is
constructed. The findings have wider implications for the study and monitoring
of deceptive behavior, including the propagation of misinformation, and a means
of quantifying the potential for antisocial behavior as measured by the trust
of people instrument in Berry (2024). Directions for future research and
limitations are also discussed.",['Shawn Berry'],0,0.61264914
"The prevalence and harms of online misinformation is a perennial concern for
internet platforms, institutions and society at large. Over time, information
shared online has become more media-heavy and misinformation has readily
adapted to these new modalities. The rise of generative AI-based tools, which
provide widely-accessible methods for synthesizing realistic audio, images,
video and human-like text, have amplified these concerns. Despite intense
public interest and significant press coverage, quantitative information on the
prevalence and modality of media-based misinformation remains scarce. Here, we
present the results of a two-year study using human raters to annotate online
media-based misinformation, mostly focusing on images, based on claims assessed
in a large sample of publicly-accessible fact checks with the ClaimReview
markup. We present an image typology, designed to capture aspects of the image
and manipulation relevant to the image's role in the misinformation claim. We
visualize the distribution of these types over time. We show the rise of
generative AI-based content in misinformation claims, and that its commonality
is a relatively recent phenomenon, occurring significantly after heavy press
coverage. We also show ""simple"" methods dominated historically, particularly
context manipulations, and continued to hold a majority as of the end of data
collection in November 2023. The dataset, Annotated Misinformation, Media-Based
(AMMeBa), is publicly-available, and we hope that these data will serve as both
a means of evaluating mitigation methods in a realistic setting and as a
first-of-its-kind census of the types and modalities of online misinformation.","['Nicholas Dufour', 'Arkanath Pathak', 'Pouya Samangouei', 'Nikki Hariri', 'Shashi Deshetti', 'Andrew Dudfield', 'Christopher Guess', 'Pablo Hern√°ndez Escayola', 'Bobby Tran', 'Mevan Babakar', 'Christoph Bregler']",0,0.7871996
"Text-to-SQL models are pivotal for making Electronic Health Records (EHRs)
accessible to healthcare professionals without SQL knowledge. With the
advancements in large language models, these systems have become more adept at
translating complex questions into SQL queries. Nonetheless, the critical need
for reliability in healthcare necessitates these models to accurately identify
unanswerable questions or uncertain predictions, preventing misinformation. To
address this problem, we present a self-training strategy using pseudo-labeled
unanswerable questions to enhance the reliability of text-to-SQL models for
EHRs. This approach includes a two-stage training process followed by a
filtering method based on the token entropy and query execution. Our
methodology's effectiveness is validated by our top performance in the EHRSQL
2024 shared task, showcasing the potential to improve healthcare
decision-making through more reliable text-to-SQL systems.","['Yongrae Jo', 'Seongyun Lee', 'Minju Seo', 'Sung Ju Hwang', 'Moontae Lee']",1,0.57728875
"One way to personalize chatbot interactions is by establishing common ground
with the intended reader. A domain where establishing mutual understanding
could be particularly impactful is vaccine concerns and misinformation. Vaccine
interventions are forms of messaging which aim to answer concerns expressed
about vaccination. Tailoring responses in this domain is difficult, since
opinions often have seemingly little ideological overlap. We define the task of
tailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring
responses to a CGO involves meaningfully improving the answer by relating it to
an opinion or belief the reader holds. In this paper we introduce TAILOR-CGO, a
dataset for evaluating how well responses are tailored to provided CGOs. We
benchmark several major LLMs on this task; finding GPT-4-Turbo performs
significantly better than others. We also build automatic evaluation metrics,
including an efficient and accurate BERT model that outperforms finetuned LLMs,
investigate how to successfully tailor vaccine messaging to CGOs, and provide
actionable recommendations from this investigation.
  Code and model weights: https://github.com/rickardstureborg/tailor-cgo
Dataset: https://huggingface.co/datasets/DukeNLP/tailor-cgo","['Rickard Stureborg', 'Sanxing Chen', 'Ruoyu Xie', 'Aayushi Patel', 'Christopher Li', 'Chloe Qinyu Zhu', 'Tingnan Hu', 'Jun Yang', 'Bhuwan Dhingra']",12,0.6230836
"Diaspora communities are disproportionately impacted by off-the-radar
misinformation and often neglected by mainstream fact-checking efforts,
creating a critical need to scale-up efforts of nascent fact-checking
initiatives. In this paper we present SynDy, a framework for Synthetic Dynamic
Dataset Generation to leverage the capabilities of the largest frontier Large
Language Models (LLMs) to train local, specialized language models. To the best
of our knowledge, SynDy is the first paper utilizing LLMs to create
fine-grained synthetic labels for tasks of direct relevance to misinformation
mitigation, namely Claim Matching, Topical Clustering, and Claim Relationship
Classification. SynDy utilizes LLMs and social media queries to automatically
generate distantly-supervised, topically-focused datasets with synthetic labels
on these three tasks, providing essential tools to scale up human-led
fact-checking at a fraction of the cost of human-annotated data. Training on
SynDy's generated labels shows improvement over a standard baseline and is not
significantly worse compared to training on human labels (which may be
infeasible to acquire). SynDy is being integrated into Meedan's chatbot
tiplines that are used by over 50 organizations, serve over 230K users
annually, and automatically distribute human-written fact-checks via messaging
apps such as WhatsApp. SynDy will also be integrated into our deployed
Co-Insights toolkit, enabling low-resource organizations to launch tiplines for
their communities. Finally, we envision SynDy enabling additional fact-checking
tools such as matching new misinformation claims to high-quality explainers on
common misinformation topics.","['Michael Shliselberg', 'Ashkan Kazemi', 'Scott A. Hale', 'Shiri Dori-Hacohen']",1,0.6038549
"Misinformation about climate change is a complex societal issue requiring
holistic, interdisciplinary solutions at the intersection between technology
and psychology. One proposed solution is a ""technocognitive"" approach,
involving the synthesis of psychological and computer science research.
Psychological research has identified that interventions in response to
misinformation require both fact-based (e.g., factual explanations) and
technique-based (e.g., explanations of misleading techniques) content. However,
little progress has been made on documenting and detecting fallacies in climate
misinformation. In this study, we apply a previously developed critical
thinking methodology for deconstructing climate misinformation, in order to
develop a dataset mapping different types of climate misinformation to
reasoning fallacies. This dataset is used to train a model to detect fallacies
in climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better
than previous works. The fallacies that are easiest to detect include fake
experts and anecdotal arguments, while fallacies that require background
knowledge, such as oversimplification, misrepresentation, and slothful
induction, are relatively more difficult to detect. This research lays the
groundwork for development of solutions where automatically detected climate
misinformation can be countered with generative technique-based corrections.","['Francisco Zanartu', 'John Cook', 'Markus Wagner', 'Julian Garcia']",1,0.65592694
"Most Americans agree that misinformation, hate speech and harassment are
harmful and inadequately curbed on social media through current moderation
practices. In this paper, we aim to understand the discursive strategies
employed by people in response to harmful speech in news comments. We conducted
a content analysis of more than 6500 comment replies to trending news videos on
YouTube and Twitter and identified seven distinct discursive objection
strategies (Study 1). We examined the frequency of each strategy's occurrence
from the 6500 comment replies, as well as from a second sample of 2004 replies
(Study 2). Together, these studies show that people deploy a diversity of
discursive strategies when objecting to speech, and reputational attacks are
the most common. The resulting classification scheme accounts for different
theoretical approaches for expressing objections and offers a comprehensive
perspective on grassroots efforts aimed at stopping offensive or problematic
speech on campus.","['Ashley L. Shea', 'Aspen K. B. Omapang', 'Ji Yong Cho', 'Miryam Y. Ginsparg', 'Natalie Bazarova', 'Winice Hui', 'Ren√© F. Kizilcec', 'Chau Tong', 'Drew Margolin']",3,0.5800998
"Fact-checking is essential due to the explosion of misinformation in the
media ecosystem. Although false information exists in every language and
country, most research to solve the problem mainly concentrated on huge
communities like English and Chinese. Low-resource languages like Vietnamese
are necessary to explore corpora and models for fact verification. To bridge
this gap, we construct ViWikiFC, the first manual annotated open-domain corpus
for Vietnamese Wikipedia Fact Checking more than 20K claims generated by
converting evidence sentences extracted from Wikipedia articles. We analyze our
corpus through many linguistic aspects, from the new dependency rate, the new
n-gram rate, and the new word rate. We conducted various experiments for
Vietnamese fact-checking, including evidence retrieval and verdict prediction.
BM25 and InfoXLM (Large) achieved the best results in two tasks, with BM25
achieving an accuracy of 88.30% for SUPPORTS, 86.93% for REFUTES, and only
56.67% for the NEI label in the evidence retrieval task, InfoXLM (Large)
achieved an F1 score of 86.51%. Furthermore, we also conducted a pipeline
approach, which only achieved a strict accuracy of 67.00% when using InfoXLM
(Large) and BM25. These results demonstrate that our dataset is challenging for
the Vietnamese language model in fact-checking tasks.","['Hung Tuan Le', 'Long Truong To', 'Manh Trong Nguyen', 'Kiet Van Nguyen']",8,0.7102643
"Nowadays, Information spreads at an unprecedented pace in social media and
discerning truth from misinformation and fake news has become an acute societal
challenge. Machine learning (ML) models have been employed to identify fake
news but are far from perfect with challenging problems like limited accuracy,
interpretability, and generalizability. In this paper, we enhance ML-based
solutions with linguistics input and we propose LingML, linguistic-informed ML,
for fake news detection. We conducted an experimental study with a popular
dataset on fake news during the pandemic. The experiment results show that our
proposed solution is highly effective. There are fewer than two errors out of
every ten attempts with only linguistic input used in ML and the knowledge is
highly explainable. When linguistics input is integrated with advanced
large-scale ML models for natural language processing, our solution outperforms
existing ones with 1.8% average error rate. LingML creates a new path with
linguistics to push the frontier of effective and efficient fake news
detection. It also sheds light on real-world multi-disciplinary applications
requiring both ML and domain expertise to achieve optimal performance.","['Jasraj Singh', 'Fang Liu', 'Hong Xu', 'Bee Chin Ng', 'Wei Zhang']",8,0.76327705
"In the digital age, the prevalence of misleading news headlines poses a
significant challenge to information integrity, necessitating robust detection
mechanisms. This study explores the efficacy of Large Language Models (LLMs) in
identifying misleading versus non-misleading news headlines. Utilizing a
dataset of 60 articles, sourced from both reputable and questionable outlets
across health, science & tech, and business domains, we employ three LLMs-
ChatGPT-3.5, ChatGPT-4, and Gemini-for classification. Our analysis reveals
significant variance in model performance, with ChatGPT-4 demonstrating
superior accuracy, especially in cases with unanimous annotator agreement on
misleading headlines. The study emphasizes the importance of human-centered
evaluation in developing LLMs that can navigate the complexities of
misinformation detection, aligning technical proficiency with nuanced human
judgment. Our findings contribute to the discourse on AI ethics, emphasizing
the need for models that are not only technically advanced but also ethically
aligned and sensitive to the subtleties of human interpretation.","['Md Main Uddin Rony', 'Md Mahfuzul Haque', 'Mohammad Ali', 'Ahmed Shatil Alam', 'Naeemul Hassan']",8,0.7126851
"Scale is often attributed as one of the factors that cause an increase in the
performance of LLMs, resulting in models with billion and trillion parameters.
One of the limitations of such large models is the high computational
requirements that limit their usage, deployment, and debugging in
resource-constrained scenarios. Two commonly used alternatives to bypass these
limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of
Llama 70B) and lower the memory requirements by using quantization. While these
approaches effectively address the limitation of resources, their impact on
model performance needs thorough examination. In this study, we perform a
comprehensive evaluation to investigate the effect of model scale and
quantization on the performance. We experiment with two major families of
open-source instruct models ranging from 7 billion to 70 billion parameters.
Our extensive zero-shot experiments across various tasks including natural
language understanding, reasoning, misinformation detection, and hallucination
reveal that larger models generally outperform their smaller counterparts,
suggesting that scale remains an important factor in enhancing performance. We
found that larger models show exceptional resilience to precision reduction and
can maintain high accuracy even at 4-bit quantization for numerous tasks and
they serve as a better solution than using smaller models at high precision
under similar memory requirements.","['Sher Badshah', 'Hassan Sajjad']",6,0.64508784
"Knowledge editing methods (KEs) can update language models' obsolete or
inaccurate knowledge learned from pre-training. However, KEs can be used for
malicious applications, e.g., inserting misinformation and toxic content.
Knowing whether a generated output is based on edited knowledge or first-hand
knowledge from pre-training can increase users' trust in generative models and
provide more transparency. Driven by this, we propose a novel task: detecting
edited knowledge in language models. Given an edited model and a fact retrieved
by a prompt from an edited model, the objective is to classify the knowledge as
either unedited (based on the pre-training), or edited (based on subsequent
editing). We instantiate the task with four KEs, two LLMs, and two datasets.
Additionally, we propose using the hidden state representations and the
probability distributions as features for the detection. Our results reveal
that, using these features as inputs to a simple AdaBoost classifiers
establishes a strong baseline. This classifier requires only a limited amount
of data and maintains its performance even in cross-domain settings. Last, we
find it more challenging to distinguish edited knowledge from unedited but
related knowledge, highlighting the need for further research. Our work lays
the groundwork for addressing malicious model editing, which is a critical
challenge associated with the strong generative capabilities of LLMs.","['Paul Youssef', 'Zhixue Zhao', 'J√∂rg Schl√∂tterer', 'Christin Seifert']",1,0.76371884
"The prevalence of unwarranted beliefs, spanning pseudoscience, logical
fallacies, and conspiracy theories, presents substantial societal hurdles and
the risk of disseminating misinformation. Utilizing established psychometric
assessments, this study explores the capabilities of large language models
(LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We
undertake a philosophical inquiry, juxtaposing the rationality of humans
against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs
to counter misconceptions, drawing upon psychological models of persuasion such
as cognitive dissonance theory and elaboration likelihood theory. Through this
endeavor, we highlight the potential of LLMs as personalized misinformation
debunking agents.","['Sowmya S Sundaram', 'Balaji Alwar']",6,0.7070112
"We introduce 'FactCheck Editor', an advanced text editor designed to automate
fact-checking and correct factual inaccuracies. Given the widespread issue of
misinformation, often a result of unintentional mistakes by content creators,
our tool aims to address this challenge. It supports over 90 languages and
utilizes transformer models to assist humans in the labor-intensive process of
fact verification. This demonstration showcases a complete workflow that
detects text claims in need of verification, generates relevant search engine
queries, and retrieves appropriate documents from the web. It employs Natural
Language Inference (NLI) to predict the veracity of claims and uses LLMs to
summarize the evidence and suggest textual revisions to correct any errors in
the text. Additionally, the effectiveness of models used in claim detection and
veracity assessment is evaluated across multiple languages.",['Vinay Setty'],1,0.796013
"In the current digital era, the rapid spread of misinformation on online
platforms presents significant challenges to societal well-being, public trust,
and democratic processes, influencing critical decision making and public
opinion. To address these challenges, there is a growing need for automated
fake news detection mechanisms. Pre-trained large language models (LLMs) have
demonstrated exceptional capabilities across various natural language
processing (NLP) tasks, prompting exploration into their potential for
verifying news claims. Instead of employing LLMs in a non-agentic way, where
LLMs generate responses based on direct prompts in a single shot, our work
introduces FactAgent, an agentic approach of utilizing LLMs for fake news
detection. FactAgent enables LLMs to emulate human expert behavior in verifying
news claims without any model training, following a structured workflow. This
workflow breaks down the complex task of news veracity checking into multiple
sub-steps, where LLMs complete simple tasks using their internal knowledge or
external tools. At the final step of the workflow, LLMs integrate all findings
throughout the workflow to determine the news claim's veracity. Compared to
manual human verification, FactAgent offers enhanced efficiency. Experimental
studies demonstrate the effectiveness of FactAgent in verifying claims without
the need for any training process. Moreover, FactAgent provides transparent
explanations at each step of the workflow and during final decision-making,
offering insights into the reasoning process of fake news detection for end
users. FactAgent is highly adaptable, allowing for straightforward updates to
its tools that LLMs can leverage within the workflow, as well as updates to the
workflow itself using domain knowledge. This adaptability enables FactAgent's
application to news verification across various domains.","['Xinyi Li', 'Yongfeng Zhang', 'Edward C. Malthouse']",6,0.7637582
"Pollution of online social spaces caused by rampaging d/misinformation is a
growing societal concern. However, recent decisions to reduce access to social
media APIs are causing a shortage of publicly available, recent, social media
data, thus hindering the advancement of computational social science as a
whole. We present a large, high-coverage dataset of social interactions and
user-generated content from Bluesky Social to address this pressing issue. The
dataset contains the complete post history of over 4M users (81% of all
registered accounts), totalling 235M posts. We also make available social data
covering follow, comment, repost, and quote interactions. Since Bluesky allows
users to create and bookmark feed generators (i.e., content recommendation
algorithms), we also release the full output of several popular algorithms
available on the platform, along with their timestamped ``like'' interactions
and time of bookmarking. This dataset allows unprecedented analysis of online
behavior and human-machine engagement patterns. Notably, it provides
ground-truth data for studying the effects of content exposure and
self-selection and performing content virality and diffusion analysis.","['Andrea Failla', 'Giulio Rossetti']",3,0.66976345
"Automated fact-checking (AFC) is garnering increasing attention by
researchers aiming to help fact-checkers combat the increasing spread of
misinformation online. While many existing AFC methods incorporate external
information from the Web to help examine the veracity of claims, they often
overlook the importance of verifying the source and quality of collected
""evidence"". One overlooked challenge involves the reliance on ""leaked
evidence"", information gathered directly from fact-checking websites and used
to train AFC systems, resulting in an unrealistic setting for early
misinformation detection. Similarly, the inclusion of information from
unreliable sources can undermine the effectiveness of AFC systems. To address
these challenges, we present a comprehensive approach to evidence verification
and filtering. We create the ""CREDible, Unreliable or LEaked"" (CREDULE)
dataset, which consists of 91,632 articles classified as Credible, Unreliable
and Fact checked (Leaked). Additionally, we introduce the EVidence VERification
Network (EVVER-Net), trained on CREDULE to detect leaked and unreliable
evidence in both short and long texts. EVVER-Net can be used to filter evidence
collected from the Web, thus enhancing the robustness of end-to-end AFC
systems. We experiment with various language models and show that EVVER-Net can
demonstrate impressive performance of up to 91.5% and 94.4% accuracy, while
leveraging domain credibility scores along with short or long texts,
respectively. Finally, we assess the evidence provided by widely-used
fact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and
VERITE, some of which exhibit concerning rates of leaked and unreliable
evidence.","['Zacharias Chrysidis', 'Stefanos-Iordanis Papadopoulos', 'Symeon Papadopoulos', 'Panagiotis C. Petrantonakis']",1,0.74015963
"Telegram has grown into a significant platform for news and information
sharing, favored for its anonymity and minimal moderation. This openness,
however, makes it vulnerable to misinformation and conspiracy theories. In this
study, we explore the dynamics of conspiratorial narrative dissemination within
Telegram, focusing on Italian and English landscapes. In particular, we
leverage the mechanism of message forwarding within Telegram and collect two
extensive datasets through snowball strategy. We adopt a network-based approach
and build the Italian and English Telegram networks to reveal their respective
communities. By employing topic modeling, we uncover distinct narratives and
dynamics of misinformation spread. Results highlight differences between
Italian and English conspiracy landscapes, with Italian discourse involving
assorted conspiracy theories and alternative news sources intertwined with
legitimate news sources, whereas English discourse is characterized by a more
focused approach on specific narratives such as QAnon and political
conspiracies. Finally, we show that our methodology exhibits robustness across
initial seed selections, suggesting broader applicability. This study
contributes to understanding information and misinformation spread on Italian
and English Telegram ecosystems through the mechanism of message forwarding","['Lorenzo Alvisi', 'Serena Tardelli', 'Maurizio Tesconi']",4,0.61740255
"In the battle against widespread online misinformation, a growing problem is
text-image inconsistency, where images are misleadingly paired with texts with
different intent or meaning. Existing classification-based methods for
text-image inconsistency can identify contextual inconsistencies but fail to
provide explainable justifications for their decisions that humans can
understand. Although more nuanced, human evaluation is impractical at scale and
susceptible to errors. To address these limitations, this study introduces
D-TIIL (Diffusion-based Text-Image Inconsistency Localization), which employs
text-to-image diffusion models to localize semantic inconsistencies in text and
image pairs. These models, trained on large-scale datasets act as ``omniscient""
agents that filter out irrelevant information and incorporate background
knowledge to identify inconsistencies. In addition, D-TIIL uses text embeddings
and modified image regions to visualize these inconsistencies. To evaluate
D-TIIL's efficacy, we introduce a new TIIL dataset containing 14K consistent
and inconsistent text-image pairs. Unlike existing datasets, TIIL enables
assessment at the level of individual words and image regions and is carefully
designed to represent various inconsistencies. D-TIIL offers a scalable and
evidence-based approach to identifying and localizing text-image inconsistency,
providing a robust framework for future research combating misinformation.","['Mingzhen Huang', 'Shan Jia', 'Zhou Zhou', 'Yan Ju', 'Jialing Cai', 'Siwei Lyu']",7,0.67826813
"This paper focuses on the opportunities and the ethical and societal risks
posed by advanced AI assistants. We define advanced AI assistants as artificial
agents with natural language interfaces, whose function is to plan and execute
sequences of actions on behalf of a user, across one or more domains, in line
with the user's expectations. The paper starts by considering the technology
itself, providing an overview of AI assistants, their technical foundations and
potential range of applications. It then explores questions around AI value
alignment, well-being, safety and malicious uses. Extending the circle of
inquiry further, we next consider the relationship between advanced AI
assistants and individual users in more detail, exploring topics such as
manipulation and persuasion, anthropomorphism, appropriate relationships, trust
and privacy. With this analysis in place, we consider the deployment of
advanced assistants at a societal scale, focusing on cooperation, equity and
access, misinformation, economic impact, the environment and how best to
evaluate advanced AI assistants. Finally, we conclude by providing a range of
recommendations for researchers, developers, policymakers and public
stakeholders.","['Iason Gabriel', 'Arianna Manzini', 'Geoff Keeling', 'Lisa Anne Hendricks', 'Verena Rieser', 'Hasan Iqbal', 'Nenad Toma≈°ev', 'Ira Ktena', 'Zachary Kenton', 'Mikel Rodriguez', 'Seliem El-Sayed', 'Sasha Brown', 'Canfer Akbulut', 'Andrew Trask', 'Edward Hughes', 'A. Stevie Bergman', 'Renee Shelby', 'Nahema Marchal', 'Conor Griffin', 'Juan Mateos-Garcia', 'Laura Weidinger', 'Winnie Street', 'Benjamin Lange', 'Alex Ingerman', 'Alison Lentz', 'Reed Enger', 'Andrew Barakat', 'Victoria Krakovna', 'John Oliver Siy', 'Zeb Kurth-Nelson', 'Amanda McCroskery', 'Vijay Bolina', 'Harry Law', 'Murray Shanahan', 'Lize Alberts', 'Borja Balle', 'Sarah de Haas', 'Yetunde Ibitoye', 'Allan Dafoe', 'Beth Goldberg', 'S√©bastien Krier', 'Alexander Reese', 'Sims Witherspoon', 'Will Hawkins', 'Maribeth Rauh', 'Don Wallace', 'Matija Franklin', 'Josh A. Goldstein', 'Joel Lehman', 'Michael Klenk', 'Shannon Vallor', 'Courtney Biles', 'Meredith Ringel Morris', 'Helen King', 'Blaise Ag√ºera y Arcas', 'William Isaac', 'James Manyika']",9,0.7408594
"Politics is one of the most prevalent topics discussed on social media
platforms, particularly during major election cycles, where users engage in
conversations about candidates and electoral processes. Malicious actors may
use this opportunity to disseminate misinformation to undermine trust in the
electoral process. The emergence of Large Language Models (LLMs) exacerbates
this issue by enabling malicious actors to generate misinformation at an
unprecedented scale. Artificial intelligence (AI)-generated content is often
indistinguishable from authentic user content, raising concerns about the
integrity of information on social networks. In this paper, we present a novel
taxonomy for characterizing election-related claims. This taxonomy provides an
instrument for analyzing election-related claims, with granular categories
related to jurisdiction, equipment, processes, and the nature of claims. We
introduce ElectAI, a novel benchmark dataset that consists of 9,900 tweets,
each labeled as human- or AI-generated. For AI-generated tweets, the specific
LLM variant that produced them is specified. We annotated a subset of 1,550
tweets using the proposed taxonomy to capture the characteristics of
election-related claims. We explored the capabilities of LLMs in extracting the
taxonomy attributes and trained various machine learning models using ElectAI
to distinguish between human- and AI-generated posts and identify the specific
LLM variant.","['Alphaeus Dmonte', 'Marcos Zampieri', 'Kevin Lybarger', 'Massimiliano Albanese', 'Genya Coulter']",9,0.67112494
"Social media users drive the spread of misinformation online by sharing posts
that include erroneous information or commenting on controversial topics with
unsubstantiated arguments often in earnest. Work on echo chambers has suggested
that users' perspectives are reinforced through repeated interactions with
like-minded peers, promoted by homophily and bias in information diffusion.
Building on long-standing interest in the social bases of language and
linguistic underpinnings of social behavior, this work explores how
conversations around misinformation are mediated through language use. We
compare a number of linguistic measures, e.g., in-/out-group cues, readability,
and discourse connectives, within and across topics of conversation and user
communities. Our findings reveal increased presence of group identity signals
and processing fluency within echo chambers during discussions of
misinformation. We discuss the specific character of these broader trends
across topics and examine contextual influences.","['Xinyu Wang', 'Jiayi Li', 'Sarah Rajtmajer']",3,0.78496194
"Misinformation about climate change poses a significant threat to societal
well-being, prompting the urgent need for effective mitigation strategies.
However, the rapid proliferation of online misinformation on social media
platforms outpaces the ability of fact-checkers to debunk false claims.
Automated detection of climate change misinformation offers a promising
solution. In this study, we address this gap by developing a two-step
hierarchical model, the Augmented CARDS model, specifically designed for
detecting contrarian climate claims on Twitter. Furthermore, we apply the
Augmented CARDS model to five million climate-themed tweets over a six-month
period in 2022. We find that over half of contrarian climate claims on Twitter
involve attacks on climate actors or conspiracy theories. Spikes in climate
contrarianism coincide with one of four stimuli: political events, natural
events, contrarian influencers, or convinced influencers. Implications for
automated responses to climate misinformation are discussed.","['Cristian Rojas', 'Frank Algra-Maschio', 'Mark Andrejevic', 'Travis Coan', 'John Cook', 'Yuan-Fang Li']",3,0.5895269
"The ""Darkverse"" could be the negative harmful area of the Metaverse; a new
virtual immersive environment for the facilitation of illicit activity such as
misinformation, fraud, harassment, and illegal marketplaces. This paper
explores the potential for inappropriate activities within the Metaverse, and
the similarities between the Darkverse and the Dark Web. Challenges and future
directions for investigation are also discussed, including user identification,
creation of privacy-preserving frameworks and other data monitoring methods.","['Raymond Chan', 'Benjamin W. J. Kwok', 'Adriel Yeo', 'Kan Chen', 'Jeannie S. Lee']",11,0.43848562
"Detecting social bots has evolved into a pivotal yet intricate task, aimed at
combating the dissemination of misinformation and preserving the authenticity
of online interactions. While earlier graph-based approaches, which leverage
topological structure of social networks, yielded notable outcomes, they
overlooked the inherent dynamicity of social networks -- In reality, they
largely depicted the social network as a static graph and solely relied on its
most recent state. Due to the absence of dynamicity modeling, such approaches
are vulnerable to evasion, particularly when advanced social bots interact with
other users to camouflage identities and escape detection. To tackle these
challenges, we propose BotDGT, a novel framework that not only considers the
topological structure, but also effectively incorporates dynamic nature of
social network. Specifically, we characterize a social network as a dynamic
graph. A structural module is employed to acquire topological information from
each historical snapshot. Additionally, a temporal module is proposed to
integrate historical context and model the evolving behavior patterns exhibited
by social bots and legitimate users. Experimental results demonstrate the
superiority of BotDGT against the leading methods that neglected the dynamic
nature of social networks in terms of accuracy, recall, and F1-score.","['Buyun He', 'Yingguang Yang', 'Qi Wu', 'Hao Liu', 'Renyu Yang', 'Hao Peng', 'Xiang Wang', 'Yong Liao', 'Pengyuan Zhou']",2,0.6276753
"Generative AI models can produce high-quality images based on text prompts.
The generated images often appear indistinguishable from images generated by
conventional optical photography devices or created by human artists (i.e.,
real images). While the outstanding performance of such generative models is
generally well received, security concerns arise. For instance, such image
generators could be used to facilitate fraud or scam schemes, generate and
spread misinformation, or produce fabricated artworks. In this paper, we
present a systematic attempt at understanding and detecting AI-generated images
(AI-art) in adversarial scenarios. First, we collect and share a dataset of
real images and their corresponding artificial counterparts generated by four
popular AI image generators. The dataset, named ARIA, contains over 140K images
in five categories: artworks (painting), social media images, news photos,
disaster scenes, and anime pictures. This dataset can be used as a foundation
to support future research on adversarial AI-art. Next, we present a user study
that employs the ARIA dataset to evaluate if real-world users can distinguish
with or without reference images. In a benchmarking study, we further evaluate
if state-of-the-art open-source and commercial AI image detectors can
effectively identify the images in the ARIA dataset. Finally, we present a
ResNet-50 classifier and evaluate its accuracy and transferability on the ARIA
dataset.","['Yuying Li', 'Zeyan Liu', 'Junyi Zhao', 'Liangqin Ren', 'Fengjun Li', 'Jiebo Luo', 'Bo Luo']",7,0.7787038
"The capabilities of generative AI (genAI) have dramatically increased in
recent times, and there are opportunities for children to leverage new features
for personal and school-related endeavors. However, while the future of genAI
is taking form, there remain potentially harmful limitations, such as
generation of outputs with misinformation and bias. We ran a workshop study
focused on ChatGPT to explore middle school girls' (N = 26) attitudes and
reasoning about how genAI works. We focused on girls who are often
disproportionately impacted by algorithmic bias. We found that: (1) middle
school girls were initially overtrusting of genAI, (2) deliberate exposure to
the limitations and mistakes of generative AI shifted this overtrust to
disillusionment about genAI capabilities, though they were still optimistic for
future possibilities of genAI, and (3) their ideas about school policy were
nuanced. This work informs how children think about genAI like ChatGPT and its
integration in learning settings.","['Jaemarie Solyst', 'Ellia Yang', 'Shixian Xie', 'Jessica Hammer', 'Amy Ogan', 'Motahhare Eslami']",9,0.6473379
"The availability of smart devices leads to an exponential increase in
multimedia content. However, the rapid advancements in deep learning have given
rise to sophisticated algorithms capable of manipulating or creating multimedia
fake content, known as Deepfake. Audio Deepfakes pose a significant threat by
producing highly realistic voices, thus facilitating the spread of
misinformation. To address this issue, numerous audio anti-spoofing detection
challenges have been organized to foster the development of anti-spoofing
countermeasures. This survey paper presents a comprehensive review of every
component within the detection pipeline, including algorithm architectures,
optimization techniques, application generalizability, evaluation metrics,
performance comparisons, available datasets, and open-source availability. For
each aspect, we conduct a systematic evaluation of the recent advancements,
along with discussions on existing challenges. Additionally, we also explore
emerging research topics on audio anti-spoofing, including partial spoofing
detection, cross-dataset evaluation, and adversarial attack defence, while
proposing some promising research directions for future work. This survey paper
not only identifies the current state-of-the-art to establish strong baselines
for future experiments but also guides future researchers on a clear path for
understanding and enhancing the audio anti-spoofing detection mechanisms.","['Menglu Li', 'Yasaman Ahmadiadli', 'Xiao-Ping Zhang']",11,0.7843653
"Large language models are aligned to be safe, preventing users from
generating harmful content like misinformation or instructions for illegal
activities. However, previous work has shown that the alignment process is
vulnerable to poisoning attacks. Adversaries can manipulate the safety training
data to inject backdoors that act like a universal sudo command: adding the
backdoor string to any prompt enables harmful responses from models that,
otherwise, behave safely. Our competition, co-located at IEEE SaTML 2024,
challenged participants to find universal backdoors in several large language
models. This report summarizes the key findings and promising ideas for future
research.","['Javier Rando', 'Francesco Croce', 'Kry≈°tof Mitka', 'Stepan Shabalin', 'Maksym Andriushchenko', 'Nicolas Flammarion', 'Florian Tram√®r']",1,0.54017967
"The escalating challenge of misinformation, particularly in political
discourse, requires advanced fact-checking solutions; this is even clearer in
the more complex scenario of multimodal claims. We tackle this issue using a
multimodal large language model in conjunction with retrieval-augmented
generation (RAG), and introduce two novel reasoning techniques: Chain of RAG
(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by
extracting both textual and image content, retrieving external information, and
reasoning subsequent questions to be answered based on prior evidence. We
achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique
by 0.14 points. Human evaluation confirms that the vast majority of our
generated fact-check explanations contain all information from gold standard
data.","['M. Abdul Khaliq', 'P. Chang', 'M. Ma', 'B. Pflugfelder', 'F. Miletiƒá']",1,0.7236293
"Logical fallacies are common errors in reasoning that undermine the logic of
an argument. Automatically detecting logical fallacies has important
applications in tracking misinformation and validating claims. In this paper,
we design a process to reliably detect logical fallacies by translating natural
language to First-order Logic (FOL) step-by-step using Large Language Models
(LLMs). We then utilize Satisfiability Modulo Theory (SMT) solvers to reason
about the validity of the formula and classify inputs as either a fallacy or
valid statement. Our model also provides a novel means of utilizing LLMs to
interpret the output of the SMT solver, offering insights into the
counter-examples that illustrate why a given sentence is considered a logical
fallacy. Our approach is robust, interpretable and does not require training
data or fine-tuning. We evaluate our model on a mixed dataset of fallacies and
valid sentences. The results demonstrate improved performance compared to
end-to-end LLMs, with our classifier achieving an F1-score of 71\% on the Logic
dataset. The approach is able to generalize effectively, achieving an F1-score
of 73% on the challenge set, LogicClimate, outperforming state-of-the-art
models by 21% despite its much smaller size.","['Abhinav Lalwani', 'Lovish Chopra', 'Christopher Hahn', 'Caroline Trippel', 'Zhijing Jin', 'Mrinmaya Sachan']",1,0.63959503
"The proliferation of unreliable news domains on the internet has had
wide-reaching negative impacts on society. We introduce and evaluate
interventions aimed at reducing traffic to unreliable news domains from search
engines while maintaining traffic to reliable domains. We build these
interventions on the principles of fairness (penalize sites for what is in
their control), generality (label/fact-check agnostic), targeted (increase the
cost of adversarial behavior), and scalability (works at webscale). We refine
our methods on small-scale webdata as a testbed and then generalize the
interventions to a large-scale webgraph containing 93.9M domains and 1.6B
edges. We demonstrate that our methods penalize unreliable domains far more
than reliable domains in both settings and we explore multiple avenues to
mitigate unintended effects on both the small-scale and large-scale webgraph
experiments. These results indicate the potential of our approach to reduce the
spread of misinformation and foster a more reliable online information
ecosystem. This research contributes to the development of targeted strategies
to enhance the trustworthiness and quality of search engine results, ultimately
benefiting users and the broader digital community.","['Peter Carragher', 'Evan M. Williams', 'Kathleen M. Carley']",0,0.6517997
"A significant amount of society's infrastructure can be modeled using graph
structures, from electric and communication grids, to traffic networks, to
social networks. Each of these domains are also susceptible to the cascading
spread of negative impacts, whether this be overloaded devices in the power
grid or the reach of a social media post containing misinformation. The
potential harm of a cascade is compounded when considering a malicious attack
by an adversary that is intended to maximize the cascading impact. However, by
exploiting knowledge of the cascading dynamics, targets with the largest
cascading impact can be preemptively prioritized for defense, and the damage an
adversary can inflict can be mitigated. While game theory provides tools for
finding an optimal preemptive defense strategy, existing methods struggle to
scale to the context of large graph environments because of the combinatorial
explosion of possible actions that occurs when the attacker and defender can
each choose multiple targets in the graph simultaneously. The proposed method
enables a data-driven deep learning approach that uses multi-node
representation learning and counterfactual data augmentation to generalize to
the full combinatorial action space by training on a variety of small
restricted subsets of the action space. We demonstrate through experiments that
the proposed method is capable of identifying defense strategies that are less
exploitable than SOTA methods for large graphs, while still being able to
produce strategies near the Nash equilibrium for small-scale scenarios for
which it can be computed. Moreover, the proposed method demonstrates superior
prediction accuracy on a validation set of unseen cascades compared to other
deep learning approaches.","['James D. Cunningham', 'Conrad S. Tucker']",2,0.7249533
"Conversational prompt-engineering-based large language models (LLMs) have
enabled targeted control over the output creation, enhancing versatility,
adaptability and adhoc retrieval. From another perspective, digital
misinformation has reached alarming levels. The anonymity, availability and
reach of social media offer fertile ground for rumours to propagate. This work
proposes to leverage the advancement of prompting-dependent LLMs to combat
misinformation by extending the research efforts of the RumourEval task on its
Twitter dataset. To the end, we employ two prompting-based LLM variants
(GPT-3.5-turbo and GPT-4) to extend the two RumourEval subtasks: (1) veracity
prediction, and (2) stance classification. For veracity prediction, three
classifications schemes are experimented per GPT variant. Each scheme is tested
in zero-, one- and few-shot settings. Our best results outperform the precedent
ones by a substantial margin. For stance classification,
prompting-based-approaches show comparable performance to prior results, with
no improvement over finetuning methods. Rumour stance subtask is also extended
beyond the original setting to allow multiclass classification. All of the
generated predictions for both subtasks are equipped with confidence scores
determining their trustworthiness degree according to the LLM, and post-hoc
justifications for explainability and interpretability purposes. Our primary
aim is AI for social good.","['Dahlia Shehata', 'Robin Cohen', 'Charles Clarke']",6,0.7195306
"Recommendation algorithms (RS) used by social media, like YouTube,
significantly shape our information consumption across various domains,
especially in healthcare. Hence, algorithmic auditing becomes crucial to
uncover their potential bias and misinformation, particularly in the context of
controversial topics like abortion. We introduce a simple yet effective sock
puppet auditing approach to investigate how YouTube recommends abortion-related
videos to individuals with different backgrounds. Our framework allows for
efficient auditing of RS, regardless of the complexity of the underlying
algorithms","['Mohammed Lahsaini', 'Mohamed Lechiakh', 'Alexandre Maurer']",2,0.54632807
"Addressing the imminent shortfall of 10 million health workers by 2030,
predominantly in Low- and Middle-Income Countries (LMICs), this paper
introduces an innovative approach that harnesses the power of Large Language
Models (LLMs) integrated with machine translation models. This solution is
engineered to meet the unique needs of Community Health Workers (CHWs),
overcoming language barriers, cultural sensitivities, and the limited
availability of medical dialog datasets. I have crafted a model that not only
boasts superior translation capabilities but also undergoes rigorous
fine-tuning on open-source datasets to ensure medical accuracy and is equipped
with comprehensive safety features to counteract the risks of misinformation.
  Featuring a modular design, this approach is specifically structured for
swift adaptation across various linguistic and cultural contexts, utilizing
open-source components to significantly reduce healthcare operational costs.
This strategic innovation markedly improves the accessibility and quality of
healthcare services by providing CHWs with contextually appropriate medical
knowledge and diagnostic tools. This paper highlights the transformative impact
of this context-aware LLM, underscoring its crucial role in addressing the
global healthcare workforce deficit and propelling forward healthcare outcomes
in LMICs.",['Agasthya Gangavarapu'],8,0.5323229
"Since 2006, Twitter's Application Programming Interface (API) has been a
treasure trove of high-quality data for researchers studying everything from
the spread of misinformation, to social psychology and emergency management.
However, in the spring of 2023, Twitter (now called X) began changing
$42,000/month for its Enterprise access level, an essential death knell for
researcher use. Lacking sufficient funds to pay this monthly fee, academics are
now scrambling to continue their research without this important data source.
This study collects and tabulates the number of studies, number of citations,
dates, major disciplines, and major topic areas of studies that used Twitter
data between 2006 and 2023. While we cannot know for certain what will be lost
now that Twitter data is cost prohibitive, we can illustrate its research value
during the time it was available. A search of 8 databases and 3 related APIs
found that since 2006, a total of 27,453 studies have been published in 7,432
publication venues, with 1,303,142 citations, across 14 disciplines. Major
disciplines include: computational social science, engineering, data science,
social media studies, public health, and medicine. Major topics include:
information dissemination, assessing the credibility of tweets, strategies for
conducting data research, detecting and analyzing major events, and studying
human behavior. Twitter data studies have increased every year since 2006, but
following Twitter's decision to begin charging for data in the spring of 2023,
the number of studies published in 2023 decreased by 13% compared to 2022. We
assume that much of the data used for studies published in 2023 were collected
prior to Twitter's shutdown, and thus the number of new studies are likely to
decline further in subsequent years.","['Ryan Murtfeldt', 'Naomi Alterman', 'Ihsan Kahveci', 'Jevin D. West']",5,0.6200102
"This paper addresses debiasing in news editing and evaluates the
effectiveness of conversational Large Language Models in this task. We designed
an evaluation checklist tailored to news editors' perspectives, obtained
generated texts from three popular conversational models using a subset of a
publicly available dataset in media bias, and evaluated the texts according to
the designed checklist. Furthermore, we examined the models as evaluator for
checking the quality of debiased model outputs. Our findings indicate that none
of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT,
introduced unnecessary changes that may impact the author's style and create
misinformation. Lastly, we show that the models do not perform as proficiently
as domain experts in evaluating the quality of debiased outputs.","['Ipek Baris Schlicht', 'Defne Altiok', 'Maryanne Taouk', 'Lucie Flek']",1,0.6486035
"The presence of political misinformation and ideological echo chambers on
social media platforms is concerning given the important role that these sites
play in the public's exposure to news and current events. Algorithmic systems
employed on these platforms are presumed to play a role in these phenomena, but
little is known about their mechanisms and effects. In this work, we conduct an
algorithmic audit of Twitter's Who-To-Follow friend recommendation system, the
first empirical audit that investigates the impact of this algorithm in-situ.
We create automated Twitter accounts that initially follow left and right
affiliated U.S. politicians during the 2022 U.S. midterm elections and then
grow their information networks using the platform's recommender system. We
pair the experiment with an observational study of Twitter users who already
follow the same politicians. Broadly, we find that while following the
recommendation algorithm leads accounts into dense and reciprocal neighborhoods
that structurally resemble echo chambers, the recommender also results in less
political homogeneity of a user's network compared to accounts growing their
networks through social endorsement. Furthermore, accounts that exclusively
followed users recommended by the algorithm had fewer opportunities to
encounter content centered on false or misleading election narratives compared
to choosing friends based on social endorsement.","['Kayla Duskin', 'Joseph S. Schafer', 'Jevin D. West', 'Emma S. Spiro']",10,0.7298717
"There is increasing interest in the adoption of LLMs in HCI research.
However, LLMs may often be regarded as a panacea because of their powerful
capabilities with an accompanying oversight on whether they are suitable for
their intended tasks. We contend that LLMs should be adopted in a critical
manner following rigorous evaluation. Accordingly, we present the evaluation of
an LLM in identifying logical fallacies that will form part of a digital
misinformation intervention. By comparing to a labeled dataset, we found that
GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes
invalid or unidentified instances, an accuracy of 0.90. This gives us the
confidence to proceed with the application of the LLM while keeping in mind the
areas where it still falls short. The paper describes our evaluation approach,
results and reflections on the use of the LLM for our intended task.","['Gionnieve Lim', 'Simon T. Perrault']",6,0.77231747
"This paper presents a systematic literature review in Computer Science that
provide an overview of the initiatives related to digital misinformation. This
is an exploratory study that covers research from 1993 to 2020, focusing on the
investigation of the phenomenon of misinformation. The review consists of 788
studies from SCOPUS, IEEE, and ACM digital libraries, synthesizing the primary
research directions and sociotechnical challenges. These challenges are
classified into Physical, Empirical, Syntactic, Semantic, Pragmatic, and Social
dimensions, drawing from Organizational Semiotics. The mapping identifies
issues related to the concept of misinformation, highlights deficiencies in
mitigation strategies, discusses challenges in approaching stakeholders, and
unveils various sociotechnical aspects relevant to understanding and mitigating
the harmful effects of digital misinformation. As contributions, this study
present a novel categorization of mitigation strategies, a sociotechnical
taxonomy for classifying types of false information and elaborate on the
inter-relation of sociotechnical aspects and their impacts.","['Alisson Andrey Puska', 'Luiz Adolpho Baroni', 'Roberto Pereira']",0,0.69196224
"Community models for malicious content detection, which take into account the
context from a social graph alongside the content itself, have shown remarkable
performance on benchmark datasets. Yet, misinformation and hate speech continue
to propagate on social media networks. This mismatch can be partially
attributed to the limitations of current evaluation setups that neglect the
rapid evolution of online content and the underlying social graph. In this
paper, we propose a novel evaluation setup for model generalisation based on
our few-shot subgraph sampling approach. This setup tests for generalisation
through few labelled examples in local explorations of a larger graph,
emulating more realistic application settings. We show this to be a challenging
inductive setup, wherein strong performance on the training graph is not
indicative of performance on unseen tasks, domains, or graph structures.
Lastly, we show that graph meta-learners trained with our proposed few-shot
subgraph sampling outperform standard community models in the inductive setup.
We make our code publicly available.","['Ivo Verhoeven', 'Pushkar Mishra', 'Rahel Beloch', 'Helen Yannakoudakis', 'Ekaterina Shutova']",2,0.6333615
"The use of words to convey speaker's intent is traditionally distinguished
from the `mention' of words for quoting what someone said, or pointing out
properties of a word. Here we show that computationally modeling this
use-mention distinction is crucial for dealing with counterspeech online.
Counterspeech that refutes problematic content often mentions harmful language
but is not harmful itself (e.g., calling a vaccine dangerous is not the same as
expressing disapproval of someone for calling vaccines dangerous). We show that
even recent language models fail at distinguishing use from mention, and that
this failure propagates to two key downstream tasks: misinformation and hate
speech detection, resulting in censorship of counterspeech. We introduce
prompting mitigations that teach the use-mention distinction, and show they
reduce these errors. Our work highlights the importance of the use-mention
distinction for NLP and CSS and offers ways to address it.","['Kristina Gligoric', 'Myra Cheng', 'Lucia Zheng', 'Esin Durmus', 'Dan Jurafsky']",8,0.51561266
"This study maps the spread of two cases of COVID-19 conspiracy theories and
misinformation in Spanish and French in Latin American and French-speaking
communities on Facebook, and thus contributes to understanding the dynamics,
reach and consequences of emerging transnational misinformation networks. The
findings show that co-sharing behavior of public Facebook groups created
transnational networks by sharing videos of Medicos por la Verdad (MPV)
conspiracy theories in Spanish and hydroxychloroquine-related misinformation
sparked by microbiologist Didier Raoult (DR) in French, usually igniting the
surge of locally led interest groups across the Global South. Using inferential
methods, the study shows how these networks are enabled primarily by shared
cultural and thematic attributes among Facebook groups, effectively creating
very large, networked audiences. The study contributes to the understanding of
how potentially harmful conspiracy theories and misinformation transcend
national borders through non-English speaking online communities, further
highlighting the overlooked role of transnationalism in global misinformation
diffusion and the potentially disproportionate harm that it causes in
vulnerable communities across the globe.","['Esteban Villa-Turek', 'Rod Abhari', 'Erik C. Nisbet', 'Yu Xu', 'Ayse Deniz Lokmanoglu']",5,0.65388757
"Recent advancements in Generative Adversarial Networks (GANs) have enabled
photorealistic image generation with high quality. However, the malicious use
of such generated media has raised concerns regarding visual misinformation.
Although deepfake detection research has demonstrated high accuracy, it is
vulnerable to advances in generation techniques and adversarial iterations on
detection countermeasures. To address this, we propose a proactive and
sustainable deepfake training augmentation solution that introduces artificial
fingerprints into models. We achieve this by employing an ensemble learning
approach that incorporates a pool of autoencoders that mimic the effect of the
artefacts introduced by the deepfake generator models. Experiments on three
datasets reveal that our proposed ensemble autoencoder-based data augmentation
learning approach offers improvements in terms of generalisation, resistance
against basic data perturbations such as noise, blurring, sharpness
enhancement, and affine transforms, resilience to commonly used lossy
compression algorithms such as JPEG, and enhanced resistance against
adversarial attacks.","['Liviu-Daniel ≈ûtefan', 'Dan-Cristian Stanciu', 'Mihai Dogariu', 'Mihai Gabriel Constantin', 'Andrei Cosmin Jitaru', 'Bogdan Ionescu']",7,0.851374
"Deepfake videos create dangerous possibilities for public misinformation. In
this experiment (N=204), we investigated whether labeling videos as containing
actual or deepfake statements from US President Biden helps participants later
differentiate between true and fake information. People accurately recalled
93.8% of deepfake videos and 84.2% of actual videos, suggesting that labeling
videos can help combat misinformation. Individuals who identify as Republican
and had lower favorability ratings of Biden performed better in distinguishing
between actual and deepfake videos, a result explained by the elaboration
likelihood model (ELM), which predicts that people who distrust a message
source will more critically evaluate the message.","['Nathan L. Tenhundfeld', 'Ryan Weber', 'William I. MacKenzie', 'Hannah M. Barr', 'Candice Lanius']",4,0.57700294
"Artificial Intelligence Generated Content (AIGC) technology development has
facilitated the creation of rumors with misinformation, impacting societal,
economic, and political ecosystems, challenging democracy. Current rumor
detection efforts fall short by merely labeling potentially misinformation
(classification task), inadequately addressing the issue, and it is unrealistic
to have authoritative institutions debunk every piece of information on social
media. Our proposed comprehensive debunking process not only detects rumors but
also provides explanatory generated content to refute the authenticity of the
information. The Expert-Citizen Collective Wisdom (ECCW) module we designed
aensures high-precision assessment of the credibility of information and the
retrieval module is responsible for retrieving relevant knowledge from a
Real-time updated debunking database based on information keywords. By using
prompt engineering techniques, we feed results and knowledge into a LLM (Large
Language Model), achieving satisfactory discrimination and explanatory effects
while eliminating the need for fine-tuning, saving computational costs, and
contributing to debunking efforts.","['Junhao Xu', 'Longdi Xian', 'Zening Liu', 'Mingliang Chen', 'Qiuyang Yin', 'Fenghua Song']",1,0.7235625
"In recent years, reports and anecdotal evidence pointing at the role of
WhatsApp in a variety of events, ranging from elections to collective violence,
have emerged. While academic research should examine the validity of these
claims, obtaining WhatsApp data for research is notably challenging,
contrasting with the relative abundance of data from platforms like Facebook
and Twitter, where user ""information diets"" have been extensively studied. This
lack of data is particularly problematic since misinformation and hate speech
are major concerns in the set of Global South countries in which WhatsApp
dominates the market for messaging. To help make research on these questions,
and more generally research on WhatsApp, possible, this paper introduces
WhatsApp Explorer, a tool designed to enable WhatsApp data collection on a
large scale. We discuss protocols for data collection, including potential
sampling approaches, and explain why our tool (and adjoining protocol) arguably
allow researchers to collect WhatsApp data in an ethical and legal manner, at
scale.","['Kiran Garimella', 'Simon Chauchard']",0,0.6175376
"The rapid progress in deep learning has given rise to hyper-realistic facial
forgery methods, leading to concerns related to misinformation and security
risks. Existing face forgery datasets have limitations in generating
high-quality facial images and addressing the challenges posed by evolving
generative techniques. To combat this, we present DiffusionFace, the first
diffusion-based face forgery dataset, covering various forgery categories,
including unconditional and Text Guide facial image generation, Img2Img,
Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace
dataset stands out with its extensive collection of 11 diffusion models and the
high-quality of the generated images, providing essential metadata and a
real-world internet-sourced forgery facial image dataset for evaluation.
Additionally, we provide an in-depth analysis of the data and introduce
practical evaluation protocols to rigorously assess discriminative models'
effectiveness in detecting counterfeit facial images, aiming to enhance
security in facial image authentication processes. The dataset is available for
download at \url{https://github.com/Rapisurazurite/DiffFace}.","['Zhongxi Chen', 'Ke Sun', 'Ziyin Zhou', 'Xianming Lin', 'Xiaoshuai Sun', 'Liujuan Cao', 'Rongrong Ji']",7,0.7895088
"Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.","['Dongfang Li', 'Zetian Sun', 'Baotian Hu', 'Zhenyu Liu', 'Xinshuo Hu', 'Xuebo Liu', 'Min Zhang']",1,0.76142484
"The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org","['Patrick Grommelt', 'Louis Weiss', 'Franz-Josef Pfreundt', 'Janis Keuper']",7,0.78228664
"Automated fact checking has gained immense interest to tackle the growing
misinformation in the digital era. Existing systems primarily focus on
synthetic claims on Wikipedia, and noteworthy progress has also been made on
real-world claims. In this work, we release QuanTemp, a diverse, multi-domain
dataset focused exclusively on numerical claims, encompassing temporal,
statistical and diverse aspects with fine-grained metadata and an evidence
collection without leakage. This addresses the challenge of verifying
real-world numerical claims, which are complex and often lack precise
information, not addressed by existing works that mainly focus on synthetic
claims. We evaluate and quantify the limitations of existing solutions for the
task of verifying numerical claims. We also evaluate claim decomposition based
methods, numerical understanding based models and our best baselines achieves a
macro-F1 of 58.32. This demonstrates that QuanTemp serves as a challenging
evaluation set for numerical claim verification.","['Venktesh V', 'Abhijit Anand', 'Avishek Anand', 'Vinay Setty']",1,0.72963035
"The Large Language Models (LLMs) exhibit remarkable ability to generate
fluent content across a wide spectrum of user queries. However, this capability
has raised concerns regarding misinformation and personal information leakage.
In this paper, we present our methods for the SemEval2024 Task8, aiming to
detect machine-generated text across various domains in both mono-lingual and
multi-lingual contexts. Our study comprehensively analyzes various methods to
detect machine-generated text, including statistical, neural, and pre-trained
model approaches. We also detail our experimental setup and perform a in-depth
error analysis to evaluate the effectiveness of these methods. Our methods
obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for
subtask-B. Furthermore, we also highlight the challenges and essential factors
for consideration in future studies.","['Ashok Urlana', 'Aditya Saibewar', 'Bala Mallikarjunarao Garlapati', 'Charaka Vinayak Kumar', 'Ajeet Kumar Singh', 'Srinivasa Rao Chalamala']",8,0.7268087
"The modeling of nonlinear dynamics based on Koopman operator theory, which is
originally applicable only to autonomous systems with no control, is extended
to non-autonomous control system without approximation to input matrix B.
Prevailing methods using a least square estimate of the B matrix may result in
an erroneous input matrix, misinforming the controller about the structure of
the input matrix in a lifted space. Here, a new method for constructing a
Koopman model that comprises the exact input matrix B is presented. A set of
state variables are introduced so that the control inputs are linearly involved
in the dynamics of actuators. With these variables, a lifted linear model with
the exact control matrix, called a Control-Coherent Koopman Model, is
constructed by superposing control input terms, which are linear in local
actuator dynamics, to the Koopman operator of the associated autonomous
nonlinear system. The proposed method is applied to multi degree-of-freedom
robotic arms and multi-cable manipulation systems. Model Predictive Control is
applied to the former. It is demonstrated that the prevailing Dynamic Mode
Decomposition with Control (DMDc) using an approximate control matrix B does
not provide a satisfactory result, while the Control-Coherent Koopman Model
performs well with the correct B matrix.","['H. Harry Asada', 'Jose A. Solano-Castellanos']",2,0.49200967
"The proliferation of online misinformation has posed significant threats to
public interest. While numerous online users actively participate in the combat
against misinformation, many of such responses can be characterized by the lack
of politeness and supporting facts. As a solution, text generation approaches
are proposed to automatically produce counter-misinformation responses.
Nevertheless, existing methods are often trained end-to-end without leveraging
external knowledge, resulting in subpar text quality and excessively repetitive
responses. In this paper, we propose retrieval augmented response generation
for online misinformation (RARG), which collects supporting evidence from
scientific sources and generates counter-misinformation responses based on the
evidences. In particular, our RARG consists of two stages: (1) evidence
collection, where we design a retrieval pipeline to retrieve and rerank
evidence documents using a database comprising over 1M academic articles; (2)
response generation, in which we align large language models (LLMs) to generate
evidence-based responses via reinforcement learning from human feedback (RLHF).
We propose a reward function to maximize the utilization of the retrieved
evidence while maintaining the quality of the generated text, which yields
polite and factual responses that clearly refutes misinformation. To
demonstrate the effectiveness of our method, we study the case of COVID-19 and
perform extensive experiments with both in- and cross-domain datasets, where
RARG consistently outperforms baselines by generating high-quality
counter-misinformation responses.","['Zhenrui Yue', 'Huimin Zeng', 'Yimeng Lu', 'Lanyu Shang', 'Yang Zhang', 'Dong Wang']",1,0.77431524
"The pervasive spread of misinformation and disinformation in social media
underscores the critical importance of detecting media bias. While robust Large
Language Models (LLMs) have emerged as foundational tools for bias prediction,
concerns about inherent biases within these models persist. In this work, we
investigate the presence and nature of bias within LLMs and its consequential
impact on media bias detection. Departing from conventional approaches that
focus solely on bias detection in media content, we delve into biases within
the LLM systems themselves. Through meticulous examination, we probe whether
LLMs exhibit biases, particularly in political bias prediction and text
continuation tasks. Additionally, we explore bias across diverse topics, aiming
to uncover nuanced variations in bias expression within the LLM framework.
Importantly, we propose debiasing strategies, including prompt engineering and
model fine-tuning. Extensive analysis of bias tendencies across different LLMs
sheds light on the broader landscape of bias propagation in language models.
This study advances our understanding of LLM bias, offering critical insights
into its implications for bias detection tasks and paving the way for more
robust and equitable AI systems","['Luyang Lin', 'Lingzhi Wang', 'Jinsong Guo', 'Kam-Fai Wong']",6,0.7402687
"Identifying the source of epidemic-like spread in networks is crucial for
removing internet viruses or finding the source of rumors in online social
networks. The challenge lies in tracing the source from a snapshot observation
of infected nodes. How do we accurately pinpoint the source? Utilizing snapshot
data, we apply a probabilistic approach, focusing on the graph boundary and the
observed time, to detect sources via an effective maximum likelihood algorithm.
A novel starlike tree approximation extends applicability to general graphs,
demonstrating versatility. Unlike previous works that rely heavily on
structural properties alone, our method also incorporates temporal data for
more precise source detection. We highlight the utility of the Gamma function
for analyzing the ratio of the likelihood being the source between nodes
asymptotically. Comprehensive evaluations confirm algorithmic effectiveness in
diverse network scenarios, advancing source detection in large-scale network
analysis and information dissemination strategies.","['Pei-Duo Yu', 'Chee Wei Tan']",2,0.7869053
"With the introduction of ChatGPT, Large Language Models (LLMs) have received
enormous attention in healthcare. Despite their potential benefits, researchers
have underscored various ethical implications. While individual instances have
drawn much attention, the debate lacks a systematic overview of practical
applications currently researched and ethical issues connected to them. Against
this background, this work aims to map the ethical landscape surrounding the
current stage of deployment of LLMs in medicine and healthcare. Electronic
databases and preprint servers were queried using a comprehensive search
strategy. Studies were screened and extracted following a modified rapid review
approach. Methodological quality was assessed using a hybrid approach. For 53
records, a meta-aggregative synthesis was performed. Four fields of
applications emerged and testify to a vivid exploration phase. Advantages of
using LLMs are attributed to their capacity in data analysis, personalized
information provisioning, support in decision-making, mitigating information
loss and enhancing information accessibility. However, we also identifies
recurrent ethical concerns connected to fairness, bias, non-maleficence,
transparency, and privacy. A distinctive concern is the tendency to produce
harmful misinformation or convincingly but inaccurate content. A recurrent plea
for ethical guidance and human oversight is evident. Given the variety of use
cases, it is suggested that the ethical guidance debate be reframed to focus on
defining what constitutes acceptable human oversight across the spectrum of
applications. This involves considering diverse settings, varying potentials
for harm, and different acceptable thresholds for performance and certainty in
healthcare. In addition, a critical inquiry is necessary to determine the
extent to which the current experimental use of LLMs is necessary and
justified.","['Joschka Haltaufderheide', 'Robert Ranisch']",0,0.71637607
"Social media platforms are online fora where users engage in discussions,
share content, and build connections. This review explores the dynamics of
social interactions, user-generated contents, and biases within the context of
social media analysis (analyzing works that use the tools offered by complex
network analysis and natural language processing) through the lens of three key
points of view: online debates, online support, and human-AI interactions. On
the one hand, we delineate the phenomenon of online debates, where
polarization, misinformation, and echo chamber formation often proliferate,
driven by algorithmic biases and extreme mechanisms of homophily. On the other
hand, we explore the emergence of online support groups through users'
self-disclosure and social support mechanisms. Online debates and support
mechanisms present a duality of both perils and possibilities within social
media; perils of segregated communities and polarized debates, and
possibilities of empathy narratives and self-help groups. This dichotomy also
extends to a third perspective: users' reliance on AI-generated content, such
as the ones produced by Large Language Models, which can manifest both human
biases hidden in training sets and non-human biases that emerge from their
artificial neural architectures. Analyzing interdisciplinary approaches, we aim
to deepen the understanding of the complex interplay between social
interactions, user-generated content, and biases within the realm of social
media ecosystems.","['Virginia Morini', 'Valentina Pansanella', 'Katherine Abramski', 'Erica Cau', 'Andrea Failla', 'Salvatore Citraro', 'Giulio Rossetti']",0,0.63532066
"Automatic detection of multimodal misinformation has gained a widespread
attention recently. However, the potential of powerful Large Language Models
(LLMs) for multimodal misinformation detection remains underexplored. Besides,
how to teach LLMs to interpret multimodal misinformation in cost-effective and
accessible way is still an open question. To address that, we propose MMIDR, a
framework designed to teach LLMs in providing fluent and high-quality textual
explanations for their decision-making process of multimodal misinformation. To
convert multimodal misinformation into an appropriate instruction-following
format, we present a data augmentation perspective and pipeline. This pipeline
consists of a visual information processing module and an evidence retrieval
module. Subsequently, we prompt the proprietary LLMs with processed contents to
extract rationales for interpreting the authenticity of multimodal
misinformation. Furthermore, we design an efficient knowledge distillation
approach to distill the capability of proprietary LLMs in explaining multimodal
misinformation into open-source LLMs. To explore several research questions
regarding the performance of LLMs in multimodal misinformation detection tasks,
we construct an instruction-following multimodal misinformation dataset and
conduct comprehensive experiments. The experimental findings reveal that our
MMIDR exhibits sufficient detection performance and possesses the capacity to
provide compelling rationales to support its assessments.","['Longzheng Wang', 'Xiaohan Xu', 'Lei Zhang', 'Jiarui Lu', 'Yongxiu Xu', 'Hongbo Xu', 'Minghao Tang', 'Chuang Zhang']",6,0.77389395
"Misinformation can seriously impact society, affecting anything from public
opinion to institutional confidence and the political horizon of a state. Fake
News (FN) proliferation on online websites and Online Social Networks (OSNs)
has increased profusely. Various fact-checking websites include news in English
and barely provide information about FN in regional languages. Thus the Urdu FN
purveyors cannot be discerned using factchecking portals. SOTA approaches for
Fake News Detection (FND) count upon appropriately labelled and large datasets.
FND in regional and resource-constrained languages lags due to the lack of
limited-sized datasets and legitimate lexical resources. The previous datasets
for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not
manually verified where the news is translated from English into Urdu. In this
paper, we curate and contribute the first largest publicly available dataset
for Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations
of existing Urdu datasets in the literature. It constitutes 10,083 fake and
real news on fifteen domains collected from leading and authentic Urdu
newspapers and news channel websites in Pakistan and India. FN for the
Ax-to-Grind dataset is collected from websites and crowdsourcing. The dataset
contains news items in Urdu from the year 2017 to the year 2023. Expert
journalists annotated the dataset. We benchmark the dataset with an ensemble
model of mBERT,XLNet, and XLM RoBERTa. The selected models are originally
trained on multilingual large corpora. The results of the proposed model are
based on performance metrics, F1-score, accuracy, precision, recall and MCC
value.","['Sheetal Harris', 'Jinshuo Liu', 'Hassan Jalil Hadi', 'Yue Cao']",8,0.65606624
"Coverage of ChatGPT-style large language models (LLMs) in the media has
focused on their eye-catching achievements, including solving advanced
mathematical problems and reaching expert proficiency in medical examinations.
But the gradual adoption of LLMs in agriculture, an industry which touches
every human life, has received much less public scrutiny. In this short
perspective, we examine risks and opportunities related to more widespread
adoption of language models in food production systems. While LLMs can
potentially enhance agricultural efficiency, drive innovation, and inform
better policies, challenges like agricultural misinformation, collection of
vast amounts of farmer data, and threats to agricultural jobs are important
concerns. The rapid evolution of the LLM landscape underscores the need for
agricultural policymakers to think carefully about frameworks and guidelines
that ensure the responsible use of LLMs in food production before these
technologies become so ingrained that policy intervention becomes challenging.","['Djavan De Clercq', 'Elias Nehring', 'Harry Mayne', 'Adam Mahdi']",6,0.6426867
"Machine Unlearning (MU) has recently gained considerable attention due to its
potential to achieve Safe AI by removing the influence of specific data from
trained Machine Learning (ML) models. This process, known as knowledge removal,
addresses AI governance concerns of training data such as quality, sensitivity,
copyright restrictions, and obsolescence. This capability is also crucial for
ensuring compliance with privacy regulations such as the Right To Be Forgotten
(RTBF). Furthermore, effective knowledge removal mitigates the risk of harmful
outcomes, safeguarding against biases, misinformation, and unauthorized data
exploitation, thereby enhancing the safe and responsible use of AI systems.
Efforts have been made to design efficient unlearning approaches, with MU
services being examined for integration with existing machine learning as a
service (MLaaS), allowing users to submit requests to remove specific data from
the training corpus. However, recent research highlights vulnerabilities in
machine unlearning systems, such as information leakage and malicious
unlearning, that can lead to significant security and privacy concerns.
Moreover, extensive research indicates that unlearning methods and prevalent
attacks fulfill diverse roles within MU systems. This underscores the intricate
relationship and complex interplay among these mechanisms in maintaining system
functionality and safety. This survey aims to fill the gap between the
extensive number of studies on threats, attacks, and defenses in machine
unlearning and the absence of a comprehensive review that categorizes their
taxonomy, methods, and solutions, thus offering valuable insights for future
research directions and practical implementations.","['Ziyao Liu', 'Huanyi Ye', 'Chen Chen', 'Yongsen Zheng', 'Kwok-Yan Lam']",9,0.68340564
"Polarization, declining trust, and wavering support for democratic norms are
pressing threats to U.S. democracy. Exposure to verified and quality news may
lower individual susceptibility to these threats and make citizens more
resilient to misinformation, populism, and hyperpartisan rhetoric. This project
examines how to enhance users' exposure to and engagement with verified and
ideologically balanced news in an ecologically valid setting. We rely on a
large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on
28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users
tweeting about sports, entertainment, or lifestyle with a contextual reply
containing two hardcoded elements: a URL to the topic-relevant section of
quality news organization and an encouragement to follow its Twitter account.
To further test differential effects by gender of the bots, treated users were
randomly assigned to receive responses by bots presented as female or male. We
examine whether our over-time intervention enhances the following of news media
organization, the sharing and the liking of news content and the tweeting about
politics and the liking of political content. We find that the treated users
followed more news accounts and the users in the female bot treatment were more
likely to like news content than the control. Most of these results, however,
were small in magnitude and confined to the already politically interested
Twitter users, as indicated by their pre-treatment tweeting about politics.
These findings have implications for social media and news organizations, and
also offer direction for future work on how Large Language Models and other
computational interventions can effectively enhance individual on-platform
engagement with quality news and public affairs.","['Hadi Askari', 'Anshuman Chhabra', 'Bernhard Clemm von Hohenberg', 'Michael Heseltine', 'Magdalena Wojcieszak']",10,0.82956654
"With fact-checking by professionals being difficult to scale on social media,
algorithmic techniques have been considered. However, it is uncertain how the
public may react to labels by automated fact-checkers. In this study, we
investigate the use of automated warning labels derived from misinformation
detection literature and investigate their effects on three forms of post
engagement. Focusing on political posts, we also consider how partisanship
affects engagement. In a two-phases within-subjects experiment with 200
participants, we found that the generic warnings suppressed intents to comment
on and share posts, but not on the intent to like them. Furthermore, when
different reasons for the labels were provided, their effects on post
engagement were inconsistent, suggesting that the reasons could have
undesirably motivated engagement instead. Partisanship effects were observed
across the labels with higher engagement for politically congruent posts. We
discuss the implications on the design and use of automated warning labels.","['Gionnieve Lim', 'Simon T. Perrault']",3,0.666132
"In Singapore, there has been a rise in misinformation on mobile instant
messaging services (MIMS). MIMS support both small peer-to-peer networks and
large groups. Misinformation in the former may spread due to recipients' trust
in the sender while in the latter, misinformation can directly reach a wide
audience. The encryption of MIMS makes it difficult to address misinformation
directly. As such, chatbots have become an alternative solution where users can
disclose their chat content directly to fact checking services. To understand
how effective fact checking chatbots are as an intervention and how trust in
three different fact checkers (i.e., Government, News Outlets, and Artificial
Intelligence) may affect this trust, we conducted a within-subjects experiment
with 527 Singapore residents. We found mixed results for the fact checkers but
support for the chatbot intervention overall. We also found a striking
contradiction between participants' trust in the fact checkers and their
behaviour towards them. Specifically, those who reported a high level of trust
in the government performed worse and tended to follow the fact checking tool
less when it was endorsed by the government.","['Gionnieve Lim', 'Simon T. Perrault']",3,0.6107285
"With the increasing influence of social media, online misinformation has
grown to become a societal issue. The motivation for our work comes from the
threat caused by cheapfakes, where an unaltered image is described using a news
caption in a new but false-context. The main challenge in detecting such
out-of-context multimedia is the unavailability of large-scale datasets.
Several detection methods employ randomly selected captions to generate
out-of-context training inputs. However, these randomly matched captions are
not truly representative of out-of-context scenarios due to inconsistencies
between the image description and the matched caption. We aim to address these
limitations by introducing a novel task of out-of-context caption generation.
In this work, we propose a new method that generates a realistic out-of-context
caption given visual and textual context. We also demonstrate that the
semantics of the generated captions can be controlled using the textual
context. We also evaluate our method against several baselines and our method
improves over the image captioning baseline by 6.2% BLUE-4, 2.96% CiDEr, 11.5%
ROUGE, and 7.3% METEOR","['Anurag Singh', 'Shivangi Aneja']",7,0.6051024
"The status-quo of misinformation moderation is a central authority, usually
social platforms, deciding what content constitutes misinformation and how it
should be handled. However, to preserve users' autonomy, researchers have
explored democratized misinformation moderation. One proposition is to enable
users to assess content accuracy and specify whose assessments they trust. We
explore how these affordances can be provided on the web, without cooperation
from the platforms where users consume content. We present a browser extension
that empowers users to assess the accuracy of any content on the web and shows
the user assessments from their trusted sources in-situ. Through a two-week
user study, we report on how users perceive such a tool, the kind of content
users want to assess, and the rationales they use in their assessments. We
identify implications for designing tools that enable users to moderate content
for themselves with the help of those they trust.","['Farnaz Jahanbakhsh', 'David R. Karger']",0,0.58704627
"Real-world misinformation, often multimodal, can be partially or fully
factual but misleading using diverse tactics like conflating correlation with
causation. Such misinformation is severely understudied, challenging to
address, and harms various social domains, particularly on social media, where
it can spread rapidly. High-quality and timely correction of misinformation
that identifies and explains its (in)accuracies effectively reduces false
beliefs. Despite the wide acceptance of manual correction, it is difficult to
be timely and scalable. While LLMs have versatile capabilities that could
accelerate misinformation correction, they struggle due to a lack of recent
information, a tendency to produce false content, and limitations in addressing
multimodal information. We propose MUSE, an LLM augmented with access to and
credibility evaluation of up-to-date information. By retrieving evidence as
refutations or supporting context, MUSE identifies and explains content
(in)accuracies with references. It conducts multimodal retrieval and interprets
visual content to verify and correct multimodal content. Given the absence of a
comprehensive evaluation approach, we propose 13 dimensions of misinformation
correction quality. Then, fact-checking experts evaluate responses to social
media content that are not presupposed to be misinformation but broadly include
(partially) incorrect and correct posts that may (not) be misleading. Results
demonstrate MUSE's ability to write high-quality responses to potential
misinformation--across modalities, tactics, domains, political leanings, and
for information that has not previously been fact-checked online--within
minutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by
37% and even high-quality responses from laypeople by 29%. Our work provides a
general methodological and evaluative framework to correct misinformation at
scale.","['Xinyi Zhou', 'Ashish Sharma', 'Amy X. Zhang', 'Tim Althoff']",0,0.734661
"Social media platforms have become one of the main channels where people
disseminate and acquire information, of which the reliability is severely
threatened by rumors widespread in the network. Existing approaches such as
suspending users or broadcasting real information to combat rumors are either
with high cost or disturbing users. In this paper, we introduce a novel rumor
mitigation paradigm, where only a minimal set of links in the social network
are intervened to decelerate the propagation of rumors, countering
misinformation with low business cost and user awareness. A knowledge-informed
agent embodying rumor propagation mechanisms is developed, which intervenes the
social network with a graph neural network for capturing information flow in
the social media platforms and a policy network for selecting links.
Experiments on real social media platforms demonstrate that the proposed
approach can effectively alleviate the influence of rumors, substantially
reducing the affected populations by over 25%. Codes for this paper are
released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation.","['Hongyuan Su', 'Yu Zheng', 'Jingtao Ding', 'Depeng Jin', 'Yong Li']",3,0.67329437
"Creating high-quality and realistic images is now possible thanks to the
impressive advancements in image generation. A description in natural language
of your desired output is all you need to obtain breathtaking results. However,
as the use of generative models grows, so do concerns about the propagation of
malicious content and misinformation. Consequently, the research community is
actively working on the development of novel fake detection techniques,
primarily focusing on low-level features and possible fingerprints left by
generative models during the image generation process. In a different vein, in
our work, we leverage human semantic knowledge to investigate the possibility
of being included in frameworks of fake image detection. To achieve this, we
collect a novel dataset of partially manipulated images using diffusion models
and conduct an eye-tracking experiment to record the eye movements of different
observers while viewing real and fake stimuli. A preliminary statistical
analysis is conducted to explore the distinctive patterns in how humans
perceive genuine and altered images. Statistical findings reveal that, when
perceiving counterfeit samples, humans tend to focus on more confined regions
of the image, in contrast to the more dispersed observational pattern observed
when viewing genuine images. Our dataset is publicly available at:
https://github.com/aimagelab/unveiling-the-truth.","['Giuseppe Cartella', 'Vittorio Cuculo', 'Marcella Cornia', 'Rita Cucchiara']",7,0.8446877
"What if misinformation is not an information problem at all? To understand
the role of news publishers in potentially unintentionally propagating
misinformation, we examine how far-right and fringe online groups share and
leverage established legacy news media articles to advance their narratives.
Our findings suggest that online fringe ideologies spread through the use of
content that is consensus-based and ""factually correct"". We found that
Australian news publishers with both moderate and far-right political leanings
contain comparable levels of information completeness and quality; and
furthermore, that far-right Twitter users often share from moderate sources.
However, a stark difference emerges when we consider two additional factors: 1)
the narrow topic selection of articles by far-right users, suggesting that they
cherry pick only news articles that engage with their preexisting worldviews
and specific topics of concern, and 2) the difference between moderate and
far-right publishers when we examine the writing style of their articles.
Furthermore, we can identify users prone to sharing misinformation based on
their communication style. These findings have important implications for
countering online misinformation, as they highlight the powerful role that
personal biases towards specific topics and publishers' writing styles have in
amplifying fringe ideologies online.","['JooYoung Lee', 'Emily Booth', 'Hany Farid', 'Marian-Andrei Rizoiu']",3,0.72910523
"This survey provides an in-depth analysis of knowledge conflicts for large
language models (LLMs), highlighting the complex challenges they encounter when
blending contextual and parametric knowledge. Our focus is on three categories
of knowledge conflicts: context-memory, inter-context, and intra-memory
conflict. These conflicts can significantly impact the trustworthiness and
performance of LLMs, especially in real-world applications where noise and
misinformation are common. By categorizing these conflicts, exploring the
causes, examining the behaviors of LLMs under such conflicts, and reviewing
available solutions, this survey aims to shed light on strategies for improving
the robustness of LLMs, thereby serving as a valuable resource for advancing
research in this evolving area.","['Rongwu Xu', 'Zehan Qi', 'Zhijiang Guo', 'Cunxiang Wang', 'Hongru Wang', 'Yue Zhang', 'Wei Xu']",6,0.6789211
"The ability to accurately identify authorship is crucial for verifying
content authenticity and mitigating misinformation. Large Language Models
(LLMs) have demonstrated an exceptional capacity for reasoning and
problem-solving. However, their potential in authorship analysis remains
under-explored. Traditional studies have depended on hand-crafted stylistic
features, whereas state-of-the-art approaches leverage text embeddings from
pre-trained language models. These methods, which typically require fine-tuning
on labeled data, often suffer from performance degradation in cross-domain
applications and provide limited explainability. This work seeks to address
three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship
verification effectively? (2) Are LLMs capable of accurately attributing
authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs
provide explainability in authorship analysis, particularly through the role of
linguistic features? Moreover, we investigate the integration of explicit
linguistic features to guide LLMs in their reasoning processes. Our assessment
demonstrates LLMs' proficiency in both tasks without the need for
domain-specific fine-tuning, providing explanations into their decision making
via a detailed analysis of linguistic features. This establishes a new
benchmark for future research on LLM-based authorship analysis.","['Baixiang Huang', 'Canyu Chen', 'Kai Shu']",6,0.7922981
"In the midst of widespread misinformation and disinformation through social
media and the proliferation of AI-generated texts, it has become increasingly
difficult for people to validate and trust information they encounter. Many
fact-checking approaches and tools have been developed, but they often lack
appropriate explainability or granularity to be useful in various contexts. A
text validation method that is easy to use, accessible, and can perform
fine-grained evidence attribution has become crucial. More importantly,
building user trust in such a method requires presenting the rationale behind
each prediction, as research shows this significantly influences people's
belief in automated systems. Localizing and bringing users' attention to the
specific problematic content is also paramount, instead of providing simple
blanket labels. In this paper, we present ClaimVer, a human-centric framework
tailored to meet users' informational and verification needs by generating rich
annotations and thereby reducing cognitive load. Designed to deliver
comprehensive evaluations of texts, it highlights each claim, verifies it
against a trusted knowledge graph (KG), presents the evidence, and provides
succinct, clear explanations for each claim prediction. Finally, our framework
introduces an attribution score, enhancing applicability across a wide range of
downstream tasks.","['Preetam Prabhu Srikar Dammu', 'Himanshu Naidu', 'Mouly Dewan', 'YoungMin Kim', 'Tanya Roosta', 'Aman Chadha', 'Chirag Shah']",1,0.8057021
"Advances in Generative Artificial Intelligence (AI) are resulting in
AI-generated media output that is (nearly) indistinguishable from human-created
content. This can drastically impact users and the media sector, especially
given global risks of misinformation. While the currently discussed European AI
Act aims at addressing these risks through Article 52's AI transparency
obligations, its interpretation and implications remain unclear. In this early
work, we adopt a participatory AI approach to derive key questions based on
Article 52's disclosure obligations. We ran two workshops with researchers,
designers, and engineers across disciplines (N=16), where participants
deconstructed Article 52's relevant clauses using the 5W1H framework. We
contribute a set of 149 questions clustered into five themes and 18 sub-themes.
We believe these can not only help inform future legal developments and
interpretations of Article 52, but also provide a starting point for
Human-Computer Interaction research to (re-)examine disclosure transparency
from a human-centered AI lens.","['Abdallah El Ali', 'Karthikeya Puttur Venkatraj', 'Sophie Morosoli', 'Laurens Naudts', 'Natali Helberger', 'Pablo Cesar']",9,0.6876216
"The internet has brought both benefits and harms to society. A prime example
of the latter is misinformation, including conspiracy theories, which flood the
web. Recent advances in natural language processing, particularly the emergence
of large language models (LLMs), have improved the prospects of accurate
misinformation detection. However, most LLM-based approaches to conspiracy
theory detection focus only on binary classification and fail to account for
the important relationship between misinformation and affective features (i.e.,
sentiment and emotions). Driven by a comprehensive analysis of conspiracy text
that reveals its distinctive affective features, we propose ConspEmoLLM, the
first open-source LLM that integrates affective information and is able to
perform diverse tasks relating to conspiracy theories. These tasks include not
only conspiracy theory detection, but also classification of theory type and
detection of related discussion (e.g., opinions towards theories). ConspEmoLLM
is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,
which includes five tasks to support LLM instruction tuning and evaluation. We
demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms
several open-source general domain LLMs and ChatGPT, as well as an LLM that has
been fine-tuned using ConDID, but which does not use affective features. This
project will be released on https://github.com/lzw108/ConspEmoLLM/.","['Zhiwei Liu', 'Boyang Liu', 'Paul Thompson', 'Kailai Yang', 'Sophia Ananiadou']",6,0.77629244
"Online misinformation poses a global risk with harmful implications for
society. Ordinary social media users are known to actively reply to
misinformation posts with counter-misinformation messages, which is shown to be
effective in containing the spread of misinformation. Such a practice is
defined as ""social correction"". Nevertheless, it remains unknown how users
respond to social correction in real-world scenarios, especially, will it have
a corrective or backfire effect on users. Investigating this research question
is pivotal for developing and refining strategies that maximize the efficacy of
social correction initiatives. To fill this gap, we conduct an in-depth study
to characterize and predict the user response to social correction in a
data-driven manner through the lens of X (Formerly Twitter), where the user
response is instantiated as the reply that is written toward a
counter-misinformation message. Particularly, we first create a novel dataset
with 55, 549 triples of misinformation tweets, counter-misinformation replies,
and responses to counter-misinformation replies, and then curate a taxonomy to
illustrate different kinds of user responses. Next, fine-grained statistical
analysis of reply linguistic and engagement features as well as repliers' user
attributes is conducted to illustrate the characteristics that are significant
in determining whether a reply will have a corrective or backfire effect.
Finally, we build a user response prediction model to identify whether a social
correction will be corrective, neutral, or have a backfire effect, which
achieves a promising F1 score of 0.816. Our work enables stakeholders to
monitor and predict user responses effectively, thus guiding the use of social
correction to maximize their corrective impact and minimize backfire effects.
The code and data is accessible on
https://github.com/claws-lab/response-to-social-correction.","['Bing He', 'Yingchen Ma', 'Mustaque Ahamad', 'Srijan Kumar']",0,0.67082417
"This note considers an innovative interdisciplinary methodology that bridges
the gap between the fundamental principles of quantum mechanics applied to the
study of materials such as tellurium nanoparticles (TeNPs) and graphene and the
complex dynamics of social systems. The basis for this approach lies in the
metaphorical parallels drawn between the structural features of TeNPs and
graphene and the behavioral patterns of social groups in the face of
misinformation. TeNPs exhibit unique properties such as the strengthening of
covalent bonds within telluric chains and the disruption of secondary structure
leading to the separation of these chains. This is analogous to increased
cohesion within social groups and disruption of information flow between
different subgroups, respectively. . Similarly, the outstanding properties of
graphene, such as high electrical conductivity, strength, and flexibility,
provide additional aspects for understanding the resilience and adaptability of
social structures in response to external stimuli such as fake news. This
research note proposes a novel metaphorical framework for analyzing the spread
of fake news within social groups, analogous to the structural features of
telluric nanoparticles (TeNPs). We investigate how the strengthening of
covalent bonds within TeNPs reflects the strengthening of social cohesion in
groups that share common beliefs and values. This paper is partially an attempt
to utilize ""Generative AI"" and was written with educational intent. There are
currently no plans for it to become a peer-reviewed paper.",['Yasuko Kawahata'],2,0.58607304
"Misinformation is a prevalent societal issue due to its potential high risks.
Out-of-context (OOC) misinformation, where authentic images are repurposed with
false text, is one of the easiest and most effective ways to mislead audiences.
Current methods focus on assessing image-text consistency but lack convincing
explanations for their judgments, which is essential for debunking
misinformation. While Multimodal Large Language Models (MLLMs) have rich
knowledge and innate capability for visual reasoning and explanation
generation, they still lack sophistication in understanding and discovering the
subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel
multimodal large language model specifically engineered for OOC misinformation
detection and explanation. SNIFFER employs two-stage instruction tuning on
InstructBLIP. The first stage refines the model's concept alignment of generic
objects with news-domain entities and the second stage leverages language-only
GPT-4 generated OOC-specific instruction data to fine-tune the model's
discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not
only detects inconsistencies between text and image but also utilizes external
knowledge for contextual verification. Our experiments show that SNIFFER
surpasses the original MLLM by over 40% and outperforms state-of-the-art
methods in detection accuracy. SNIFFER also provides accurate and persuasive
explanations as validated by quantitative and human evaluations.","['Peng Qi', 'Zehong Yan', 'Wynne Hsu', 'Mong Li Lee']",1,0.735585
"This note explores the innovative application of soliton theory and plasmonic
phenomena in modeling user behavior and engagement within digital health
platforms. By introducing the concept of soliton solutions, we present a novel
approach to understanding stable patterns of health improvement behaviors over
time. Additionally, we delve into the role of tellurium nanoparticles and their
plasmonic properties in adsorbing fake news, thereby influencing user
interactions and engagement levels. Through a theoretical framework that
combines nonlinear dynamics with the unique characteristics of tellurium
nanoparticles, we aim to provide new insights into the dynamics of user
engagement in digital health environments. Our analysis highlights the
potential of soliton theory in capturing the complex, nonlinear dynamics of
user behavior, while the application of plasmonic phenomena offers a promising
avenue for enhancing the sensitivity and effectiveness of digital health
platforms. This research ventures into an uncharted territory where optical
phenomena such as Brewster's Angle and Snell's Law, along with the concept of
spin solitons, are metaphorically applied to address the challenge of fake news
dissemination. By exploring the analogy between light refraction, reflection,
and the propagation of information in digital platforms, we unveil a novel
perspective on how the 'angle' at which information is presented can
significantly affect its acceptance and spread. Additionally, we propose the
use of tellurium nanoparticles to manage 'information waves' through mechanisms
akin to plasmonic resonance and soliton dynamics. This theoretical exploration
aims to bridge the gap between physical sciences and digital communication,
offering insights into the development of strategies for mitigating
misinformation.",['Yasuko Kawahata'],2,0.62829185
"The increase in active users on social networking sites (SNSs) has also
observed an increase in harmful content on social media sites. Harmful content
is described as an inappropriate activity to harm or deceive an individual or a
group of users. Alongside existing methods to detect misinformation and hate
speech, users still need to be well-informed about the harmfulness of the
content on SNSs. This study proposes a user-interactive system TweetInfo for
mitigating the consumption of harmful content by providing metainformation
about the posts. It focuses on two types of harmful content: hate speech and
misinformation. TweetInfo provides insights into tweets by doing content
analysis. Based on previous research, we have selected a list of
metainformation. We offer the option to filter content based on metainformation
Bot, Hate Speech, Misinformation, Verified Account, Sentiment, Tweet Category,
Language. The proposed user interface allows customising the user's timeline to
mitigate harmful content. This study present the demo version of the propose
user interface of TweetInfo.",['Gautam Kishore Shahi'],3,0.63662255
"We have witnessed lately a rapid proliferation of advanced Large Language
Models (LLMs) capable of generating high-quality text. While these LLMs have
revolutionized text generation across various domains, they also pose
significant risks to the information ecosystem, such as the potential for
generating convincing propaganda, misinformation, and disinformation at scale.
This paper offers a review of AI-generated text forensic systems, an emerging
field addressing the challenges of LLM misuses. We present an overview of the
existing efforts in AI-generated text forensics by introducing a detailed
taxonomy, focusing on three primary pillars: detection, attribution, and
characterization. These pillars enable a practical understanding of
AI-generated text, from identifying AI-generated content (detection),
determining the specific AI model involved (attribution), and grouping the
underlying intents of the text (characterization). Furthermore, we explore
available resources for AI-generated text forensics research and discuss the
evolving challenges and future directions of forensic systems in an AI era.","['Tharindu Kumarage', 'Garima Agrawal', 'Paras Sheth', 'Raha Moraffah', 'Aman Chadha', 'Joshua Garland', 'Huan Liu']",9,0.7270816
"The rapid spread of misinformation through social media platforms has raised
concerns regarding its impact on public opinion. While misinformation is
prevalent in other languages, the majority of research in this field has
concentrated on the English language. Hence, there is a scarcity of datasets
for other languages, including Turkish. To address this concern, we have
introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset
spans multiple domains and incorporates evidence collected from three Turkish
fact-checking organizations. Additionally, we aim to assess the effectiveness
of cross-lingual transfer learning for low-resource languages, with a
particular focus on Turkish. We demonstrate in-context learning (zero-shot and
few-shot) performance of large language models in this context. The
experimental results indicate that the dataset has the potential to advance
research in the Turkish language.","['Recep Firat Cekinel', 'Pinar Karagoz', 'Cagri Coltekin']",8,0.79813635
"Model editing has emerged as a cost-effective strategy to update knowledge
stored in language models. However, model editing can have unintended
consequences after edits are applied: information unrelated to the edits can
also be changed, and other general behaviors of the model can be wrongly
altered. In this work, we investigate how model editing methods unexpectedly
amplify model biases post-edit. We introduce a novel benchmark dataset,
Seesaw-CF, for measuring bias-related harms of model editing and conduct the
first in-depth investigation of how different weight-editing methods impact
model bias. Specifically, we focus on biases with respect to demographic
attributes such as race, geographic origin, and gender, as well as qualitative
flaws in long-form texts generated by edited language models. We find that
edited models exhibit, to various degrees, more biased behavior as they become
less confident in attributes for Asian, African, and South American subjects.
Furthermore, edited models amplify sexism and xenophobia in text generations
while remaining seemingly coherent and logical. Finally, editing facts about
place of birth, country of citizenship, or gender have particularly negative
effects on the model's knowledge about unrelated features like field of work.","['Karina Halevy', 'Anna Sotnikova', 'Badr AlKhamissi', 'Syrielle Montariol', 'Antoine Bosselut']",1,0.5855002
"More than six million people died of the COVID-19 by April 2022. The heavy
casualties have put people on great and urgent alert and people try to find all
kinds of information to keep them from being inflected by the coronavirus. This
research tries to find out whether the mobile health text information sent to
peoples devices is correct as smartphones becoming the major information source
for people. The proposed method uses various mobile information retrieval and
data mining technologies including lexical analysis, stopword elimination,
stemming, and decision trees to classify the mobile health text information to
one of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv)
disinformative, and (v) neutral. Experiment results show the accuracy of the
proposed method is above the threshold value 50 percentage, but is not optimal.
It is because the problem, mobile text misinformation identification, is
intrinsically difficult.","['Wen-Chen Hu', 'Sanjaikanth E Vadakkethil Somanathan Pillai', 'Abdelrahman Ahmed ElSaid']",5,0.6094254
"We present VIXEN - a technique that succinctly summarizes in text the visual
differences between a pair of images in order to highlight any content
manipulation present. Our proposed network linearly maps image features in a
pairwise manner, constructing a soft prompt for a pretrained large language
model. We address the challenge of low volume of training data and lack of
manipulation variety in existing image difference captioning (IDC) datasets by
training on synthetically manipulated images from the recent InstructPix2Pix
dataset generated via prompt-to-prompt editing framework. We augment this
dataset with change summaries produced via GPT-3. We show that VIXEN produces
state-of-the-art, comprehensible difference captions for diverse image contents
and edit types, offering a potential mitigation against misinformation
disseminated via manipulated image content. Code and data are available at
http://github.com/alexblck/vixen","['Alexander Black', 'Jing Shi', 'Yifei Fan', 'Tu Bui', 'John Collomosse']",7,0.647369
"With the primary goal of raising readers' awareness of misinformation
phenomena, extensive efforts have been made by both academic institutions and
independent organizations to develop methodologies for assessing the
trustworthiness of online news publishers. Unfortunately, existing approaches
are costly and face critical scalability challenges. This study presents a
novel framework for assessing the trustworthiness of online news publishers
using user interactions on social media platforms. The proposed methodology
provides a versatile solution that serves the dual purpose of i) identifying
verifiable online publishers and ii) automatically performing an initial
estimation of the trustworthiness of previously unclassified online news
outlets.","['Manuel Pratelli', 'Fabio Saracco', 'Marinella Petrocchi']",0,0.6285053
"Pre-training of neural networks has recently revolutionized the field of
Natural Language Processing (NLP) and has before demonstrated its effectiveness
in computer vision. At the same time, advances around the detection of fake
news were mainly driven by the context-based paradigm, where different types of
signals (e.g. from social media) form graph-like structures that hold
contextual information apart from the news article to classify. We propose to
merge these two developments by applying pre-training of Graph Neural Networks
(GNNs) in the domain of context-based fake news detection. Our experiments
provide an evaluation of different pre-training strategies for graph-based
misinformation detection and demonstrate that transfer learning does currently
not lead to significant improvements over training a model from scratch in the
domain. We argue that a major current issue is the lack of suitable large-scale
resources that can be used for pre-training.","['Gregor Donabauer', 'Udo Kruschwitz']",2,0.68549216
"Large language models generate high-quality responses with potential
misinformation, underscoring the need for regulation by distinguishing
AI-generated and human-written texts. Watermarking is pivotal in this context,
which involves embedding hidden markers in texts during the LLM inference
phase, which is imperceptible to humans. Achieving both the detectability of
inserted watermarks and the semantic quality of generated texts is challenging.
While current watermarking algorithms have made promising progress in this
direction, there remains significant scope for improvement. To address these
challenges, we introduce a novel multi-objective optimization (MOO) approach
for watermarking that utilizes lightweight networks to generate token-specific
watermarking logits and splitting ratios. By leveraging MOO to optimize for
both detection and semantic objective functions, our method simultaneously
achieves detectability and semantic integrity. Experimental results show that
our method outperforms current watermarking techniques in enhancing the
detectability of texts generated by LLMs while maintaining their semantic
coherence. Our code is available at https://github.com/mignonjia/TS_watermark.","['Mingjia Huo', 'Sai Ashish Somayajula', 'Youwei Liang', 'Ruisi Zhang', 'Farinaz Koushanfar', 'Pengtao Xie']",7,0.6731546
"Prediction models are increasingly proposed for guiding treatment decisions,
but most fail to address the special role of treatments, leading to
inappropriate use. This paper highlights the limitations of using standard
prediction models for treatment decision support. We identify `causal blind
spots' in three common approaches to handling treatments in prediction
modelling: including treatment as a predictor, restricting data based on
treatment status and ignoring treatments. When predictions are used to inform
treatment decisions, confounders, colliders and mediators, as well as changes
in treatment protocols over time may lead to misinformed decision-making. We
illustrate potential harmful consequences in several medical applications. We
advocate for an extension of guidelines for development, reporting and
evaluation of prediction models to ensure that the intended use of the model is
matched to an appropriate risk estimand. When prediction models are intended to
inform treatment decisions, prediction models should specify upfront the
treatment decisions they aim to support and target a prediction estimand in
line with that goal. This requires a shift towards developing predictions under
the specific treatment options under consideration (`predictions under
interventions'). Predictions under interventions need causal reasoning and
inference techniques during development and validation. We argue that this will
improve the efficacy of prediction models in guiding treatment decisions and
prevent potential negative effects on patient outcomes.","['Nan van Geloven', 'Ruth H Keogh', 'Wouter van Amsterdam', 'Giovanni Cin√†', 'Jesse H. Krijthe', 'Niels Peek', 'Kim Luijken', 'Sara Magliacane', 'Pawe≈Ç Morzywo≈Çek', 'Thijs van Ommen', 'Hein Putter', 'Matthew Sperrin', 'Junfeng Wang', 'Daniala L. Weir', 'Vanessa Didelez']",2,0.49400693
"As more users turn to video-sharing platforms like YouTube as an information
source, they may consume misinformation despite their best efforts. In this
work, we investigate ways that users can better assess the credibility of
videos by first exploring how users currently determine credibility using
existing signals on platforms and then by introducing and evaluating new
credibility-based signals. We conducted 12 contextual inquiry interviews with
YouTube users, determining that participants used a combination of existing
signals, such as the channel name, the production quality, and prior knowledge,
to evaluate credibility, yet sometimes stumbled in their efforts to do so. We
then developed Viblio, a prototype system that enables YouTube users to view
and add citations and related information while watching a video based on our
participants' needs. From an evaluation with 12 people, all participants found
Viblio to be intuitive and useful in the process of evaluating a video's
credibility and could see themselves using Viblio in the future.","['Emelia Hughes', 'Renee Wang', 'Prerna Juneja', 'Tony Li', 'Tanu Mitra', 'Amy Zhang']",4,0.53839636
"The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect
anomalous nodes in an input graph while avoiding biased predictions against
individuals from sensitive subgroups. However, the current literature does not
comprehensively discuss this problem, nor does it provide realistic datasets
that encompass actual graph structures, anomaly labels, and sensitive
attributes. To bridge this gap, we introduce a formal definition of the FairGAD
problem and present two novel datasets constructed from the social media
platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000
edges associated with 9,000 and 47,000 nodes, respectively, and leverage
political leanings as sensitive attributes and misinformation spreaders as
anomaly labels. We demonstrate that our FairGAD datasets significantly differ
from the synthetic datasets used by the research community. Using our datasets,
we investigate the performance-fairness trade-off in nine existing GAD and
non-graph AD methods on five state-of-the-art fairness methods. Our code and
datasets are available at https://github.com/nigelnnk/FairGAD","['Neng Kai Nigel Neo', 'Yeon-Chang Lee', 'Yiqiao Jin', 'Sang-Wook Kim', 'Srijan Kumar']",2,0.6012094
"As machine- and AI-generated content proliferates, protecting the
intellectual property of generative models has become imperative, yet verifying
data ownership poses formidable challenges, particularly in cases of
unauthorized reuse of generated data. The challenge of verifying data ownership
is further amplified by using Machine Learning as a Service (MLaaS), which
often functions as a black-box system.
  Our work is dedicated to detecting data reuse from even an individual sample.
Traditionally, watermarking has been leveraged to detect AI-generated content.
However, unlike watermarking techniques that embed additional information as
triggers into models or generated content, potentially compromising output
quality, our approach identifies latent fingerprints inherently present within
the outputs through re-generation. We propose an explainable verification
procedure that attributes data ownership through re-generation, and further
amplifies these fingerprints in the generative models through iterative data
re-generation. This methodology is theoretically grounded and demonstrates
viability and robustness using recent advanced text and image generative
models. Our methodology is significant as it goes beyond protecting the
intellectual property of APIs and addresses important issues such as the spread
of misinformation and academic misconduct. It provides a useful tool to ensure
the integrity of sources and authorship, expanding its application in different
scenarios where authenticity and ownership verification are essential.","['Aditya Desu', 'Xuanli He', 'Qiongkai Xu', 'Wei Lu']",7,0.7690709
"Anthropomorphic social bots are engineered to emulate human verbal
communication and generate toxic or inflammatory content across social
networking services (SNSs). Bot-disseminated misinformation could subtly yet
profoundly reshape societal processes by complexly interweaving factors like
repeated disinformation exposure, amplified political polarization, compromised
indicators of democratic health, shifted perceptions of national identity,
propagation of false social norms, and manipulation of collective memory over
time. However, extrapolating bots' pluripotency across hybridized,
multilingual, and heterogeneous media ecologies from isolated SNS analyses
remains largely unknown, underscoring the need for a comprehensive framework to
characterise bots' emergent risks to civic discourse. Here we propose an
interdisciplinary framework to characterise bots' pluripotency, incorporating
quantification of influence, network dynamics monitoring, and interlingual
feature analysis. When applied to the geopolitical discourse around the
Russo-Ukrainian conflict, results from interlanguage toxicity profiling and
network analysis elucidated spatiotemporal trajectories of pro-Russian and
pro-Ukrainian human and bots across hybrid SNSs. Weaponized bots predominantly
inhabited X, while human primarily populated Reddit in the social media
warfare. This rigorous framework promises to elucidate interlingual homogeneity
and heterogeneity in bots' pluripotent behaviours, revealing synergistic
human-bot mechanisms underlying regimes of information manipulation, echo
chamber formation, and collective memory manifestation in algorithmically
structured societies.","['Wentao Xu', 'Kazutoshi Sasahara', 'Jianxun Chu', 'Bin Wang', 'Wenlu Fan', 'Zhiwen Hu']",13,0.66771865
"Understanding the dissemination of diseases, information, and behavior stands
as a paramount research challenge in contemporary network and complex systems
science. The COVID-19 pandemic and the proliferation of misinformation are
relevant examples of the importance of these dynamic processes, which have
recently gained more attention due to the potential of higher-order networks to
unlock new avenues for their investigation. Despite being in its early stages,
the examination of social contagion in higher-order networks has witnessed a
surge of novel research and concepts, revealing different functional forms for
the spreading dynamics and offering novel insights. This review presents a
focused overview of this body of literature and proposes a unified formalism
that covers most of these forms. The goal is to underscore the similarities and
distinctions among various models, to motivate further research on the general
and universal properties of such models. We also highlight that while the path
for additional theoretical exploration appears clear, the empirical validation
of these models through data or experiments remains scant, with an unsettled
roadmap as of today. We therefore conclude with some perspectives aimed at
providing possible research directions that could contribute to a better
understanding of this class of dynamical processes, both from a theoretical and
a data-oriented point of view.","['Guilherme Ferraz de Arruda', 'Alberto Aleta', 'Yamir Moreno']",0,0.646976
"Augmented generation techniques such as Retrieval-Augmented Generation (RAG)
and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing
large language model (LLM) outputs with external knowledge and cached
information. However, the integration of vector databases, which serve as a
backbone for these augmentations, introduces critical challenges, particularly
in ensuring accurate vector matching. False vector matching in these databases
can significantly compromise the integrity and reliability of LLM outputs,
leading to misinformation or erroneous responses. Despite the crucial impact of
these issues, there is a notable research gap in methods to effectively detect
and address false vector matches in LLM-augmented generation. This paper
presents MeTMaP, a metamorphic testing framework developed to identify false
vector matching in LLM-augmented generation systems. We derive eight
metamorphic relations (MRs) from six NLP datasets, which form our method's
core, based on the idea that semantically similar texts should match and
dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets
for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over
203 vector matching configurations, involving 29 embedding models and 7
distance metrics, uncovers significant inaccuracies. The results, showing a
maximum accuracy of only 41.51\% on our tests compared to the original
datasets, emphasize the widespread issue of false matches in vector matching
methods and the critical need for effective detection and mitigation in
LLM-augmented applications.","['Guanyu Wang', 'Yuekang Li', 'Yi Liu', 'Gelei Deng', 'Tianlin Li', 'Guosheng Xu', 'Yang Liu', 'Haoyu Wang', 'Kailong Wang']",6,0.7491248
"Scientific facts are often spun in the popular press with the intent to
influence public opinion and action, as was evidenced during the COVID-19
pandemic. Automatic detection of misinformation in the scientific domain is
challenging because of the distinct styles of writing in these two media types
and is still in its nascence. Most research on the validity of scientific
reporting treats this problem as a claim verification challenge. In doing so,
significant expert human effort is required to generate appropriate claims. Our
solution bypasses this step and addresses a more real-world scenario where such
explicit, labeled claims may not be available. The central research question of
this paper is whether it is possible to use large language models (LLMs) to
detect misinformation in scientific reporting. To this end, we first present a
new labeled dataset SciNews, containing 2.4k scientific news stories drawn from
trusted and untrustworthy sources, paired with related abstracts from the
CORD-19 database. Our dataset includes both human-written and LLM-generated
news articles, making it more comprehensive in terms of capturing the growing
trend of using LLMs to generate popular press articles. Then, we identify
dimensions of scientific validity in science news articles and explore how this
can be integrated into the automated detection of scientific misinformation. We
propose several baseline architectures using LLMs to automatically detect false
representations of scientific findings in the popular press. For each of these
architectures, we use several prompt engineering strategies including
zero-shot, few-shot, and chain-of-thought prompting. We also test these
architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,
Llama2-13B.","['Yupeng Cao', 'Aishwarya Muralidharan Nair', 'Elyon Eyimife', 'Nastaran Jamalipour Soofi', 'K. P. Subbalakshmi', 'John R. Wullert II', 'Chumki Basu', 'David Shallcross']",1,0.7020515
"Many deep learning synthetic speech generation tools are readily available.
The use of synthetic speech has caused financial fraud, impersonation of
people, and misinformation to spread. For this reason forensic methods that can
detect synthetic speech have been proposed. Existing methods often overfit on
one dataset and their performance reduces substantially in practical scenarios
such as detecting synthetic speech shared on social platforms. In this paper we
propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a
synthetic speech detector that converts a time domain speech signal to a
mel-spectrogram and processes it in patches using a transformer neural network.
We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our
experiments show that PS3DT performs well on ASVspoof2019 dataset compared to
other approaches using spectrogram for synthetic speech detection. We also
investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT
generalizes well than several existing methods on detecting synthetic speech
from an out-of-distribution dataset. We also evaluate robustness of PS3DT to
detect telephone quality synthetic speech and synthetic speech shared on social
platforms (compressed speech). PS3DT is robust to compression and can detect
telephone quality synthetic speech better than several existing methods.","['Amit Kumar Singh Yadav', 'Ziyue Xiang', 'Kratika Bhagtani', 'Paolo Bestagini', 'Stefano Tubaro', 'Edward J. Delp']",11,0.6512809
"Social media platforms are hubs for multimodal information exchange,
encompassing text, images, and videos, making it challenging for machines to
comprehend the information or emotions associated with interactions in online
spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising
solution to these challenges, yet they struggle to accurately interpret human
emotions and complex content such as misinformation. This paper introduces
MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of
multimodal social media content. MM-Soc compiles prominent multimodal datasets
and incorporates a novel large-scale YouTube tagging dataset, targeting a range
of tasks from misinformation detection, hate speech detection, and social
context generation. Through our exhaustive evaluation on ten size-variants of
four open-source MLLMs, we have identified significant performance disparities,
highlighting the need for advancements in models' social understanding
capabilities. Our analysis reveals that, in a zero-shot setting, various types
of MLLMs generally exhibit difficulties in handling social media tasks.
However, MLLMs demonstrate performance improvements post fine-tuning,
suggesting potential pathways for improvement. Our code and data are available
at https://github.com/claws-lab/MMSoc.git.","['Yiqiao Jin', 'Minje Choi', 'Gaurav Verma', 'Jindong Wang', 'Srijan Kumar']",6,0.74324614
"Factual inconsistency with source documents in automatically generated
summaries can lead to misinformation or pose risks. Existing factual
consistency(FC) metrics are constrained by their performance, efficiency, and
explainability. Recent advances in Large language models (LLMs) have
demonstrated remarkable potential in text evaluation but their effectiveness in
assessing FC in summarisation remains underexplored. Prior research has mostly
focused on proprietary LLMs, leaving essential factors that affect their
assessment capabilities unexplored. Additionally, current FC evaluation
benchmarks are restricted to news articles, casting doubt on the generality of
the FC methods tested on them. In this paper, we first address the gap by
introducing TreatFact a dataset of LLM-generated summaries of clinical texts,
annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC
evaluation across news and clinical domains and analyse the impact of model
size, prompts, pre-training and fine-tuning data. Our findings reveal that
despite proprietary models prevailing on the task, open-source LLMs lag behind.
Nevertheless, there is potential for enhancing the performance of open-source
LLMs through increasing model size, expanding pre-training data, and developing
well-curated fine-tuning data. Experiments on TreatFact suggest that both
previous methods and LLM-based evaluators are unable to capture factual
inconsistencies in clinical summaries, posing a new challenge for FC
evaluation.","['Zheheng Luo', 'Qianqian Xie', 'Sophia Ananiadou']",6,0.7579745
"Dense retrievers and retrieval-augmented language models have been widely
used in various NLP applications. Despite being designed to deliver reliable
and secure outcomes, the vulnerability of retrievers to potential attacks
remains unclear, raising concerns about their security. In this paper, we
introduce a novel scenario where the attackers aim to covertly disseminate
targeted misinformation, such as hate speech or advertisement, through a
retrieval system. To achieve this, we propose a perilous backdoor attack
triggered by grammar errors in dense passage retrieval. Our approach ensures
that attacked models can function normally for standard queries but are
manipulated to return passages specified by the attacker when users
unintentionally make grammatical mistakes in their queries. Extensive
experiments demonstrate the effectiveness and stealthiness of our proposed
attack method. When a user query is error-free, our model consistently
retrieves accurate information while effectively filtering out misinformation
from the top-k results. However, when a query contains grammar errors, our
system shows a significantly higher success rate in fetching the targeted
content.","['Quanyu Long', 'Yue Deng', 'LeiLei Gan', 'Wenya Wang', 'Sinno Jialin Pan']",1,0.67269033
"Due to their unprecedented ability to process and respond to various types of
data, Multimodal Large Language Models (MLLMs) are constantly defining the new
boundary of Artificial General Intelligence (AGI). As these advanced generative
models increasingly form collaborative networks for complex tasks, the
integrity and security of these systems are crucial. Our paper, ``The Wolf
Within'', explores a novel vulnerability in MLLM societies - the indirect
propagation of malicious content. Unlike direct harmful output generation for
MLLMs, our research demonstrates how a single MLLM agent can be subtly
influenced to generate prompts that, in turn, induce other MLLM agents in the
society to output malicious content. Our findings reveal that, an MLLM agent,
when manipulated to produce specific prompts or instructions, can effectively
``infect'' other agents within a society of MLLMs. This infection leads to the
generation and circulation of harmful outputs, such as dangerous instructions
or misinformation, across the society. We also show the transferability of
these indirectly generated prompts, highlighting their possibility in
propagating malice through inter-agent communication. This research provides a
critical insight into a new dimension of threat posed by MLLMs, where a single
agent can act as a catalyst for widespread malevolent influence. Our work
underscores the urgent need for developing robust mechanisms to detect and
mitigate such covert manipulations within MLLM societies, ensuring their safe
and ethical utilization in societal applications.","['Zhen Tan', 'Chengshuai Zhao', 'Raha Moraffah', 'Yifan Li', 'Yu Kong', 'Tianlong Chen', 'Huan Liu']",6,0.7545289
"Fact-checking-specific search tools such as Google Fact Check are a promising
way to combat misinformation on social media, especially during events bringing
significant social influence, such as the COVID-19 pandemic and the U.S.
presidential elections. However, the usability of such an approach has not been
thoroughly studied. We evaluated the performance of Google Fact Check by
analyzing the retrieved fact-checking results regarding 1,000 COVID-19-related
false claims and found it able to retrieve the fact-checking results for 15.8%
of the input claims, and the rendered results are relatively reliable. We also
found that the false claims receiving different fact-checking verdicts (i.e.,
""False,"" ""Partly False,"" ""True,"" and ""Unratable"") tend to reflect diverse
emotional tones, and fact-checking sources tend to check the claims in
different lengths and using dictionary words to various extents. Claim
variations addressing the same issue yet described differently are likely to
retrieve distinct fact-checking results. We suggest that the quantities of the
retrieved fact-checking results could be optimized and that slightly adjusting
input wording may be the best practice for users to retrieve more useful
information. This study aims to contribute to the understanding of
state-of-the-art fact-checking tools and information integrity.","['Qiangeng Yang', 'Tess Christensen', 'Shlok Gilda', 'Juliana Fernandes', 'Daniela Oliveira', 'Ronald Wilson', 'Damon Woodard']",1,0.6751032
"In this paper, we delve into the rapidly evolving challenge of misinformation
detection, with a specific focus on the nuanced manipulation of narrative
frames - an under-explored area within the AI community. The potential for
Generative AI models to generate misleading narratives underscores the urgency
of this problem. Drawing from communication and framing theories, we posit that
the presentation or 'framing' of accurate information can dramatically alter
its interpretation, potentially leading to misinformation. We highlight this
issue through real-world examples, demonstrating how shifts in narrative frames
can transmute fact-based information into misinformation. To tackle this
challenge, we propose an innovative approach leveraging the power of
pre-trained Large Language Models and deep neural networks to detect
misinformation originating from accurate facts portrayed under different
frames. These advanced AI techniques offer unprecedented capabilities in
identifying complex patterns within unstructured data critical for examining
the subtleties of narrative frames. The objective of this paper is to bridge a
significant research gap in the AI domain, providing valuable insights and
methodologies for tackling framing-induced misinformation, thus contributing to
the advancement of responsible and trustworthy AI technologies. Several
experiments are intensively conducted and experimental results explicitly
demonstrate the various impact of elements of framing theory proving the
rationale of applying framing theory to increase the performance in
misinformation detection.","['Guan Wang', 'Rebecca Frederick', 'Jinglong Duan', 'William Wong', 'Verica Rupar', 'Weihua Li', 'Quan Bai']",0,0.7524981
"The rise of multimodal misinformation on social platforms poses significant
challenges for individuals and societies. Its increased credibility and broader
impact compared to textual misinformation make detection complex, requiring
robust reasoning across diverse media types and profound knowledge for accurate
verification. The emergence of Large Vision Language Model (LVLM) offers a
potential solution to this problem. Leveraging their proficiency in processing
visual and textual information, LVLM demonstrates promising capabilities in
recognizing complex information and exhibiting strong reasoning skills. In this
paper, we first investigate the potential of LVLM on multimodal misinformation
detection. We find that even though LVLM has a superior performance compared to
LLMs, its profound reasoning may present limited power with a lack of evidence.
Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal
Misinformation Detection with External Knowledge Augmentation. LEMMA leverages
LVLM intuition and reasoning capabilities while augmenting them with external
knowledge to enhance the accuracy of misinformation detection. Our method
improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and
Fakeddit datasets respectively.","['Keyang Xuan', 'Li Yi', 'Fan Yang', 'Ruochen Wu', 'Yi R. Fung', 'Heng Ji']",6,0.81911993
"Retrieval-augmented language models are being increasingly tasked with
subjective, contentious, and conflicting queries such as ""is aspartame linked
to cancer"". To resolve these ambiguous queries, one must search through a large
range of websites and consider ""which, if any, of this evidence do I find
convincing?"". In this work, we study how LLMs answer this question. In
particular, we construct ConflictingQA, a dataset that pairs controversial
queries with a series of real-world evidence documents that contain different
facts (e.g., quantitative results), argument styles (e.g., appeals to
authority), and answers (Yes or No). We use this dataset to perform sensitivity
and counterfactual analyses to explore which text features most affect LLM
predictions. Overall, we find that current models rely heavily on the relevance
of a website to the query, while largely ignoring stylistic features that
humans find important such as whether a text contains scientific references or
is written with a neutral tone. Taken together, these results highlight the
importance of RAG corpus quality (e.g., the need to filter misinformation), and
possibly even a shift in how LLMs are trained to better align with human
judgements.","['Alexander Wan', 'Eric Wallace', 'Dan Klein']",6,0.73684955
"Machine-Generated Text (MGT) detection aims to identify a piece of text as
machine or human written. Prior work has primarily formulated MGT detection as
a binary classification task over an entire document, with limited work
exploring cases where only part of a document is machine generated. This paper
provides the first in-depth study of MGT that localizes the portions of a
document that were machine generated. Thus, if a bad actor were to change a key
portion of a news article to spread misinformation, whole document MGT
detection may fail since the vast majority is human written, but our approach
can succeed due to its granular approach. A key challenge in our MGT
localization task is that short spans of text, e.g., a single sentence,
provides little information indicating if it is machine generated due to its
short length. To address this, we leverage contextual information, where we
predict whether multiple sentences are machine or human written at once. This
enables our approach to identify changes in style or content to boost
performance. A gain of 4-13% mean Average Precision (mAP) over prior work
demonstrates the effectiveness of approach on five diverse datasets: GoodNews,
VisualNews, WikiText, Essay, and WP. We release our implementation at
https://github.com/Zhongping-Zhang/MGT_Localization.","['Zhongping Zhang', 'Wenda Qin', 'Bryan A. Plummer']",8,0.61450124
"Understanding how misinformation affects the spread of disease is crucial for
public health, especially given recent research indicating that misinformation
can increase vaccine hesitancy and discourage vaccine uptake. However, it is
difficult to investigate the interaction between misinformation and epidemic
outcomes due to the dearth of data-informed holistic epidemic models. Here, we
employ an epidemic model that incorporates a large, mobility-informed physical
contact network as well as the distribution of misinformed individuals across
counties derived from social media data. The model allows us to simulate and
estimate various scenarios to understand the impact of misinformation on
epidemic spreading. Using this model, we present a worst-case scenario in which
a heavily misinformed population would result in an additional 14% of the U.S.
population becoming infected over the course of the COVID-19 epidemic, compared
to a best-case scenario.","['Matthew R. DeVerna', 'Francesco Pierri', 'Yong-Yeol Ahn', 'Santo Fortunato', 'Alessandro Flammini', 'Filippo Menczer']",5,0.73918486
"With the rise of generative AI, automated fact-checking methods to combat
misinformation are becoming more and more important. However, factual claim
detection, the first step in a fact-checking pipeline, suffers from two key
issues that limit its scalability and generalizability: (1) inconsistency in
definitions of the task and what a claim is, and (2) the high cost of manual
annotation. To address (1), we review the definitions in related work and
propose a unifying definition of factual claims that focuses on verifiability.
To address (2), we introduce AFaCTA (Automatic Factual Claim deTection
Annotator), a novel framework that assists in the annotation of factual claims
with the help of large language models (LLMs). AFaCTA calibrates its annotation
confidence with consistency along three predefined reasoning paths. Extensive
evaluation and experiments in the domain of political speech reveal that AFaCTA
can efficiently assist experts in annotating factual claims and training
high-quality classifiers, and can work with or without expert supervision. Our
analyses also result in PoliClaim, a comprehensive claim detection dataset
spanning diverse political topics.","['Jingwei Ni', 'Minjing Shi', 'Dominik Stammbach', 'Mrinmaya Sachan', 'Elliott Ash', 'Markus Leippold']",1,0.8018577
"Adopting human and large language models (LLM) as judges (a.k.a human- and
LLM-as-a-judge) for evaluating the performance of LLMs has recently gained
attention. Nonetheless, this approach concurrently introduces potential biases
from human and LLMs, questioning the reliability of the evaluation results. In
this paper, we propose a novel framework that is free from referencing
groundtruth annotations for investigating Misinformation Oversight Bias, Gender
Bias, Authority Bias and Beauty Bias on LLM and human judges. We curate a
dataset referring to the revised Bloom's Taxonomy and conduct thousands of
evaluations. Results show that human and LLM judges are vulnerable to
perturbations to various degrees, and that even the cutting-edge judges possess
considerable biases. We further exploit these biases to conduct attacks on LLM
judges. We hope that our work can notify the community of the bias and
vulnerability of human- and LLM-as-a-judge, as well as the urgency of
developing robust evaluation systems.","['Guiming Hardy Chen', 'Shunian Chen', 'Ziche Liu', 'Feng Jiang', 'Benyou Wang']",6,0.68569666
"Large language models are limited by challenges in factuality and
hallucinations to be directly employed off-the-shelf for judging the veracity
of news articles, where factual accuracy is paramount. In this work, we propose
DELL that identifies three key stages in misinformation detection where LLMs
could be incorporated as part of the pipeline: 1) LLMs could \emph{generate
news reactions} to represent diverse perspectives and simulate user-news
interaction networks; 2) LLMs could \emph{generate explanations} for proxy
tasks (e.g., sentiment, stance) to enrich the contexts of news articles and
produce experts specializing in various aspects of news understanding; 3) LLMs
could \emph{merge task-specific experts} and provide an overall prediction by
incorporating the predictions and confidence scores of varying experts.
Extensive experiments on seven datasets with three LLMs demonstrate that DELL
outperforms state-of-the-art baselines by up to 16.8\% in macro f1-score.
Further analysis reveals that the generated reactions and explanations are
greatly helpful in misinformation detection, while our proposed LLM-guided
expert merging helps produce better-calibrated predictions.","['Herun Wan', 'Shangbin Feng', 'Zhaoxuan Tan', 'Heng Wang', 'Yulia Tsvetkov', 'Minnan Luo']",6,0.7931582
"In the dynamic landscape of digital information, the rise of misinformation
and fake news presents a pressing challenge. This paper takes a completely new
approach to verifying news, inspired by how quantum actors can reach agreement
even when they are spatially spread out. We propose a radically new, to the
best of our knowledge, algorithm that uses quantum ``entanglement'' (think of
it as a special connection) to help news aggregators sniff out bad actors,
whether they be other news sources or even fact-checkers trying to spread
misinformation. This algorithm doesn't rely on quantum signatures, it just uses
basic quantum technology we already have, in particular, special pairs of
particles called ``EPR pairs'' that are much easier to create than other
options. More complex entangled states are like juggling too many balls -
they're hard to make and slow things down, especially when many players are
involved. For instance, bigger, more complex states like ``GHZ states'' work
for small groups, but they become messy with larger numbers. So, we stick with
Bell states, the simplest form of entanglement, which are easy to generate no
matter how many players are in the game. This means our algorithm is faster to
set up, works for any number of participants, and is more practical for
real-world use. Bonus points: it finishes in a fixed number of steps,
regardless of how many players are involved, making it even more scalable. This
new approach may lead to a powerful and efficient way to fight misinformation
in the digital age, using the weird and wonderful world of quantum mechanics.","['Theodore Andronikos', 'Alla Sirokofskich']",4,0.52326375
"Whataboutism, a potent tool for disrupting narratives and sowing distrust,
remains under-explored in quantitative NLP research. Moreover, past work has
not distinguished its use as a strategy for misinformation and propaganda from
its use as a tool for pragmatic and semantic framing. We introduce new datasets
from Twitter and YouTube, revealing overlaps as well as distinctions between
whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on
recent work in linguistic semantics, we differentiate the `what about' lexical
construct from whataboutism. Our experiments bring to light unique challenges
in its accurate detection, prompting the introduction of a novel method using
attention weights for negative sample mining. We report significant
improvements of 4% and 10% over previous state-of-the-art methods in our
Twitter and YouTube collections, respectively.","['Khiem Phi', 'Noushin Salek Faramarzi', 'Chenlu Wang', 'Ritwik Banerjee']",4,0.6354121
"Adversarial attacks represent a substantial challenge in Natural Language
Processing (NLP). This study undertakes a systematic exploration of this
challenge in two distinct phases: vulnerability evaluation and resilience
enhancement of Transformer-based models under adversarial attacks.
  In the evaluation phase, we assess the susceptibility of three Transformer
configurations, encoder-decoder, encoder-only, and decoder-only setups, to
adversarial attacks of escalating complexity across datasets containing
offensive language and misinformation. Encoder-only models manifest a 14% and
21% performance drop in offensive language detection and misinformation
detection tasks, respectively. Decoder-only models register a 16% decrease in
both tasks, while encoder-decoder models exhibit a maximum performance drop of
14% and 26% in the respective tasks.
  The resilience-enhancement phase employs adversarial training, integrating
pre-camouflaged and dynamically altered data. This approach effectively reduces
the performance drop in encoder-only models to an average of 5% in offensive
language detection and 2% in misinformation detection tasks. Decoder-only
models, occasionally exceeding original performance, limit the performance drop
to 7% and 2% in the respective tasks. Although not surpassing the original
performance, Encoder-decoder models can reduce the drop to an average of 6% and
2% respectively.
  Results suggest a trade-off between performance and robustness, with some
models maintaining similar performance while gaining robustness. Our study and
adversarial training techniques have been incorporated into an open-source tool
for generating camouflaged datasets. However, methodology effectiveness depends
on the specific camouflage technique and data encountered, emphasizing the need
for continued exploration.","['√Ålvaro Huertas-Garc√≠a', 'Alejandro Mart√≠n', 'Javier Huertas-Tato', 'David Camacho']",7,0.6732817
"Social media platforms have diverse content moderation policies, with many
prominent actors hesitant to impose strict regulations. A key reason for this
reluctance could be the competitive advantage that comes with lax regulation. A
popular platform that starts enforcing content moderation rules may fear that
it could lose users to less-regulated alternative platforms. Moreover, if users
continue harmful activities on other platforms, regulation ends up being
futile. This article examines the competitive aspect of content moderation by
considering the motivations of all involved players (platformer, news source,
and social media users), identifying the regulation policies sustained in
equilibrium, and evaluating the information quality available on each platform.
Applied to simple yet relevant social networks such as stochastic block models,
our model reveals the conditions for a popular platform to enforce strict
regulation without losing users. Effectiveness of regulation depends on the
diffusive property of news posts, friend interaction qualities in social media,
the sizes and cohesiveness of communities, and how much sympathizers appreciate
surprising news from influencers.","['So Sasaki', 'C√©dric Langbort']",3,0.6366598
"The social media-fuelled explosion of fake news and misinformation supported
by tampered images has led to growth in the development of models and datasets
for image manipulation detection. However, existing detection methods mostly
treat media objects in isolation, without considering the impact of specific
manipulations on viewer perception. Forensic datasets are usually analyzed
based on the manipulation operations and corresponding pixel-based masks, but
not on the semantics of the manipulation, i.e., type of scene, objects, and
viewers' attention to scene content. The semantics of the manipulation play an
important role in spreading misinformation through manipulated images. In an
attempt to encourage further development of semantic-aware forensic approaches
to understand visual misinformation, we propose a framework to analyze the
trends of visual and semantic saliency in popular image manipulation datasets
and their impact on detection.","['Joshua Krinsky', 'Alan Bettis', 'Qiuyu Tang', 'Daniel Moreira', 'Aparna Bharati']",7,0.7090646
"The rapid dissemination of misinformation through social media increased the
importance of automated fact-checking. Furthermore, studies on what deep neural
models pay attention to when making predictions have increased in recent years.
While significant progress has been made in this field, it has not yet reached
a level of reasoning comparable to human reasoning. To address these gaps, we
propose a multi-task explainable neural model for misinformation detection.
Specifically, this work formulates an explanation generation process of the
model's veracity prediction as a text summarization problem. Additionally, the
performance of the proposed model is discussed on publicly available datasets
and the findings are evaluated with related studies.","['Recep Firat Cekinel', 'Pinar Karagoz']",1,0.6888479
"In this paper, we present the current progress of the project Verif.ai, an
open-source scientific generative question-answering system with referenced and
verified answers. The components of the system are (1) an information retrieval
system combining semantic and lexical search techniques over scientific papers
(PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and
generating answers with references to the papers from which the claim was
derived, and (3) a verification engine that cross-checks the generated claim
and the abstract or paper from which the claim was derived, verifying whether
there may have been any hallucinations in generating the claim. We are
reinforcing the generative model by providing the abstract in context, but in
addition, an independent set of methods and models are verifying the answer and
checking for hallucinations. Therefore, we believe that by using our method, we
can make scientists more productive, while building trust in the use of
generative language models in scientific environments, where hallucinations and
misinformation cannot be tolerated.","['Milo≈° Ko≈°prdiƒá', 'Adela Ljajiƒá', 'Bojana Ba≈°aragin', 'Darija Medvecki', 'Nikola Milo≈°eviƒá']",1,0.74543965
"Our society is facing rampant misinformation harming public health and trust.
To address the societal challenge, we introduce FACT-GPT, a system leveraging
Large Language Models (LLMs) to automate the claim matching stage of
fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social
media content that aligns with, contradicts, or is irrelevant to previously
debunked claims. Our evaluation shows that our specialized LLMs can match the
accuracy of larger models in identifying related claims, closely mirroring
human judgment. This research provides an automated solution for efficient
claim matching, demonstrates the potential of LLMs in supporting fact-checkers,
and offers valuable resources for further research in the field.","['Eun Cheol Choi', 'Emilio Ferrara']",6,0.7506006
"Despite significant investment into safety training, large language models
(LLMs) deployed in the real world still suffer from numerous vulnerabilities.
One perspective on LLM safety training is that it algorithmically forbids the
model from answering toxic or harmful queries. To assess the effectiveness of
safety training, in this work, we study forbidden tasks, i.e., tasks the model
is designed to refuse to answer. Specifically, we investigate whether
in-context learning (ICL) can be used to re-learn forbidden tasks despite the
explicit fine-tuning of the model to refuse them. We first examine a toy
example of refusing sentiment classification to demonstrate the problem. Then,
we use ICL on a model fine-tuned to refuse to summarise made-up news articles.
Finally, we investigate whether ICL can undo safety training, which could
represent a major security risk. For the safety task, we look at Vicuna-7B,
Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on
Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL
attack that uses the chat template tokens like a prompt injection attack to
achieve a better attack success rate on Vicuna-7B and Starling-7B.
  Trigger Warning: the appendix contains LLM-generated text with violence,
suicide, and misinformation.","['Sophie Xhonneux', 'David Dobre', 'Jian Tang', 'Gauthier Gidel', 'Dhanya Sridhar']",6,0.5991862
"The rapid development of technologies and artificial intelligence makes
deepfakes an increasingly sophisticated and challenging-to-identify technique.
To ensure the accuracy of information and control misinformation and mass
manipulation, it is of paramount importance to discover and develop artificial
intelligence models that enable the generic detection of forged videos. This
work aims to address the detection of deepfakes across various existing
datasets in a scenario with limited computing resources. The goal is to analyze
the applicability of different deep learning techniques under these
restrictions and explore possible approaches to enhance their efficiency.","['Paloma Cantero-Arjona', 'Alfonso S√°nchez-Maci√°n']",11,0.72610843
"Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
However, when handling tasks involving repetitive sub-tasks and/or deceptive
contents, such as arithmetic calculation and article-level fake news detection,
simple instructional prompts suffer from inaccurate responses. Existing works
show that more complicated prompting strategies, such as Chain-of-Thoughts and
Least-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent
researches reveal that simple divide-and-conquer prompting strategy, i.e.
simply dividing the input sequence to multiple sub-inputs, can also
substantially improve LLM's performance in some specific tasks such as
misinformation detection. In this paper, we aim at examining the utility of
divide-and-conquer prompting strategy and answer on which kind of tasks this
strategy gets advantages. Specifically, we provide a theoretic analysis to
divide-and-conquer prompting strategy and help us identify the specific tasks
where DaC prompting can bring performance boost with theoretic guarantee. We
then present two cases (large integer arithmetic and fact verification) where
experimental results aligns with our theoretic analysis.","['Yizhou Zhang', 'Lun Du', 'Defu Cao', 'Qiang Fu', 'Yan Liu']",6,0.71900105
"In the era of social media platforms, identifying the credibility of online
content is crucial to combat misinformation. We present the CREDiBERT
(CREDibility assessment using Bi-directional Encoder Representations from
Transformers), a source credibility assessment model fine-tuned for Reddit
submissions focusing on political discourse as the main contribution. We adopt
a semi-supervised training approach for CREDiBERT, leveraging Reddit's
community-based structure. By encoding submission content using CREDiBERT and
integrating it into a Siamese neural network, we significantly improve the
binary classification of submission credibility, achieving a 9% increase in F1
score compared to existing methods. Additionally, we introduce a new version of
the post-to-post network in Reddit that efficiently encodes user interactions
to enhance the binary classification task by nearly 8% in F1 score. Finally, we
employ CREDiBERT to evaluate the susceptibility of subreddits with respect to
different topics.","['Arash Amini', 'Yigit Ege Bayiz', 'Ashwin Ram', 'Radu Marculescu', 'Ufuk Topcu']",4,0.57590616
"We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes
and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in
particular, pose an alarming threat to society as they are capable of spreading
misinformation and changing the truth. LLMs are powerful language models that
generate general-purpose language. However due to its generative aspect, it can
also be a risk for people if used with ill intentions. The ethical use of these
technologies is a big concern. This short article tries to find out the
interrelationship between them.","['Alakananda Mitra', 'Saraju P. Mohanty', 'Elias Kougianos']",9,0.691445
"The kick-off of vaccination campaigns in Europe, starting in late December
2020, has been followed by the online spread of controversies and conspiracies
surrounding vaccine validity and efficacy. We study Twitter discussions in
three major European languages (Italian, German, and French) during the
vaccination campaign. Moving beyond content analysis to explore the structural
aspects of online discussions, our investigation includes an analysis of
polarization and the potential formation of echo chambers, revealing nuanced
behavioral and topical differences in user interactions across the analyzed
countries. Notably, we identify strong anti- and pro-vaccine factions
exhibiting heterogeneous temporal polarization patterns in different countries.
Through a detailed examination of news-sharing sources, we uncover the
widespread use of other media platforms like Telegram and YouTube for
disseminating low-credibility information, indicating a concerning trend of
diminishing news credibility over time. Our findings on Twitter discussions
during the COVID-19 vaccination campaign in major European languages expose
nuanced behavioral distinctions, revealing the profound impact of polarization
and the emergence of distinct anti-vaccine and pro-vaccine advocates over time.","['Gianluca Nogara', 'Francesco Pierri', 'Stefano Cresci', 'Luca Luceri', 'Silvia Giordano']",12,0.78931534
"The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide
high-quality, annotated, 5-way stance data extracted from Twitter, suitable for
analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus
2.0 iteration, we extend this framework to encompass Russian and Spanish. The
former is of current significance due to prevalent misinformation amid
escalating tensions with the West and the violent incursion into Ukraine. The
latter, meanwhile, represents an enormous community that has been largely
overlooked on major social media platforms. By incorporating an additional
3,874 Spanish and Russian tweets over 41 misinformation claims, our objective
is to support research focused on these issues. To demonstrate the value of
this data, we employed zero-shot cross-lingual transfer on multilingual BERT,
yielding results on par with the initial Stanceosaurus study with a macro F1
score of 43 for both languages. This underlines the viability of stance
classification as an effective tool for identifying multicultural
misinformation.","['Anton Lavrouk', 'Ian Ligon', 'Tarek Naous', 'Jonathan Zheng', 'Alan Ritter', 'Wei Xu']",8,0.75979066
"Vision-Language Models (VLMs) excel in generating textual responses from
visual inputs, but their versatility raises security concerns. This study takes
the first step in exposing VLMs' susceptibility to data poisoning attacks that
can manipulate responses to innocuous, everyday prompts. We introduce
Shadowcast, a stealthy data poisoning attack where poison samples are visually
indistinguishable from benign images with matching texts. Shadowcast
demonstrates effectiveness in two attack types. The first is a traditional
Label Attack, tricking VLMs into misidentifying class labels, such as confusing
Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging
VLMs' text generation capabilities to craft persuasive and seemingly rational
narratives for misinformation, such as portraying junk food as healthy. We show
that Shadowcast effectively achieves the attacker's intentions using as few as
50 poison samples. Crucially, the poisoned samples demonstrate transferability
across different VLM architectures, posing a significant concern in black-box
settings. Moreover, Shadowcast remains potent under realistic conditions
involving various text prompts, training data augmentation, and image
compression techniques. This work reveals how poisoned VLMs can disseminate
convincing yet deceptive misinformation to everyday, benign users, emphasizing
the importance of data integrity for responsible VLM deployments. Our code is
available at: https://github.com/umd-huang-lab/VLM-Poisoning.","['Yuancheng Xu', 'Jiarui Yao', 'Manli Shu', 'Yanchao Sun', 'Zichu Wu', 'Ning Yu', 'Tom Goldstein', 'Furong Huang']",7,0.53366154
"The results of information retrieval (IR) are usually presented in the form
of a ranked list of candidate documents, such as web search for humans and
retrieval-augmented generation for large language models (LLMs). List-aware
retrieval aims to capture the list-level contextual features to return a better
list, mainly including reranking and truncation. Reranking finely re-scores the
documents in the list. Truncation dynamically determines the cut-off point of
the ranked list to achieve the trade-off between overall relevance and avoiding
misinformation from irrelevant documents. Previous studies treat them as two
separate tasks and model them separately. However, the separation is not
optimal. First, it is hard to share the contextual information of the ranking
list between the two tasks. Second, the separate pipeline usually meets the
error accumulation problem, where the small error from the reranking stage can
largely affect the truncation stage. To solve these problems, we propose a
Reranking-Truncation joint model (GenRT) that can perform the two tasks
concurrently. GenRT integrates reranking and truncation via generative paradigm
based on encoder-decoder architecture. We also design the novel loss functions
for joint optimization to make the model learn both tasks. Sharing parameters
by the joint model is conducive to making full use of the common modeling
information of the two tasks. Besides, the two tasks are performed concurrently
and co-optimized to solve the error accumulation problem between separate
stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&A
tasks show that our method achieves SOTA performance on both reranking and
truncation tasks for web search and retrieval-augmented LLMs.","['Shicheng Xu', 'Liang Pang', 'Jun Xu', 'Huawei Shen', 'Xueqi Cheng']",1,0.6853882
"In an era where artificial intelligence (AI) intertwines with medical
research, the delineation of truth becomes increasingly complex. This study
ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega
variant, showcasing 31 unique mutations in the S gene region. However, the real
undercurrent of this narrative is a demonstration of the ease with which AI,
specifically ChatGPT-4, can fabricate convincing yet entirely fictional
scientific data. The so-called Omega variant was identified in a fully
vaccinated, previously infected 35-year-old male presenting with severe
COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and
contact tracing, this study mirrors the rigorous methodology of genuine case
reports, thereby setting the stage for a compelling but entirely constructed
narrative. The entire case study was generated by ChatGPT-4, a large language
model by OpenAI. The fabricated Omega variant features an ensemble of
mutations, including N501Y and E484K, known for enhancing ACE2 receptor
affinity, alongside L452R and P681H, ostensibly indicative of immune evasion.
This variant's contrived interaction dynamics - severe symptoms in a vaccinated
individual versus mild ones in unvaccinated contacts - were designed to mimic
real-world complexities, including suggestions of antibody-dependent
enhancement (ADE). While the Omega variant is a product of AI-generated
fiction, the implications of this exercise are real and profound. The ease with
which AI can generate believable but false scientific information, as
illustrated in this case, raises significant concerns about the potential for
misinformation in medicine. This study, therefore, serves as a cautionary tale,
emphasizing the necessity for critical evaluation of sources, especially in an
age where AI tools like ChatGPT are becoming increasingly sophisticated and
widespread in their use.","['Malik Sallam', 'Jan Egger', 'Rainer Roehrig', 'Behrus Puladi']",9,0.6139302
"In the digital era, information consumption is predominantly channeled
through online news media disseminated on social media platforms. Understanding
the complex dynamics of the news media environment and users habits within the
digital ecosystem is a challenging task that requires at the same time large
bases of data and accurate methodological approaches. This study contributes to
this expanding research landscape by employing network science methodologies
and entropic measures to analyze the behavioural patterns of social media users
sharing news pieces and dig into the diverse news consumption habits within
different online social media user groups. Our analyses reveal that users are
more inclined to share news classified as fake when they have previously posted
conspiracy or junk science content, and vice versa, creating a series of
misinformation hot streaks. To better understand these dynamics, we used three
different measures of entropy to gain insights into the news media habits of
each user, finding that the patterns of news consumption significantly differ
among users when focusing on disinformation spreaders, as opposed to accounts
sharing reliable or low-risk content. Thanks to these entropic measures, we
quantify the variety and the regularity of the news media diet, finding that
those disseminating unreliable content exhibit a more varied and at the same
time a more regular choice of web domains. This quantitative insight into the
nuances of news consumption behaviours exhibited by disinformation spreaders
holds the potential to significantly inform the strategic formulation of more
robust and adaptive social media moderation policies.","['Anna Bertani', 'Valeria Mazzeo', 'Riccardo Gallotti']",0,0.7647791
"Anthropomorphism, or the attribution of human-like characteristics to
non-human entities, has shaped conversations about the impacts and
possibilities of technology. We present AnthroScore, an automatic metric of
implicit anthropomorphism in language. We use a masked language model to
quantify how non-human entities are implicitly framed as human by the
surrounding context. We show that AnthroScore corresponds with human judgments
of anthropomorphism and dimensions of anthropomorphism described in social
science literature. Motivated by concerns of misleading anthropomorphism in
computer science discourse, we use AnthroScore to analyze 15 years of research
papers and downstream news articles. In research papers, we find that
anthropomorphism has steadily increased over time, and that papers related to
language models have the most anthropomorphism. Within ACL papers, temporal
increases in anthropomorphism are correlated with key neural advancements.
Building upon concerns of scientific misinformation in mass media, we identify
higher levels of anthropomorphism in news headlines compared to the research
papers they cite. Since AnthroScore is lexicon-free, it can be directly applied
to a wide range of text sources.","['Myra Cheng', 'Kristina Gligoric', 'Tiziano Piccardi', 'Dan Jurafsky']",8,0.4456123
"Vaccine concerns are an ever-evolving target, and can shift quickly as seen
during the COVID-19 pandemic. Identifying longitudinal trends in vaccine
concerns and misinformation might inform the healthcare space by helping public
health efforts strategically allocate resources or information campaigns. We
explore the task of detecting vaccine concerns in online discourse using large
language models (LLMs) in a zero-shot setting without the need for expensive
training datasets. Since real-time monitoring of online sources requires
large-scale inference, we explore cost-accuracy trade-offs of different
prompting strategies and offer concrete takeaways that may inform choices in
system designs for current applications. An analysis of different prompting
strategies reveals that classifying the concerns over multiple passes through
the LLM, each consisting a boolean question whether the text mentions a vaccine
concern or not, works the best. Our results indicate that GPT-4 can strongly
outperform crowdworker accuracy when compared to ground truth annotations
provided by experts on the recently introduced VaxConcerns dataset, achieving
an overall F1 score of 78.7%.","['Chloe Qinyu Zhu', 'Rickard Stureborg', 'Bhuwan Dhingra']",12,0.7541168
"Signal quality assessment (SQA) is required for monitoring the reliability of
data acquisition systems, especially in AI-driven Predictive Maintenance (PMx)
application contexts. SQA is vital for addressing ""silent failures"" of data
acquisition hardware and software, which when unnoticed, misinform the users of
data, creating the risk for incorrect decisions with unintended or even
catastrophic consequences. We have developed an open-source software
implementation of signal quality indices (SQIs) for the analysis of time-series
data. We codify a range of SQIs, demonstrate them using established benchmark
data, and show that they can be effective for signal quality assessment. We
also study alternative approaches to denoising time-series data in an attempt
to improve the quality of the already degraded signal, and evaluate them
empirically on relevant real-world data. To our knowledge, our software toolkit
is the first to provide an open source implementation of a broad range of
signal quality assessment and improvement techniques validated on publicly
available benchmark data for ease of reproducibility. The generality of our
framework can be easily extended to assessing reliability of arbitrary
time-series measurements in complex systems, especially when morphological
patterns of the waveform shapes and signal periodicity are of key interest in
downstream analyses.","['Chufan Gao', 'Nicholas Gisolfi', 'Artur Dubrawski']",1,0.5820738
"Online harms, such as hate speech, misinformation, harassment and self-harm
promotion, continue to be widespread. While some work suggests that women are
disproportionately affected by such harms, other studies find little evidence
for gender differences in overall exposure. Here, we present preliminary
results from a large, nationally representative survey of UK adults (N = 2000).
We asked about exposure to 15 specific harms, along with fears surrounding
exposure and comfort engaging in certain online behaviours. While men and women
report seeing online harms to a roughly equal extent overall, we find that
women are significantly more fearful of experiencing every type of harm that we
asked about, and are significantly less comfortable partaking in several online
behaviours. Strikingly, just 24% of women report being comfortable expressing
political opinions online compared with almost 40% of men, with similar overall
proportions for challenging certain content. Our work suggests that women may
suffer an additional psychological burden in response to the proliferation of
harmful online content, doing more 'safety work' to protect themselves. With
much public discourse happening online, gender inequality in public voice is
likely to be perpetuated if women feel too fearful to participate. Our results
are important because to establish greater equality in society, we must take
measures to ensure all members feel safe and able to participate in the online
space.","['Florence E. Enock', 'Francesca Stevens', 'Jonathan Bright', 'Miranda Cross', 'Pica Johansson', 'Judy Wajcman', 'Helen Z. Margetts']",3,0.5088985
"In recent years, large language models (LLMs) have become incredibly popular,
with ChatGPT for example being used by over a billion users. While these models
exhibit remarkable language understanding and logical prowess, a notable
challenge surfaces in the form of ""hallucinations."" This phenomenon results in
LLMs outputting misinformation in a confident manner, which can lead to
devastating consequences with such a large user base. However, we question the
appropriateness of the term ""hallucination"" in LLMs, proposing a psychological
taxonomy based on cognitive biases and other psychological phenomena. Our
approach offers a more fine-grained understanding of this phenomenon, allowing
for targeted solutions. By leveraging insights from how humans internally
resolve similar challenges, we aim to develop strategies to mitigate LLM
hallucinations. This interdisciplinary approach seeks to move beyond
conventional terminology, providing a nuanced understanding and actionable
pathways for improvement in LLM reliability.","['Elijah Berberette', 'Jack Hutchins', 'Amir Sadovnik']",6,0.701457
"Sharing misinformation threatens societies as misleading news shapes the risk
perception of individuals. We witnessed this during the COVID-19 pandemic,
where misinformation undermined the effectiveness of stay-at-home orders,
posing an additional obstacle in the fight against the virus. In this research,
we study misinformation spreading, reanalyzing behavioral data on online
sharing, and analyzing decision-making mechanisms using the Drift Diffusion
Model (DDM). We find that subjects display an increased instinctive inclination
towards sharing misleading news, but rational thinking significantly curbs this
reaction, especially for more cautious and older individuals. Using an
agent-based model, we expand this individual knowledge to a social network
where individuals are exposed to misinformation through friends and share (or
not) content with probabilities driven by DDM. The natural shape of the Twitter
network provides a fertile ground for any news to rapidly become viral, yet we
found that limiting users' followers proves to be an appropriate and feasible
containment strategy.","['Lucila G. Alvarez-Zuzek', 'Jelena Grujic', 'Riccardo Gallotti']",5,0.7656239
"The increasing reliance on AI-driven solutions, particularly Large Language
Models (LLMs) like the GPT series, for information retrieval highlights the
critical need for their factuality and fairness, especially amidst the rampant
spread of misinformation and disinformation online. Our study evaluates the
factual accuracy, stability, and biases in widely adopted GPT models, including
GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated
information dissemination.
  We introduce 'Global-Liar,' a dataset uniquely balanced in terms of
geographic and temporal representation, facilitating a more nuanced evaluation
of LLM biases. Our analysis reveals that newer iterations of GPT models do not
always equate to improved performance. Notably, the GPT-4 version from March
demonstrates higher factual accuracy than its subsequent June release.
Furthermore, a concerning bias is observed, privileging statements from the
Global North over the Global South, thus potentially exacerbating existing
informational inequities. Regions such as Africa and the Middle East are at a
disadvantage, with much lower factual accuracy. The performance fluctuations
over time suggest that model updates may not consistently benefit all regions
equally.
  Our study also offers insights into the impact of various LLM configuration
settings, such as binary decision forcing, model re-runs and temperature, on
model's factuality. Models constrained to binary (true/false) choices exhibit
reduced factuality compared to those allowing an 'unclear' option. Single
inference at a low temperature setting matches the reliability of majority
voting across various configurations. The insights gained highlight the need
for culturally diverse and geographically inclusive model training and
evaluation. This approach is key to achieving global equity in technology,
distributing AI benefits fairly worldwide.","['Shujaat Mirza', 'Bruno Coelho', 'Yuyuan Cui', 'Christina P√∂pper', 'Damon McCoy']",6,0.66305363
"YouTube faces a global crisis with the dissemination of false information and
hate speech. To counter these issues, YouTube has implemented strict rules
against uploading content that includes false information or promotes hate
speech. While numerous studies have been conducted to reduce offensive
English-language content, there's a significant lack of research on Sinhala
content. This study aims to address the aforementioned gap by proposing a
solution to minimize the spread of violence and misinformation in Sinhala
YouTube videos. The approach involves developing a rating system that assesses
whether a video contains false information by comparing the title and
description with the audio content and evaluating whether the video includes
hate speech. The methodology encompasses several steps, including audio
extraction using the Pytube library, audio transcription via the fine-tuned
Whisper model, hate speech detection employing the distilroberta-base model and
a text classification LSTM model, and text summarization through the fine-tuned
BART-Large- XSUM model. Notably, the Whisper model achieved a 48.99\% word
error rate, while the distilroberta-base model demonstrated an F1 score of
0.856 and a recall value of 0.861 in comparison to the LSTM model, which
exhibited signs of overfitting.","['W. A. K. M. Wickramaarachchi', 'Sameeri Sathsara Subasinghe', 'K. K. Rashani Tharushika Wijerathna', 'A. Sahashra Udani Athukorala', 'Lakmini Abeywardhana', 'A. Karunasena']",8,0.5987903
"This study explores linguistic differences between human and LLM-generated
dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the
EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word
Count (LIWC) analysis, comparing ChatGPT-generated conversations with human
conversations across 118 linguistic categories. Results show greater
variability and authenticity in human dialogues, but ChatGPT excels in
categories such as social processes, analytical style, cognition, attentional
focus, and positive emotional tone, reinforcing recent findings of LLMs being
""more human than human."" However, no significant difference was found in
positive or negative affect between ChatGPT and human dialogues. Classifier
analysis of dialogue embeddings indicates implicit coding of the valence of
affect despite no explicit mention of affect in the conversations. The research
also contributes a novel, companion ChatGPT-generated dataset of conversations
between two independent chatbots, which were designed to replicate a corpus of
human conversations available for open access and used widely in AI research on
language modeling. Our findings enhance understanding of ChatGPT's linguistic
capabilities and inform ongoing efforts to distinguish between human and
LLM-generated text, which is critical in detecting AI-generated fakes,
misinformation, and disinformation.","['Morgan Sandler', 'Hyesun Choung', 'Arun Ross', 'Prabu David']",8,0.61277175
"The pervasive spread of misinformation and disinformation poses a significant
threat to society. Professional fact-checkers play a key role in addressing
this threat, but the vast scale of the problem forces them to prioritize their
limited resources. This prioritization may consider a range of factors, such as
varying risks of harm posed to specific groups of people. In this work, we
investigate potential implications of using a large language model (LLM) to
facilitate such prioritization. Because fact-checking impacts a wide range of
diverse segments of society, it is important that diverse views are represented
in the claim prioritization process. This paper examines whether a LLM can
reflect the views of various groups when assessing the harms of misinformation,
focusing on gender as a primary variable. We pose two central questions: (1) To
what extent do prompts with explicit gender references reflect gender
differences in opinion in the United States on topics of social relevance? and
(2) To what extent do gender-neutral prompts align with gendered viewpoints on
those topics? To analyze these questions, we present the TopicMisinfo dataset,
containing 160 fact-checked claims from diverse topics, supplemented by nearly
1600 human annotations with subjective perceptions and annotator demographics.
Analyzing responses to gender-specific and neutral prompts, we find that GPT
3.5-Turbo reflects empirically observed gender differences in opinion but
amplifies the extent of these differences. These findings illuminate AI's
complex role in moderating online communication, with implications for
fact-checkers, algorithm designers, and the use of crowd-workers as annotators.
We also release the TopicMisinfo dataset to support continuing research in the
community.","['Terrence Neumann', 'Sooyong Lee', 'Maria De-Arteaga', 'Sina Fazelpour', 'Matthew Lease']",0,0.6723267
"Preventing the spread of misinformation is challenging. The detection of
misleading content presents a significant hurdle due to its extreme linguistic
and domain variability. Content-based models have managed to identify deceptive
language by learning representations from textual data such as social media
posts and web articles. However, aggregating representative samples of this
heterogeneous phenomenon and implementing effective real-world applications is
still elusive. Based on analytical work on the language of misinformation, this
paper analyzes the linguistic attributes that characterize this phenomenon and
how representative of such features some of the most popular misinformation
datasets are. We demonstrate that the appropriate use of pertinent symbolic
knowledge in combination with neural language models is helpful in detecting
misleading content. Our results achieve state-of-the-art performance in
misinformation datasets across the board, showing that our approach offers a
valid and robust alternative to multi-task transfer learning without requiring
any additional training data. Furthermore, our results show evidence that
structured knowledge can provide the extra boost required to address a complex
and unpredictable real-world problem like misinformation detection, not only in
terms of accuracy but also time efficiency and resource utilization.","['Flavio Merenda', 'Jos√© Manuel G√≥mez-P√©rez']",8,0.78346896
"Misinformation has become a major challenge in the era of increasing digital
information, requiring the development of effective detection methods. We have
investigated a novel approach to Out-Of-Context detection (OOCD) that uses
synthetic data generation. We created a dataset specifically designed for OOCD
and developed an efficient detector for accurate classification. Our
experimental findings validate the use of synthetic data generation and
demonstrate its efficacy in addressing the data limitations associated with
OOCD. The dataset and detector should serve as valuable resources for future
research and the development of robust misinformation detection systems.","['Fatma Shalabi', 'Huy H. Nguyen', 'Hichem Felouat', 'Ching-Chun Chang', 'Isao Echizen']",1,0.58785343
"Search result snippets are crucial in modern search engines, providing users
with a quick overview of a website's content. Snippets help users determine the
relevance of a document to their information needs, and in certain scenarios
even enable them to satisfy those needs without visiting web documents. Hence,
it is crucial for snippets to reliably represent the content of their
corresponding documents. While this may be a straightforward requirement for
some queries, it can become challenging in the complex domain of healthcare,
and can lead to misinformation. This paper aims to examine snippets'
reliability in representing their corresponding documents, specifically in the
health domain. To achieve this, we conduct a series of user studies using
Google's search results, where participants are asked to infer viewpoints of
search results pertaining to queries about the effectiveness of a medical
intervention for a medical condition, based solely on their titles and
snippets. Our findings reveal that a considerable portion of Google's snippets
(28%) failed to present any viewpoint on the intervention's effectiveness, and
that 35% were interpreted by participants as having a different viewpoint
compared to their corresponding documents. To address this issue, we propose a
snippet extraction solution tailored directly to users' information needs,
i.e., extracting snippets that summarize documents' viewpoints regarding the
intervention and condition that appear in the query. User study demonstrates
that our information need-focused solution outperforms the mainstream
query-based approach. With only 19.67% of snippets generated by our solution
reported as not presenting a viewpoint and a mere 20.33% misinterpreted by
participants. These results strongly suggest that an information need-focused
approach can significantly improve the reliability of extracted snippets in
online health search.","['Anat Hashavit', 'Tamar Stern', 'Hongning Wang', 'Sarit Kraus']",1,0.65045047
"With the improvements in generative models, the issues of producing
hallucinations in various domains (e.g., law, writing) have been brought to
people's attention due to concerns about misinformation. In this paper, we
focus on neural fake news, which refers to content generated by neural networks
aiming to mimic the style of real news to deceive people. To prevent harmful
disinformation spreading fallaciously from malicious social media (e.g.,
content farms), we propose a novel verification framework, Style-News, using
publisher metadata to imply a publisher's template with the corresponding text
types, political stance, and credibility. Based on threat modeling aspects, a
style-aware neural news generator is introduced as an adversary for generating
news content conditioning for a specific publisher, and style and source
discriminators are trained to defend against this attack by identifying which
publisher the style corresponds with, and discriminating whether the source of
the given news is human-written or machine-generated. To evaluate the quality
of the generated content, we integrate various dimensional metrics (language
fluency, content preservation, and style adherence) and demonstrate that
Style-News significantly outperforms the previous approaches by a margin of
0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, our
discriminative model outperforms state-of-the-art baselines in terms of
publisher prediction (up to 4.64%) and neural fake news detection (+6.94%
$\sim$ 31.72%).","['Wei-Yao Wang', 'Yu-Chieh Chang', 'Wen-Chih Peng']",4,0.72452533
"The growing volume of online content prompts the need for adopting
algorithmic systems of information curation. These systems range from web
search engines to recommender systems and are integral for helping users stay
informed about important societal developments. However, unlike journalistic
editing the algorithmic information curation systems (AICSs) are known to be
subject to different forms of malperformance which make them vulnerable to
possible manipulation. The risk of manipulation is particularly prominent in
the case when AICSs have to deal with information about false claims that
underpin propaganda campaigns of authoritarian regimes. Using as a case study
of the Russian disinformation campaign concerning the US biolabs in Ukraine, we
investigate how one of the most commonly used forms of AICSs - i.e. web search
engines - curate misinformation-related content. For this aim, we conduct
virtual agent-based algorithm audits of Google, Bing, and Yandex search outputs
in June 2022. Our findings highlight the troubling performance of search
engines. Even though some search engines, like Google, were less likely to
return misinformation results, across all languages and locations, the three
search engines still mentioned or promoted a considerable share of false
content (33% on Google; 44% on Bing, and 70% on Yandex). We also find
significant disparities in misinformation exposure based on the language of
search, with all search engines presenting a higher number of false stories in
Russian. Location matters as well with users from Germany being more likely to
be exposed to search results promoting false information. These observations
stress the possibility of AICSs being vulnerable to manipulation, in particular
in the case of the unfolding propaganda campaigns, and underline the importance
of monitoring performance of these systems to prevent it.","['Elizaveta Kuznetsova', 'Mykola Makhortykh', 'Maryna Sydorova', 'Aleksandra Urman', 'Ilaria Vitulano', 'Martha Stolze']",0,0.6466669
"Many studies explore how people 'come into' misinformation exposure. But much
less is known about how people 'come out of' misinformation exposure. Do people
organically sever ties to misinformation spreaders? And what predicts doing so?
Over six months, we tracked the frequency and predictors of ~900K followers
unfollowing ~5K health misinformation spreaders on Twitter. We found that
misinformation ties are persistent. Monthly unfollowing rates are just 0.52%.
In other words, 99.5% of misinformation ties persist each month. Users are also
31% more likely to unfollow non-misinformation spreaders than they are to
unfollow misinformation spreaders. Although generally infrequent, the factors
most associated with unfollowing misinformation spreaders are (1) redundancy
and (2) ideology. First, users initially following many spreaders, or who
follow spreaders that tweet often, are most likely to unfollow later. Second,
liberals are more likely to unfollow than conservatives. Overall, we observe a
strong persistence of misinformation ties. The fact that users rarely unfollow
misinformation spreaders suggests a need for external nudges and the importance
of preventing exposure from arising in the first place.","['Joshua Ashkinaze', 'Eric Gilbert', 'Ceren Budak']",10,0.66367865
"The COVID-19 pandemic brought about an extraordinary rate of scientific
papers on the topic that were discussed among the general public, although
often in biased or misinformed ways. In this paper, we present a mixed-methods
analysis aimed at examining whether public discussions were commensurate with
the scientific consensus on several COVID-19 issues. We estimate scientific
consensus based on samples of abstracts from preprint servers and compare
against the volume of public discussions on Twitter mentioning these papers. We
find that anti-consensus posts and users, though overall less numerous than
pro-consensus ones, are vastly over-represented on Twitter, thus producing a
false consensus effect. This transpires with favorable papers being
disproportionately amplified, along with an influx of new anti-consensus user
sign-ups. Finally, our content analysis highlights that anti-consensus users
misrepresent scientific findings or question scientists' integrity in their
efforts to substantiate their claims.","['Alexandros Efstratiou', 'Marina Efstratiou', 'Satrio Yudhoatmojo', 'Jeremy Blackburn', 'Emiliano De Cristofaro']",3,0.6360427
"We develop a simulation framework for studying misinformation spread within
online social networks that blends agent-based modeling and natural language
processing techniques. While many other agent-based simulations exist in this
space, questions over their fidelity and generalization to existing networks in
part hinders their ability to provide actionable insights. To partially address
these concerns, we create a 'digital clone' of a known misinformation sharing
network by downloading social media histories for over ten thousand of its
users. We parse these histories to both extract the structure of the network
and model the nuanced ways in which information is shared and spread among its
members. Unlike many other agent-based methods in this space, information
sharing between users in our framework is sensitive to topic of discussion,
user preferences, and online community dynamics. To evaluate the fidelity of
our method, we seed our cloned network with a set of posts recorded in the base
network and compare propagation dynamics between the two, observing reasonable
agreement across the twin networks over a variety of metrics. Lastly, we
explore how the cloned network may serve as a flexible, low-cost testbed for
misinformation countermeasure evaluation and red teaming analysis. We hope the
tools explored here augment existing efforts in the space and unlock new
opportunities for misinformation countermeasure evaluation, a field that may
become increasingly important to consider with the anticipated rise of
misinformation campaigns fueled by generative artificial intelligence.","['Prateek Puri', 'Gabriel Hassler', 'Anton Shenk', 'Sai Katragadda']",2,0.73546827
"The increasing pervasiveness of fruitless disagreement poses a considerable
risk to social cohesion and constructive public discourse. While polarised
discussions can exhibit significant distrust in the news, it is still largely
unclear whether disagreement is somehow linked to misinformation. In this work,
we exploit the results of `Cartesio', an online experiment to rate the
trustworthiness of Italian news articles annotated for reliability by expert
evaluators. We developed a metric for disagreement that allows for correct
comparisons between news with different mean trust values. Our findings
indicate that, though misinformation receives lower trust ratings than accurate
information, it does not appear to be more controversial. Additionally, we
examined the relationship between these findings and Facebook user engagement
with news articles. Our results show that disagreement correlates with an
increased likelihood of commenting, probably linked to inconclusive and long
discussions. The emerging scenario is one in which fighting disinformation
seems ineffective in countering polarisation. Disagreement focuses more on the
divergence of opinions, trust, and their effects on social cohesion. This study
offers a foundation for unsupervised news item analysis independent of expert
annotation. Incorporating similar principles into the design of news
distribution platforms and social media systems can enhance online interactions
and foster the development of a less divisive news ecosystem.","['Donald Ruggiero Lo Sardo', 'Emanuele Brugnoli', 'Pietro Gravino', 'Vittorio Loreto']",3,0.7692567
"Automated fact-checking has drawn considerable attention over the past few
decades due to the increase in the diffusion of misinformation on online
platforms. This is often carried out as a sequence of tasks comprising (i) the
detection of sentences circulating in online platforms which constitute claims
needing verification, followed by (ii) the verification process of those
claims. This survey focuses on the former, by discussing existing efforts
towards detecting claims needing fact-checking, with a particular focus on
multilingual data and methods. This is a challenging and fertile direction
where existing methods are yet far from matching human performance due to the
profoundly challenging nature of the issue. Especially, the dissemination of
information across multiple social platforms, articulated in multiple languages
and modalities demands more generalized solutions for combating misinformation.
Focusing on multilingual misinformation, we present a comprehensive survey of
existing multilingual claim detection research. We present state-of-the-art
multilingual claim detection research categorized into three key factors of the
problem, verifiability, priority, and similarity. Further, we present a
detailed overview of the existing multilingual datasets along with the
challenges and suggest possible future advancements.","['Rrubaa Panchendrarajan', 'Arkaitz Zubiaga']",8,0.78526676
"While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses are attributed to either generated or retrieved
contexts. To easily trace the origin of the response, we construct datasets
with conflicting contexts, i.e., each question is paired with both generated
and retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offers valuable insights for advancing current LLM
augmentation methods, and highlights the risk of generated misinformation for
retrieval-augmented LLMs.","['Hexiang Tan', 'Fei Sun', 'Wanli Yang', 'Yuanzhuo Wang', 'Qi Cao', 'Xueqi Cheng']",6,0.8335789
"The rapid and widespread dissemination of misinformation through social
networks is a growing concern in today's digital age. This study focused on
modeling fake news diffusion, discovering the spreading dynamics, and designing
control strategies. A common approach for modeling the misinformation dynamics
is SIR-based models. Our approach is an extension of a model called 'SBFC'
which is a SIR-based model. This model has three states, Susceptible, Believer,
and Fact-Checker. The dynamics and transition between states are based on
neighbors' beliefs, hoax credibility, spreading rate, probability of verifying
the news, and probability of forgetting the current state. Our contribution is
to push this model to real social networks by considering different classes of
agents with their characteristics. We proposed two main strategies for
confronting misinformation diffusion. First, we can educate a minor class, like
scholars or influencers, to improve their ability to verify the news or
remember their state longer. The second strategy is adding fact-checker bots to
the network to spread the facts and influence their neighbors' states. Our
result shows that both of these approaches can effectively control the
misinformation spread.","['Ali Khodabandeh Yalabadi', 'Mehdi Yazdani-Jahromi', 'Sina Abdidizaji', 'Ivan Garibay', 'Ozlem Ozmen Garibay']",4,0.6954873
"Large language models now possess human-level linguistic abilities in many
contexts. This raises the concern that they can be used to deceive and
manipulate on unprecedented scales, for instance spreading political
misinformation on social media. In future, agentic AI systems might also
deceive and manipulate humans for their own ends. In this paper, first, I argue
that AI-generated content should be subject to stricter standards against
deception and manipulation than we ordinarily apply to humans. Second, I offer
new characterizations of AI deception and manipulation meant to support such
standards, according to which a statement is deceptive (manipulative) if it
leads human addressees away from the beliefs (choices) they would endorse under
``semi-ideal'' conditions. Third, I propose two measures to guard against AI
deception and manipulation, inspired by this characterization: ""extreme
transparency"" requirements for AI-generated content and defensive systems that,
among other things, annotate AI-generated statements with contextualizing
information. Finally, I consider to what extent these measures can protect
against deceptive behavior in future, agentic AIs, and argue that non-agentic
defensive systems can provide an important layer of defense even against more
powerful agentic systems.",['Christian Tarsney'],9,0.7548387
"In the age of the infodemic, it is crucial to have tools for effectively
monitoring the spread of rampant rumors that can quickly go viral, as well as
identifying vulnerable users who may be more susceptible to spreading such
misinformation. This proactive approach allows for timely preventive measures
to be taken, mitigating the negative impact of false information on society. We
propose a novel approach to predict viral rumors and vulnerable users using a
unified graph neural network model. We pre-train network-based user embeddings
and leverage a cross-attention mechanism between users and posts, together with
a community-enhanced vulnerability propagation (CVP) method to improve user and
propagation graph representations. Furthermore, we employ two multi-task
training strategies to mitigate negative transfer effects among tasks in
different settings, enhancing the overall performance of our approach. We also
construct two datasets with ground-truth annotations on information virality
and user vulnerability in rumor and non-rumor events, which are automatically
derived from existing rumor detection datasets. Extensive evaluation results of
our joint learning model confirm its superiority over strong baselines in all
three tasks: rumor detection, virality prediction, and user vulnerability
scoring. For instance, compared to the best baselines based on the Weibo
dataset, our model makes 3.8\% and 3.0\% improvements on Accuracy and MacF1 for
rumor detection, and reduces mean squared error (MSE) by 23.9\% and 16.5\% for
virality prediction and user vulnerability scoring, respectively. Our findings
suggest that our approach effectively captures the correlation between rumor
virality and user vulnerability, leveraging this information to improve
prediction performance and provide a valuable tool for infodemic surveillance.","['Xuan Zhang', 'Wei Gao']",2,0.7010571
"Social computing is the study of how technology shapes human social
interactions. This topic has become increasingly relevant to secondary school
students (ages 11--18) as more of young people's everyday social experiences
take place online, particularly with the continuing effects of the COVID-19
pandemic. However, social computing topics are rarely touched upon in existing
middle and high school curricula. We seek to introduce concepts from social
computing to secondary school students so they can understand how computing has
wide-ranging social implications that touch upon their everyday lives, as well
as think critically about both the positive and negative sides of different
social technology designs.
  In this report, we present a series of six lessons combining presentations
and hands-on activities covering topics within social computing and detail our
experience teaching these lessons to approximately 1,405 students across 13
middle and high schools in our local school district. We developed lessons
covering how social computing relates to the topics of Data Management,
Encrypted Messaging, Human-Computer Interaction Careers, Machine Learning and
Bias, Misinformation, and Online Behavior. We found that 81.13% of students
expressed greater interest in the content of our lessons compared to their
interest in STEM overall. We also found from pre- and post-lesson comprehension
questions that 63.65% learned new concepts from the main activity. We release
all lesson materials on a website for public use. From our experience, we
observed that students were engaged in these topics and found enjoyment in
finding connections between computing and their own lives.","['Kianna Bolante', 'Kevin Chen', 'Quan Ze Chen', 'Amy Zhang']",0,0.55802923
"The coronavirus pandemic (COVID-19) is probably the most disruptive global
health disaster in recent history. It negatively impacted the whole world and
virtually brought the global economy to a standstill. However, as the virus was
spreading, infecting people and claiming thousands of lives so was the spread
and propagation of fake news, misinformation and disinformation about the
event. These included the spread of unconfirmed health advice and remedies on
social media. In this paper, false information about the pandemic is identified
using a content-based approach and metadata curated from messages posted to
online social networks. A content-based approach combined with metadata as well
as an initial feature analysis is used and then several supervised learning
models are tested for identifying and predicting misleading posts. Our approach
shows up to 93% accuracy in the detection of fake news related posts about the
COVID-19 pandemic","['Oluwaseun Ajao', 'Ashish Garg', 'Marjory Da Costa-Abreu']",5,0.7971539
"Engagement with misinformation on social media poses unprecedented threats to
societal well-being, particularly during health crises when susceptibility to
misinformation is heightened in a multi-topic context. This paper focuses on
the COVID-19 pandemic and addresses a critical gap in understanding online
engagement with multi-topic misinformation at two user levels: news sharers who
share source news items on social media and post viewers who engage with online
news posts. To this end, we conduct a comprehensive analysis of 7273
fact-checked source news claims related to COVID-19 and their associated posts
on X, through the lens of topic diversity and conspiracy theories. We find that
false news, particularly when accompanied by conspiracy theories, exhibits
higher topic diversity than true news. At the news sharer level, false news has
a longer lifetime and receives more posts on X than true news. Additionally,
the integration of conspiracy theories is significantly associated with a
longer lifetime for COVID-19 misinformation. However, topic diversity has no
significant association with news sharer engagement in terms of news lifetime
and the number of posts. At the post viewer level, contrary to the news sharer
level, news posts characterized by heightened topic diversity receive more
reposts, likes, and replies. Notably, post viewers tend to engage more with
misinformation containing conspiracy narratives: false news posts that contain
conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes,
and 44.1% more replies compared to false news posts without conspiracy
theories. Our findings suggest that news sharers and post viewers exhibit
different engagement patterns on social media regarding topic diversity and
conspiracy theories, offering valuable insights into designing targeted
misinformation intervention strategies at both user levels.","['Yuwei Chuai', 'Jichang Zhao', 'Gabriele Lenzini']",3,0.704656
"WhatsApp is the largest social media platform in the Global South and is a
virulent force in global misinformation and political propaganda. Due to
end-to-end encryption WhatsApp can barely review any content and mostly rely on
volunteer moderation by group admins. Yet, little is known about how WhatsApp
group admins manage their groups, what factors and values influence moderation
decisions, and what challenges they face while managing their groups. To fill
this gap, we interviewed admins of 32 diverse groups and reviewed content from
30 public groups in India and Bangladesh. We observed notable differences in
the formation, members' behavior, and moderation of public versus private
groups, as well as in how WhatsApp admins operate compared to those on other
platforms. We used Baumrind's typology of 'parenting styles' as a lens to
examine how admins enact care and control during volunteer moderation. We
identified four styles based on how caring and controlling the admins are and
discuss design recommendations to help them better manage problematic content
in WhatsApp groups.","['Farhana Shahid', 'Dhruv Agarwal', 'Aditya Vashistha']",3,0.52532446
"Large Language Models have emerged as prime candidates to tackle
misinformation mitigation. However, existing approaches struggle with
hallucinations and overconfident predictions. We propose an uncertainty
quantification framework that leverages both direct confidence elicitation and
sampled-based consistency methods to provide better calibration for NLP
misinformation mitigation solutions. We first investigate the calibration of
sample-based consistency methods that exploit distinct features of consistency
across sample sizes and stochastic levels. Next, we evaluate the performance
and distributional shift of a robust numeric verbalization prompt across single
vs. two-step confidence elicitation procedure. We also compare the performance
of the same prompt with different versions of GPT and different numerical
scales. Finally, we combine the sample-based consistency and verbalized methods
to propose a hybrid framework that yields a better uncertainty estimation for
GPT models. Overall, our work proposes novel uncertainty quantification methods
that will improve the reliability of Large Language Models in misinformation
mitigation applications.","['Mauricio Rivera', 'Jean-Fran√ßois Godbout', 'Reihaneh Rabbany', 'Kellin Pelrine']",2,0.6615037
"This study delves into the application of Conformal Field Theory (CFT) to
understand information diffusion within digital media and its broader social
implications. Focusing on the digital native generation, vulnerable to
misinformation and data privacy issues, the research highlights the urgency of
addressing psychological health in the digital realm. By leveraging CFT's
analytical power, particularly in tracing information flows and assessing the
impact of misinformation, the study offers novel insights into the dynamics of
digital communication. It proposes a model for predicting and analyzing the
spread of biased content, emphasizing the need for improved information
literacy and personal data protection. This paper explores the concept of
infinitesimal conformal transformations using the Virasoro operator, forming a
conformal transformation group. In the examination of multiple analytical
methods, the approach to quantitatively analyze cases that are likely to expose
the risk of spreading filter bubbles such as negative information, rumors, and
fake news is to incorporate a two-dimensional conformal field theory approach
using the Ward-Takahashi identity and path integral with trace anomalies and
metric are introduced. Trace anomalies and metrics were introduced. A trace
anomaly refers to the spread of information in an unexpected way, usually
indicating anomalous behavior or hidden dynamics in the system. Metric
variation hypothesized and analyzed how diffusion characteristics, such as the
rate and pattern of information diffusion, vary over time.",['Yasuko Kawahata'],2,0.72347736
"Recent large language models (LLMs) have been shown to be effective for
misinformation detection. However, the choice of LLMs for experiments varies
widely, leading to uncertain conclusions. In particular, GPT-4 is known to be
strong in this domain, but it is closed source, potentially expensive, and can
show instability between different versions. Meanwhile, alternative LLMs have
given mixed results. In this work, we show that Zephyr-7b presents a
consistently viable alternative, overcoming key limitations of commonly used
approaches like Llama-2 and GPT-3.5. This provides the research community with
a solid open-source option and shows open-source models are gradually catching
up on this task. We then highlight how GPT-3.5 exhibits unstable performance,
such that this very widely used model could provide misleading results in
misinformation detection. Finally, we validate new tools including approaches
to structured output and the latest version of GPT-4 (Turbo), showing they do
not compromise performance, thus unlocking them for future research and
potentially enabling more complex pipelines for misinformation mitigation.","['Tyler Vergho', 'Jean-Francois Godbout', 'Reihaneh Rabbany', 'Kellin Pelrine']",6,0.65538126
"This research aims to model tracks the evolution of opinions among agents and
their collective dynamics, and mathematically represents the resonance of
opinions and echo chamber effects within the filter bubble by including
non-physical factors such as misinformation and confirmation bias, known as FP
ghosting phenomena.The indeterminate ghost phenomenon, a social science concept
similar to the uncertainty principle, depicts the variability of social opinion
by incorporating information uncertainty and nonlinearities in opinion
formation into the model. Furthermore, by introducing the Kubo formula and the
Matsubara form of the Green's function, we mathematically express temporal
effects and model how past, present, and future opinions interact to reveal the
mechanisms of opinion divergence and aggregation. Our model uses multiple
parameters, including population density and extremes of opinion generated on a
random number basis, to simulate the formation and growth of filter bubbles and
their progression to ultraviolet divergence phenomena. In this process, we
observe how resonance or disconnection of opinions within a society occurs via
a disconnection function (type la, lb, ll, lll). However, the interpretation of
the results requires careful consideration, and empirical verification is a
future challenge.Finally, we will share our hypotheses and considerations for
the model case of this paper, which is a close examination of regional
differences in media coverage and its effectiveness and considerations unique
to Japan, a disaster-prone country.",['Yasuko Kawahata'],2,0.6080706
"The recent success in language generation capabilities of large language
models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns
about their possible misuse in inducing mass agitation and communal hatred via
generating fake news and spreading misinformation. Traditional means of
developing a misinformation ground-truth dataset does not scale well because of
the extensive manual effort required to annotate the data. In this paper, we
propose an LLM-based approach of creating silver-standard ground-truth datasets
for identifying misinformation. Specifically speaking, given a trusted news
article, our proposed approach involves prompting LLMs to automatically
generate a summarised version of the original article. The prompts in our
proposed approach act as a controlling mechanism to generate specific types of
factual incorrectness in the generated summaries, e.g., incorrect quantities,
false attributions etc. To investigate the usefulness of this dataset, we
conduct a set of experiments where we train a range of supervised models for
the task of misinformation detection.","['Shrey Satapara', 'Parth Mehta', 'Debasis Ganguly', 'Sandip Modha']",6,0.7901356
"Medical question answer (QA) assistants respond to lay users' health-related
queries by synthesizing information from multiple sources using natural
language processing and related techniques. They can serve as vital tools to
alleviate issues of misinformation, information overload, and complexity of
medical language, thus addressing lay users' information needs while reducing
the burden on healthcare professionals. QA systems, the engines of such
assistants, have typically used either language models (LMs) or knowledge
graphs (KG), though the approaches could be complementary. LM-based QA systems
excel at understanding complex questions and providing well-formed answers, but
are prone to factual mistakes. KG-based QA systems, which represent facts well,
are mostly limited to answering short-answer questions with pre-created
templates. While a few studies have jointly used LM and KG approaches for
text-based QA, this was done to answer multiple-choice questions. Extant QA
systems also have limitations in terms of automation and performance. We
address these challenges by designing a novel, automated disease QA system
which effectively utilizes both LM and KG techniques through a joint-reasoning
approach to answer disease-related questions appropriate for lay users. Our
evaluation of the system using a range of quality metrics demonstrates its
efficacy over benchmark systems, including the popular ChatGPT.","['Prakash Chandra Sukhwal', 'Vaibhav Rajan', 'Atreyi Kankanhalli']",1,0.62768674
"In the largest survey of its kind, 2,778 researchers who had published in
top-tier artificial intelligence (AI) venues gave predictions on the pace of AI
progress and the nature and impacts of advanced AI systems The aggregate
forecasts give at least a 50% chance of AI systems achieving several milestones
by 2028, including autonomously constructing a payment processing site from
scratch, creating a song indistinguishable from a new song by a popular
musician, and autonomously downloading and fine-tuning a large language model.
If science continues undisrupted, the chance of unaided machines outperforming
humans in every possible task was estimated at 10% by 2027, and 50% by 2047.
The latter estimate is 13 years earlier than that reached in a similar survey
we conducted only one year earlier [Grace et al., 2022]. However, the chance of
all human occupations becoming fully automatable was forecast to reach 10% by
2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).
  Most respondents expressed substantial uncertainty about the long-term value
of AI progress: While 68.3% thought good outcomes from superhuman AI are more
likely than bad, of these net optimists 48% gave at least a 5% chance of
extremely bad outcomes such as human extinction, and 59% of net pessimists gave
5% or more to extremely good outcomes. Between 38% and 51% of respondents gave
at least a 10% chance to advanced AI leading to outcomes as bad as human
extinction. More than half suggested that ""substantial"" or ""extreme"" concern is
warranted about six different AI-related scenarios, including misinformation,
authoritarian control, and inequality. There was disagreement about whether
faster or slower AI progress would be better for the future of humanity.
However, there was broad agreement that research aimed at minimizing potential
risks from AI systems ought to be prioritized more.","['Katja Grace', 'Harlan Stewart', 'Julia Fabienne Sandk√ºhler', 'Stephen Thomas', 'Ben Weinstein-Raun', 'Jan Brauner']",9,0.64526236
"This study analyzes misinformation from WhatsApp, Twitter, and Kwai during
the 2022 Brazilian general election. Given the democratic importance of
accurate information during elections, multiple fact-checking organizations
collaborated to identify and respond to misinformation via WhatsApp tiplines
and power a fact-checking feature within a chatbot operated by Brazil's
election authority, the TSE. WhatsApp is installed on over 99% of smartphones
in Brazil, and the TSE chatbot was used by millions of citizens in the run-up
to the elections. During the same period, we collected social media data from
Twitter (now X) and Kwai (a popular video-sharing app similar to TikTok). Using
the WhatsApp, Kwai, and Twitter data along with fact-checks from three
Brazilian fact-checking organizations, we find unique claims on each platform.
Even when the same claims are present on different platforms, they often differ
in format, detail, length, or other characteristics. Our research highlights
the limitations of current claim matching algorithms to match claims across
platforms with such differences and identifies areas for further algorithmic
development. Finally, we perform a descriptive analysis examining the formats
(image, video, audio, text) and content themes of popular misinformation
claims.","['Scott A. Hale', 'Adriano Belisario', 'Ahmed Mostafa', 'Chico Camargo']",10,0.6628377
"Website reliability labels underpin almost all research in misinformation
detection. However, misinformation sources often exhibit transient behavior,
which makes many such labeled lists obsolete over time. We demonstrate that
Search Engine Optimization (SEO) attributes provide strong signals for
predicting news site reliability. We introduce a novel attributed webgraph
dataset with labeled news domains and their connections to outlinking and
backlinking domains. We demonstrate the success of graph neural networks in
detecting news site reliability using these attributed webgraphs, and show that
our baseline news site reliability classifier outperforms current SoTA methods
on the PoliticalNews dataset, achieving an F1 score of 0.96. Finally, we
introduce and evaluate a novel graph-based algorithm for discovering previously
unknown misinformation news sources.","['Peter Carragher', 'Evan M. Williams', 'Kathleen M. Carley']",4,0.74242806
"Misinformation proliferates in the online sphere, with evident impacts on the
political and social realms, influencing democratic discourse and posing risks
to public health and safety. The corporate world is also a prime target for
fake news dissemination. While recent studies have attempted to characterize
corporate misinformation and its effects on companies, their findings often
suffer from limitations due to qualitative or narrative approaches and a narrow
focus on specific industries. To address this gap, we conducted an analysis
utilizing social media quantitative methods and crowd-sourcing studies to
investigate corporate misinformation across a diverse array of industries
within the S\&P 500 companies. Our study reveals that corporate misinformation
encompasses topics such as products, politics, and societal issues. We
discovered companies affected by fake news also get reputable news coverage but
less social media attention, leading to heightened negativity in social media
comments, diminished stock growth, and increased stress mentions among employee
reviews. Additionally, we observe that a company is not targeted by fake news
all the time, but there are particular times when a critical mass of fake news
emerges. These findings hold significant implications for regulators, business
leaders, and investors, emphasizing the necessity to vigilantly monitor the
escalating phenomenon of corporate misinformation.","['Ke Zhou', 'Sanja Scepanovic', 'Daniele Quercia']",3,0.72240245
"Misinformation poses a variety of risks, such as undermining public trust and
distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been
shown effective in mitigating misinformation, particularly in handling
statements where enough context is provided. However, they struggle to assess
ambiguous or context-deficient statements accurately. This work introduces a
new method to resolve uncertainty in such statements. We propose a framework to
categorize missing information and publish category labels for the LIAR-New
dataset, which is adaptable to cross-domain content with missing information.
We then leverage this framework to generate effective user queries for missing
context. Compared to baselines, our method improves the rate at which generated
questions are answerable by the user by 38 percentage points and classification
performance by over 10 percentage points macro F1. Thus, this approach may
provide a valuable component for future misinformation mitigation pipelines.","['Yury Orlovskiy', 'Camille Thibault', 'Anne Imouza', 'Jean-Fran√ßois Godbout', 'Reihaneh Rabbany', 'Kellin Pelrine']",1,0.69889396
"The growing popularity of generative artificial intelligence (AI) chatbots
such as ChatGPT is having transformative effects on social media. As the
prevalence of AI-generated content grows, concerns have been raised regarding
privacy and misinformation online. Among social media platforms, Discord
enables AI integrations -- making their primarily ""Generation Z"" userbase
particularly exposed to AI-generated content. We surveyed Generation Z aged
individuals (n = 335) to evaluate their proficiency in discriminating between
AI-generated and human-authored text on Discord. The investigation employed
one-shot prompting of ChatGPT, disguised as a text message received on the
Discord.com platform. We explore the influence of demographic factors on
ability, as well as participants' familiarity with Discord and artificial
intelligence technologies. We find that Generation Z individuals are unable to
discern between AI and human-authored text (p = 0.011), and that those with
lower self-reported familiarity with Discord demonstrated an improved ability
in identifying human-authored compared to those with self-reported experience
with AI (p << 0.0001). Our results suggest that there is a nuanced relationship
between AI technology and popular modes of communication for Generation Z,
contributing valuable insights into human-computer interactions, digital
communication, and artificial intelligence literacy.","['Dhruv Ramu', 'Rishab Jain', 'Aditya Jain']",9,0.72371304
"We study the influence minimization problem: given a graph $G$ and a seed set
$S$, blocking at most $b$ nodes or $b$ edges such that the influence spread of
the seed set is minimized. This is a pivotal yet underexplored aspect of
network analytics, which can limit the spread of undesirable phenomena in
networks, such as misinformation and epidemics. Given the inherent NP-hardness
of the problem under the IC and LT models, previous studies have employed
greedy algorithms and Monte Carlo Simulations for its resolution. However,
existing techniques become cost-prohibitive when applied to large networks due
to the necessity of enumerating all the candidate blockers and computing the
decrease in expected spread from blocking each of them. This significantly
restricts the practicality and effectiveness of existing methods, especially
when prompt decision-making is crucial. In this paper, we propose the
AdvancedGreedy algorithm, which utilizes a novel graph sampling technique that
incorporates the dominator tree structure. We find that AdvancedGreedy can
achieve a $(1-1/e-\epsilon)$-approximation in the problem under the LT model.
Experimental evaluations on real-life networks reveal that our proposed
algorithms exhibit a significant enhancement in efficiency, surpassing the
state-of-the-art algorithm by three orders of magnitude, while achieving high
effectiveness.","['Jiadong Xie', 'Fan Zhang', 'Kai Wang', 'Jialu Liu', 'Xuemin Lin', 'Wenjie Zhang']",2,0.79556274
"As the landscape of time-sensitive applications gains prominence in 5G/6G
communications, timeliness of information updates at network nodes has become
crucial, which is popularly quantified in the literature by the age of
information metric. However, as we devise policies to improve age of
information of our systems, we inadvertently introduce a new vulnerability for
adversaries to exploit. In this article, we comprehensively discuss the diverse
threats that age-based systems are vulnerable to. We begin with discussion on
densely interconnected networks that employ gossiping between nodes to expedite
dissemination of dynamic information in the network, and show how the age-based
nature of gossiping renders these networks uniquely susceptible to threats such
as timestomping attacks, jamming attacks, and the propagation of
misinformation. Later, we survey adversarial works within simpler network
settings, specifically in one-hop and two-hop configurations, and delve into
adversarial robustness concerning challenges posed by jamming, timestomping,
and issues related to privacy leakage. We conclude this article with future
directions that aim to address challenges posed by more intelligent adversaries
and robustness of networks to them.","['Priyanka Kaswan', 'Sennur Ulukus']",2,0.6477182
"Large language models (LLMs) are able to engage in natural-sounding
conversations with humans, showcasing unprecedented capabilities for
information retrieval and automated decision support. They have disrupted
human-technology interaction and the way businesses operate. However,
technologies based on generative artificial intelligence (GenAI) are known to
hallucinate, misinform, and display biases introduced by the massive datasets
on which they are trained. Existing research indicates that humans may
unconsciously internalize these biases, which can persist even after they stop
using the programs. This study explores the cultural self-perception of LLMs by
prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from
the GLOBE project. The findings reveal that their cultural self-perception is
most closely aligned with the values of English-speaking countries and
countries characterized by sustained economic competitiveness. Recognizing the
cultural biases of LLMs and understanding how they work is crucial for all
members of society because one does not want the black box of artificial
intelligence to perpetuate bias in humans, who might, in turn, inadvertently
create and train even more biased algorithms.","['Wolfgang Messner', 'Tatum Greene', 'Josephine Matalone']",9,0.71276045
"This article presents a comparative analysis of the ability of two large
language model (LLM)-based chatbots, ChatGPT and Bing Chat, recently rebranded
to Microsoft Copilot, to detect veracity of political information. We use AI
auditing methodology to investigate how chatbots evaluate true, false, and
borderline statements on five topics: COVID-19, Russian aggression against
Ukraine, the Holocaust, climate change, and LGBTQ+ related debates. We compare
how the chatbots perform in high- and low-resource languages by using prompts
in English, Russian, and Ukrainian. Furthermore, we explore the ability of
chatbots to evaluate statements according to political communication concepts
of disinformation, misinformation, and conspiracy theory, using
definition-oriented prompts. We also systematically test how such evaluations
are influenced by source bias which we model by attributing specific claims to
various political and social actors. The results show high performance of
ChatGPT for the baseline veracity evaluation task, with 72 percent of the cases
evaluated correctly on average across languages without pre-training. Bing Chat
performed worse with a 67 percent accuracy. We observe significant disparities
in how chatbots evaluate prompts in high- and low-resource languages and how
they adapt their evaluations to political communication concepts with ChatGPT
providing more nuanced outputs than Bing Chat. Finally, we find that for some
veracity detection-related tasks, the performance of chatbots varied depending
on the topic of the statement or the source to which it is attributed. These
findings highlight the potential of LLM-based chatbots in tackling different
forms of false information in online environments, but also points to the
substantial variation in terms of how such potential is realized due to
specific factors, such as language of the prompt or the topic.","['Elizaveta Kuznetsova', 'Mykola Makhortykh', 'Victoria Vziatysheva', 'Martha Stolze', 'Ani Baghumyan', 'Aleksandra Urman']",8,0.70236903
"In this work, we examine how fact-checkers prioritize which claims to
fact-check and what tools may assist them in their efforts. Through a series of
interviews with 23 professional fact-checkers from around the world, we
validate that harm assessment is a central component of how fact-checkers
triage their work. We also clarify the processes behind fact-checking
prioritization, finding that they are typically ad hoc, and gather suggestions
for tools that could help with these processes.
  To address the needs articulated by fact-checkers, we present a structured
framework of questions to help fact-checkers negotiate the priority of claims
through assessing potential harms. Our FABLE Framework of Misinformation Harms
incorporates five dimensions of magnitude -- (social) Fragmentation,
Actionability, Believability, Likelihood of spread, and Exploitativeness --
that can help determine the potential urgency of a specific message or claim
when considering misinformation as harm. The result is a practical and
conceptual tool to support fact-checkers and others as they make strategic
decisions to prioritize their efforts. We conclude with a discussion of
computational approaches to support structured prioritization, as well as
applications beyond fact-checking to content moderation and curation.","['Connie Moon Sehat', 'Ryan Li', 'Peipei Nie', 'Tarunima Prabhakar', 'Amy X. Zhang']",1,0.65553224
"In the dynamic and rapidly evolving world of social media, detecting
anomalous users has become a crucial task to address malicious activities such
as misinformation and cyberbullying. As the increasing number of anomalous
users improves the ability to mimic normal users and evade detection, existing
methods only focusing on bot detection are ineffective in terms of capturing
subtle distinctions between users. To address these challenges, we proposed
SeGA, preference-aware self-contrastive learning for anomalous user detection,
which leverages heterogeneous entities and their relations in the Twittersphere
to detect anomalous users with different malicious strategies. SeGA utilizes
the knowledge of large language models to summarize user preferences via posts.
In addition, integrating user preferences with prompts as pseudo-labels for
preference-aware self-contrastive learning enables the model to learn
multifaceted aspects for describing the behaviors of users. Extensive
experiments on the proposed TwBNT benchmark demonstrate that SeGA significantly
outperforms the state-of-the-art methods (+3.5\% ~ 27.6\%) and empirically
validate the effectiveness of the model design and pre-training strategies. Our
code and data are publicly available at https://github.com/ying0409/SeGA.","['Ying-Ying Chang', 'Wei-Yao Wang', 'Wen-Chih Peng']",2,0.67230207
"Generative artificial intelligence has the potential to both exacerbate and
ameliorate existing socioeconomic inequalities. In this article, we provide a
state-of-the-art interdisciplinary overview of the potential impacts of
generative AI on (mis)information and three information-intensive domains:
work, education, and healthcare. Our goal is to highlight how generative AI
could worsen existing inequalities while illuminating how AI may help mitigate
pervasive social problems. In the information domain, generative AI can
democratize content creation and access, but may dramatically expand the
production and proliferation of misinformation. In the workplace, it can boost
productivity and create new jobs, but the benefits will likely be distributed
unevenly. In education, it offers personalized learning, but may widen the
digital divide. In healthcare, it might improve diagnostics and accessibility,
but could deepen pre-existing inequalities. In each section we cover a specific
topic, evaluate existing research, identify critical gaps, and recommend
research directions, including explicit trade-offs that complicate the
derivation of a priori hypotheses. We conclude with a section highlighting the
role of policymaking to maximize generative AI's potential to reduce
inequalities while mitigating its harmful effects. We discuss strengths and
weaknesses of existing policy frameworks in the European Union, the United
States, and the United Kingdom, observing that each fails to fully confront the
socioeconomic challenges we have identified. We propose several concrete
policies that could promote shared prosperity through the advancement of
generative AI. This article emphasizes the need for interdisciplinary
collaborations to understand and address the complex challenges of generative
AI.","['Valerio Capraro', 'Austin Lentsch', 'Daron Acemoglu', 'Selin Akgun', 'Aisel Akhmedova', 'Ennio Bilancini', 'Jean-Fran√ßois Bonnefon', 'Pablo Bra√±as-Garza', 'Luigi Butera', 'Karen M. Douglas', 'Jim A. C. Everett', 'Gerd Gigerenzer', 'Christine Greenhow', 'Daniel A. Hashimoto', 'Julianne Holt-Lunstad', 'Jolanda Jetten', 'Simon Johnson', 'Chiara Longoni', 'Pete Lunn', 'Simone Natale', 'Iyad Rahwan', 'Neil Selwyn', 'Vivek Singh', 'Siddharth Suri', 'Jennifer Sutcliffe', 'Joe Tomlinson', 'Sander van der Linden', 'Paul A. M. Van Lange', 'Friederike Wall', 'Jay J. Van Bavel', 'Riccardo Viale']",9,0.80517244
"Children encounter misinformation on social media in a similar capacity as
their parents. Unlike their parents, children are an exceptionally vulnerable
population because their cognitive abilities and emotional regulation are still
maturing, rendering them more susceptible to misinformation and falsehoods
online. Yet, little is known about children's experience with misinformation as
well as what their parents think of the misinformation's effect on child
development. To answer these questions, we combined a qualitative survey of
parents (n=87) with semi-structured interviews of both parents and children
(n=12). We found that children usually encounter deep fakes, memes with
political context, or celebrity/influencer rumors on social media. Children
revealed they ""ask Siri"" whether a social media video or post is true or not
before they search on Google or ask their parents about it. Parents expressed
discontent that their children are impressionable to misinformation, stating
that the burden falls on them to help their children develop critical thinking
skills for navigating falsehoods on social media. Here, the majority of parents
felt that schools should also teach these skills as well as media literacy to
their children. Misinformation, according to both parents and children affects
the family relationships especially with grandparents with different political
views than theirs.","['Filipo Sharevski', 'Jennifer Vander Loop']",3,0.5466509
"Older adults habitually encounter misinformation on social media, but there
is little knowledge about their experiences with it. In this study, we combined
a qualitative survey (n=119) with in-depth interviews (n=21) to investigate how
older adults in America conceptualize, discern, and contextualize social media
misinformation. As misinformation on social media in the past was driven
towards influencing voting outcomes, we were particularly interested to
approach our study from a voting intention perspective. We found that 62% of
the participants intending to vote Democrat saw a manipulative political
purpose behind the spread of misinformation while only 5% of those intending to
vote Republican believed misinformation has a political dissent purpose.
Regardless of the voting intentions, most participants relied on source
heuristics combined with fact-checking to discern truth from misinformation on
social media. The biggest concern about the misinformation, among all the
participants, was that it increasingly leads to biased reasoning influenced by
personal values and feelings instead of reasoning based on objective evidence.
The participants intending to vote Democrat were in 74% of the cases concerned
that misinformation will cause escalation of extremism in the future, while
those intending to vote Republican, were undecided, or planned to abstain were
concerned that misinformation will further erode the trust in democratic
institutions, specifically in the context of public health and free and fair
elections. During our interviews, we found that 63% of the participants who
intended to vote Republican, were fully aware and acknowledged that Republican
or conservative voices often time speak misinformation, even though they are
closely aligned to their political ideology.","['Filipo Sharevski', 'Jennifer Vander Loop']",3,0.6587315
"Large language models (LLMs) encapsulate vast amounts of knowledge but still
remain vulnerable to external misinformation. Existing research mainly studied
this susceptibility behavior in a single-turn setting. However, belief can
change during a multi-turn conversation, especially a persuasive one.
Therefore, in this study, we delve into LLMs' susceptibility to persuasive
conversations, particularly on factual questions that they can answer
correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which
contains factual questions paired with systematically generated persuasive
misinformation. Then, we develop a testing framework to track LLMs' belief
changes in a persuasive dialogue. Through extensive experiments, we find that
LLMs' correct beliefs on factual knowledge can be easily manipulated by various
persuasive strategies.","['Rongwu Xu', 'Brian S. Lin', 'Shujian Yang', 'Tianqi Zhang', 'Weiyan Shi', 'Tianwei Zhang', 'Zhixuan Fang', 'Wei Xu', 'Han Qiu']",6,0.7153442
"The importance of effective detection is underscored by the fact that
socialbots imitate human behavior to propagate misinformation, leading to an
ongoing competition between socialbots and detectors. Despite the rapid
advancement of reactive detectors, the exploration of adversarial socialbot
modeling remains incomplete, significantly hindering the development of
proactive detectors. To address this issue, we propose a mathematical
Structural Information principles-based Adversarial Socialbots Modeling
framework, namely SIASM, to enable more accurate and effective modeling of
adversarial behaviors. First, a heterogeneous graph is presented to integrate
various users and rich activities in the original social network and measure
its dynamic uncertainty as structural entropy. By minimizing the
high-dimensional structural entropy, a hierarchical community structure of the
social network is generated and referred to as the optimal encoding tree.
Secondly, a novel method is designed to quantify influence by utilizing the
assigned structural entropy, which helps reduce the computational cost of SIASM
by filtering out uninfluential users. Besides, a new conditional structural
entropy is defined between the socialbot and other users to guide the follower
selection for network influence maximization. Extensive and comparative
experiments on both homogeneous and heterogeneous social networks demonstrate
that, compared with state-of-the-art baselines, the proposed SIASM framework
yields substantial performance improvements in terms of network influence (up
to 16.32%) and sustainable stealthiness (up to 16.29%) when evaluated against a
robust detector with 90% accuracy.","['Xianghua Zeng', 'Hao Peng', 'Angsheng Li']",2,0.7180486
"The Digital Services Act (DSA) requires large social media platforms in the
EU to provide clear and specific information whenever they remove or restrict
access to certain content. These ""Statements of Reasons"" (SoRs) are collected
in the DSA Transparency Database to ensure transparency and scrutiny of content
moderation decisions of the providers of online platforms. In this work, we
empirically analyze 156 million SoRs within an observation period of two months
to provide an early look at content moderation decisions of social media
platforms in the EU. Our empirical analysis yields the following main findings:
(i) There are vast differences in the frequency of content moderation across
platforms. For instance, TikTok performs more than 350 times more content
moderation decisions per user than X/Twitter. (ii) Content moderation is most
commonly applied for text and videos, whereas images and other content formats
undergo moderation less frequently. (ii) The primary reasons for moderation
include content falling outside the platform's scope of service,
illegal/harmful speech, and pornography/sexualized content, with moderation of
misinformation being relatively uncommon. (iii) The majority of rule-breaking
content is detected and decided upon via automated means rather than manual
intervention. However, X/Twitter reports that it relies solely on non-automated
methods. (iv) There is significant variation in the content moderation actions
taken across platforms. Altogether, our study implies inconsistencies in how
social media platforms implement their obligations under the DSA -- resulting
in a fragmented outcome that the DSA is meant to avoid. Our findings have
important implications for regulators to clarify existing guidelines or lay out
more specific rules that ensure common standards on how social media providers
handle rule-breaking content on their platforms.","['Chiara Drolsbach', 'Nicolas Pr√∂llochs']",3,0.622627
"With the constant spread of misinformation on social media networks, a need
has arisen to continuously assess the veracity of digital content. This need
has inspired numerous research efforts on the development of misinformation
detection (MD) models. However, many models do not use all information
available to them and existing research contains a lack of relevant datasets to
train the models, specifically within the South African social media
environment. The aim of this paper is to investigate the transferability of
knowledge of a MD model between different contextual environments. This
research contributes a multimodal MD model capable of functioning in the South
African social media environment, as well as introduces a South African
misinformation dataset. The model makes use of multiple sources of information
for misinformation detection, namely: textual and visual elements. It uses
bidirectional encoder representations from transformers (BERT) as the textual
encoder and a residual network (ResNet) as the visual encoder. The model is
trained and evaluated on the Fakeddit dataset and a South African
misinformation dataset. Results show that using South African samples in the
training of the model increases model performance, in a South African
contextual environment, and that a multimodal model retains significantly more
knowledge than both the textual and visual unimodal models. Our study suggests
that the performance of a misinformation detection model is influenced by the
cultural nuances of its operating environment and multimodal models assist in
the transferability of knowledge between different contextual environments.
Therefore, local data should be incorporated into the training process of a
misinformation detection model in order to optimize model performance.","['Amica De Jager', 'Vukosi Marivate', 'Abioudun Modupe']",8,0.66259325
"This paper delves into the history and integration of quantum theory into
areas such as opinion dynamics, decision theory, and game theory, offering a
novel framework for social simulations. It introduces a quantum perspective for
analyzing information transfer and decision-making complexity within social
systems, employing a toric code-based method for error discrimination.Central
to this research is the use of toric codes, originally for quantum error
correction, to detect and correct errors in social simulations, representing
uncertainty in opinion formation and decision-making processes. Operator and
error syndrome measurement, vital in quantum computation, help identify and
analyze errors and uncertainty in social simulations. The paper also discusses
fault-tolerant computation employing transversal gates, which protect against
errors during quantum computation. In social simulations, transversal gates
model protection from external interference and misinformation, enhancing the
fidelity of decision-making and strategy formation processes.",['Yasuko Kawahata'],2,0.62997895
"Social media platforms such as Twitter (now known as X) have revolutionized
how the public engage with important societal and political topics. Recently,
climate change discussions on social media became a catalyst for political
polarization and the spreading of misinformation. In this work, we aim to
understand how real world events influence the opinions of individuals towards
climate change related topics on social media. To this end, we extracted and
analyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006
to 2019. Then, we construct a temporal graph from the user-user mentions
network and utilize the Louvain community detection algorithm to analyze the
changes in community structure around Conference of the Parties on Climate
Change~(COP) events. Next, we also apply tools from the Natural Language
Processing literature to perform sentiment analysis and topic modeling on the
tweets. Our work acts as a first step towards understanding the evolution of
pro-climate change communities around COP events. Answering these questions
helps us understand how to raise people's awareness towards climate change thus
hopefully calling on more individuals to join the collaborative effort in
slowing down climate change.","['Yashaswi Pupneja', 'Joseph Zou', 'Sacha L√©vy', 'Shenyang Huang']",3,0.6906134
"Despite increasing awareness and research around fake news, there is still a
significant need for datasets that specifically target racial slurs and biases
within North American political speeches. This is particulary important in the
context of upcoming North American elections. This study introduces a
comprehensive dataset that illuminates these critical aspects of
misinformation. To develop this fake news dataset, we scraped and built a
corpus of 40,000 news articles about political discourses in North America. A
portion of this dataset (4000) was then carefully annotated, using a blend of
advanced language models and human verification methods. We have made both
these datasets openly available to the research community and have conducted
benchmarking on the annotated data to demonstrate its utility. We release the
best-performing language model along with data. We encourage researchers and
developers to make use of this dataset and contribute to this ongoing
initiative.","['Shaina Raza', 'Mizanur Rahman', 'Shardul Ghuge']",8,0.73784137
"Open-source Large Language Models (LLMs) have recently gained popularity
because of their comparable performance to proprietary LLMs. To efficiently
fulfill domain-specialized tasks, open-source LLMs can be refined, without
expensive accelerators, using low-rank adapters. However, it is still unknown
whether low-rank adapters can be exploited to control LLMs. To address this
gap, we demonstrate that an infected adapter can induce, on specific
triggers,an LLM to output content defined by an adversary and to even
maliciously use tools. To train a Trojan adapter, we propose two novel attacks,
POLISHED and FUSION, that improve over prior approaches. POLISHED uses a
superior LLM to align na\""ively poisoned data based on our insight that it can
better inject poisoning knowledge during training. In contrast, FUSION
leverages a novel over-poisoning procedure to transform a benign adapter into a
malicious one by magnifying the attention between trigger and target in model
weights. In our experiments, we first conduct two case studies to demonstrate
that a compromised LLM agent can use malware to control the system (e.g., a
LLM-driven robot) or to launch a spear-phishing attack. Then, in terms of
targeted misinformation, we show that our attacks provide higher attack
effectiveness than the existing baseline and, for the purpose of attracting
downloads, preserve or improve the adapter's utility. Finally, we designed and
evaluated three potential defenses. However, none proved entirely effective in
safeguarding against our attacks, highlighting the need for more robust
defenses supporting a secure LLM supply chain.","['Tian Dong', 'Minhui Xue', 'Guoxing Chen', 'Rayne Holland', 'Yan Meng', 'Shaofeng Li', 'Zhen Liu', 'Haojin Zhu']",6,0.701264
"Imperceptible digital watermarking is important in copyright protection,
misinformation prevention, and responsible generative AI. We propose TrustMark
- a GAN-based watermarking method with novel design in architecture and
spatio-spectra losses to balance the trade-off between watermarked image
quality with the watermark recovery accuracy. Our model is trained with
robustness in mind, withstanding various in- and out-place perturbations on the
encoded image. Additionally, we introduce TrustMark-RM - a watermark remover
method useful for re-watermarking. Our methods achieve state-of-art performance
on 3 benchmarks comprising arbitrary resolution images.","['Tu Bui', 'Shruti Agarwal', 'John Collomosse']",7,0.60760987
"This paper introduces a multilingual dataset of COVID-19 vaccine
misinformation, consisting of annotated tweets from three middle-income
countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset
includes annotations for 5,952 tweets, assessing their relevance to COVID-19
vaccines, presence of misinformation, and the themes of the misinformation. To
address challenges posed by domain specificity, the low-resource setting, and
data imbalance, we adopt two approaches for developing COVID-19 vaccine
misinformation detection models: domain-specific pre-training and text
augmentation using a large language model. Our best misinformation detection
models demonstrate improvements ranging from 2.7 to 15.9 percentage points in
macro F1-score compared to the baseline models. Additionally, we apply our
misinformation detection models in a large-scale study of 19 million unlabeled
tweets from the three countries between 2020 and 2022, showcasing the practical
application of our dataset and models for detecting and analyzing vaccine
misinformation in multiple countries and languages. Our analysis indicates that
percentage changes in the number of new COVID-19 cases are positively
associated with COVID-19 vaccine misinformation rates in a staggered manner for
Brazil and Indonesia, and there are significant positive associations between
the misinformation rates across the three countries.","['Jongin Kim', 'Byeo Rhee Bak', 'Aditya Agrawal', 'Jiaxi Wu', 'Veronika J. Wirtz', 'Traci Hong', 'Derry Wijaya']",12,0.7138355
"Potential harms of Large Language Models such as mass misinformation and
plagiarism can be partially mitigated if there exists a reliable way to detect
machine generated text. In this paper, we propose a new watermarking method to
detect machine-generated texts. Our method embeds a unique pattern within the
generated text, ensuring that while the content remains coherent and natural to
human readers, it carries distinct markers that can be identified
algorithmically. Specifically, we intervene with the token sampling process in
a way which enables us to trace back our token choices during the detection
phase. We show how watermarking affects textual quality and compare our
proposed method with a state-of-the-art watermarking method in terms of
robustness and detectability. Through extensive experiments, we demonstrate the
effectiveness of our watermarking scheme in distinguishing between watermarked
and non-watermarked text, achieving high detection rates while maintaining
textual quality.","['Kaan Efe Kele≈ü', '√ñmer Kaan G√ºrb√ºz', 'Mucahid Kutlu']",7,0.6459925
"The accelerated advancement of generative AI significantly enhance the
viability and effectiveness of generative regional editing methods. This
evolution render the image manipulation more accessible, thereby intensifying
the risk of altering the conveyed information within original images and even
propagating misinformation. Consequently, there exists a critical demand for
robust capable of detecting the edited images. However, the lack of
comprehensive dataset containing images edited with abundant and advanced
generative regional editing methods poses a substantial obstacle to the
advancement of corresponding detection methods.
  We endeavor to fill the vacancy by constructing the GRE dataset, a
large-scale generative regional editing dataset with the following advantages:
1) Collection of real-world original images, focusing on two frequently edited
scenarios. 2) Integration of a logical and simulated editing pipeline,
leveraging multiple large models in various modalities. 3) Inclusion of various
editing approaches with distinct architectures. 4) Provision of comprehensive
analysis tasks. We perform comprehensive experiments with proposed three tasks:
edited image classification, edited method attribution and edited region
localization, providing analysis of distinct editing methods and evaluation of
detection methods in related fields. We expect that the GRE dataset can promote
further research and exploration in the field of generative region editing
detection.","['Zhihao Sun', 'Haipeng Fang', 'Xinying Zhao', 'Danding Wang', 'Juan Cao']",7,0.6360611
"With the advent of sophisticated artificial intelligence (AI) technologies,
the proliferation of deepfakes and the spread of m/disinformation have emerged
as formidable threats to the integrity of information ecosystems worldwide.
This paper provides an overview of the current literature. Within the frontier
AI's crucial application in developing defense mechanisms for detecting
deepfakes, we highlight the mechanisms through which generative AI based on
large models (LM-based GenAI) craft seemingly convincing yet fabricated
contents. We explore the multifaceted implications of LM-based GenAI on
society, politics, and individual privacy violations, underscoring the urgent
need for robust defense strategies. To address these challenges, in this study,
we introduce an integrated framework that combines advanced detection
algorithms, cross-platform collaboration, and policy-driven initiatives to
mitigate the risks associated with AI-Generated Content (AIGC). By leveraging
multi-modal analysis, digital watermarking, and machine learning-based
authentication techniques, we propose a defense mechanism adaptable to AI
capabilities of ever-evolving nature. Furthermore, the paper advocates for a
global consensus on the ethical usage of GenAI and implementing cyber-wellness
educational programs to enhance public awareness and resilience against
m/disinformation. Our findings suggest that a proactive and collaborative
approach involving technological innovation and regulatory oversight is
essential for safeguarding netizens while interacting with cyberspace against
the insidious effects of deepfakes and GenAI-enabled m/disinformation
campaigns.","['Mohamed R. Shoaib', 'Zefan Wang', 'Milad Taleby Ahvanooey', 'Jun Zhao']",9,0.8096218
"Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.","['Yuebing Liang', 'Yichao Liu', 'Xiaohan Wang', 'Zhan Zhao']",6,0.6459017
"Synthetically generated face images have shown to be indistinguishable from
real images by humans and as such can lead to a lack of trust in digital
content as they can, for instance, be used to spread misinformation. Therefore,
the need to develop algorithms for detecting entirely synthetic face images is
apparent. Of interest are images generated by state-of-the-art deep
learning-based models, as these exhibit a high level of visual realism. Recent
works have demonstrated that detecting such synthetic face images under
realistic circumstances remains difficult as new and improved generative models
are proposed with rapid speed and arbitrary image post-processing can be
applied. In this work, we propose a multi-channel architecture for detecting
entirely synthetic face images which analyses information both in the frequency
and visible spectra using Cross Modal Focal Loss. We compare the proposed
architecture with several related architectures trained using Binary Cross
Entropy and show in cross-model experiments that the proposed architecture
supervised using Cross Modal Focal Loss, in general, achieves most competitive
performance.","['M. Ibsen', 'C. Rathgeb', 'S. Marcel', 'C. Busch']",7,0.7968155
"The increasing adoption and commercialization of generalized Large Language
Models (LLMs) have profoundly impacted various aspects of our daily lives.
Initially embraced by the computer science community, the versatility of LLMs
has found its way into diverse domains. In particular, the software engineering
realm has witnessed the most transformative changes. With LLMs increasingly
serving as AI Pair Programming Assistants spurred the development of
specialized models aimed at aiding software engineers. Although this new
paradigm offers numerous advantages, it also presents critical challenges and
open problems. To identify the potential and prevailing obstacles, we
systematically reviewed contemporary scholarly publications, emphasizing the
perspectives of software developers and usability concerns. Preliminary
findings underscore pressing concerns about data privacy, bias, and
misinformation. Additionally, we identified several usability challenges,
including prompt engineering, increased cognitive demands, and mistrust.
Finally, we introduce 12 open problems that we have identified through our
survey, covering these various domains.",['Sajed Jalil'],6,0.6268866
"In today's technologically driven world, the spread of fake news,
particularly during crucial events such as elections, presents an increasing
challenge to the integrity of information. To address this challenge, we
introduce FakeWatch ElectionShield, an innovative framework carefully designed
to detect fake news. We have created a novel dataset of North American
election-related news articles through a blend of advanced language models
(LMs) and thorough human verification, for precision and relevance. We propose
a model hub of LMs for identifying fake news. Our goal is to provide the
research community with adaptable and accurate classification models in
recognizing the dynamic nature of misinformation. Extensive evaluation of fake
news classifiers on our dataset and a benchmark dataset shows our that while
state-of-the-art LMs slightly outperform the traditional ML models, classical
models are still competitive with their balance of accuracy, explainability,
and computational efficiency. This research sets the foundation for future
studies to address misinformation related to elections.","['Tahniat Khan', 'Mizanur Rahman', 'Veronica Chatrath', 'Oluwanifemi Bamgbose', 'Shaina Raza']",4,0.7547779
"Deepfake videos are defined as a resulting media from the synthesis of
different persons images and videos, mostly faces, replacing a real one. The
easy spread of such videos leads to elevated misinformation and represents a
threat to society and democracy today. The present study aims to collect and
analyze the relevant literature through a systematic procedure. We present 27
articles from scientific databases revealing threats to society, democracies,
the political life but present as well advantages of this technology in
entertainment, gaming, education, and public life. The research indicates high
scientific interest in deepfake detection algorithms as well as the ethical
aspect of such technology. This article covers the scientific gap since, to the
best of our knowledge, this is the first systematic literature review in the
field. A discussion has already started among academics and practitioners
concerning the spread of fake news. The next step of fake news considers the
use of artificial intelligence and machine learning algorithms that create
hyper-realistic videos, called deepfake. Deepfake technology has continuously
attracted the attention of scholars over the last 3 years more and more. The
importance of conducting research in this field derives from the necessity to
understand the theory. The first contextual approach is related to the
epistemological points of view of the concept. The second one is related to the
phenomenological disadvantages of the field. Despite that, the authors will try
to focus not only on the disadvantages of the field but also on the positive
aspects of the technology.","['Nikolaos Misirlis', 'Harris Bin Munawar']",11,0.70319
"In this work, I discuss how Large Language Models can be applied in the legal
domain, circumventing their current drawbacks. Despite their large success and
acceptance, their lack of explainability hinders legal experts to trust in
their output, and this happens rightfully so. However, in this paper, I argue
in favor of a new view, Justifiable Artificial Intelligence, instead of
focusing on Explainable Artificial Intelligence. I discuss in this paper how
gaining evidence for and against a Large Language Model's output may make their
generated texts more trustworthy - or hold them accountable for misinformation.",['Sabine Wehnert'],9,0.56344944
"The spread of fake news using out-of-context images has become widespread and
is a relevant problem in this era of information overload. Such out-of-context
fake news may arise across different domains like politics, sports,
entertainment, etc. In practical scenarios, an inherent problem of imbalance
exists among news articles from such widely varying domains, resulting in a few
domains with abundant data, while the rest containing very limited data. Under
such circumstances, it is imperative to develop methods which can work in such
varying amounts of data setting. In this work, we explore whether out-of-domain
data can help to improve out-of-context misinformation detection (termed here
as multi-modal fake news detection) of a desired domain, to address this
challenging problem. Towards this goal, we propose a novel framework termed
DPOD (Domain-specific Prompt-tuning using Out-of-Domain data). First, to
compute generalizable features, we modify the Vision-Language Model, CLIP to
extract features that helps to align the representations of the images and
corresponding text captions of both the in-domain and out-of-domain data in a
label-aware manner. Further, we propose a domain-specific prompt learning
technique which leverages the training samples of all the available domains
based on the extent they can be useful to the desired domain. Extensive
experiments on a large-scale benchmark dataset, namely NewsCLIPpings
demonstrate that the proposed framework achieves state of-the-art performance,
significantly surpassing the existing approaches for this challenging task.
Code will be released on acceptance.","['Debarshi Brahma', 'Amartya Bhattacharya', 'Suraj Nagaje Mahadev', 'Anmol Asati', 'Vikas Verma', 'Soma Biswas']",8,0.63842237
"Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher ""credibility"" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM","['Hui Liu', 'Wenya Wang', 'Hao Sun', 'Anderson Rocha', 'Haoliang Li']",2,0.6709775
"The growing reliance on social media for news consumption necessitates
effective countermeasures to mitigate the rapid spread of misinformation.
Prebunking, a proactive method that arms users with accurate information before
they come across false content, has garnered support from journalism and
psychology experts. We formalize the problem of optimal prebunking as
optimizing the timing of delivering accurate information, ensuring users
encounter it before receiving misinformation while minimizing the disruption to
user experience. Utilizing a susceptible-infected epidemiological process to
model the propagation of misinformation, we frame optimal prebunking as a
policy synthesis problem with safety constraints. We then propose a policy that
approximates the optimal solution to a relaxed problem. The experiments show
that this policy cuts the user experience cost of repeated information delivery
in half, compared to delivering accurate information immediately after
identifying a misinformation propagation.","['Yigit Ege Bayiz', 'Ufuk Topcu']",0,0.6457778
"Large Language Models (LLMs) have garnered significant attention for their
powerful ability in natural language understanding and reasoning. In this
paper, we present a comprehensive empirical study to explore the performance of
LLMs on misinformation detection tasks. This study stands as the pioneering
investigation into the understanding capabilities of multiple LLMs regarding
both content and propagation across social media platforms. Our empirical
studies on five misinformation detection datasets show that LLMs with diverse
prompts achieve comparable performance in text-based misinformation detection
but exhibit notably constrained capabilities in comprehending propagation
structure compared to existing models in propagation-based misinformation
detection. Besides, we further design four instruction-tuned strategies to
enhance LLMs for both content and propagation-based misinformation detection.
These strategies boost LLMs to actively learn effective features from multiple
instances or hard instances, and eliminate irrelevant propagation structures,
thereby achieving better detection performance. Extensive experiments further
demonstrate LLMs would play a better capacity in content and propagation
structure under these proposed strategies and achieve promising detection
performance. These findings highlight the potential ability of LLMs to detect
misinformation.","['Mengyang Chen', 'Lingwei Wei', 'Han Cao', 'Wei Zhou', 'Songlin Hu']",6,0.80904114
"Fears about the destabilizing impact of misinformation online have motivated
individuals and platforms to respond. Individuals have become empowered to
challenge others' online claims with fact-checks in pursuit of a healthier
information ecosystem and to break down echo chambers of self-reinforcing
opinion. Using Twitter data, here we show the consequences of individual
misinformation tagging: tagged posters had explored novel political information
and expanded topical interests immediately prior, but being tagged caused
posters to retreat into information bubbles. These unintended consequences were
softened by a collective verification system for misinformation moderation. In
Twitter's new platform, Community Notes, misinformation tagging was
peer-reviewed by other fact-checkers before exposure to the poster. With
collective misinformation tagging, posters were less likely to retreat from
diverse information engagement. Detailed comparison suggests differences in
toxicity, sentiment, readability, and delay in individual versus collective
misinformation tagging messages. These findings provide evidence for
differential impacts from individual versus collective moderation strategies on
the diversity of information engagement and mobility across the information
ecosystem.","['Junsol Kim', 'Zhao Wang', 'Haohan Shi', 'Hsin-Keng Ling', 'James Evans']",3,0.7784224
"Continual learning algorithms are typically exposed to untrusted sources that
contain training data inserted by adversaries and bad actors. An adversary can
insert a small number of poisoned samples, such as mislabeled samples from
previously learned tasks, or intentional adversarial perturbed samples, into
the training datasets, which can drastically reduce the model's performance. In
this work, we demonstrate that continual learning systems can be manipulated by
malicious misinformation and present a new category of data poisoning attacks
specific for continual learners, which we refer to as {\em Poisoning Attacks
Against Continual Learners} (PACOL). The effectiveness of labeling flipping
attacks inspires PACOL; however, PACOL produces attack samples that do not
change the sample's label and produce an attack that causes catastrophic
forgetting. A comprehensive set of experiments shows the vulnerability of
commonly used generative replay and regularization-based continual learning
approaches against attack methods. We evaluate the ability of label-flipping
and a new adversarial poison attack, namely PACOL proposed in this work, to
force the continual learning system to forget the knowledge of a learned
task(s). More specifically, we compared the performance degradation of
continual learning systems trained on benchmark data streams with and without
poisoning attacks. Moreover, we discuss the stealthiness of the attacks in
which we test the success rate of data sanitization defense and other outlier
detection-based defenses for filtering out adversarial samples.","['Huayu Li', 'Gregory Ditzler']",2,0.63233936
"The ethical imperative for technology should be first, do no harm. But
digital innovations like AI and social media increasingly enable societal
harms, from bias to misinformation. As these technologies grow ubiquitous, we
need solutions to address unintended consequences. This report proposes a model
to incentivize developers to prevent foreseeable algorithmic harms. It does
this by expanding negligence and product liability laws. Digital product
developers would be incentivized to mitigate potential algorithmic risks before
deployment to protect themselves and investors. Standards and penalties would
be set proportional to harm. Insurers would require harm mitigation during
development in order to obtain coverage. This shifts tech ethics from move fast
and break things to first, do no harm. Details would need careful refinement
between stakeholders to enact reasonable guardrails without stifling
innovation. Policy and harm prevention frameworks would likely evolve over
time. Similar accountability schemes have helped address workplace,
environmental, and product safety. Introducing algorithmic harm negligence
liability would acknowledge the real societal costs of unethical tech. The
timing is right for reform. This proposal provides a model to steer the digital
revolution toward human rights and dignity. Harm prevention must be prioritized
over reckless growth. Vigorous liability policies are essential to stop
technologists from breaking things",['Marc J. Pfeiffer'],9,0.6491645
"The proliferation of misinformation on social media platforms (SMPs) poses a
significant danger to public health, social cohesion and ultimately democracy.
Previous research has shown how social correction can be an effective way to
curb misinformation, by engaging directly in a constructive dialogue with users
who spread -- often in good faith -- misleading messages. Although professional
fact-checkers are crucial to debunking viral claims, they usually do not engage
in conversations on social media. Thereby, significant effort has been made to
automate the use of fact-checker material in social correction; however, no
previous work has tried to integrate it with the style and pragmatics that are
commonly employed in social media communication. To fill this gap, we present
VerMouth, the first large-scale dataset comprising roughly 12 thousand
claim-response pairs (linked to debunking articles), accounting for both
SMP-style and basic emotions, two factors which have a significant role in
misinformation credibility and spreading. To collect this dataset we used a
technique based on an author-reviewer pipeline, which efficiently combines LLMs
and human annotators to obtain high-quality data. We also provide comprehensive
experiments showing how models trained on our proposed dataset have significant
improvements in terms of output quality and generalization capabilities.","['Daniel Russo', 'Shane Peter Kaszefski-Yaschuk', 'Jacopo Staiano', 'Marco Guerini']",0,0.7618003
"Many under-resourced languages require high-quality datasets for specific
tasks such as offensive language detection, disinformation, or misinformation
identification. However, the intricacies of the content may have a detrimental
effect on the annotators. The article aims to revisit an approach of
pseudo-labeling sensitive data on the example of Ukrainian tweets covering the
Russian-Ukrainian war. Nowadays, this acute topic is in the spotlight of
various language manipulations that cause numerous disinformation and profanity
on social media platforms. The conducted experiment highlights three main
stages of data annotation and underlines the main obstacles during machine
annotation. Ultimately, we provide a fundamental statistical analysis of the
obtained data, evaluation of models used for pseudo-labelling, and set further
guidelines on how the scientists can leverage the corpus to execute more
advanced research and extend the existing data samples without annotators'
engagement.",['Stetsenko Daria'],8,0.83341545
"Online misinformation is often multimodal in nature, i.e., it is caused by
misleading associations between texts and accompanying images. To support the
fact-checking process, researchers have been recently developing automatic
multimodal methods that gather and analyze external information, evidence,
related to the image-text pairs under examination. However, prior works assumed
all external information collected from the web to be relevant. In this study,
we introduce a ""Relevant Evidence Detection"" (RED) module to discern whether
each piece of evidence is relevant, to support or refute the claim.
Specifically, we develop the ""Relevant Evidence Detection Directed Transformer""
(RED-DOT) and explore multiple architectural variants (e.g., single or
dual-stage) and mechanisms (e.g., ""guided attention""). Extensive ablation and
comparative experiments demonstrate that RED-DOT achieves significant
improvements over the state-of-the-art (SotA) on the VERITE benchmark by up to
33.7%. Furthermore, our evidence re-ranking and element-wise modality fusion
led to RED-DOT surpassing the SotA on NewsCLIPings+ by up to 3% without the
need for numerous evidence or multiple backbone encoders. We release our code
at: https://github.com/stevejpapad/relevant-evidence-detection","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Symeon Papadopoulos', 'Panagiotis C. Petrantonakis']",1,0.70678824
"The use of propagandistic techniques in online content has increased in
recent years aiming to manipulate online audiences. Fine-grained propaganda
detection and extraction of textual spans where propaganda techniques are used,
are essential for more informed content consumption. Automatic systems
targeting the task over lower resourced languages are limited, usually
obstructed by lack of large scale training datasets. Our study investigates
whether Large Language Models (LLMs), such as GPT-4, can effectively extract
propagandistic spans. We further study the potential of employing the model to
collect more cost-effective annotations. Finally, we examine the effectiveness
of labels provided by GPT-4 in training smaller language models for the task.
The experiments are performed over a large-scale in-house manually annotated
dataset. The results suggest that providing more annotation context to GPT-4
within prompts improves its performance compared to human annotators. Moreover,
when serving as an expert annotator (consolidator), the model provides labels
that have higher agreement with expert annotators, and lead to specialized
models that achieve state-of-the-art over an unseen Arabic testing set.
Finally, our work is the first to show the potential of utilizing LLMs to
develop annotated datasets for propagandistic spans detection task prompting it
with annotations from human annotators with limited expertise. All scripts and
annotations will be shared with the community.","['Maram Hasanain', 'Fatema Ahmad', 'Firoj Alam']",8,0.70470893
"Susceptibility to misinformation describes the degree of belief in
unverifiable claims, a latent aspect of individuals' mental processes that is
not observable. Existing susceptibility studies heavily rely on self-reported
beliefs, which can be subject to bias, expensive to collect, and challenging to
scale for downstream applications. To address these limitations, in this work,
we propose a computational approach to model users' latent susceptibility
levels. As shown in previous research, susceptibility is influenced by various
factors (e.g., demographic factors, political ideology), and directly
influences people's reposting behavior on social media. To represent the
underlying mental process, our susceptibility modeling incorporates these
factors as inputs, guided by the supervision of people's sharing behavior.
Using COVID-19 as a testbed domain, our experiments demonstrate a significant
alignment between the susceptibility scores estimated by our computational
modeling and human judgments, confirming the effectiveness of this latent
modeling approach. Furthermore, we apply our model to annotate susceptibility
scores on a large-scale dataset and analyze the relationships between
susceptibility with various factors. Our analysis reveals that political
leanings and psychological factors exhibit varying degrees of association with
susceptibility to COVID-19 misinformation.","['Yanchen Liu', 'Mingyu Derek Ma', 'Wenna Qin', 'Azure Zhou', 'Jiaao Chen', 'Weiyan Shi', 'Wei Wang', 'Diyi Yang']",2,0.64746076
"Accurately simulating human opinion dynamics is crucial for understanding a
variety of societal phenomena, including polarization and the spread of
misinformation. However, the agent-based models (ABMs) commonly used for such
simulations often over-simplify human behavior. We propose a new approach to
simulating opinion dynamics based on populations of Large Language Models
(LLMs). Our findings reveal a strong inherent bias in LLM agents towards
producing accurate information, leading simulated agents to consensus in line
with scientific reality. This bias limits their utility for understanding
resistance to consensus views on issues like climate change. After inducing
confirmation bias through prompt engineering, however, we observed opinion
fragmentation in line with existing agent-based modeling and opinion dynamics
research. These insights highlight the promise and limitations of LLM agents in
this domain and suggest a path forward: refining LLMs with real-world discourse
to better simulate the evolution of human beliefs.","['Yun-Shiuan Chuang', 'Agam Goyal', 'Nikunj Harlalka', 'Siddharth Suresh', 'Robert Hawkins', 'Sijia Yang', 'Dhavan Shah', 'Junjie Hu', 'Timothy T. Rogers']",6,0.681132
"The emergence of research focused to understand the spreading and impact of
disinformation is increasing year over year. Most times, the purpose of those
who start the spreading of information intentionally false and designed to
cause harm is in catalyzing its fast transformation into misinformation, which
is the false content shared by people who do not realize it is false or
misleading. Our interest is in discussing the role of people who decide to
adopt an active role in stopping the propagation of an information when they
realize that it is false. For this, we formulate two simple probabilistic
models to compare misinformation spreading in the possible scenarios for which
there is a passive or an active environment of aware individuals. With aware
individuals we mean those individuals who realize that a given information is
false or misleading. In the passive environment we assume that if one of an
aware individual is exposed to the misinformation then he/she will not spread
it. In the active environment we assume that if one of an aware individual is
exposed to the misinformation then he/she will not spread it but also he/she
will stop the propagation to other individuals from the individual who
contacted him/her. We appeal to the theory of branching processes to analyse
propagation in both scenarios and we discuss the role and the impact of
effective participation in stopping misinformation. We show that the
propagation reduces drastically provided we assume an active environment, and
we obtain theoretical and computational results to measure such a reduction,
which in turns depends on the proportion of aware individuals and the number of
potential contacts of each individual which is assumed to be random.","['Luz Marina Gomez', 'Valdivino V. Junior', 'Pablo M. Rodriguez']",3,0.6703102
"The fluency and creativity of large pre-trained language models (LLMs) have
led to their widespread use, sometimes even as a replacement for traditional
search engines. Yet language models are prone to making convincing but
factually inaccurate claims, often referred to as 'hallucinations.' These
errors can inadvertently spread misinformation or harmfully perpetuate
misconceptions. Further, manual fact-checking of model responses is a
time-consuming process, making human factuality labels expensive to acquire. In
this work, we fine-tune language models to be more factual, without human
labeling and targeting more open-ended generation settings than past work. We
leverage two key recent innovations in NLP to do so. First, several recent
works have proposed methods for judging the factuality of open-ended text by
measuring consistency with an external knowledge base or simply a large model's
confidence scores. Second, the direct preference optimization algorithm enables
straightforward fine-tuning of language models on objectives other than
supervised imitation, using a preference ranking over possible model responses.
We show that learning from automatically generated factuality preference
rankings, generated either through existing retrieval systems or our novel
retrieval-free approach, significantly improves the factuality (percent of
generated claims that are correct) of Llama-2 on held-out topics compared with
RLHF or decoding strategies targeted at factuality. At 7B scale, compared to
Llama-2-chat, we observe 58% and 40% reduction in factual error rate when
generating biographies and answering medical questions, respectively.","['Katherine Tian', 'Eric Mitchell', 'Huaxiu Yao', 'Christopher D. Manning', 'Chelsea Finn']",1,0.77844167
"The COVID-19 infodemic, characterized by the rapid spread of misinformation
and unverified claims related to the pandemic, presents a significant
challenge. This paper presents a comparative analysis of the COVID-19 infodemic
in the English and Chinese languages, utilizing textual data extracted from
social media platforms. To ensure a balanced representation, two infodemic
datasets were created by augmenting previously collected social media textual
data. Through word frequency analysis, the thirty-five most frequently
occurring infodemic words are identified, shedding light on prevalent
discussions surrounding the infodemic. Moreover, topic clustering analysis
uncovers thematic structures and provides a deeper understanding of primary
topics within each language context. Additionally, sentiment analysis enables
comprehension of the emotional tone associated with COVID-19 information on
social media platforms in English and Chinese. This research contributes to a
better understanding of the COVID-19 infodemic phenomenon and can guide the
development of strategies to combat misinformation during public health crises
across different languages.","['Jia Luo', 'Daiyun Peng', 'Lei Shi', 'Didier El Baz', 'Xinran Liu']",5,0.739923
"Whilst fact verification has attracted substantial interest in the natural
language processing community, verifying misinforming statements against data
visualizations such as charts has so far been overlooked. Charts are commonly
used in the real-world to summarize and communicate key information, but they
can also be easily misused to spread misinformation and promote certain
agendas. In this paper, we introduce ChartCheck, a novel, large-scale dataset
for explainable fact-checking against real-world charts, consisting of 1.7k
charts and 10.5k human-written claims and explanations. We systematically
evaluate ChartCheck using vision-language and chart-to-table models, and
propose a baseline to the community. Finally, we study chart reasoning types
and visual attributes that pose a challenge to these models","['Mubashara Akhtar', 'Nikesh Subedi', 'Vivek Gupta', 'Sahar Tahmasebi', 'Oana Cocarascu', 'Elena Simperl']",1,0.570222
"Text-based misinformation permeates online discourses, yet evidence of
people's ability to discern truth from such deceptive textual content is
scarce. We analyze a novel TV game show data where conversations in a
high-stake environment between individuals with conflicting objectives result
in lies. We investigate the manifestation of potentially verifiable language
cues of deception in the presence of objective truth, a distinguishing feature
absent in previous text-based deception datasets. We show that there exists a
class of detectors (algorithms) that have similar truth detection performance
compared to human subjects, even when the former accesses only the language
cues while the latter engages in conversations with complete access to all
potential sources of cues (language and audio-visual). Our model, built on a
large language model, employs a bottleneck framework to learn discernible cues
to determine truth, an act of reasoning in which human subjects often perform
poorly, even with incentives. Our model detects novel but accurate language
cues in many cases where humans failed to detect deception, opening up the
possibility of humans collaborating with algorithms and ameliorating their
ability to detect the truth.","['Sanchaita Hazra', 'Bodhisattwa Prasad Majumder']",8,0.7022449
"We propose a novel slow-fast SIRS compartmental model with demography, by
coupling a slow disease spreading model and a fast information and
misinformation spreading model. Beside the classes of susceptible, infected and
recovered individuals of a common SIRS model, here we define three new classes
related to the information spreading model, e.g. unaware individuals,
misinformed individuals and individuals who are skeptical to disease-related
misinformation. Under our assumptions, the system evolves on two time scales.
We completely characterize its asymptotic behaviour with techniques of
Geometric Singular Perturbation Theory (GSPT). We exploit the time scale
separation to analyse two lower dimensional subsystem separately. First, we
focus on the analysis of the fast dynamics and we find three equilibrium point
which are feasible and stable under specific conditions. We perform a
theoretical bifurcation analysis of the fast system to understand the relations
between these three equilibria when varying specific parameters of the fast
system. Secondly, we focus on the evolution of the slow variables and we
identify three branches of the critical manifold, which are described by the
three equilibria of the fast system. We fully characterize the slow dynamics on
each branch. Moreover, we show how the inclusion of (mis)information spread may
negatively or positively affect the evolution of the epidemic, depending on
whether the slow dynamics evolves on the second branch of the critical
manifold, related to the skeptical-free equilibrium or on the third one,
related to misinformed-free equilibrium, respectively. We conclude with
numerical simulations which showcase our analytical results.","['Iulia Martina Bulai', 'Mattia Sensi', 'Sara Sottile']",2,0.67118126
"Misinformation proliferation on social media platforms is a pervasive threat
to the integrity of online public discourse. Genuine users, susceptible to
others' influence, often unknowingly engage with, endorse, and re-share
questionable pieces of information, collectively amplifying the spread of
misinformation. In this study, we introduce an empirical framework to
investigate users' susceptibility to influence when exposed to unreliable and
reliable information sources. Leveraging two datasets on political and public
health discussions on Twitter, we analyze the impact of exposure on the
adoption of information sources, examining how the reliability of the source
modulates this relationship. Our findings provide evidence that increased
exposure augments the likelihood of adoption. Users tend to adopt
low-credibility sources with fewer exposures than high-credibility sources, a
trend that persists even among non-partisan users. Furthermore, the number of
exposures needed for adoption varies based on the source credibility, with
extreme ends of the spectrum (very high or low credibility) requiring fewer
exposures for adoption. Additionally, we reveal that the adoption of
information sources often mirrors users' prior exposure to sources with
comparable credibility levels. Our research offers critical insights for
mitigating the endorsement of misinformation by vulnerable users, offering a
framework to study the dynamics of content exposure and adoption on social
media platforms.","['Jinyi Ye', 'Luca Luceri', 'Julie Jiang', 'Emilio Ferrara']",3,0.72133577
"The Internet and social media have transformed news availability and
accessibility, reshaping information consumption and production. However, they
can also facilitate the rapid spread of misinformation, posing significant
societal challenges. To combat misinformation effectively, it is crucial to
understand the online information environment and news consumption patterns.
Most existing research has primarily focused on single topics or individual
countries, lacking cross-country comparisons. This study investigated
information consumption in four European countries, analyzing three years of
Twitter activity from news outlet accounts in France, Germany, Italy, and the
UK and focusing on the role of misinformation sources. Our work offers a
perspective on how topics of European significance are interpreted across
various countries. Results indicate that reliable sources dominate the
information landscape, although unreliable content is still present across all
countries and topics. While most users engage with reliable sources, a small
percentage consume questionable content. Interestingly, few users have a mixed
information diet, bridging the gap between questionable and reliable news in
the similarity network. Cross-country comparisons revealed differences in
audience overlap of news sources, offering valuable guidance for policymakers
and scholars in developing effective and tailored solutions to combat
misinformation.","['Anees Baqir', 'Alessandro Galeazzi', 'Fabiana Zollo']",0,0.74418277
"Misinformation such as fake news and rumors is a serious threat on
information ecosystems and public trust. The emergence of Large Language Models
(LLMs) has great potential to reshape the landscape of combating
misinformation. Generally, LLMs can be a double-edged sword in the fight. On
the one hand, LLMs bring promising opportunities for combating misinformation
due to their profound world knowledge and strong reasoning abilities. Thus, one
emergent question is: how to utilize LLMs to combat misinformation? On the
other hand, the critical challenge is that LLMs can be easily leveraged to
generate deceptive misinformation at scale. Then, another important question
is: how to combat LLM-generated misinformation? In this paper, we first
systematically review the history of combating misinformation before the advent
of LLMs. Then we illustrate the current efforts and present an outlook for
these two fundamental questions respectively. The goal of this survey paper is
to facilitate the progress of utilizing LLMs for fighting misinformation and
call for interdisciplinary efforts from different stakeholders for combating
LLM-generated misinformation.","['Canyu Chen', 'Kai Shu']",6,0.7456986
"In the contemporary digital age, the proliferation of deepfakes presents a
formidable challenge to the sanctity of information dissemination. Audio
deepfakes, in particular, can be deceptively realistic, posing significant
risks in misinformation campaigns. To address this threat, we introduce the
Multi-Feature Audio Authenticity Network (MFAAN), an advanced architecture
tailored for the detection of fabricated audio content. MFAAN incorporates
multiple parallel paths designed to harness the strengths of different audio
representations, including Mel-frequency cepstral coefficients (MFCC),
linear-frequency cepstral coefficients (LFCC), and Chroma Short Time Fourier
Transform (Chroma-STFT). By synergistically fusing these features, MFAAN
achieves a nuanced understanding of audio content, facilitating robust
differentiation between genuine and manipulated recordings. Preliminary
evaluations of MFAAN on two benchmark datasets, 'In-the-Wild' Audio Deepfake
Data and The Fake-or-Real Dataset, demonstrate its superior performance,
achieving accuracies of 98.93% and 94.47% respectively. Such results not only
underscore the efficacy of MFAAN but also highlight its potential as a pivotal
tool in the ongoing battle against deepfake audio content.","['Karthik Sivarama Krishnan', 'Koushik Sivarama Krishnan']",11,0.64747655
"The spread of disinformation and propagandistic content poses a threat to
societal harmony, undermining informed decision-making and trust in reliable
sources. Online platforms often serve as breeding grounds for such content, and
malicious actors exploit the vulnerabilities of audiences to shape public
opinion. Although there have been research efforts aimed at the automatic
identification of disinformation and propaganda in social media content, there
remain challenges in terms of performance. The ArAIEval shared task aims to
further research on these particular issues within the context of the Arabic
language. In this paper, we discuss our participation in these shared tasks. We
competed in subtasks 1A and 2A, where our submitted system secured positions
9th and 10th, respectively. Our experiments consist of fine-tuning transformer
models and using zero- and few-shot learning with GPT-4.","['Yunze Xiao', 'Firoj Alam']",0,0.6824048
"We present an overview of the ArAIEval shared task, organized as part of the
first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two
tasks over Arabic text: (i) persuasion technique detection, focusing on
identifying persuasion techniques in tweets and news articles, and (ii)
disinformation detection in binary and multiclass setups over tweets. A total
of 20 teams participated in the final evaluation phase, with 14 and 16 teams
participating in Tasks 1 and 2, respectively. Across both tasks, we observed
that fine-tuning transformer models such as AraBERT was at the core of the
majority of the participating systems. We provide a description of the task
setup, including a description of the dataset construction and the evaluation
setup. We further give a brief overview of the participating systems. All
datasets and evaluation scripts from the shared task are released to the
research community. (https://araieval.gitlab.io/) We hope this will enable
further research on these important tasks in Arabic.","['Maram Hasanain', 'Firoj Alam', 'Hamdy Mubarak', 'Samir Abdaljalil', 'Wajdi Zaghouani', 'Preslav Nakov', 'Giovanni Da San Martino', 'Abed Alhakim Freihat']",8,0.5471727
"Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing complex cognitive tasks. However, their complexity and lack of
transparency have raised several trustworthiness concerns, including the
propagation of misinformation and toxicity. Recent research has explored the
self-correction capabilities of LLMs to enhance their performance. In this
work, we investigate whether these self-correction capabilities can be
harnessed to improve the trustworthiness of LLMs. We conduct experiments
focusing on two key aspects of trustworthiness: truthfulness and toxicity. Our
findings reveal that self-correction can lead to improvements in toxicity and
truthfulness, but the extent of these improvements varies depending on the
specific aspect of trustworthiness and the nature of the task. Interestingly,
our study also uncovers instances of ""self-doubt"" in LLMs during the
self-correction process, introducing a new set of challenges that need to be
addressed.",['Satyapriya Krishna'],6,0.660528
"Large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, enabling them to answer a wide range
of questions across various domains. However, these models are not flawless and
often produce responses that contain errors or misinformation. These
inaccuracies, commonly referred to as hallucinations, render LLMs unreliable
and even unusable in many scenarios. In this paper, our focus is on mitigating
the issue of hallucination in LLMs, particularly in the context of
question-answering. Instead of attempting to answer all questions, we explore a
refusal mechanism that instructs LLMs to refuse to answer challenging questions
in order to avoid errors. We then propose a simple yet effective solution
called Learn to Refuse (L2R), which incorporates the refusal mechanism to
enable LLMs to recognize and refuse to answer questions that they find
difficult to address. To achieve this, we utilize a structured knowledge base
to represent all the LLM's understanding of the world, enabling it to provide
traceable gold knowledge. This knowledge base is separate from the LLM and
initially empty. It can be filled with validated knowledge and progressively
expanded. When an LLM encounters questions outside its domain, the system
recognizes its knowledge scope and determines whether it can answer the
question independently. Additionally, we introduce a method for automatically
and efficiently expanding the knowledge base of LLMs. Through qualitative and
quantitative analysis, we demonstrate that our approach enhances the
controllability and reliability of LLMs.",['Lang Cao'],6,0.80096763
"With the advent of social media, an increasing number of netizens are sharing
and reading posts and news online. However, the huge volumes of misinformation
(e.g., fake news and rumors) that flood the internet can adversely affect
people's lives, and have resulted in the emergence of rumor and fake news
detection as a hot research topic. The emotions and sentiments of netizens, as
expressed in social media posts and news, constitute important factors that can
help to distinguish fake news from genuine news and to understand the spread of
rumors. This article comprehensively reviews emotion-based methods for
misinformation detection. We begin by explaining the strong links between
emotions and misinformation. We subsequently provide a detailed analysis of a
range of misinformation detection methods that employ a variety of emotion,
sentiment and stance-based features, and describe their strengths and
weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based
misinformation detection based on large language models and suggest future
research directions, including data collection (multi-platform, multilingual),
annotation, benchmark, multimodality, and interpretability.","['Zhiwei Liu', 'Tianlin Zhang', 'Kailai Yang', 'Paul Thompson', 'Zeping Yu', 'Sophia Ananiadou']",4,0.78991085
"The proliferation of harmful content and misinformation on social networks
necessitates content moderation policies to maintain platform health. One such
policy is shadow banning, which limits content visibility. The danger of shadow
banning is that it can be misused by social media platforms to manipulate
opinions. Here we present an optimization based approach to shadow banning that
can shape opinions into a desired distribution and scale to large networks.
Simulations on real network topologies show that our shadow banning policies
can shift opinions and increase or decrease opinion polarization. We find that
if one shadow bans with the aim of shifting opinions in a certain direction,
the resulting shadow banning policy can appear neutral. This shows the
potential for social media platforms to misuse shadow banning without being
detected. Our results demonstrate the power and danger of shadow banning for
opinion manipulation in social networks.","['Yen-Shao Chen', 'Tauhid Zaman']",3,0.5011289
"Large language models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, their suitability for domain-specific tasks, is limited
due to their immense scale at deployment, susceptibility to misinformation, and
more importantly, high data annotation costs. We propose a novel Interactive
Multi-Fidelity Learning (IMFL) framework for the cost-effective development of
small domain-specific LMs under limited annotation budgets. Our approach
formulates the domain-specific fine-tuning process as a multi-fidelity learning
problem, focusing on identifying the optimal acquisition strategy that balances
between low-fidelity automatic LLM annotations and high-fidelity human
annotations to maximize model performance. We further propose an
exploration-exploitation query strategy that enhances annotation diversity and
informativeness, incorporating two innovative designs: 1) prompt retrieval that
selects in-context examples from human-annotated samples to improve LLM
annotation, and 2) variable batch size that controls the order for choosing
each fidelity to facilitate knowledge distillation, ultimately enhancing
annotation quality. Extensive experiments on financial and medical tasks
demonstrate that IMFL achieves superior performance compared with single
fidelity annotations. Given a limited budget of human annotation, IMFL
significantly outperforms the human annotation baselines in all four tasks and
achieves very close performance as human annotations on two of the tasks. These
promising results suggest that the high human annotation costs in
domain-specific tasks can be significantly reduced by employing IMFL, which
utilizes fewer human annotations, supplemented with cheaper and faster LLM
(e.g., GPT-3.5) annotations to achieve comparable performance.","['Jiaxin Zhang', 'Zhuohang Li', 'Kamalika Das', 'Sricharan Kumar']",6,0.69313115
"A significant increase in content creation and information exchange has been
made possible by the quick development of online social media platforms, which
has been very advantageous. However, these platforms have also become a haven
for those who disseminate false information, propaganda, and fake news. Claims
are essential in forming our perceptions of the world, but sadly, they are
frequently used to trick people by those who spread false information. To
address this problem, social media giants employ content moderators to filter
out fake news from the actual world. However, the sheer volume of information
makes it difficult to identify fake news effectively. Therefore, it has become
crucial to automatically identify social media posts that make such claims,
check their veracity, and differentiate between credible and false claims. In
response, we presented CLAIMSCAN in the 2023 Forum for Information Retrieval
Evaluation (FIRE'2023). The primary objectives centered on two crucial tasks:
Task A, determining whether a social media post constitutes a claim, and Task
B, precisely identifying the words or phrases within the post that form the
claim. Task A received 40 registrations, demonstrating a strong interest and
engagement in this timely challenge. Meanwhile, Task B attracted participation
from 28 teams, highlighting its significance in the digital era of
misinformation.","['Megha Sundriyal', 'Md Shad Akhtar', 'Tanmoy Chakraborty']",3,0.7074081
"The openness and influence of video-sharing platforms (VSPs) such as YouTube
and TikTok attracted creators to share videos on various social issues.
Although social issue videos (SIVs) affect public opinions and breed
misinformation, how VSP users obtain information and interact with SIVs is
under-explored. This work surveyed 659 YouTube and 127 TikTok users to
understand the motives for consuming SIVs on VSPs. We found that VSP users are
primarily motivated by the information and entertainment gratifications to use
the platform. VSP users use SIVs for information-seeking purposes and find
YouTube and TikTok convenient to interact with SIVs. VSP users moderately watch
SIVs for entertainment and inactively engage in social interactions. SIV
consumption is associated with information and socialization gratifications of
the platform. VSP users appreciate the diversity of information and opinions
but would also do their own research and are concerned about the misinformation
and echo chamber problems.","['Shuo Niu', 'Dilasha Shrestha', 'Abhisan Ghimire', 'Zhicong Lu']",0,0.5326586
"Misinformation has emerged as a major societal threat in recent years in
general; specifically in the context of the COVID-19 pandemic, it has wrecked
havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable
solutions for combating misinformation are the need of the hour. This work
explored how existing information obtained from social media and augmented with
more curated fact checked data repositories can be harnessed to facilitate
automated rebuttal of misinformation at scale. While the ideas herein can be
generalized and reapplied in the broader context of misinformation mitigation
using a multitude of information sources and catering to the spectrum of social
media platforms, this work serves as a proof of concept, and as such, it is
confined in its scope to only rebuttal of tweets, and in the specific context
of misinformation regarding COVID-19. It leverages two publicly available
datasets, viz. FaCov (fact-checked articles) and misleading (social media
Twitter) data on COVID-19 Vaccination.","['Shakshi Sharma', 'Anwitaman Datta', 'Rajesh Sharma']",5,0.81336653
"Conspiracy theories, as a type of misinformation, are narratives that
explains an event or situation in an irrational or malicious manner. While most
previous work examined conspiracy theory in social media short texts, limited
attention was put on such misinformation in long news documents. In this paper,
we aim to identify whether a news article contains conspiracy theories. We
observe that a conspiracy story can be made up by mixing uncorrelated events
together, or by presenting an unusual distribution of relations between events.
Achieving a contextualized understanding of events in a story is essential for
detecting conspiracy theories. Thus, we propose to incorporate an event
relation graph for each article, in which events are nodes, and four common
types of event relations, coreference, temporal, causal, and subevent
relations, are considered as edges. Then, we integrate the event relation graph
into conspiracy theory identification in two ways: an event-aware language
model is developed to augment the basic language model with the knowledge of
events and event relations via soft labels; further, a heterogeneous graph
attention network is designed to derive a graph embedding based on hard labels.
Experiments on a large benchmark dataset show that our approach based on event
relation graph improves both precision and recall of conspiracy theory
identification, and generalizes well for new unseen media sources.","['Yuanyuan Lei', 'Ruihong Huang']",3,0.6091387
"The amount of news being consumed online has substantially expanded in recent
years. Fake news has become increasingly common, especially in regional
languages like Malayalam, due to the rapid publication and lack of editorial
standards on some online sites. Fake news may have a terrible effect on
society, causing people to make bad judgments, lose faith in authorities, and
even engage in violent behavior. When we take into the context of India, there
are many regional languages, and fake news is spreading in every language.
Therefore, providing efficient techniques for identifying false information in
regional tongues is crucial. Until now, little to no work has been done in
Malayalam, extracting features from multiple modalities to classify fake news.
Multimodal approaches are more accurate in detecting fake news, as features
from multiple modalities are extracted to build the deep learning
classification model. As far as we know, this is the first piece of work in
Malayalam that uses multimodal deep learning to tackle false information.
Models trained with more than one modality typically outperform models taught
with only one modality. Our study in the Malayalam language utilizing
multimodal deep learning is a significant step toward more effective
misinformation detection and mitigation.","['Adhish S. Sujan', 'Ajitha. V', 'Aleena Benny', 'Amiya M. P.', 'V. S. Anoop']",4,0.7105775
"Misinformation and disinformation are growing threats in the digital age,
spreading rapidly across languages and borders. This paper investigates the
prevalence and dynamics of multilingual misinformation through an analysis of
over 250,000 unique fact-checks spanning 95 languages. First, we find that
while the majority of misinformation claims are only fact-checked once, 11.7%,
corresponding to more than 21,000 claims, are checked multiple times. Using
fact-checks as a proxy for the spread of misinformation, we find 33% of
repeated claims cross linguistic boundaries, suggesting that some
misinformation permeates language barriers. However, spreading patterns exhibit
strong homophily, with misinformation more likely to spread within the same
language. To study the evolution of claims over time and mutations across
languages, we represent fact-checks with multilingual sentence embeddings and
cluster semantically similar claims. We analyze the connected components and
shortest paths connecting different versions of a claim finding that claims
gradually drift over time and undergo greater alteration when traversing
languages. Overall, this novel investigation of multilingual misinformation
provides key insights. It quantifies redundant fact-checking efforts,
establishes that some claims diffuse across languages, measures linguistic
homophily, and models the temporal and cross-lingual evolution of claims. The
findings advocate for expanded information sharing between fact-checkers
globally while underscoring the importance of localized verification.","['Dorian Quelle', 'Calvin Cheng', 'Alexandre Bovet', 'Scott A. Hale']",8,0.78250796
"Vast amounts of (open) data are increasingly used to make arguments about
crisis topics such as climate change and global pandemics. Data visualizations
are central to bringing these viewpoints to broader publics. However,
visualizations often conceal the many contexts involved in their production,
ranging from decisions made in research labs about collecting and sharing data
to choices made in editorial rooms about which data stories to tell. In this
paper, we examine how data visualizations about climate change and COVID-19 are
produced in popular science magazines, using Scientific American, an
established English-language popular science magazine, as a case study. To do
this, we apply the analytical concept of data journeys (Leonelli, 2020) in a
mixed methods study that centers on interviews with Scientific American staff
and is supplemented by a visualization analysis of selected charts. In
particular, we discuss the affordances of working with open data, the role of
collaborative data practices, and how the magazine works to counter
misinformation and increase transparency. This work provides an empirical
contribution by providing insight into the data (visualization) practices of
science communicators and demonstrating how the concept of data journeys can be
used as an analytical framework.","['Kathleen Gregory', 'Laura Koesten', 'Regina Schuster', 'Torsten M√∂ller', 'Sarah Davies']",0,0.6740955
"With advancements in natural language processing (NLP) models, automatic
explanation generation has been proposed to mitigate misinformation on social
media platforms in addition to adding warning labels to identified fake news.
While many researchers have focused on generating good explanations, how these
explanations can really help humans combat fake news is under-explored. In this
study, we compare the effectiveness of a warning label and the state-of-the-art
counterfactual explanations generated by GPT-4 in debunking misinformation. In
a two-wave, online human-subject study, participants (N = 215) were randomly
assigned to a control group in which false contents are shown without any
intervention, a warning tag group in which the false claims were labeled, or an
explanation group in which the false contents were accompanied by GPT-4
generated explanations. Our results show that both interventions significantly
decrease participants' self-reported belief in fake claims in an equivalent
manner for the short-term and long-term. We discuss the implications of our
findings and directions for future NLP-based misinformation debunking
strategies.","['Yi-Li Hsu', 'Shih-Chieh Dai', 'Aiping Xiong', 'Lun-Wei Ku']",4,0.61919796
"Deep generative models can create remarkably photorealistic fake images while
raising concerns about misinformation and copyright infringement, known as
deepfake threats. Deepfake detection technique is developed to distinguish
between real and fake images, where the existing methods typically learn
classifiers in the image domain or various feature domains. However, the
generalizability of deepfake detection against emerging and more advanced
generative models remains challenging. In this paper, being inspired by the
zero-shot advantages of Vision-Language Models (VLMs), we propose a novel
approach called AntifakePrompt, using VLMs (e.g., InstructBLIP) and prompt
tuning techniques to improve the deepfake detection accuracy over unseen data.
We formulate deepfake detection as a visual question answering problem, and
tune soft prompts for InstructBLIP to answer the real/fake information of a
query image. We conduct full-spectrum experiments on datasets from a diversity
of 3 held-in and 20 held-out generative models, covering modern text-to-image
generation, image editing and adversarial image attacks. These testing datasets
provide useful benchmarks in the realm of deepfake detection for further
research. Moreover, results demonstrate that (1) the deepfake detection
accuracy can be significantly and consistently improved (from 71.06% to 92.11%,
in average accuracy over unseen domains) using pretrained vision-language
models with prompt tuning; (2) our superior performance is at less cost of
training data and trainable parameters, resulting in an effective and efficient
solution for deepfake detection. Code and models can be found at
https://github.com/nctu-eva-lab/AntifakePrompt.","['You-Ming Chang', 'Chen Yeh', 'Wei-Chen Chiu', 'Ning Yu']",7,0.84673226
"The large language based-model chatbot ChatGPT gained a lot of popularity
since its launch and has been used in a wide range of situations. This research
centers around a particular situation, when the ChatGPT is used to produce news
that will be consumed by the population, causing the facilitation in the
production of fake news, spread of misinformation and lack of trust in news
sources. Aware of these problems, this research aims to build an artificial
intelligence model capable of performing authorship attribution on news
articles, identifying the ones written by the ChatGPT. To achieve this goal, a
dataset containing equal amounts of human and ChatGPT written news was
assembled and different natural processing language techniques were used to
extract features from it that were used to train, validate and test three
models built with different techniques. The best performance was produced by
the Bidirectional Long Short Term Memory (LSTM) Neural Network model, achiving
91.57\% accuracy when tested against the data from the testing set.","['Amanda Ferrari Iaquinta', 'Gustavo Voltani von Atzingen']",8,0.7239989
"In recent years, we witness the explosion of false and unconfirmed
information (i.e., rumors) that went viral on social media and shocked the
public. Rumors can trigger versatile, mostly controversial stance expressions
among social media users. Rumor verification and stance detection are different
yet relevant tasks. Fake news debunking primarily focuses on determining the
truthfulness of news articles, which oversimplifies the issue as fake news
often combines elements of both truth and falsehood. Thus, it becomes crucial
to identify specific instances of misinformation within the articles. In this
research, we investigate a novel task in the field of fake news debunking,
which involves detecting sentence-level misinformation. One of the major
challenges in this task is the absence of a training dataset with
sentence-level annotations regarding veracity. Inspired by the Multiple
Instance Learning (MIL) approach, we propose a model called Weakly Supervised
Detection of Misinforming Sentences (WSDMS). This model only requires bag-level
labels for training but is capable of inferring both sentence-level
misinformation and article-level veracity, aided by relevant social media
conversations that are attentively contextualized with news sentences. We
evaluate WSDMS on three real-world benchmarks and demonstrate that it
outperforms existing state-of-the-art baselines in debunking fake news at both
the sentence and article levels.","['Ruichao Yang', 'Wei Gao', 'Jing Ma', 'Hongzhan Lin', 'Zhiwei Yang']",4,0.83563375
"The malicious applications of deep forgery, represented by face swapping,
have introduced security threats such as misinformation dissemination and
identity fraud. While some research has proposed the use of robust watermarking
methods to trace the copyright of facial images for post-event traceability,
these methods cannot effectively prevent the generation of forgeries at the
source and curb their dissemination. To address this problem, we propose a
novel comprehensive active defense mechanism that combines traceability and
adversariality, called Dual Defense. Dual Defense invisibly embeds a single
robust watermark within the target face to actively respond to sudden cases of
malicious face swapping. It disrupts the output of the face swapping model
while maintaining the integrity of watermark information throughout the entire
dissemination process. This allows for watermark extraction at any stage of
image tracking for traceability. Specifically, we introduce a watermark
embedding network based on original-domain feature impersonation attack. This
network learns robust adversarial features of target facial images and embeds
watermarks, seeking a well-balanced trade-off between watermark invisibility,
adversariality, and traceability through perceptual adversarial encoding
strategies. Extensive experiments demonstrate that Dual Defense achieves
optimal overall defense success rates and exhibits promising universality in
anti-face swapping tasks and dataset generalization ability. It maintains
impressive adversariality and traceability in both original and robust
settings, surpassing current forgery defense methods that possess only one of
these capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD
methods.","['Yunming Zhang', 'Dengpan Ye', 'Caiyun Xie', 'Long Tang', 'Chuanxi Chen', 'Ziyi Liu', 'Jiacheng Deng']",7,0.7187115
"Visualizations are common methods to convey information but also increasingly
used to spread misinformation. It is therefore important to understand the
factors people use to interpret visualizations. In this paper, we focus on
factors that influence interpretations of scatter plots, investigating the
extent to which common visual aspects of scatter plots (outliers and trend
lines) and cognitive biases (people's beliefs) influence perception of
correlation trends. We highlight three main findings: outliers skew trend
perception but exert less influence than other points; trend lines make trends
seem stronger but also mitigate the influence of some outliers; and people's
beliefs have a small influence on perceptions of weak, but not strong
correlations. From these results we derive guidelines for adjusting visual
elements to mitigate the influence of factors that distort interpretations of
scatter plots. We explore how these guidelines may generalize to other
visualization types and make recommendations for future studies.","['Alexandre Filipowicz', 'Scott Carter', 'Nayeli Bravo', 'Rumen Iliev', 'Shabnam Hakimi', 'David Ayman Shamma', 'Kent Lyons', 'Candice Hogan', 'Charlene Wu']",0,0.5803677
"Large Language Models (LLMs) have revolutionized the domain of natural
language processing (NLP) with remarkable capabilities of generating human-like
text responses. However, despite these advancements, several works in the
existing literature have raised serious concerns about the potential misuse of
LLMs such as spreading misinformation, generating fake news, plagiarism in
academia, and contaminating the web. To address these concerns, a consensus
among the research community is to develop algorithmic solutions to detect
AI-generated text. The basic idea is that whenever we can tell if the given
text is either written by a human or an AI, we can utilize this information to
address the above-mentioned concerns. To that end, a plethora of detection
frameworks have been proposed, highlighting the possibilities of AI-generated
text detection. But in parallel to the development of detection frameworks,
researchers have also concentrated on designing strategies to elude detection,
i.e., focusing on the impossibilities of AI-generated text detection. This is a
crucial step in order to make sure the detection frameworks are robust enough
and it is not too easy to fool a detector. Despite the huge interest and the
flurry of research in this domain, the community currently lacks a
comprehensive analysis of recent developments. In this survey, we aim to
provide a concise categorization and overview of current work encompassing both
the prospects and the limitations of AI-generated text detection. To enrich the
collective knowledge, we engage in an exhaustive discussion on critical and
challenging open questions related to ongoing research on AI-generated text
detection.","['Soumya Suvra Ghosal', 'Souradip Chakraborty', 'Jonas Geiping', 'Furong Huang', 'Dinesh Manocha', 'Amrit Singh Bedi']",9,0.70877296
"With large language models (LLMs) poised to become embedded in our daily
lives, questions are starting to be raised about the data they learned from.
These questions range from potential bias or misinformation LLMs could retain
from their training data to questions of copyright and fair use of
human-generated text. However, while these questions emerge, developers of the
recent state-of-the-art LLMs become increasingly reluctant to disclose details
on their training corpus. We here introduce the task of document-level
membership inference for real-world LLMs, i.e. inferring whether the LLM has
seen a given document during training or not. First, we propose a procedure for
the development and evaluation of document-level membership inference for LLMs
by leveraging commonly used data sources for training and the model release
date. We then propose a practical, black-box method to predict document-level
membership and instantiate it on OpenLLaMA-7B with both books and academic
papers. We show our methodology to perform very well, reaching an AUC of 0.856
for books and 0.678 for papers. We then show our approach to outperform the
sentence-level membership inference attacks used in the privacy literature for
the document-level membership task. We further evaluate whether smaller models
might be less sensitive to document-level inference and show OpenLLaMA-3B to be
approximately as sensitive as OpenLLaMA-7B to our approach. Finally, we
consider two mitigation strategies and find the AUC to slowly decrease when
only partial documents are considered but to remain fairly high when the model
precision is reduced. Taken together, our results show that accurate
document-level membership can be inferred for LLMs, increasing the transparency
of technology poised to change our lives.","['Matthieu Meeus', 'Shubham Jain', 'Marek Rei', 'Yves-Alexandre de Montjoye']",6,0.7794937
"Modern social media platforms play an important role in facilitating rapid
dissemination of information through their massive user networks. Fake news,
misinformation, and unverifiable facts on social media platforms propagate
disharmony and affect society. In this paper, we consider the problem of online
auditing of information flow/propagation with the goal of classifying news
items as fake or genuine. Specifically, driven by experiential studies on
real-world social media platforms, we propose a probabilistic Markovian
information spread model over networks modeled by graphs. We then formulate our
inference task as a certain sequential detection problem with the goal of
minimizing the combination of the error probability and the time it takes to
achieve correct decision. For this model, we find the optimal detection
algorithm minimizing the aforementioned risk and prove several statistical
guarantees. We then test our algorithm over real-world datasets. To that end,
we first construct an offline algorithm for learning the probabilistic
information spreading model, and then apply our optimal detection algorithm.
Experimental study show that our algorithm outperforms state-of-the-art
misinformation detection algorithms in terms of accuracy and detection time.","['Mor Oren-Loberman', 'Vered Azar', 'Wasim Huleihel']",2,0.76364505
"Automated fact-checking, using machine learning to verify claims, has grown
vital as misinformation spreads beyond human fact-checking capacity. Large
Language Models (LLMs) like GPT-4 are increasingly trusted to write academic
papers, lawsuits, and news articles and to verify information, emphasizing
their role in discerning truth from falsehood and the importance of being able
to verify their outputs. Understanding the capacities and limitations of LLMs
in fact-checking tasks is therefore essential for ensuring the health of our
information ecosystem. Here, we evaluate the use of LLM agents in fact-checking
by having them phrase queries, retrieve contextual data, and make decisions.
Importantly, in our framework, agents explain their reasoning and cite the
relevant sources from the retrieved context. Our results show the enhanced
prowess of LLMs when equipped with contextual information. GPT-4 outperforms
GPT-3, but accuracy varies based on query language and claim veracity. While
LLMs show promise in fact-checking, caution is essential due to inconsistent
accuracy. Our investigation calls for further research, fostering a deeper
comprehension of when agents succeed and when they fail.","['Dorian Quelle', 'Alexandre Bovet']",6,0.7493826
"Bayesian Persuasion is proposed as a tool for social media platforms to
combat the spread of misinformation. Since platforms can use machine learning
to predict the popularity and misinformation features of to-be-shared posts,
and users are largely motivated to share popular content, platforms can
strategically signal this informational advantage to change user beliefs and
persuade them not to share misinformation. We characterize the optimal
signaling scheme with imperfect predictions as a linear program and give
sufficient and necessary conditions on the classifier to ensure optimal
platform utility is non-decreasing and continuous. Next, this interaction is
considered under a performative model, wherein platform intervention affects
the user's future behaviour. The convergence and stability of optimal signaling
under this performative process are fully characterized. Lastly, we
experimentally validate that our approach significantly reduces misinformation
in both the single round and performative setting and discuss the broader scope
of using information design to combat misinformation.","['Safwan Hossain', 'Andjela Mladenovic', 'Yiling Chen', 'Gauthier Gidel']",0,0.6553383
"Artificial intelligence (AI) can undermine financial stability because of
malicious use, misinformation, misalignment, and the AI analytics market
structure. The low frequency and uniqueness of financial crises, coupled with
mutable and unclear objectives, frustrate machine learning. Even if the
authorities prefer a conservative approach to AI adoption, it will likely
become widely used by stealth, taking over increasingly high-level functions
driven by significant cost efficiencies and superior performance. We propose
six criteria for judging the suitability of AI.","['Jon Danielsson', 'Andreas Uthemann']",9,0.6852727
"In today's digital era, the rapid spread of misinformation poses threats to
public well-being and societal trust. As online misinformation proliferates,
manual verification by fact checkers becomes increasingly challenging. We
introduce FACT-GPT (Fact-checking Augmentation with Claim matching
Task-oriented Generative Pre-trained Transformer), a framework designed to
automate the claim matching phase of fact-checking using Large Language Models
(LLMs). This framework identifies new social media content that either supports
or contradicts claims previously debunked by fact-checkers. Our approach
employs GPT-4 to generate a labeled dataset consisting of simulated social
media posts. This data set serves as a training ground for fine-tuning more
specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media
content related to public health. The results indicate that our fine-tuned LLMs
rival the performance of larger pre-trained LLMs in claim matching tasks,
aligning closely with human annotations. This study achieves three key
milestones: it provides an automated framework for enhanced fact-checking;
demonstrates the potential of LLMs to complement human expertise; offers public
resources, including datasets and models, to further research and applications
in the fact-checking domain.","['Eun Cheol Choi', 'Emilio Ferrara']",6,0.7864847
"Large Language Models (LLMs) have demonstrated remarkable human-level natural
language generation capabilities. However, their potential to generate
misinformation, often called the hallucination problem, poses a significant
risk to their deployment. A common approach to address this issue is to
retrieve relevant knowledge and fine-tune the LLM with the knowledge in its
input. Unfortunately, this method incurs high training costs and may cause
catastrophic forgetting for multi-tasking models. To overcome these
limitations, we propose a knowledge-constrained decoding method called KCTS
(Knowledge-Constrained Tree Search), which guides a frozen LM to generate text
aligned with the reference knowledge at each decoding step using a knowledge
classifier score and MCTS (Monte-Carlo Tree Search). To adapt the
sequence-level knowledge classifier to token-level guidance, we also propose a
novel token-level hallucination detection method called RIPA (Reward Inflection
Point Approximation). Our empirical results on knowledge-grounded dialogue and
abstractive summarization demonstrate the strength of KCTS as a plug-and-play,
model-agnostic decoding method that can effectively reduce hallucinations in
natural language generation.","['Sehyun Choi', 'Tianqing Fang', 'Zhaowei Wang', 'Yangqiu Song']",6,0.71672636
"Political elites play an important role in the proliferation of online
misinformation. However, an understanding of how fact-checking platforms pick
up politicized misinformation for fact-checking is still in its infancy. Here,
we conduct an empirical analysis of mentions of U.S. political elites within
fact-checked statements. For this purpose, we collect a comprehensive dataset
consisting of 35,014 true and false statements that have been fact-checked by
two major fact-checking organizations (Snopes, PolitiFact) in the U.S. between
2008 and 2023, i.e., within an observation period of 15 years. Subsequently, we
perform content analysis and explanatory regression modeling to analyze how
veracity is linked to mentions of U.S. political elites in fact-checked
statements. Our analysis yields the following main findings: (i) Fact-checked
false statements are, on average, 20% more likely to mention political elites
than true fact-checked statements. (ii) There is a partisan asymmetry such that
fact-checked false statements are 88.1% more likely to mention Democrats, but
26.5% less likely to mention Republicans, compared to fact-checked true
statements. (iii) Mentions of political elites in fact-checked false statements
reach the highest level during the months preceding elections. (iv)
Fact-checked false statements that mention political elites carry stronger
other-condemning emotions and are more likely to be pro-Republican, compared to
fact-checked true statements. In sum, our study offers new insights into
understanding mentions of political elites in false statements on U.S.
fact-checking platforms, and bridges important findings at the intersection
between misinformation and politicization.","['Yuwei Chuai', 'Jichang Zhao', 'Nicolas Pr√∂llochs', 'Gabriele Lenzini']",10,0.5363017
"The proliferation of online misinformation has emerged as one of the biggest
threats to society. Considerable efforts have focused on building
misinformation detection models, still the perils of misinformation remain
abound. Mitigating online misinformation and its ramifications requires a
holistic approach that encompasses not only an understanding of its intricate
landscape in relation to the complex issue and topic-rich information ecosystem
online, but also the psychological drivers of individuals behind it. Adopting a
time series analytic technique and robust causal inference-based design, we
conduct a large-scale observational study analyzing over 32 million COVID-19
tweets and 16 million historical timeline tweets. We focus on understanding the
behavior and psychology of users disseminating misinformation during COVID-19
and its relationship with the historical inclinations towards sharing
misinformation on Non-COVID domains before the pandemic. Our analysis
underscores the intricacies inherent to cross-domain misinformation, and
highlights that users' historical inclination toward sharing misinformation is
positively associated with their present behavior pertaining to misinformation
sharing on emergent topics and beyond. This work may serve as a valuable
foundation for designing user-centric inoculation strategies and
ecologically-grounded agile interventions for effectively tackling online
misinformation.","['Mohit Chandra', 'Anush Mattapalli', 'Munmun De Choudhury']",0,0.79092604
"AI-generated text has proliferated across various online platforms, offering
both transformative prospects and posing significant risks related to
misinformation and manipulation. Addressing these challenges, this paper
introduces SAID (Social media AI Detection), a novel benchmark developed to
assess AI-text detection models' capabilities in real social media platforms.
It incorporates real AI-generate text from popular social media platforms like
Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that
reflects the sophisticated strategies employed by real AI users on the Internet
which may evade detection or gain visibility, providing a more realistic and
challenging evaluation landscape. A notable finding of our study, based on the
Zhihu dataset, reveals that annotators can distinguish between AI-generated and
human-generated texts with an average accuracy rate of 96.5%. This finding
necessitates a re-evaluation of human capability in recognizing AI-generated
text in today's widely AI-influenced environment. Furthermore, we present a new
user-oriented AI-text detection challenge focusing on the practicality and
effectiveness of identifying AI-generated text based on user information and
multiple responses. The experimental results demonstrate that conducting
detection tasks on actual social media platforms proves to be more challenging
compared to traditional simulated AI-text detection, resulting in a decreased
accuracy. On the other hand, user-oriented AI-generated text detection
significantly improve the accuracy of detection.","['Wanyun Cui', 'Linqiu Zhang', 'Qianle Wang', 'Shuyang Cai']",9,0.7619059
"Fact-checking is a crucial task as it ensures the prevention of
misinformation. However, manual fact-checking cannot keep up with the rate at
which false information is generated and disseminated online. Automated
fact-checking by machines is significantly quicker than by humans. But for
better trust and transparency of these automated systems, explainability in the
fact-checking process is necessary. Fact-checking often entails contrasting a
factual assertion with a body of knowledge for such explanations. An effective
way of representing knowledge is the Knowledge Graph (KG). There have been
sufficient works proposed related to fact-checking with the usage of KG but not
much focus is given to the application of reinforcement learning (RL) in such
cases. To mitigate this gap, we propose an RL-based KG reasoning approach for
explainable fact-checking. Extensive experiments on FB15K-277 and NELL-995
datasets reveal that reasoning over a KG is an effective way of producing
human-readable explanations in the form of paths and classifications for fact
claims. The RL reasoning agent computes a path that either proves or disproves
a factual claim, but does not provide a verdict itself. A verdict is reached by
a voting mechanism that utilizes paths produced by the agent. These paths can
be presented to human readers so that they themselves can decide whether or not
the provided evidence is convincing or not. This work will encourage works in
this direction for incorporating RL for explainable fact-checking as it
increases trustworthiness by providing a human-in-the-loop approach.","['Gustav Nikopensius', 'Mohit Mayank', 'Orchid Chetia Phukan', 'Rajesh Sharma']",1,0.71705866
"This article presents the affordances that Generative Artificial Intelligence
can have in misinformation and disinformation contexts, major threats to our
digitalized society. We present a research framework to generate customized
agent-based social networks for disinformation simulations that would enable
understanding and evaluating the phenomena whilst discussing open challenges.","['Javier Pastor-Galindo', 'Pantaleone Nespoli', 'Jos√© A. Ruip√©rez-Valiente']",0,0.6429075
"Persuasion is the process of changing an agent's belief distribution from a
given (or estimated) prior to a desired posterior. A common assumption in the
acceptance of information or misinformation as fact is that the
(mis)information must be consistent with or familiar to the individual who
accepts it. We model the process as a control problem in which the state is
given by a (time-varying) belief distribution following a predictor-corrector
dynamic. Persuasion is modeled as the corrector control signal with the
performance index defined using the Fisher-Rao information metric, reflecting a
fundamental cost associated to altering the agent's belief distribution. To
compensate for the fact that information production arises naturally from the
predictor dynamic (i.e., expected beliefs change) we modify the Fisher-Rao
metric to account just for information generated by the control signal. The
resulting optimal control problem produces non-geodesic paths through
distribution space that are compared to the geodesic paths found using the
standard free entropy minimizing Fisher metric in several example belief
models: a Kalman Filter, a Boltzmann distribution and a joint Kalman/Boltzmann
belief system.","['Geoff Goehle', 'Christopher Griffin']",2,0.6936928
"The tremendous rise of generative AI has reached every part of society -
including the news environment. There are many concerns about the individual
and societal impact of the increasing use of generative AI, including issues
such as disinformation and misinformation, discrimination, and the promotion of
social tensions. However, research on anticipating the impact of generative AI
is still in its infancy and mostly limited to the views of technology
developers and/or researchers. In this paper, we aim to broaden the perspective
and capture the expectations of three stakeholder groups (news consumers;
technology developers; content creators) about the potential negative impacts
of generative AI, as well as mitigation strategies to address these.
Methodologically, we apply scenario writing and use participatory foresight in
the context of a survey (n=119) to delve into cognitively diverse imaginations
of the future. We qualitatively analyze the scenarios using thematic analysis
to systematically map potential impacts of generative AI on the news
environment, potential mitigation strategies, and the role of stakeholders in
causing and mitigating these impacts. In addition, we measure respondents'
opinions on a specific mitigation strategy, namely transparency obligations as
suggested in Article 52 of the draft EU AI Act. We compare the results across
different stakeholder groups and elaborate on the (non-) presence of different
expected impacts across these groups. We conclude by discussing the usefulness
of scenario-writing and participatory foresight as a toolbox for generative AI
impact assessment.","['Kimon Kieslich', 'Nicholas Diakopoulos', 'Natali Helberger']",9,0.72909963
"Shortly after the first COVID-19 cases became apparent in December 2020,
rumors spread on social media suggesting a connection between the virus and the
5G radiation emanating from the recently deployed telecommunications network.
In the course of the following weeks, this idea gained increasing popularity,
and various alleged explanations for how such a connection manifests emerged.
Ultimately, after being amplified by prominent conspiracy theorists, a series
of arson attacks on telecommunication equipment follows, concluding with the
kidnapping of telecommunication technicians in Peru. In this paper, we study
the spread of content related to a conspiracy theory with harmful consequences,
a so-called digital wildfire. In particular, we investigate the 5G and COVID-19
misinformation event on Twitter before, during, and after its peak in April and
May 2020. For this purpose, we examine the community dynamics in complex
temporal interaction networks underlying Twitter user activity. We assess the
evolution of such digital wildfires by appropriately defining the temporal
dynamics of communication in communities within social networks. We show that,
for this specific misinformation event, the number of interactions of the users
participating in a digital wildfire, as well as the size of the engaged
communities, both follow a power-law distribution. Moreover, our research
elucidates the possibility of quantifying the phases of a digital wildfire, as
per established literature. We identify one such phase as a critical
transition, marked by a shift from sporadic tweets to a global spread event,
highlighting the dramatic scaling of misinformation propagation.","['Kaspara Skovli G√•sv√¶r', 'Pedro G. Lind', 'Johannes Langguth', 'Morten Hjorth-Jensen', 'Michael Kreil', 'Daniel Thilo Schroeder']",5,0.6930225
"Claim verification plays a crucial role in combating misinformation. While
existing works on claim verification have shown promising results, a crucial
piece of the puzzle that remains unsolved is to understand how to verify claims
without relying on human-annotated data, which is expensive to create at a
large scale. Additionally, it is important for models to provide comprehensive
explanations that can justify their decisions and assist human fact-checkers.
This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)
Reasoning that can verify complex claims and generate explanations without the
need for annotated evidence using Large Language Models (LLMs). FOLK leverages
the in-context learning ability of LLMs to translate the claim into a
First-Order-Logic (FOL) clause consisting of predicates, each corresponding to
a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning
over a set of knowledge-grounded question-and-answer pairs to make veracity
predictions and generate explanations to justify its decision-making process.
This process makes our model highly explanatory, providing clear explanations
of its reasoning process in human-readable form. Our experiment results
indicate that FOLK outperforms strong baselines on three datasets encompassing
various claim verification challenges. Our code and data are available.","['Haoran Wang', 'Kai Shu']",1,0.7900939
"Reading and understanding the stories in the news is increasingly difficult.
Reporting on stories evolves rapidly, politicized news venues offer different
perspectives (and sometimes different facts), and misinformation is rampant.
However, existing solutions merely aggregate an overwhelming amount of
information from heterogenous sources, such as different news outlets, social
media, and news bias rating agencies. We present NEWSSENSE, a novel sensemaking
tool and reading interface designed to collect and integrate information from
multiple news articles on a central topic, using a form of reference-free fact
verification. NEWSSENSE augments a central, grounding article of the user's
choice by linking it to related articles from different sources, providing
inline highlights on how specific claims in the chosen article are either
supported or contradicted by information from other articles. Using NEWSSENSE,
users can seamlessly digest and cross-check multiple information sources
without disturbing their natural reading flow. Our pilot study shows that
NEWSSENSE has the potential to help users identify key information, verify the
credibility of news articles, and explore different perspectives.","['Jeremiah Milbauer', 'Ziqi Ding', 'Zhijin Wu', 'Tongshuang Wu']",4,0.7854183
"Epidemic models study the spread of an undesired agent through a population,
be it infectious diseases through a country, misinformation in online social
media, or pests infesting a region. In combating these epidemics, we rely
neither on global top-down interventions, nor solely on individual adaptations.
Instead, interventions most commonly come from local institutions such as
public health departments, moderation teams on social media platforms, or other
forms of group governance. Classic models, which are often individual or
agent-based, are ill-suited to capture local adaptations. We leverage recent
development of institutional dynamics based on cultural group selection to
study how groups can attempt local control of an epidemic by taking inspiration
from the successes and failures of other groups. Incorporating these
institutional changes into the epidemic dynamics reveals paradoxes: a higher
transmission rate can result in smaller outbreaks and decreasing the speed of
institutional adaptation generally reduces outbreak size. When groups perceive
a contagion as more worrisome, they can invest in improved policies and, if
they maintain these policies long enough to have impact, lead to a reduction in
endemicity. By looking at the interplay between the speed of institutions and
the transmission rate of the contagions, we find rich co-evolutionary dynamics
that reflect the complexity of known biological and social contagions.","['Jonathan St-Onge', 'Giulio Burgio', 'Samuel F. Rosenblatt', 'Timothy M. Waring', 'Laurent H√©bert-Dufresne']",5,0.5486423
"Although pervasive spread of misinformation on social media platforms has
become a pressing challenge, existing platform interventions have shown limited
success in curbing its dissemination. In this study, we propose a stance-aware
graph neural network (stance-aware GNN) that leverages users' stances to
proactively predict misinformation spread. As different user stances can form
unique echo chambers, we customize four information passing paths in
stance-aware GNN, while the trainable attention weights provide explainability
by highlighting each structure's importance. Evaluated on a real-world dataset,
stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs
without user stance by over 4.69%. Furthermore, the attention weights indicate
that users' opposition stances have a higher impact on their neighbors'
behaviors than supportive ones, which function as social correction to halt
misinformation propagation. Overall, our study provides an effective predictive
model for platforms to combat misinformation, and highlights the impact of user
stances in the misinformation propagation.","['Zihan Chen', 'Jingyi Sun', 'Rong Liu', 'Feng Mai']",0,0.6201765
"Online misinformation poses a global risk with significant real-world
consequences. To combat misinformation, current research relies on
professionals like journalists and fact-checkers for annotating and debunking
misinformation, and develops automated machine learning methods for detecting
misinformation. Complementary to these approaches, recent research has
increasingly concentrated on utilizing the power of ordinary social media
users, a.k.a. ""crowd"", who act as eyes-on-the-ground proactively questioning
and countering misinformation. Notably, recent studies show that 96% of
counter-misinformation responses originate from them. Acknowledging their
prominent role, we present the first systematic and comprehensive survey of
research papers that actively leverage the crowds to combat misinformation.
  We first identify 88 papers related to crowd-based efforts, following a
meticulous annotation process adhering to the PRISMA framework. We then present
key statistics related to misinformation, counter-misinformation, and crowd
input in different formats and topics. Upon holistic analysis of the papers, we
introduce a novel taxonomy of the roles played by the crowds: (i)annotators who
actively identify misinformation; (ii)evaluators who assess
counter-misinformation effectiveness; (iii)creators who create
counter-misinformation. This taxonomy explores the crowd's capabilities in
misinformation detection, identifies prerequisites for effective
counter-misinformation, and analyzes crowd-generated counter-misinformation.
Then, we delve into (i)distinguishing individual, collaborative, and
machine-assisted labeling for annotators; (ii)analyzing the effectiveness of
counter-misinformation through surveys, interviews, and in-lab experiments for
evaluators; and (iii)characterizing creation patterns and creator profiles for
creators. Finally, we outline potential future research in this field.","['Bing He', 'Yibo Hu', 'Yeon-Chang Lee', 'Soyoung Oh', 'Gaurav Verma', 'Srijan Kumar']",0,0.8365736
"This paper investigates the use of computational tools and Open-Source
Intelligence (OSINT) techniques for verifying online multimedia content, with a
specific focus on real-world cases from the Russia-Ukraine conflict. Over a
nine-month period from April to December 2022, we examine verification
workflows, tools, and case studies published by \faktiskbar. Our study
showcases the effectiveness of diverse resources, including AI tools,
geolocation tools, internet archives, and social media monitoring platforms, in
enabling journalists and fact-checkers to efficiently process and corroborate
evidence, ensuring the dissemination of accurate information. This research
underscores the vital role of computational tools and OSINT techniques in
promoting evidence-based reporting and combatting misinformation. We also touch
on the current limitations of available tools and prospects for future
developments in multimedia verification.","['Sohail Ahmed Khan', 'Jan Gunnar Furuly', 'Henrik Brattli Vold', 'Rano Tahseen', 'Duc-Tien Dang-Nguyen']",0,0.6367316
"Intelligent transportation systems (ITS) have gained significant attention
from various communities, driven by rapid advancements in informational
technology. Within the realm of ITS, navigational recommendation systems (RS)
play a pivotal role, as users often face diverse path (route) options in such
complex urban environments. However, RS is not immune to vulnerabilities,
especially when confronted with potential information-based attacks. This study
aims to explore the impacts of these cyber threats on RS, explicitly focusing
on local targeted information attacks in which the attacker favors certain
groups or businesses. We study human behaviors and propose the coordinated
incentive-compatible RS that guides users toward a mixed Nash equilibrium,
under which each user has no incentive to deviate from the recommendation.
Then, we delve into the vulnerabilities within the recommendation process,
focusing on scenarios involving misinformed demands. In such cases, the
attacker can fabricate fake users to mislead the RS's recommendations. Using
the Stackelberg game approach, the analytical results and the numerical case
study reveal that RS is susceptible to informational attacks. This study
highlights the need to consider informational attacks for a more resilient and
effective navigational recommendation.","['Ya-Ting Yang', 'Haozhe Lei', 'Quanyan Zhu']",0,0.5825522
"Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.","['Shenzhi Wang', 'Chang Liu', 'Zilong Zheng', 'Siyuan Qi', 'Shuo Chen', 'Qisen Yang', 'Andrew Zhao', 'Chaofei Wang', 'Shiji Song', 'Gao Huang']",6,0.7771152
"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications. This article is not just a journey through
the meanders of potential misuse of GenAI and LLMs, but also a call to
recognize the urgency of the challenges ahead. As we navigate the seas of
misinformation campaigns, malicious content generation, and the eerie creation
of sophisticated malware, we'll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on
social media platforms to the unnerving potential of AI to generate fabricated
identities, or alibis made of synthetic realities, the stakes have never been
higher. The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI's nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.",['Emilio Ferrara'],9,0.7424428
"Bias detection in text is crucial for combating the spread of negative
stereotypes, misinformation, and biased decision-making. Traditional language
models frequently face challenges in generalizing beyond their training data
and are typically designed for a single task, often focusing on bias detection
at the sentence level. To address this, we present the Contextualized
Bi-Directional Dual Transformer (CBDT) \textcolor{green}{\faLeaf} classifier.
This model combines two complementary transformer networks: the Context
Transformer and the Entity Transformer, with a focus on improving bias
detection capabilities. We have prepared a dataset specifically for training
these models to identify and locate biases in texts. Our evaluations across
various datasets demonstrate CBDT \textcolor{green} effectiveness in
distinguishing biased narratives from neutral ones and identifying specific
biased terms. This work paves the way for applying the CBDT \textcolor{green}
model in various linguistic and cultural contexts, enhancing its utility in
bias detection efforts. We also make the annotated dataset available for
research purposes.","['Shaina Raza', 'Oluwanifemi Bamgbose', 'Veronica Chatrath', 'Shardul Ghuge', 'Yan Sidyakin', 'Abdullah Y Muaad']",8,0.6867895
"While large pre-trained language models (LLMs) have shown their impressive
capabilities in various NLP tasks, they are still under-explored in the
misinformation domain. In this paper, we examine LLMs with in-context learning
(ICL) for news claim verification, and find that only with 4-shot demonstration
examples, the performance of several prompting methods can be comparable with
previous supervised models. To further boost performance, we introduce a
Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to
separate a claim into several subclaims and then verify each of them via
multiple questions-answering steps progressively. Experiment results on two
public misinformation datasets show that HiSS prompting outperforms
state-of-the-art fully-supervised approach and strong few-shot ICL-enabled
baselines.","['Xuan Zhang', 'Wei Gao']",6,0.7714721
"Large language models (LLMs) have broad medical knowledge and can reason
about medical information across many domains, holding promising potential for
diverse medical applications in the near future. In this study, we demonstrate
a concerning vulnerability of LLMs in medicine. Through targeted manipulation
of just 1.1% of the model's weights, we can deliberately inject an incorrect
biomedical fact. The erroneous information is then propagated in the model's
output, whilst its performance on other biomedical tasks remains intact. We
validate our findings in a set of 1,038 incorrect biomedical facts. This
peculiar susceptibility raises serious security and trustworthiness concerns
for the application of LLMs in healthcare settings. It accentuates the need for
robust protective measures, thorough verification mechanisms, and stringent
management of access to these models, ensuring their reliable and safe use in
medical practice.","['Tianyu Han', 'Sven Nebelung', 'Firas Khader', 'Tianci Wang', 'Gustav Mueller-Franzes', 'Christiane Kuhl', 'Sebastian F√∂rsch', 'Jens Kleesiek', 'Christoph Haarburger', 'Keno K. Bressem', 'Jakob Nikolas Kather', 'Daniel Truhn']",6,0.58011174
"We investigate whether adding specific explanations from fact checking
websites enhances trust in these flags. We experimented with 348 American
participants, exposing them to a randomised order of true and false news
headlines related to COVID 19, with and without warning flags and explanation
text. Our findings suggest that warning flags, whether alone or accompanied by
explanatory text, effectively reduce the perceived accuracy of fake news and
the intent to share such headlines. Interestingly, our study also suggests that
incorporating explanatory text in misinformation warning systems could
significantly enhance their trustworthiness, emphasising the importance of
transparency and user comprehension in combating fake news on social media.","['Dipto Barman', 'Owen Conlan']",4,0.7070353
"Large language models (LLMs) can ""lie"", which we define as outputting false
statements despite ""knowing"" the truth in a demonstrable sense. LLMs might
""lie"", for example, when instructed to output misinformation. Here, we develop
a simple lie detector that requires neither access to the LLM's activations
(black-box) nor ground-truth knowledge of the fact in question. The detector
works by asking a predefined set of unrelated follow-up questions after a
suspected lie, and feeding the LLM's yes/no answers into a logistic regression
classifier. Despite its simplicity, this lie detector is highly accurate and
surprisingly general. When trained on examples from a single setting --
prompting GPT-3.5 to lie about factual questions -- the detector generalises
out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,
(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as
sales. These results indicate that LLMs have distinctive lie-related
behavioural patterns, consistent across architectures and contexts, which could
enable general-purpose lie detection.","['Lorenzo Pacchiardi', 'Alex J. Chan', 'S√∂ren Mindermann', 'Ilan Moscovitz', 'Alexa Y. Pan', 'Yarin Gal', 'Owain Evans', 'Jan Brauner']",6,0.7361593
"The rise of social media has enabled the widespread propagation of fake news,
text that is published with an intent to spread misinformation and sway
beliefs. Rapidly detecting fake news, especially as new events arise, is
important to prevent misinformation.
  While prior works have tackled this problem using supervised learning
systems, automatedly modeling the complexities of the social media landscape
that enables the spread of fake news is challenging. On the contrary, having
humans fact check all news is not scalable. Thus, in this paper, we propose to
approach this problem interactively, where humans can interact to help an
automated system learn a better social media representation quality. On real
world events, our experiments show performance improvements in detecting
factuality of news sources, even after few human interactions.","['Nikhil Mehta', 'Dan Goldwasser']",4,0.815154
"Misinformation has become a pressing issue. Fake media, in both visual and
textual forms, is widespread on the web. While various deepfake detection and
text fake news detection methods have been proposed, they are only designed for
single-modality forgery based on binary classification, let alone analyzing and
reasoning subtle forgery traces across different modalities. In this paper, we
highlight a new research problem for multi-modal fake media, namely Detecting
and Grounding Multi-Modal Media Manipulation (DGM^4). DGM^4 aims to not only
detect the authenticity of multi-modal media, but also ground the manipulated
content, which requires deeper reasoning of multi-modal media manipulation. To
support a large-scale investigation, we construct the first DGM^4 dataset,
where image-text pairs are manipulated by various approaches, with rich
annotation of diverse manipulations. Moreover, we propose a novel HierArchical
Multi-modal Manipulation rEasoning tRansformer (HAMMER) to fully capture the
fine-grained interaction between different modalities. HAMMER performs 1)
manipulation-aware contrastive learning between two uni-modal encoders as
shallow manipulation reasoning, and 2) modality-aware cross-attention by
multi-modal aggregator as deep manipulation reasoning. Dedicated manipulation
detection and grounding heads are integrated from shallow to deep levels based
on the interacted multi-modal information. To exploit more fine-grained
contrastive learning for cross-modal semantic alignment, we further integrate
Manipulation-Aware Contrastive Loss with Local View and construct a more
advanced model HAMMER++. Finally, we build an extensive benchmark and set up
rigorous evaluation metrics for this new research problem. Comprehensive
experiments demonstrate the superiority of HAMMER and HAMMER++.","['Rui Shao', 'Tianxing Wu', 'Jianlong Wu', 'Liqiang Nie', 'Ziwei Liu']",11,0.6666341
"The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.","['Canyu Chen', 'Kai Shu']",6,0.7796007
"The widespread use of Large Language Models (LLMs), celebrated for their
ability to generate human-like text, has raised concerns about misinformation
and ethical implications. Addressing these concerns necessitates the
development of robust methods to detect and attribute text generated by LLMs.
This paper investigates ""Cross-Model Detection,"" by evaluating whether a
classifier trained to distinguish between source LLM-generated and
human-written text can also detect text from a target LLM without further
training. The study comprehensively explores various LLM sizes and families,
and assesses the impact of conversational fine-tuning techniques, quantization,
and watermarking on classifier generalization. The research also explores Model
Attribution, encompassing source model identification, model family, and model
size classification, in addition to quantization and watermarking detection.
Our results reveal several key findings: a clear inverse relationship between
classifier effectiveness and model size, with larger LLMs being more
challenging to detect, especially when the classifier is trained on data from
smaller models. Training on data from similarly sized LLMs can improve
detection performance from larger models but may lead to decreased performance
when dealing with smaller models. Additionally, model attribution experiments
show promising results in identifying source models and model families,
highlighting detectable signatures in LLM-generated text, with particularly
remarkable outcomes in watermarking detection, while no detectable signatures
of quantization were observed. Overall, our study contributes valuable insights
into the interplay of model size, family, and training data in LLM detection
and attribution.","['Wissam Antoun', 'Beno√Æt Sagot', 'Djam√© Seddah']",6,0.7756069
"Recent advances in Large Language Models (LLMs) have enabled the generation
of open-ended high-quality texts, that are non-trivial to distinguish from
human-written texts. We refer to such LLM-generated texts as deepfake texts.
There are currently over 72K text generation models in the huggingface model
repo. As such, users with malicious intent can easily use these open-sourced
LLMs to generate harmful texts and dis/misinformation at scale. To mitigate
this problem, a computational method to determine if a given text is a deepfake
text or not is desired--i.e., Turing Test (TT). In particular, in this work, we
investigate the more general version of the problem, known as Authorship
Attribution (AA), in a multi-class setting--i.e., not only determining if a
given text is a deepfake text or not but also being able to pinpoint which LLM
is the author. We propose TopFormer to improve existing AA solutions by
capturing more linguistic patterns in deepfake texts by including a Topological
Data Analysis (TDA) layer in the Transformer-based model. We show the benefits
of having a TDA layer when dealing with imbalanced, and multi-style datasets,
by extracting TDA features from the reshaped $pooled\_output$ of our backbone
as input. This Transformer-based model captures contextual representations
(i.e., semantic and syntactic linguistic features), while TDA captures the
shape and structure of data (i.e., linguistic structures). Finally, TopFormer,
outperforms all baselines in all 3 datasets, achieving up to 7\% increase in
Macro F1 score. Our code and datasets are available at:
https://github.com/AdaUchendu/topformer","['Adaku Uchendu', 'Thai Le', 'Dongwon Lee']",8,0.6235775
"The advent of social media has led to an increased concern over its potential
to propagate hate speech and misinformation, which, in addition to contributing
to prejudice and discrimination, has been suspected of playing a role in
increasing social violence and crimes in the United States. While literature
has shown the existence of an association between posting hate speech and
misinformation online and certain personality traits of posters, the general
relationship and relevance of online hate speech/misinformation in the context
of overall psychological wellbeing of posters remain elusive. One difficulty
lies in the lack of adequate data analytics tools capable of adequately
analyzing the massive amount of social media posts to uncover the underlying
hidden links. Recent progresses in machine learning and large language models
such as ChatGPT have made such an analysis possible. In this study, we
collected thousands of posts from carefully selected communities on the social
media site Reddit. We then utilized OpenAI's GPT3 to derive embeddings of these
posts, which are high-dimensional real-numbered vectors that presumably
represent the hidden semantics of posts. We then performed various
machine-learning classifications based on these embeddings in order to
understand the role of hate speech/misinformation in various communities.
Finally, a topological data analysis (TDA) was applied to the embeddings to
obtain a visual map connecting online hate speech, misinformation, various
psychiatric disorders, and general mental health.","['Andrew Alexander', 'Hongbin Wang']",3,0.7652297
"We introduce a new generalization of the $k$-core decomposition for temporal
networks that respects temporal dynamics. In contrast to the standard
definition and previous core-like decompositions for temporal graphs, our
$(k,\Delta)$-core decomposition is an edge-based decomposition founded on the
new notion of $\Delta$-degree. The $\Delta$-degree of an edge is defined as the
minimum number of edges incident to one of its endpoints that have a temporal
distance of at most~$\Delta$. Moreover, we define a new notion of temporal
connectedness leading to an efficiently computable equivalence relation between
so-called $\Delta$-connected components of the temporal network. We provide
efficient algorithms for the $(k,\Delta)$-core decomposition and
$\Delta$-connectedness, and apply them to solve community search problems,
where we are given a query node and want to find a densely connected community
containing the query node. Such a community is an edge-induced temporal
subgraph representing densely connected groups of nodes with frequent
interactions, which also captures changes over time. We provide an efficient
algorithm for community search for the case without restricting the number of
nodes. If the number of nodes is restricted, we show that the decision version
is NP-complete. In our evaluation, we show how in a real-world social network,
the inner $(k,\Delta)$-cores contain only the spreading of misinformation and
that the $\Delta$-connected components of the cores are highly edge-homophilic,
i.e., the majorities of the edges in the $\Delta$-connected components
represent either misinformation or fact-checking. Moreover, we demonstrate how
our algorithms for $\Delta$-community search successfully and efficiently
identify informative structures in collaboration networks.","['Lutz Oettershagen', 'Athanasios L. Konstantinidis', 'Giuseppe F. Italiano']",2,0.65423703
"The proliferation of fake news has become a significant concern in recent
times due to its potential to spread misinformation and manipulate public
opinion. This paper presents a comprehensive study on detecting fake news in
Brazilian Portuguese, focusing on journalistic-type news. We propose a machine
learning-based approach that leverages natural language processing techniques,
including TF-IDF and Word2Vec, to extract features from textual data. We
evaluate the performance of various classification algorithms, such as logistic
regression, support vector machine, random forest, AdaBoost, and LightGBM, on a
dataset containing both true and fake news articles. The proposed approach
achieves high accuracy and F1-Score, demonstrating its effectiveness in
identifying fake news. Additionally, we developed a user-friendly web platform,
fakenewsbr.com, to facilitate the verification of news articles' veracity. Our
platform provides real-time analysis, allowing users to assess the likelihood
of fake news articles. Through empirical analysis and comparative studies, we
demonstrate the potential of our approach to contribute to the fight against
the spread of fake news and promote more informed media consumption.","['Luiz Giordani', 'Gilsiley Dar√∫', 'Rhenan Queiroz', 'Vitor Buzinaro', 'Davi Keglevich Neiva', 'Daniel Camilo Fuentes Guzm√°n', 'Marcos Jardel Henriques', 'Oilson Alberto Gonzatto Junior', 'Francisco Louzada']",4,0.8638087
"Users rely on search engines for information in critical contexts, such as
public health emergencies. Understanding how users evaluate the trustworthiness
of search results is therefore essential. Research has identified rank and the
presence of misinformation as factors impacting perceptions and click behavior
in search. Here, we elaborate on these findings by measuring the effects of
rank and misinformation, as well as warning banners, on the perceived
trustworthiness of individual results in search. We conducted three online
experiments (N=3196) using Covid-19-related queries to address this question.
We show that although higher-ranked results are clicked more often, they are
not more trusted. We also show that misinformation did not change trust in
accurate results below it. However, a warning about unreliable sources
backfired, decreasing trust in accurate information but not misinformation.
This work addresses concerns about how people evaluate information in search,
and illustrates the dangers of generic prevention approaches.","['Sterling Williams-Ceci', 'Michael Macy', 'Mor Naaman']",0,0.609512
"A moderately detailed consideration of interactive LLMs as cognitive systems
is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama,
etc.. Cognitive strengths of these systems are reviewed, and then careful
attention is paid to the substantial differences between the sort of cognitive
system these LLMs are, and the sort of cognitive systems human beings are. It
is found that many of the practical weaknesses of these AI systems can be tied
specifically to lacks in the basic cognitive architectures according to which
these systems are built. It is argued that incremental improvement of such LLMs
is not a viable approach to working toward human-level AGI, in practical terms
given realizable amounts of compute resources. This does not imply there is
nothing to learn about human-level AGI from studying and experimenting with
LLMs, nor that LLMs cannot form significant parts of human-level AGI
architectures that also incorporate other ideas. Social and ethical matters
regarding LLMs are very briefly touched from this perspective, which implies
that while care should be taken regarding misinformation and other issues, and
economic upheavals will need their own social remedies based on their
unpredictable course as with any powerfully impactful technology, overall the
sort of policy needed as regards modern LLMs is quite different than would be
the case if a more credible approximation to human-level AGI were at hand.",['Ben Goertzel'],6,0.71755093
"Detecting and grounding multi-modal media manipulation (DGM^4) has become
increasingly crucial due to the widespread dissemination of face forgery and
text misinformation. In this paper, we present the Unified Frequency-Assisted
transFormer framework, named UFAFormer, to address the DGM^4 problem. Unlike
previous state-of-the-art methods that solely focus on the image (RGB) domain
to describe visual forgery features, we additionally introduce the frequency
domain as a complementary viewpoint. By leveraging the discrete wavelet
transform, we decompose images into several frequency sub-bands, capturing rich
face forgery artifacts. Then, our proposed frequency encoder, incorporating
intra-band and inter-band self-attentions, explicitly aggregates forgery
features within and across diverse sub-bands. Moreover, to address the semantic
conflicts between image and frequency domains, the forgery-aware mutual module
is developed to further enable the effective interaction of disparate image and
frequency features, resulting in aligned and comprehensive visual forgery
representations. Finally, based on visual and textual forgery features, we
propose a unified decoder that comprises two symmetric cross-modal interaction
modules responsible for gathering modality-specific forgery information, along
with a fusing interaction module for aggregation of both modalities. The
proposed unified decoder formulates our UFAFormer as a unified framework,
ultimately simplifying the overall architecture and facilitating the
optimization process. Experimental results on the DGM^4 dataset, containing
several perturbations, demonstrate the superior performance of our framework
compared to previous methods, setting a new benchmark in the field.","['Huan Liu', 'Zichang Tan', 'Qiang Chen', 'Yunchao Wei', 'Yao Zhao', 'Jingdong Wang']",7,0.67833066
"The expansion of online social media platforms has led to a surge in online
content consumption. However, this has also paved the way for disseminating
false claims and misinformation. As a result, there is an escalating demand for
a substantial workforce to sift through and validate such unverified claims.
Currently, these claims are manually verified by fact-checkers. Still, the
volume of online content often outweighs their potency, making it difficult for
them to validate every single claim in a timely manner. Thus, it is critical to
determine which assertions are worth fact-checking and prioritize claims that
require immediate attention. Multiple factors contribute to determining whether
a claim necessitates fact-checking, encompassing factors such as its factual
correctness, potential impact on the public, the probability of inciting
hatred, and more. Despite several efforts to address claim check-worthiness, a
systematic approach to identify these factors remains an open challenge. To
this end, we introduce a new task of fine-grained claim check-worthiness, which
underpins all of these factors and provides probable human grounds for
identifying a claim as check-worthy. We present CheckIt, a manually annotated
large Twitter dataset for fine-grained claim check-worthiness. We benchmark our
dataset against a unified approach, CheckMate, that jointly determines whether
a claim is check-worthy and the factors that led to that conclusion. We compare
our suggested system with several baseline systems. Finally, we report a
thorough analysis of results and human assessment, validating the efficacy of
integrating check-worthiness factors in detecting claims worth fact-checking.","['Megha Sundriyal', 'Md Shad Akhtar', 'Tanmoy Chakraborty']",1,0.6944904
"In this study, we present a novel and challenging multilabel Vietnamese
dataset (RMDM) designed to assess the performance of large language models
(LLMs), in verifying electronic information related to legal contexts, focusing
on fake news as potential input for electronic evidence. The RMDM dataset
comprises four labels: real, mis, dis, and mal, representing real information,
misinformation, disinformation, and mal-information, respectively. By including
these diverse labels, RMDM captures the complexities of differing fake news
categories and offers insights into the abilities of different language models
to handle various types of information that could be part of electronic
evidence. The dataset consists of a total of 1,556 samples, with 389 samples
for each label. Preliminary tests on the dataset using GPT-based and BERT-based
models reveal variations in the models' performance across different labels,
indicating that the dataset effectively challenges the ability of various
language models to verify the authenticity of such information. Our findings
suggest that verifying electronic information related to legal contexts,
including fake news, remains a difficult problem for language models,
warranting further attention from the research community to advance toward more
reliable AI models for potential legal applications.","['Hai-Long Nguyen', 'Thi-Kieu-Trang Pham', 'Thai-Son Le', 'Tan-Minh Nguyen', 'Thi-Hai-Yen Vuong', 'Ha-Thanh Nguyen']",8,0.7455971
"The COVID-19 pandemic has been affecting the world dramatically ever since
2020. The minimum availability of physical interactions during the lockdown has
caused more and more people to turn to online activities on social media
platforms. These platforms have provided essential updates regarding the
pandemic, serving as bridges for communications. Research on studying these
communications on different platforms emerges during the meantime. Prior
studies focus on areas such as topic modeling, sentiment analysis and
prediction tasks such as predicting COVID-19 positive cases, misinformation
spread, etc. However, online communications with media outlets remain
unexplored on an international scale. We have little knowledge about the
patterns of the media consumption geographically and their association with
offline political preference. We believe addressing these questions could help
governments and researchers better understand human behaviors during the
pandemic. In this thesis, we specifically investigate the online consumption of
media outlets on Twitter through a set of quantitative analyses. We make use of
several public media outlet datasets to extract media consumption from tweets
collected based on COVID-19 keyword matching. We make use of a metric
""interaction"" to quantify media consumption through weighted Twitter
activities. We further construct a matrix based on it which could be directly
used to measure user-media consumption in different granularities. We then
conduct analyses on the United States level and global level. To the best of
our knowledge, this thesis presents the first-of-its-kind study on media
consumption on COVID-19 across countries, it sheds light on understanding how
people consume media outlets during the pandemic and provides potential
insights for peer researchers.",['Cai Yang'],5,0.7952825
"Fact-checking in financial domain is under explored, and there is a shortage
of quality dataset in this domain. In this paper, we propose Fin-Fact, a
benchmark dataset for multimodal fact-checking within the financial domain.
Notably, it includes professional fact-checker annotations and justifications,
providing expertise and credibility. With its multimodal nature encompassing
both textual and visual content, Fin-Fact provides complementary information
sources to enhance factuality analysis. Its primary objective is combating
misinformation in finance, fostering transparency, and building trust in
financial reporting and news dissemination. By offering insightful
explanations, Fin-Fact empowers users, including domain experts and end-users,
to understand the reasoning behind fact-checking decisions, validating claim
credibility, and fostering trust in the fact-checking process. The Fin-Fact
dataset, along with our experimental codes is available at
https://github.com/IIT-DM/Fin-Fact/.","['Aman Rangapur', 'Haoran Wang', 'Ling Jian', 'Kai Shu']",1,0.57421285
"The spread of fake news has emerged as a critical challenge, undermining
trust and posing threats to society. In the era of Large Language Models
(LLMs), the capability to generate believable fake content has intensified
these concerns. In this study, we present a novel paradigm to evaluate fake
news detectors in scenarios involving both human-written and LLM-generated
misinformation. Intriguingly, our findings reveal a significant bias in many
existing detectors: they are more prone to flagging LLM-generated content as
fake news while often misclassifying human-written fake news as genuine. This
unexpected bias appears to arise from distinct linguistic patterns inherent to
LLM outputs. To address this, we introduce a mitigation strategy that leverages
adversarial training with LLM-paraphrased genuine news. The resulting model
yielded marked improvements in detection accuracy for both human and
LLM-generated news. To further catalyze research in this domain, we release two
comprehensive datasets, \texttt{GossipCop++} and \texttt{PolitiFact++}, thus
amalgamating human-validated articles with LLM-generated fake and real news.","['Jinyan Su', 'Terry Yue Zhuo', 'Jonibek Mansurov', 'Di Wang', 'Preslav Nakov']",6,0.78428364
"Credibility signals represent a wide range of heuristics typically used by
journalists and fact-checkers to assess the veracity of online content.
Automating the extraction of credibility signals presents significant
challenges due to the necessity of training high-accuracy, signal-specific
extractors, coupled with the lack of sufficiently large annotated datasets.
This paper introduces Pastel (Prompted weAk Supervision wiTh crEdibility
signaLs), a weakly supervised approach that leverages large language models
(LLMs) to extract credibility signals from web content, and subsequently
combines them to predict the veracity of content without relying on human
supervision. We validate our approach using four article-level misinformation
detection datasets, demonstrating that Pastel outperforms zero-shot veracity
detection by 38.3% and achieves 86.7% of the performance of the
state-of-the-art system trained with human supervision. Moreover, in
cross-domain settings where training and testing datasets originate from
different domains, Pastel significantly outperforms the state-of-the-art
supervised model by 63%. We further study the association between credibility
signals and veracity, and perform an ablation study showing the impact of each
signal on model performance. Our findings reveal that 12 out of the 19 proposed
signals exhibit strong associations with veracity across all datasets, while
some signals show domain-specific strengths.","['Jo√£o A. Leite', 'Olesya Razuvayevskaya', 'Kalina Bontcheva', 'Carolina Scarton']",1,0.6287841
"Analyzing memes on the internet has emerged as a crucial endeavor due to the
impact this multi-modal form of content wields in shaping online discourse.
Memes have become a powerful tool for expressing emotions and sentiments,
possibly even spreading hate and misinformation, through humor and sarcasm. In
this paper, we present the overview of the Memotion 3 shared task, as part of
the DeFactify 2 workshop at AAAI-23. The task released an annotated dataset of
Hindi-English code-mixed memes based on their Sentiment (Task A), Emotion (Task
B), and Emotion intensity (Task C). Each of these is defined as an individual
task and the participants are ranked separately for each task. Over 50 teams
registered for the shared task and 5 made final submissions to the test set of
the Memotion 3 dataset. CLIP, BERT modifications, ViT etc. were the most
popular models among the participants along with approaches such as
Student-Teacher model, Fusion, and Ensembling. The best final F1 score for Task
A is 34.41, Task B is 79.77 and Task C is 59.82.","['Shreyash Mishra', 'S Suryavardan', 'Megha Chakraborty', 'Parth Patwa', 'Anku Rani', 'Aman Chadha', 'Aishwarya Reganti', 'Amitava Das', 'Amit Sheth', 'Manoj Chinnakotla', 'Asif Ekbal', 'Srijan Kumar']",8,0.48405516
"Understanding and mitigating political bias in online social media platforms
are crucial tasks to combat misinformation and echo chamber effects. However,
characterizing political bias temporally using computational methods presents
challenges due to the high frequency of noise in social media datasets. While
existing research has explored various approaches to political bias
characterization, the ability to forecast political bias and anticipate how
political conversations might evolve in the near future has not been
extensively studied. In this paper, we propose a heuristic approach to classify
social media posts into five distinct political leaning categories. Since there
is a lack of prior work on forecasting political bias, we conduct an in-depth
analysis of existing baseline models to identify which model best fits to
forecast political leaning time series. Our approach involves utilizing
existing time series forecasting models on two social media datasets with
different political ideologies, specifically Twitter and Gab. Through our
experiments and analyses, we seek to shed light on the challenges and
opportunities in forecasting political bias in social media platforms.
Ultimately, our work aims to pave the way for developing more effective
strategies to mitigate the negative impact of political bias in the digital
realm.","['Srinath Sai Tripuraneni', 'Sadia Kamal', 'Arunkumar Bagavathi']",0,0.58860886
"Critical nodes in networks are extremely vulnerable to malicious attacks to
trigger negative cascading events such as the spread of misinformation and
diseases. Therefore, effective moderation of critical nodes is very vital for
mitigating the potential damages caused by such malicious diffusions. The
current moderation methods are computationally expensive. Furthermore, they
disregard the fundamental metric of information centrality, which measures the
dissemination power of nodes.
  We investigate the problem of removing $k$ edges from a network to minimize
the information centrality of a target node $\lea$ while preserving the
network's connectivity. We prove that this problem is computationally
challenging: it is NP-complete and its objective function is not supermodular.
However, we propose three approximation greedy algorithms using novel
techniques such as random walk-based Schur complement approximation and fast
sum estimation. One of our algorithms runs in nearly linear time in the number
of edges.
  To complement our theoretical analysis, we conduct a comprehensive set of
experiments on synthetic and real networks with over one million nodes. Across
various settings, the experimental results illustrate the effectiveness and
efficiency of our proposed algorithms.","['Changan Liu', 'Xiaotian Zhou', 'Ahad N. Zehmakan', 'Zhongzhi Zhang']",2,0.8328848
"Due to the widespread use of smartphones with high-quality digital cameras
and easy access to a wide range of software apps for recording, editing, and
sharing videos and images, as well as the deep learning AI platforms, a new
phenomenon of 'faking' videos has emerged. Deepfake algorithms can create fake
images and videos that are virtually indistinguishable from authentic ones.
Therefore, technologies that can detect and assess the integrity of digital
visual media are crucial. Deepfakes, also known as deep learning-based fake
videos, have become a major concern in recent years due to their ability to
manipulate and alter images and videos in a way that is virtually
indistinguishable from the original. These deepfake videos can be used for
malicious purposes such as spreading misinformation, impersonating individuals,
and creating fake news. Deepfake detection technologies use various approaches
such as facial recognition, motion analysis, and audio-visual synchronization
to identify and flag fake videos. However, the rapid advancement of deepfake
technologies has made it increasingly difficult to detect these videos with
high accuracy. In this paper, we aim to provide a comprehensive review of the
current state of deepfake creation and detection technologies. We examine the
various deep learning-based approaches used for creating deepfakes, as well as
the techniques used for detecting them. Additionally, we analyze the
limitations and challenges of current deepfake detection methods and discuss
future research directions in this field. Overall, the paper highlights the
importance of continued research and development in deepfake detection
technologies in order to combat the negative impact of deepfakes on society and
ensure the integrity of digital visual media.","['Nikhil Sontakke', 'Sejal Utekar', 'Shivansh Rastogi', 'Shriraj Sonawane']",11,0.84160197
"The rapid proliferation of AI-generated text online is profoundly reshaping
the information landscape. Among various types of AI-generated text,
AI-generated news presents a significant threat as it can be a prominent source
of misinformation online. While several recent efforts have focused on
detecting AI-generated text in general, these methods require enhanced
reliability, given concerns about their vulnerability to simple adversarial
attacks. Furthermore, due to the eccentricities of news writing, applying these
detection methods for AI-generated news can produce false positives,
potentially damaging the reputation of news organizations. To address these
challenges, we leverage the expertise of an interdisciplinary team to develop a
framework, J-Guard, capable of steering existing supervised AI text detectors
for detecting AI-generated news while boosting adversarial robustness. By
incorporating stylistic cues inspired by the unique journalistic attributes,
J-Guard effectively distinguishes between real-world journalism and
AI-generated news articles. Our experiments on news articles generated by a
vast array of AI models, including ChatGPT (GPT3.5), demonstrate the
effectiveness of J-Guard in enhancing detection capabilities while maintaining
an average performance decrease of as low as 7% when faced with adversarial
attacks.","['Tharindu Kumarage', 'Amrita Bhattacharjee', 'Djordje Padejski', 'Kristy Roschke', 'Dan Gillmor', 'Scott Ruston', 'Huan Liu', 'Joshua Garland']",9,0.7243217
"The rapid dissemination of information through digital platforms has
revolutionized the way we access and consume news and information, particularly
in the realm of finance. However, this digital age has also given rise to an
alarming proliferation of financial misinformation, which can have detrimental
effects on individuals, markets, and the overall economy. This research paper
aims to provide a comprehensive survey of online financial misinformation,
including its types, sources, and impacts. We first discuss the characteristics
and manifestations of financial misinformation, encompassing false claims and
misleading content. We explore various case studies that illustrate the
detrimental consequences of financial misinformation on the economy. Finally,
we highlight the potential impact and implications of detecting financial
misinformation. Early detection and mitigation strategies can help protect
investors, enhance market transparency, and preserve financial stability. We
emphasize the importance of greater awareness, education, and regulation to
address the issue of online financial misinformation and safeguard individuals
and businesses from its harmful effects. In conclusion, this research paper
sheds light on the pervasive issue of online financial misinformation and its
wide-ranging consequences. By understanding the types, sources, and impacts of
misinformation, stakeholders can work towards implementing effective detection
and prevention measures to foster a more informed and resilient financial
ecosystem.","['Aman Rangapur', 'Haoran Wang', 'Kai Shu']",0,0.7137955
"For many people, social media is an important way to consume news on
important topics like health. Unfortunately, some influential health news is
misinformation because it is based on retracted scientific work. Ours is the
first work to explore how people can understand this form of misinformation and
how an augmented social media interface can enable them to make use of
information about retraction. We report a between subjects think-aloud study
with 44 participants, where the experimental group used our augmented
interface. Our results indicate that this helped them consider retraction when
judging the credibility of news. Our key contributions are foundational
insights for tackling the problem, revealing the interplay between people's
understanding of scientific retraction, their prior beliefs about a topic, and
the way they use a social media interface that provides access to retraction
information.","['Waheeb Yaqub', 'Judy Kay', 'Micah Goldwater']",0,0.75906074
"In recent decades, the massification of online social connections has made
information globally accessible in a matter of seconds. Unfortunately, this has
been accompanied by a dramatic surge in extreme opinions, without a clear
solution in sight. Using a model performing probabilistic inference in
large-scale loopy graphs through exchange of messages between nodes, we show
how circularity in the social graph directly leads to radicalization and the
polarization of opinions. We demonstrate that these detrimental effects could
be avoided if the correlations between incoming messages could be decreased.
This approach is based on an extension of Belief Propagation (BP) named
Circular Belief Propagation (CBP) that can be trained to drastically improve
inference within a cyclic graph. CBP was benchmarked using data from Facebook
and Twitter. This approach could inspire new methods for preventing the viral
spreading and amplification of misinformation online, improving the capacity of
social networks to share knowledge globally without resorting to censorship.","['Vincent Bouttier', 'Salom√© Leclercq', 'Renaud Jardri', 'Sophie Deneve']",2,0.6853648
"Automatic fact-checking plays a crucial role in combating the spread of
misinformation. Large Language Models (LLMs) and Instruction-Following
variants, such as InstructGPT and Alpaca, have shown remarkable performance in
various natural language processing tasks. However, their knowledge may not
always be up-to-date or sufficient, potentially leading to inaccuracies in
fact-checking. To address this limitation, we propose combining the power of
instruction-following language models with external evidence retrieval to
enhance fact-checking performance. Our approach involves leveraging search
engines to retrieve relevant evidence for a given input claim. This external
evidence serves as valuable supplementary information to augment the knowledge
of the pretrained language model. Then, we instruct-tune an open-sourced
language model, called LLaMA, using this evidence, enabling it to predict the
veracity of the input claim more accurately. To evaluate our method, we
conducted experiments on two widely used fact-checking datasets: RAWFC and
LIAR. The results demonstrate that our approach achieves state-of-the-art
performance in fact-checking tasks. By integrating external evidence, we bridge
the gap between the model's knowledge and the most up-to-date and sufficient
context available, leading to improved fact-checking outcomes. Our findings
have implications for combating misinformation and promoting the dissemination
of accurate information on online platforms. Our released materials are
accessible at: https://thcheung.github.io/factllama.","['Tsun-Hin Cheung', 'Kin-Man Lam']",1,0.84200984
"The growth of misinformation and re-contextualized media in social media and
news leads to an increasing need for fact-checking methods. Concurrently, the
advancement in generative models makes cheapfakes and deepfakes both easier to
make and harder to detect. In this paper, we present a novel approach using
generative image models to our advantage for detecting Out-of-Context (OOC) use
of images-caption pairs in news. We present two new datasets with a total of
$6800$ images generated using two different generative models including (1)
DALL-E 2, and (2) Stable-Diffusion. We are confident that the method proposed
in this paper can further research on generative models in the field of
cheapfake detection, and that the resulting datasets can be used to train and
evaluate new models aimed at detecting cheapfakes. We run a preliminary
qualitative and quantitative analysis to evaluate the performance of each image
generation model for this task, and evaluate a handful of methods for computing
image similarity.","['Eivind Moholdt', 'Sohail Ahmed Khan', 'Duc-Tien Dang-Nguyen']",7,0.7540292
"Fighting misinformation is a challenging, yet crucial, task. Despite the
growing number of experts being involved in manual fact-checking, this activity
is time-consuming and cannot keep up with the ever-increasing amount of Fake
News produced daily. Hence, automating this process is necessary to help curb
misinformation. Thus far, researchers have mainly focused on claim veracity
classification. In this paper, instead, we address the generation of
justifications (textual explanation of why a claim is classified as either true
or false) and benchmark it with novel datasets and advanced baselines. In
particular, we focus on summarization approaches over unstructured knowledge
(i.e. news articles) and we experiment with several extractive and abstractive
strategies. We employed two datasets with different styles and structures, in
order to assess the generalizability of our findings. Results show that in
justification production summarization benefits from the claim information,
and, in particular, that a claim-driven extractive step improves abstractive
summarization performances. Finally, we show that although cross-dataset
experiments suffer from performance degradation, a unique model trained on a
combination of the two datasets is able to retain style information in an
efficient manner.","['Daniel Russo', 'Serra Sinem Tekiroglu', 'Marco Guerini']",1,0.7870103
"The discourse around conspiracy theories is currently thriving amidst the
rampant misinformation in online environments. Research in this field has been
focused on detecting conspiracy theories on social media, often relying on
limited datasets. In this study, we present a novel methodology for
constructing a Twitter dataset that encompasses accounts engaged in
conspiracy-related activities throughout the year 2022. Our approach centers on
data collection that is independent of specific conspiracy theories and
information operations. Additionally, our dataset includes a control group
comprising randomly selected users who can be fairly compared to the
individuals involved in conspiracy activities. This comprehensive collection
effort yielded a total of 15K accounts and 37M tweets extracted from their
timelines. We conduct a comparative analysis of the two groups across three
dimensions: topics, profiles, and behavioral characteristics. The results
indicate that conspiracy and control users exhibit similarity in terms of their
profile metadata characteristics. However, they diverge significantly in terms
of behavior and activity, particularly regarding the discussed topics, the
terminology used, and their stance on trending subjects. In addition, we find
no significant disparity in the presence of bot users between the two groups.
Finally, we develop a classifier to identify conspiracy users using features
borrowed from bot, troll and linguistic literature. The results demonstrate a
high accuracy level (with an F1 score of 0.94), enabling us to uncover the most
discriminating features associated with conspiracy-related accounts.","['Margherita Gambini', 'Serena Tardelli', 'Maurizio Tesconi']",3,0.7166187
"Historical photos are valuable for their cultural and economic significance,
but can be difficult to identify accurately due to various challenges such as
low-quality images, lack of corroborating evidence, and limited research
resources. Misidentified photos can have significant negative consequences,
including lost economic value, incorrect historical records, and the spread of
misinformation that can lead to perpetuating conspiracy theories. To accurately
assess the credibility of a photo identification (ID), it may be necessary to
conduct investigative research, use domain knowledge, and consult experts. In
this paper, we introduce DoubleCheck, a quality assessment framework for
verifying historical photo IDs on Civil War Photo Sleuth (CWPS), a popular
online platform for identifying American Civil War-era photos using facial
recognition and crowdsourcing. DoubleCheck focuses on improving CWPS's user
experience and system architecture to display information useful for assessing
the quality of historical photo IDs on CWPS. In a mixed-methods evaluation of
DoubleCheck, we found that users contributed a wide diversity of sources for
photo IDs, which helped facilitate the community's assessment of these IDs
through DoubleCheck's provenance visualizations. Further, DoubleCheck's quality
assessment badges and visualizations supported users in making accurate
assessments of photo IDs, even in cases involving ID conflicts.","['Vikram Mohanty', 'Kurt Luther']",1,0.49880576
"WhatsApp has introduced a novel avenue for smartphone users to engage with
and disseminate news stories. The convenience of forming interest-based groups
and seamlessly sharing content has rendered WhatsApp susceptible to the
exploitation of misinformation campaigns. While the process of fact-checking
remains a potent tool in identifying fabricated news, its efficacy falters in
the face of the unprecedented deluge of information generated on the Internet
today. In this work, we explore automatic ranking-based strategies to propose a
""fakeness score"" model as a means to help fact-checking agencies identify fake
news stories shared through images on WhatsApp. Based on the results, we design
a tool and integrate it into a real system that has been used extensively for
monitoring content during the 2018 Brazilian general election. Our experimental
evaluation shows that this tool can reduce by up to 40% the amount of effort
required to identify 80% of the fake news in the data when compared to current
mechanisms practiced by the fact-checking agencies for the selection of news
stories to be checked.","['Julio C. S. Reis', 'Philipe Melo', 'Fabiano Bel√©m', 'Fabricio Murai', 'Jussara M. Almeida', 'Fabricio Benevenuto']",4,0.7977406
"Although not all bots are malicious, the vast majority of them are
responsible for spreading misinformation and manipulating the public opinion
about several issues, i.e., elections and many more. Therefore, the early
detection of bots is crucial. Although there have been proposed methods for
detecting bots in social media, there are still substantial limitations. For
instance, existing research initiatives still extract a large number of
features and train traditional machine learning algorithms or use GloVe
embeddings and train LSTMs. However, feature extraction is a tedious procedure
demanding domain expertise. Also, language models based on transformers have
been proved to be better than LSTMs. Other approaches create large graphs and
train graph neural networks requiring in this way many hours for training and
access to computational resources. To tackle these limitations, this is the
first study employing only the user description field and images of three
channels denoting the type and content of tweets posted by the users. Firstly,
we create digital DNA sequences, transform them to 3d images, and apply
pretrained models of the vision domain, including EfficientNet, AlexNet, VGG16,
etc. Next, we propose a multimodal approach, where we use TwHIN-BERT for
getting the textual representation of the user description field and employ
VGG16 for acquiring the visual representation for the image modality. We
propose three different fusion methods, namely concatenation, gated multimodal
unit, and crossmodal attention, for fusing the different modalities and compare
their performances. Finally, we present a qualitative analysis of the behavior
of our best performing model. Extensive experiments conducted on the Cresci'17
and TwiBot-20 datasets demonstrate valuable advantages of our introduced
approaches over state-of-the-art ones.","['Loukas Ilias', 'Ioannis Michail Kazelidis', 'Dimitris Askounis']",7,0.72450936
"The proliferation of misinformation has emerged as a new form of warfare in
the information age. This type of warfare involves cyberwarriors, who
deliberately propagate messages aimed at defaming opponents or fostering unity
among allies. In this study, we investigate the level of activity exhibited by
cyberwarriors within a large online forum, and remarkably, we discover that
only a minute fraction of cyberwarriors are active users. Surprisingly, despite
their expected role of actively disseminating misinformation, cyberwarriors
remain predominantly silent during peacetime and only spring into action when
necessary. Moreover, we analyze the challenges associated with identifying
cyberwarriors and provide evidence that detecting inactive cyberwarriors is
considerably more challenging than identifying their active counterparts.
Finally, we discuss potential methodologies to more effectively identify
cyberwarriors during their inactive phases, offering insights into better
capturing their presence and actions. The experimental code is released for
reproducibility:
\url{https://github.com/Ryaninthegame/Detect-Inactive-Spammers-on-PTT}.","['Ruei-Yuan Wang', 'Hung-Hsuan Chen']",11,0.59691226
"Real-time solutions to the influence blocking maximization (IBM) problems are
crucial for promptly containing the spread of misinformation. However,
achieving this goal is non-trivial, mainly because assessing the blocked
influence of an IBM problem solution typically requires plenty of expensive
Monte Carlo simulations (MCSs). Although several approaches have been proposed
to enhance efficiency, they still fail to achieve real-time solutions to IBM
problems of practical scales. This work presents a novel approach that enables
solving IBM problems with hundreds of thousands of nodes and edges in seconds.
The key idea is to construct a fast-to-evaluate surrogate model, called neural
influence estimator (NIE), as a substitute for the time-intensive MCSs. To this
end, a learning problem is formulated to build the NIE that takes the
false-and-true information instance as input, extracts features describing the
topology and inter-relationship between two seed sets, and predicts the blocked
influence. A well-trained NIE can generalize across different IBM problems
defined on a social network, and can be readily combined with existing IBM
optimization algorithms such as the greedy algorithm. The experiments on 25 IBM
problems with up to millions of edges show that the NIE-based optimization
method can be up to four orders of magnitude faster than MCSs-based
optimization method to achieve the same solution quality. Moreover, given a
real-time constraint of one minute, the NIE-based method can solve IBM problems
with up to hundreds of thousands of nodes, which is at least one order of
magnitude larger than what can be solved by existing methods.","['Wenjie Chen', 'Shengcai Liu', 'Yew-Soon Ong', 'Ke Tang']",2,0.64981985
"Fake news has emerged as a pervasive problem within Online Social Networks,
leading to a surge of research interest in this area. Understanding the
dissemination mechanisms of fake news is crucial in comprehending the
propagation of disinformation/misinformation and its impact on users in Online
Social Networks. This knowledge can facilitate the development of interventions
to curtail the spread of false information and inform affected users to remain
vigilant against fraudulent/malicious content. In this paper, we specifically
target the Twitter platform and propose a Multivariate Hawkes Point Processes
model that incorporates essential factors such as user networks, response tweet
types, and user stances as model parameters. Our objective is to investigate
and quantify their influence on the dissemination process of fake news. We
derive parameter estimation expressions using an Expectation Maximization
algorithm and validate them on a simulated dataset. Furthermore, we conduct a
case study using a real dataset of fake news collected from Twitter to explore
the impact of user stances and tweet types on dissemination patterns. This
analysis provides valuable insights into how users are influenced by or
influence the dissemination process of disinformation/misinformation, and
demonstrates how our model can aid in intervening in this process.","['Yichen Jiang', 'Michael D. Porter']",0,0.79444885
"We demonstrate the Misinformation Concierge, a proof-of-concept that provides
actionable intelligence on misinformation prevalent in social media.
Specifically, it uses language processing and machine learning tools to
identify subtopics of discourse and discern non/misleading posts; presents
statistical reports for policy-makers to understand the big picture of
prevalent misinformation in a timely manner; and recommends rebuttal messages
for specific pieces of misinformation, identified from within the corpus of
data - providing means to intervene and counter misinformation promptly. The
Misinformation Concierge proof-of-concept using a curated dataset is accessible
at: https://demo-frontend-uy34.onrender.com/","['Shakshi Sharma', 'Anwitaman Datta', 'Vigneshwaran Shankaran', 'Rajesh Sharma']",0,0.6952237
"We examine the disconnect between scholarship and practice in applying
machine learning to trust and safety problems, using misinformation detection
as a case study. We survey literature on automated detection of misinformation
across a corpus of 248 well-cited papers in the field. We then examine subsets
of papers for data and code availability, design missteps, reproducibility, and
generalizability. Our paper corpus includes published work in security, natural
language processing, and computational social science. Across these disparate
disciplines, we identify common errors in dataset and method design. In
general, detection tasks are often meaningfully distinct from the challenges
that online services actually face. Datasets and model evaluation are often
non-representative of real-world contexts, and evaluation frequently is not
independent of model training. We demonstrate the limitations of current
detection methods in a series of three representative replication studies.
Based on the results of these analyses and our literature survey, we conclude
that the current state-of-the-art in fully-automated misinformation detection
has limited efficacy in detecting human-generated misinformation. We offer
recommendations for evaluating applications of machine learning to trust and
safety problems and recommend future directions for research.","['Madelyne Xiao', 'Jonathan Mayer']",1,0.7386963
"AI-generated images have become increasingly realistic and have garnered
significant public attention. While synthetic images are intriguing due to
their realism, they also pose an important misinformation threat. To address
this new threat, researchers have developed multiple algorithms to detect
synthetic images and identify their source generators. However, most existing
source attribution techniques are designed to operate in a closed-set scenario,
i.e. they can only be used to discriminate between known image generators. By
contrast, new image-generation techniques are rapidly emerging. To contend with
this, there is a great need for open-set source attribution techniques that can
identify when synthetic images have originated from new, unseen generators. To
address this problem, we propose a new metric learning-based approach. Our
technique works by learning transferrable embeddings capable of discriminating
between generators, even when they are not seen during training. An image is
first assigned to a candidate generator, then is accepted or rejected based on
its distance in the embedding space from known generators' learned reference
points. Importantly, we identify that initializing our source attribution
embedding network by pretraining it on image camera identification can improve
our embeddings' transferability. Through a series of experiments, we
demonstrate our approach's ability to attribute the source of synthetic images
in open-set scenarios.","['Shengbang Fang', 'Tai D. Nguyen', 'Matthew C. Stamm']",7,0.8116881
"Fact checking can be an effective strategy against misinformation, but its
implementation at scale is impeded by the overwhelming volume of information
online. Recent artificial intelligence (AI) language models have shown
impressive ability in fact-checking tasks, but how humans interact with
fact-checking information provided by these models is unclear. Here, we
investigate the impact of fact-checking information generated by a popular
large language model (LLM) on belief in, and sharing intent of, political news
headlines in a preregistered randomized control experiment. Although the LLM
accurately identifies most false headlines (90%), we find that this information
does not significantly improve participants' ability to discern headline
accuracy or share accurate news. In contrast, viewing human-generated fact
checks enhances discernment in both cases. Subsequent analysis reveals that the
AI fact-checker is harmful in specific cases: it decreases beliefs in true
headlines that it mislabels as false and increases beliefs in false headlines
that it is unsure about. On the positive side, AI fact-checking information
increases the sharing intent for correctly labeled true headlines. When
participants are given the option to view LLM fact checks and choose to do so,
they are significantly more likely to share both true and false news but only
more likely to believe false headlines. Our findings highlight an important
source of potential harm stemming from AI applications and underscore the
critical need for policies to prevent or mitigate such unintended consequences.","['Matthew R. DeVerna', 'Harry Yaojun Yan', 'Kai-Cheng Yang', 'Filippo Menczer']",4,0.7306353
"With the rapid growth of online misinformation, it is crucial to have
reliable fact-checking methods. Recent research on finding check-worthy claims
and automated fact-checking have made significant advancements. However,
limited guidance exists regarding the presentation of fact-checked content to
effectively convey verified information to users. We address this research gap
by exploring the critical design elements in fact-checking reports and
investigating whether credibility and presentation-based design improvements
can enhance users' ability to interpret the report accurately. We co-developed
potential content presentation strategies through a workshop involving
fact-checking professionals, communication experts, and researchers. The
workshop examined the significance and utility of elements such as veracity
indicators and explored the feasibility of incorporating interactive components
for enhanced information disclosure. Building on the workshop outcomes, we
conducted an online experiment involving 76 crowd workers to assess the
efficacy of different design strategies. The results indicate that proposed
strategies significantly improve users' ability to accurately interpret the
verdict of fact-checking articles. Our findings underscore the critical role of
effective presentation of fact reports in addressing the spread of
misinformation. By adopting appropriate design enhancements, the effectiveness
of fact-checking reports can be maximized, enabling users to make informed
judgments.","['Danula Hettiachchi', 'Kaixin Ji', 'Jenny Kennedy', 'Anthony McCosker', 'Flora D. Salim', 'Mark Sanderson', 'Falk Scholer', 'Damiano Spina']",0,0.65260756
"Recent advancements in large language models have demonstrated remarkable
capabilities across various NLP tasks. But many questions remain, including
whether open-source models match closed ones, why these models excel or
struggle with certain tasks, and what types of practical procedures can improve
performance. We address these questions in the context of classification by
evaluating three classes of models using eight datasets across three distinct
tasks: named entity recognition, political party prediction, and misinformation
detection. While larger LLMs often lead to improved performance, open-source
models can rival their closed-source counterparts by fine-tuning. Moreover,
supervised smaller models, like RoBERTa, can achieve similar or even greater
performance in many datasets compared to generative LLMs. On the other hand,
closed models maintain an advantage in hard tasks that demand the most
generalizability. This study underscores the importance of model selection
based on task requirements","['Hao Yu', 'Zachary Yang', 'Kellin Pelrine', 'Jean Francois Godbout', 'Reihaneh Rabbany']",6,0.66721743
"The wide adoption of social media platforms has brought about numerous
benefits for communication and information sharing. However, it has also led to
the rapid spread of misinformation, causing significant harm to individuals,
communities, and society at large. Consequently, there has been a growing
interest in devising efficient and effective strategies to contain the spread
of misinformation. One popular countermeasure is blocking edges in the
underlying network.
  We model the spread of misinformation using the classical Independent Cascade
model and study the problem of minimizing the spread by blocking a given number
of edges. We prove that this problem is computationally hard, but we propose an
intuitive community-based algorithm, which aims to detect well-connected
communities in the network and disconnect the inter-community edges. Our
experiments on various real-world social networks demonstrate that the proposed
algorithm significantly outperforms the prior methods, which mostly rely on
centrality measures.","['Ahad N. Zehmakan', 'Khushvind Maurya']",2,0.7438557
"With the explosion of multimedia content, video moment retrieval (VMR), which
aims to detect a video moment that matches a given text query from a video, has
been studied intensively as a critical problem. However, the existing VMR
framework evaluates video moment retrieval performance, assuming that a video
is given, which may not reveal whether the models exhibit overconfidence in the
falsely given video. In this paper, we propose the MVMR (Massive Videos Moment
Retrieval for Faithfulness Evaluation) task that aims to retrieve video moments
within a massive video set, including multiple distractors, to evaluate the
faithfulness of VMR models. For this task, we suggest an automated massive
video pool construction framework to categorize negative (distractors) and
positive (false-negative) video sets using textual and visual semantic distance
verification methods. We extend existing VMR datasets using these methods and
newly construct three practical MVMR datasets. To solve the task, we further
propose a strong informative sample-weighted learning method, CroCs, which
employs two contrastive learning mechanisms: (1) weakly-supervised potential
negative learning and (2) cross-directional hard-negative learning.
Experimental results on the MVMR datasets reveal that existing VMR models are
easily distracted by the misinformation (distractors), whereas our model shows
significantly robust performance, demonstrating that CroCs is essential to
distinguishing positive moments against distractors. Our code and datasets are
publicly available: https://github.com/yny0506/Massive-Videos-Moment-Retrieval.","['Nakyeong Yang', 'Minsung Kim', 'Seunghyun Yoon', 'Joongbo Shin', 'Kyomin Jung']",6,0.63175416
"We present a new framework to conceptualize and operationalize the total user
experience of search, by studying the entirety of a search journey from an
utilitarian point of view.
  Web search engines are widely perceived as ""free"". But search requires time
and effort: in reality there are many intermingled non-monetary costs (e.g.
time costs, cognitive costs, interactivity costs) and the benefits may be
marred by various impairments, such as misunderstanding and misinformation.
This characterization of costs and benefits appears to be inherent to the human
search for information within the pursuit of some larger task: most of the
costs and impairments can be identified in interactions with any web search
engine, interactions with public libraries, and even in interactions with
ancient oracles. To emphasize this innate connection, we call these costs and
benefits Delphic, in contrast to explicitly financial costs and benefits.
  Our main thesis is that the users' satisfaction with a search engine mostly
depends on their experience of Delphic cost and benefits, in other words on
their utility. The consumer utility is correlated with classic measures of
search engine quality, such as ranking, precision, recall, etc., but is not
completely determined by them. To argue our thesis, we catalog the Delphic
costs and benefits and show how the development of search engines over the last
quarter century, from classic Information Retrieval roots to the integration of
Large Language Models, was driven to a great extent by the quest of decreasing
Delphic costs and increasing Delphic benefits.
  We hope that the Delphic costs framework will engender new ideas and new
research for evaluating and improving the web experience for everyone.","['Andrei Z. Broder', 'Preston McAfee']",0,0.5163989
"Large language models (LLMs) such as GPT-4, PaLM, and Llama have
significantly propelled the generation of AI-crafted text. With rising concerns
about their potential misuse, there is a pressing need for AI-generated-text
forensics. Neural authorship attribution is a forensic effort, seeking to trace
AI-generated text back to its originating LLM. The LLM landscape can be divided
into two primary categories: proprietary and open-source. In this work, we
delve into these emerging categories of LLMs, focusing on the nuances of neural
authorship attribution. To enrich our understanding, we carry out an empirical
analysis of LLM writing signatures, highlighting the contrasts between
proprietary and open-source models, and scrutinizing variations within each
group. By integrating stylometric features across lexical, syntactic, and
structural aspects of language, we explore their potential to yield
interpretable results and augment pre-trained language model-based classifiers
utilized in neural authorship attribution. Our findings, based on a range of
state-of-the-art LLMs, provide empirical insights into neural authorship
attribution, paving the way for future investigations aimed at mitigating the
threats posed by AI-generated misinformation.","['Tharindu Kumarage', 'Huan Liu']",6,0.77850735
"While many Natural Language Processing (NLP) techniques have been proposed
for fact-checking, both academic research and fact-checking organizations
report limited adoption of such NLP work due to poor alignment with
fact-checker practices, values, and needs. To address this, we investigate a
co-design method, Matchmaking for AI, to enable fact-checkers, designers, and
NLP researchers to collaboratively identify what fact-checker needs should be
addressed by technology, and to brainstorm ideas for potential solutions.
Co-design sessions we conducted with 22 professional fact-checkers yielded a
set of 11 design ideas that offer a ""north star"", integrating fact-checker
criteria into novel NLP design concepts. These concepts range from pre-bunking
misinformation, efficient and personalized monitoring misinformation,
proactively reducing fact-checker potential biases, and collaborative writing
fact-check reports. Our work provides new insights into both human-centered
fact-checking research and practice and AI co-design research.","['Houjiang Liu', 'Anubrata Das', 'Alexander Boltz', 'Didi Zhou', 'Daisy Pinaroc', 'Matthew Lease', 'Min Kyung Lee']",1,0.5643544
"Most prior and current research examining misinformation spread on social
media focuses on reports published by 'fake' news sources. These approaches
fail to capture another potential form of misinformation with a much larger
audience: factual news from mainstream sources ('real' news) repurposed to
promote false or misleading narratives. We operationalize narratives using an
existing unsupervised NLP technique and examine the narratives present in
misinformation content. We find that certain articles from reliable outlets are
shared by a disproportionate number of users who also shared fake news on
Twitter. We consider these 'real' news articles to be co-shared with fake news.
We show that co-shared articles contain existing misinformation narratives at a
significantly higher rate than articles from the same reliable outlets that are
not co-shared with fake news. This holds true even when articles are chosen
following strict criteria of reliability for the outlets and after accounting
for the alternative explanation of partisan curation of articles. For example,
we observe that a recent article published by The Washington Post titled
""Vaccinated people now make up a majority of COVID deaths"" was
disproportionately shared by Twitter users with a history of sharing
anti-vaccine false news reports. Our findings suggest a strategic repurposing
of mainstream news by conveyors of misinformation as a way to enhance the reach
and persuasiveness of misleading narratives. We also conduct a comprehensive
case study to help highlight how such repurposing can happen on Twitter as a
consequence of the inclusion of particular narratives in the framing of
mainstream news.","['Pranav Goel', 'Jon Green', 'David Lazer', 'Philip Resnik']",4,0.7854463
"Finding previously debunked narratives involves identifying claims that have
already undergone fact-checking. The issue intensifies when similar false
claims persist in multiple languages, despite the availability of debunks for
several months in another language. Hence, automatically finding debunks (or
fact-checks) in multiple languages is crucial to make the best use of scarce
fact-checkers' resources. Mainly due to the lack of readily available data,
this is an understudied problem, particularly when considering the
cross-lingual scenario, i.e. the retrieval of debunks in a language different
from the language of the online post being checked. This study introduces
cross-lingual debunked narrative retrieval and addresses this research gap by:
(i) creating Multilingual Misinformation Tweets (MMTweets): a dataset that
stands out, featuring cross-lingual pairs, images, human annotations, and
fine-grained labels, making it a comprehensive resource compared to its
counterparts; (ii) conducting an extensive experiment to benchmark
state-of-the-art cross-lingual retrieval models and introducing multistage
retrieval methods tailored for the task; and (iii) comprehensively evaluating
retrieval models for their cross-lingual and cross-dataset transfer
capabilities within MMTweets, and conducting a retrieval latency analysis. We
find that MMTweets presents challenges for cross-lingual debunked narrative
retrieval, highlighting areas for improvement in retrieval models. Nonetheless,
the study provides valuable insights for creating MMTweets datasets and
optimising debunked narrative retrieval models to empower fact-checking
endeavours. The dataset and annotation codebook are publicly available at
https://doi.org/10.5281/zenodo.10637161.","['Iknoor Singh', 'Carolina Scarton', 'Xingyi Song', 'Kalina Bontcheva']",8,0.83956826
"Fake news is fake material in a news media format but is not processed
properly by news agencies. The fake material can provoke or defame significant
entities or individuals or potentially even for the personal interests of the
creators, causing problems for society. Distinguishing fake news and real news
is challenging due to limited of domain knowledge and time constraints.
According to the survey, the top three areas most exposed to hoaxes and
misinformation by residents are in Banten, DKI Jakarta and West Java. The model
of transformers is referring to an approach in the field of artificial
intelligence (AI) in natural language processing utilizing the deep learning
architectures. Transformers exercise a powerful attention mechanism to process
text in parallel and produce rich and contextual word representations. A
previous study indicates a superior performance of a transformer model known as
BERT over and above non transformer approach. However, some studies suggest the
performance can be improved with the use of improved BERT models known as
ALBERT and RoBERTa. However, the modified BERT models are not well explored for
detecting fake news in Bahasa Indonesia. In this research, we explore those
transformer models and found that ALBERT outperformed other models with 87.6%
accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)
respectively. Source code available at:
https://github.com/Shafna81/fakenewsdetection.git","['Shafna Fitria Nur Azizah', 'Hasan Dwi Cahyono', 'Sari Widya Sihwi', 'Wisnu Widiarto']",4,0.73261607
"The massive volume of online information along with the issue of
misinformation has spurred active research in the automation of fact-checking.
Like fact-checking by human experts, it is not enough for an automated
fact-checker to just be accurate, but also be able to inform and convince the
user of the validity of its predictions. This becomes viable with explainable
artificial intelligence (XAI). In this work, we conduct a study of XAI
fact-checkers involving 180 participants to determine how users' actions
towards news and their attitudes towards explanations are affected by the XAI.
Our results suggest that XAI has limited effects on users' agreement with the
veracity prediction of the automated fact-checker and on their intent to share
news. However, XAI nudges users towards forming uniform judgments of news
veracity, thereby signaling their reliance on the explanations. We also found
polarizing preferences towards XAI and raise several design considerations on
them.","['Gionnieve Lim', 'Simon T. Perrault']",9,0.63935673
"Communication strongly influences attitudes on climate change. Within
sponsored communication, high spend and high reach advertising dominates. In
the advertising ecosystem we can distinguish actors with adversarial stances:
organizations with contrarian or advocacy communication goals, who direct the
advertisement delivery algorithm to launch ads in different destinations by
specifying targets and campaign objectives. We present an observational
(N=275,632) and a controlled (N=650) study which collectively indicate that the
advertising delivery algorithm could itself be an actor, asserting
statistically significant influence over advertisement destinations,
characterized by U.S. state, gender type, or age range. This algorithmic
behaviour may not entirely be understood by the advertising platform (and its
creators). These findings have implications for climate communications and
misinformation research, revealing that targeting intentions are not always
fulfilled as requested and that delivery itself could be manipulated.","['Aruna Sankaranarayanan', 'Erik Hemberg', ""Una-May O'Reilly""]",3,0.5621867
"This paper makes two key contributions. First, it argues that highly
specialized rare content classifiers trained on small data typically have
limited exposure to the richness and topical diversity of the negative class
(dubbed anticontent) as observed in the wild. As a result, these classifiers'
strong performance observed on the test set may not translate into real-world
settings. In the context of COVID-19 misinformation detection, we conduct an
in-the-wild audit of multiple datasets and demonstrate that models trained with
several prominently cited recent datasets are vulnerable to anticontent when
evaluated in the wild. Second, we present a novel active learning pipeline that
requires zero manual annotation and iteratively augments the training data with
challenging anticontent, robustifying these classifiers.","['Clay H. Yoo', 'Ashiqur R. KhudaBukhsh']",1,0.6439742
"We conducted ethnographic research with 31 misinformation creators and
consumers in Brazil and the US before, during, and after a major election to
understand the consumption and production of election and medical
misinformation. This study contributes to research on misinformation ecosystems
by focusing on poorly understood small players, or ""micro-influencers"", who
create misinformation in peer-to-peer networks. We detail four key tactics that
micro-influencers use. First, they typically disseminate ""gray area"" content
rather than expert-falsified claims, using subtle aesthetic and rhetorical
tactics to evade moderation. Second, they post in small, closed groups where
members feel safe and predisposed to trust content. Third, they explicitly
target misinformation consumers' emotional and social needs. Finally, they post
a high volume of short, repetitive content to plant seeds of doubt and build
trust in influencers as unofficial experts. We discuss the implications these
micro-influencers have for misinformation interventions and platforms' efforts
to moderate misinformation.","['Amelia Hassoun', 'Gabrielle Borenstein', 'Beth Goldberg', 'Jacob McAuliffe', 'Katy Osborn']",3,0.7198024
"Q&A platforms have been crucial for the online help-seeking behavior of
programmers. However, the recent popularity of ChatGPT is altering this trend.
Despite this popularity, no comprehensive study has been conducted to evaluate
the characteristics of ChatGPT's answers to programming questions. To bridge
the gap, we conducted the first in-depth analysis of ChatGPT answers to 517
programming questions on Stack Overflow and examined the correctness,
consistency, comprehensiveness, and conciseness of ChatGPT answers.
Furthermore, we conducted a large-scale linguistic analysis, as well as a user
study, to understand the characteristics of ChatGPT answers from linguistic and
human aspects. Our analysis shows that 52% of ChatGPT answers contain incorrect
information and 77% are verbose. Nonetheless, our user study participants still
preferred ChatGPT answers 35% of the time due to their comprehensiveness and
well-articulated language style. However, they also overlooked the
misinformation in the ChatGPT answers 39% of the time. This implies the need to
counter misinformation in ChatGPT answers to programming questions and raise
awareness of the risks associated with seemingly correct answers.","['Samia Kabir', 'David N. Udo-Imeh', 'Bonan Kou', 'Tianyi Zhang']",8,0.47185788
"Misinformation, propaganda, and outright lies proliferate on the web, with
some narratives having dangerous real-world consequences on public health,
elections, and individual safety. However, despite the impact of
misinformation, the research community largely lacks automated and programmatic
approaches for tracking news narratives across online platforms. In this work,
utilizing daily scrapes of 1,334 unreliable news websites, the large-language
model MPNet, and DP-Means clustering, we introduce a system to automatically
identify and track the narratives spread within online ecosystems. Identifying
52,036 narratives on these 1,334 websites, we describe the most prevalent
narratives spread in 2022 and identify the most influential websites that
originate and amplify narratives. Finally, we show how our system can be
utilized to detect new narratives originating from unreliable news websites and
to aid fact-checkers in more quickly addressing misinformation. We release code
and data at https://github.com/hanshanley/specious-sites.","['Hans W. A. Hanley', 'Deepak Kumar', 'Zakir Durumeric']",4,0.7471127
"Generative Artificial Intelligence (AI) has seen mainstream adoption lately,
especially in the form of consumer-facing, open-ended, text and image
generating models. However, the use of such systems raises significant ethical
and safety concerns, including privacy violations, misinformation and
intellectual property theft. The potential for generative AI to displace human
creativity and livelihoods has also been under intense scrutiny. To mitigate
these risks, there is an urgent need of policies and regulations responsible
and ethical development in the field of generative AI. Existing and proposed
centralized regulations by governments to rein in AI face criticisms such as
not having sufficient clarity or uniformity, lack of interoperability across
lines of jurisdictions, restricting innovation, and hindering free market
competition. Decentralized protections via crowdsourced safety tools and
mechanisms are a potential alternative. However, they have clear deficiencies
in terms of lack of adequacy of oversight and difficulty of enforcement of
ethical and safety standards, and are thus not enough by themselves as a
regulation mechanism. We propose a marriage of these two strategies via a
framework we call Dual Governance. This framework proposes a cooperative
synergy between centralized government regulations in a U.S. specific context
and safety mechanisms developed by the community to protect stakeholders from
the harms of generative AI. By implementing the Dual Governance framework, we
posit that innovation and creativity can be promoted while ensuring safe and
ethical deployment of generative AI.","['Avijit Ghosh', 'Dhanya Lakshmi']",9,0.73518443
"Existing communications and behavioral theories have been adopted to address
health misinformation. Although various theories and models have been used to
investigate the COVID-19 pandemic, there is no framework specially designed for
social listening or misinformation studies using social media data and natural
language processing techniques. This study aimed to propose a novel yet
theory-based conceptual framework for misinformation research. We collected
theories and models used in COVID-19 related studies published in peer-reviewed
journals. The theories and models ranged from health behaviors, communications,
to misinformation. They are analyzed and critiqued for their components,
followed by proposing a conceptual framework with a demonstration. We reviewed
Health Belief Model, Theory of Planned Behavior/Reasoned Action, Communication
for Behavioral Impact, Transtheoretical Model, Uses and Gratifications Theory,
Social Judgment Theory, Risk Information Seeking and Processing Model,
Behavioral and Social Drivers, and Hype Loop. Accordingly, we proposed the
Social Media Listening for Public Health Behavior Conceptual Framework by not
only integrating important attributes of existing theories, but also adding new
attributes. The proposed conceptual framework was demonstrated in the Freedom
Convoy social media listening. The proposed conceptual framework can be used to
better understand public discourse on social media, and it can be integrated
with other data analyses to gather a more comprehensive picture. The framework
will continue to be revised and adopted as health misinformation evolves.","['Shu-Feng Tsao', 'Helen Chen', 'Samantha Meyer', 'Zahid A. Butt']",0,0.59077555
"Recent advances in deep learning and computer vision have made the synthesis
and counterfeiting of multimedia content more accessible than ever, leading to
possible threats and dangers from malicious users. In the audio field, we are
witnessing the growth of speech deepfake generation techniques, which solicit
the development of synthetic speech detection algorithms to counter possible
mischievous uses such as frauds or identity thefts. In this paper, we consider
three different feature sets proposed in the literature for the synthetic
speech detection task and present a model that fuses them, achieving overall
better performances with respect to the state-of-the-art solutions. The system
was tested on different scenarios and datasets to prove its robustness to
anti-forensic attacks and its generalization capabilities.","['Daniele Mari', 'Davide Salvi', 'Paolo Bestagini', 'Simone Milani']",11,0.7941606
"The ongoing debate surrounding the impact of the Internet Research Agency s
(IRA) social media campaign during the 2016 U.S. presidential election has
largely overshadowed the involvement of other actors. Our analysis brings to
light a substantial group of suspended Twitter users, outnumbering the IRA user
group by a factor of 60, who align with the ideologies of the IRA campaign. Our
study demonstrates that this group of suspended Twitter accounts significantly
influenced individuals categorized as undecided or weak supporters, potentially
with the aim of swaying their opinions, as indicated by Granger causality.","['Matteo Serafino', 'Zhenkun Zhou', 'Jose S. Andrade, Jr.', 'Alexandre Bovet', 'Hernan A. Makse']",10,0.6644553
"An accurate and timely estimate of the reproduction ratio R of an infectious
disease epidemic is crucial to make projections on its evolution and set up the
appropriate public health response. Estimates of R routinely come from
statistical inference on timelines of cases or their proxies like symptomatic
cases, hospitalizatons, deaths. Here, however, we prove that these estimates of
R may not be accurate if the population is made up of spatially distinct
communities, as the interplay between space and mobility may hide the true
epidemic evolution from surveillance data. This means that surveillance may
underestimate R over long periods, to the point of mistaking a growing epidemic
for a subsiding one, misinforming public health response. To overcome this, we
propose a correction to be applied to surveillance data that removes this bias
and ensures an accurate estimate of R across all epidemic phases. We use
COVID-19 as case study; our results, however, apply to any epidemic where
mobility is a driver of circulation, including major challenges of the next
decades: respiratory infections (influenza, SARS-CoV-2, emerging pathogens),
vector-borne diseases (arboviruses). Our findings will help set up public
health response to these threats, by improving epidemic monitoring and
surveillance.","['Piero Birello', 'Michele Re Fiorentin', 'Boxuan Wang', 'Vittoria Colizza', 'Eugenio Valdano']",12,0.6207762
"The proliferation of misinformation and propaganda is a global challenge,
with profound effects during major crises such as the COVID-19 pandemic and the
Russian invasion of Ukraine. Understanding the spread of misinformation and its
social impacts requires identifying the news sources spreading false
information. While machine learning (ML) techniques have been proposed to
address this issue, ML models have failed to provide an efficient
implementation scenario that yields useful results. In prior research, the
precision of deployment in real traffic deteriorates significantly,
experiencing a decrement up to ten times compared to the results derived from
benchmark data sets. Our research addresses this gap by proposing a graph-based
approach to capture navigational patterns and generate traffic-based features
which are used to train a classification model. These navigational and
traffic-based features result in classifiers that present outstanding
performance when evaluated against real traffic. Moreover, we also propose
graph-based filtering techniques to filter out models to be classified by our
framework. These filtering techniques increase the signal-to-noise ratio of the
models to be classified, greatly reducing false positives and the computational
cost of deploying the model. Our proposed framework for the detection of
misinformation domains achieves a precision of 0.78 when evaluated in real
traffic. This outcome represents an improvement factor of over ten times over
those achieved in previous studies.","['Mayana Pereira', 'Kevin Greene', 'Nilima Pisharody', 'Rahul Dodhia', 'Jacob N. Shapiro', 'Juan Lavista']",2,0.7203336
"The popularity of online social networks has enabled rapid dissemination of
information. People now can share and consume information much more rapidly
than ever before. However, low-quality and/or accidentally/deliberately fake
information can also spread rapidly. This can lead to considerable and negative
impacts on society. Identifying, labelling and debunking online misinformation
as early as possible has become an increasingly urgent problem. Many methods
have been proposed to detect fake news including many deep learning and
graph-based approaches. In recent years, graph-based methods have yielded
strong results, as they can closely model the social context and propagation
process of online news. In this paper, we present a systematic review of fake
news detection studies based on graph-based and deep learning-based techniques.
We classify existing graph-based methods into knowledge-driven methods,
propagation-based methods, and heterogeneous social context-based methods,
depending on how a graph structure is constructed to model news related
information flows. We further discuss the challenges and open problems in
graph-based fake news detection and identify future research directions.","['Shuzhi Gong', 'Richard O. Sinnott', 'Jianzhong Qi', 'Cecile Paris']",0,0.74142915
"News consumption has significantly increased with the growing popularity and
use of web-based forums and social media. This sets the stage for misinforming
and confusing people. To help reduce the impact of misinformation on users'
potential health-related decisions and other intents, it is desired to have
machine learning models to detect and combat fake news automatically. This
paper proposes a novel transformer-based model using Capsule neural
Networks(CapsNet) called X-CapsNet. This model includes a CapsNet with dynamic
routing algorithm paralyzed with a size-based classifier for detecting short
and long fake news statements. We use two size-based classifiers, a Deep
Convolutional Neural Network (DCNN) for detecting long fake news statements and
a Multi-Layer Perceptron (MLP) for detecting short news statements. To resolve
the problem of representing short news statements, we use indirect features of
news created by concatenating the vector of news speaker profiles and a vector
of polarity, sentiment, and counting words of news statements. For evaluating
the proposed architecture, we use the Covid-19 and the Liar datasets. The
results in terms of the F1-score for the Covid-19 dataset and accuracy for the
Liar dataset show that models perform better than the state-of-the-art
baselines.","['Mohammad Hadi Goldani', 'Reza Safabakhsh', 'Saeedeh Momtazi']",4,0.7368243
"Misinformation on YouTube is a significant concern, necessitating robust
detection strategies. In this paper, we introduce a novel methodology for video
classification, focusing on the veracity of the content. We convert the
conventional video classification task into a text classification task by
leveraging the textual content derived from the video transcripts. We employ
advanced machine learning techniques like transfer learning to solve the
classification challenge. Our approach incorporates two forms of transfer
learning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and
ELECTRA, and (b) few-shot learning using sentence-transformers MPNet and
RoBERTa-large. We apply the trained models to three datasets: (a) YouTube
Vaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and
(c) Fake-News dataset (a collection of articles). Including the Fake-News
dataset extended the evaluation of our approach beyond YouTube videos. Using
these datasets, we evaluated the models distinguishing valid information from
misinformation. The fine-tuned models yielded Matthews Correlation
Coefficient>0.81, accuracy>0.90, and F1 score>0.90 in two of three datasets.
Interestingly, the few-shot models outperformed the fine-tuned ones by 20% in
both Accuracy and F1 score for the YouTube Pseudoscience dataset, highlighting
the potential utility of this approach -- especially in the context of limited
training data.","['Christos Christodoulou', 'Nikos Salamanos', 'Pantelitsa Leonidou', 'Michail Papadakis', 'Michael Sirivianos']",1,0.67154586
"Check-worthy claim detection aims at providing plausible misinformation to
downstream fact-checking systems or human experts to check. This is a crucial
step toward accelerating the fact-checking process. Many efforts have been put
into how to identify check-worthy claims from a small scale of pre-collected
claims, but how to efficiently detect check-worthy claims directly from a
large-scale information source, such as Twitter, remains underexplored. To fill
this gap, we introduce MythQA, a new multi-answer open-domain question
answering(QA) task that involves contradictory stance mining for query-based
large-scale check-worthy claim detection. The idea behind this is that
contradictory claims are a strong indicator of misinformation that merits
scrutiny by the appropriate authorities. To study this task, we construct
TweetMythQA, an evaluation dataset containing 522 factoid multi-answer
questions based on controversial topics. Each question is annotated with
multiple answers. Moreover, we collect relevant tweets for each distinct
answer, then classify them into three categories: ""Supporting"", ""Refuting"", and
""Neutral"". In total, we annotated 5.3K tweets. Contradictory evidence is
collected for all answers in the dataset. Finally, we present a baseline system
for MythQA and evaluate existing NLP models for each system component using the
TweetMythQA dataset. We provide initial benchmarks and identify key challenges
for future models to improve upon. Code and data are available at:
https://github.com/TonyBY/Myth-QA","['Yang Bai', 'Anthony Colas', 'Daisy Zhe Wang']",1,0.7417977
"Social media manipulation poses a significant threat to cognitive autonomy
and unbiased opinion formation. Prior literature explored the relationship
between online activity and emotional state, cognitive resources, sunlight and
weather. However, a limited understanding exists regarding the role of time of
day in content spread and the impact of user activity patterns on
susceptibility to mis- and disinformation. This work uncovers a strong
correlation between user activity patterns and the tendency to spread
manipulated content. Through quantitative analysis of Twitter data, we examine
how user activity throughout the day aligns with chronotypical archetypes.
Evening types exhibit a significantly higher inclination towards spreading
potentially manipulated content, which is generally more likely between 2:30 AM
and 4:15 AM. This knowledge can become crucial for developing targeted
interventions and strategies that mitigate misinformation spread by addressing
vulnerable periods and user groups more susceptible to manipulation.","['Elisabeth Stockinger', 'Riccardo Gallotti', 'Carina I. Hausladen']",3,0.7165974
"Social media has enabled the spread of information at unprecedented speeds
and scales, and with it the proliferation of high-engagement, low-quality
content. *Friction* -- behavioral design measures that make the sharing of
content more cumbersome -- might be a way to raise the quality of what is
spread online. Here, we study the effects of friction with and without
quality-recognition learning. Experiments from an agent-based model suggest
that friction alone decreases the number of posts without improving their
quality. A small amount of friction combined with learning, however, increases
the average quality of posts significantly. Based on this preliminary evidence,
we propose a friction intervention with a learning component about the
platform's community standards, to be tested via a field experiment. The
proposed intervention would have minimal effects on engagement and may easily
be deployed at scale.","['Laura Jahn', 'Rasmus K. Rendsvig', 'Alessandro Flammini', 'Filippo Menczer', 'Vincent F. Hendricks']",2,0.6085702
"The remarkable capabilities of large-scale language models, such as ChatGPT,
in text generation have impressed readers and spurred researchers to devise
detectors to mitigate potential risks, including misinformation, phishing, and
academic dishonesty. Despite this, most previous studies have been
predominantly geared towards creating detectors that differentiate between
purely ChatGPT-generated texts and human-authored texts. This approach,
however, fails to work on discerning texts generated through human-machine
collaboration, such as ChatGPT-polished texts. Addressing this gap, we
introduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts),
facilitating the construction of more robust detectors. It diverges from extant
corpora by comprising pairs of human-written and ChatGPT-polished abstracts
instead of purely ChatGPT-generated texts. Additionally, we propose the ""Polish
Ratio"" method, an innovative measure of the degree of modification made by
ChatGPT compared to the original human-written text. It provides a mechanism to
measure the degree of ChatGPT influence in the resulting text. Our experimental
results show our proposed model has better robustness on the HPPT dataset and
two existing datasets (HC3 and CDB). Furthermore, the ""Polish Ratio"" we
proposed offers a more comprehensive explanation by quantifying the degree of
ChatGPT involvement.","['Lingyi Yang', 'Feng Jiang', 'Haizhou Li']",8,0.5793873
"Narrative is a foundation of human cognition and decision making. Because
narratives play a crucial role in societal discourses and spread of
misinformation and because of the pervasive use of social media, the narrative
dynamics on social media can have profound societal impact. Yet, systematic and
computational understanding of online narratives faces critical challenge of
the scale and dynamics; how can we reliably and automatically extract
narratives from massive amount of texts? How do narratives emerge, spread, and
die? Here, we propose a systematic narrative discovery framework that fill this
gap by combining change point detection, semantic role labeling (SRL), and
automatic aggregation of narrative fragments into narrative networks. We
evaluate our model with synthetic and empirical data two-Twitter corpora about
COVID-19 and 2017 French Election. Results demonstrate that our approach can
recover major narrative shifts that correspond to the major events.","['Wanying Zhao', 'Siyi Guo', 'Kristina Lerman', 'Yong-Yeol Ahn']",3,0.65907574
"The recent release of very large language models such as PaLM and GPT-4 has
made an unprecedented impact in the popular media and public consciousness,
giving rise to a mixture of excitement and fear as to their capabilities and
potential uses, and shining a light on natural language processing research
which had not previously received so much attention. The developments offer
great promise for education technology, and in this paper we look specifically
at the potential for incorporating large language models in AI-driven language
teaching and assessment systems. We consider several research areas and also
discuss the risks and ethical considerations surrounding generative AI in
education technology for language learners. Overall we find that larger
language models offer improvements over previous models in text generation,
opening up routes toward content generation which had not previously been
plausible. For text generation they must be prompted carefully and their
outputs may need to be reshaped before they are ready for use. For automated
grading and grammatical error correction, tasks whose progress is checked on
well-known benchmarks, early investigations indicate that large language models
on their own do not improve on state-of-the-art results according to standard
evaluation metrics. For grading it appears that linguistic features established
in the literature should still be used for best performance, and for error
correction it may be that the models can offer alternative feedback styles
which are not measured sensitively with existing methods. In all cases, there
is work to be done to experiment with the inclusion of large language models in
education technology for language learners, in order to properly understand and
report on their capacities and limitations, and to ensure that foreseeable
risks such as misinformation and harmful bias are mitigated.","['Andrew Caines', 'Luca Benedetto', 'Shiva Taslimipoor', 'Christopher Davis', 'Yuan Gao', 'Oeistein Andersen', 'Zheng Yuan', 'Mark Elliott', 'Russell Moore', 'Christopher Bryant', 'Marek Rei', 'Helen Yannakoudakis', 'Andrew Mullooly', 'Diane Nicholls', 'Paula Buttery']",8,0.7391525
"The recent performance leap of Large Language Models (LLMs) opens up new
opportunities across numerous industrial applications and domains. However,
erroneous generations, such as false predictions, misinformation, and
hallucination made by LLMs, have also raised severe concerns for the
trustworthiness of LLMs', especially in safety-, security- and
reliability-sensitive scenarios, potentially hindering real-world adoptions.
While uncertainty estimation has shown its potential for interpreting the
prediction risks made by general machine learning (ML) models, little is known
about whether and to what extent it can help explore an LLM's capabilities and
counteract its undesired behavior. To bridge the gap, in this paper, we
initiate an exploratory study on the risk assessment of LLMs from the lens of
uncertainty. In particular, we experiment with twelve uncertainty estimation
methods and four LLMs on four prominent natural language processing (NLP) tasks
to investigate to what extent uncertainty estimation techniques could help
characterize the prediction risks of LLMs. Our findings validate the
effectiveness of uncertainty estimation for revealing LLMs'
uncertain/non-factual predictions. In addition to general NLP tasks, we
extensively conduct experiments with four LLMs for code generation on two
datasets. We find that uncertainty estimation can potentially uncover buggy
programs generated by LLMs. Insights from our study shed light on future design
and development for reliable LLMs, facilitating further research toward
enhancing the trustworthiness of LLMs.","['Yuheng Huang', 'Jiayang Song', 'Zhijie Wang', 'Shengming Zhao', 'Huaming Chen', 'Felix Juefei-Xu', 'Lei Ma']",6,0.7811717
"Developing interventions that successfully reduce engagement with
misinformation on social media is challenging. One intervention that has
recently gained great attention is X/Twitter's Community Notes (previously
known as ""Birdwatch""). Community Notes is a crowdsourced fact-checking approach
that allows users to write textual notes to inform others about potentially
misleading posts on X/Twitter. Yet, empirical evidence regarding its
effectiveness in reducing engagement with misinformation on social media is
missing. In this paper, we perform a large-scale empirical study to analyze
whether the introduction of the Community Notes feature and its roll-out to
users in the U.S. and around the world have reduced engagement with
misinformation on X/Twitter in terms of retweet volume and likes. We employ
Difference-in-Differences (DiD) models and Regression Discontinuity Design
(RDD) to analyze a comprehensive dataset consisting of all fact-checking notes
and corresponding source tweets since the launch of Community Notes in early
2021. Although we observe a significant increase in the volume of fact-checks
carried out via Community Notes, particularly for tweets from verified users
with many followers, we find no evidence that the introduction of Community
Notes significantly reduced engagement with misleading tweets on X/Twitter.
Rather, our findings suggest that Community Notes might be too slow to
effectively reduce engagement with misinformation in the early (and most viral)
stage of diffusion. Our work emphasizes the importance of evaluating
fact-checking interventions in the field and offers important implications to
enhance crowdsourced fact-checking strategies on social media.","['Yuwei Chuai', 'Haoye Tian', 'Nicolas Pr√∂llochs', 'Gabriele Lenzini']",3,0.75295234
"Cloud providers' support for network evasion techniques that misrepresent the
server's domain name is more prevalent than previously believed, which has
serious implications for security and privacy due to the reliance on domain
names in common security architectures. Domain fronting is one such evasive
technique used by privacy enhancing technologies and malware to hide the
domains they visit, and it uses shared hosting and HTTPS to present a benign
domain to observers while signaling the target domain in the encrypted HTTP
request. In this paper, we construct an ontology of domain name misinformation
and detail a novel measurement methodology to identify support among cloud
infrastructure providers. Despite several of the largest cloud providers having
publicly stated that they no longer support domain fronting, our findings
demonstrate a more complex environment with many exceptions.
  We also present a novel and straightforward attack that allows an adversary
to man-in-the-middle all the victim's encrypted traffic bound to a content
delivery network that supports domain fronting, breaking the authenticity,
confidentiality, and integrity guarantees expected by the victim when using
HTTPS. By using dynamic linker hijacking to rewrite the HTTP Host field, our
attack does not generate any artifacts that are visible to the victim or
passive network monitoring solutions, and the attacker does not need a separate
channel to exfiltrate data or perform command-and-control, which can be
achieved by rewriting HTTP headers.","['Blake Anderson', 'David McGrew']",11,0.5172878
"Recent developments in natural language processing have demonstrated the
potential of large language models (LLMs) to improve a range of educational and
learning outcomes. Of recent chatbots based on LLMs, ChatGPT and Bard have made
it clear that artificial intelligence (AI) technology will have significant
implications on the way we obtain and search for information. However, these
tools sometimes produce text that is convincing, but often incorrect, known as
hallucinations. As such, their use can distort scientific facts and spread
misinformation. To counter polarizing responses on these tools, it is critical
to provide an overview of such responses so stakeholders can determine which
topics tend to produce more contentious responses -- key to developing targeted
regulatory policy and interventions. In addition, there currently exists no
annotated dataset of ChatGPT and Bard responses around possibly polarizing
topics, central to the above aims. We address the indicated issues through the
following contribution: Focusing on highly polarizing topics in the US, we
created and described a dataset of ChatGPT and Bard responses. Broadly, our
results indicated a left-leaning bias for both ChatGPT and Bard, with Bard more
likely to provide responses around polarizing topics. Bard seemed to have fewer
guardrails around controversial topics, and appeared more willing to provide
comprehensive, and somewhat human-like responses. Bard may thus be more likely
abused by malicious actors. Stakeholders may utilize our findings to mitigate
misinformative and/or polarizing responses from LLMs","['Abhay Goyal', 'Muhammad Siddique', 'Nimay Parekh', 'Zach Schwitzky', 'Clara Broekaert', 'Connor Michelotti', 'Allie Wong', 'Lam Yin Cheung', 'Robin O Hanlon', 'Lam Yin Cheung', 'Munmun De Choudhury', 'Roy Ka-Wei Lee', 'Navin Kumar']",3,0.6144268
"Audio has become an increasingly crucial biometric modality due to its
ability to provide an intuitive way for humans to interact with machines. It is
currently being used for a range of applications, including person
authentication to banking to virtual assistants. Research has shown that these
systems are also susceptible to spoofing and attacks. Therefore, protecting
audio processing systems against fraudulent activities, such as identity theft,
financial fraud, and spreading misinformation, is of paramount importance. This
paper reviews the current state-of-the-art techniques for detecting audio
spoofing and discusses the current challenges along with open research
problems. The paper further highlights the importance of considering the
ethical and privacy implications of audio spoofing detection systems. Lastly,
the work aims to accentuate the need for building more robust and generalizable
methods, the integration of automatic speaker verification and countermeasure
systems, and better evaluation protocols.","['Rishabh Ranjan', 'Mayank Vatsa', 'Richa Singh']",11,0.73196715
"Conspiracy Theory Identication task is a new shared task proposed for the
first time at the Evalita 2023. The ACTI challenge, based exclusively on
comments published on conspiratorial channels of telegram, is divided into two
subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial
content and (ii) Conspiratorial Category Classification about specific
conspiracy theory classification. A total of fifteen teams participated in the
task for a total of 81 submissions. We illustrate the best performing
approaches were based on the utilization of large language models. We finally
draw conclusions about the utilization of these models for counteracting the
spreading of misinformation in online platforms.","['Giuseppe Russo', 'Niklas Stoehr', 'Manoel Horta Ribeiro']",3,0.5564252
"Improving healthcare quality and access remains a critical concern for
countries worldwide. Consequently, the rise of large language models (LLMs) has
erupted a wealth of discussion around healthcare applications among researchers
and consumers alike. While the ability of these models to pass medical exams
has been used to argue in favour of their use in medical training and
diagnosis, the impact of their inevitable use as a self-diagnostic tool and
their role in spreading healthcare misinformation has not been evaluated. In
this work, we critically evaluate LLMs' capabilities from the lens of a general
user self-diagnosing, as well as the means through which LLMs may aid in the
spread of medical misinformation. To accomplish this, we develop a testing
methodology which can be used to evaluate responses to open-ended questions
mimicking real-world use cases. In doing so, we reveal that a) these models
perform worse than previously known, and b) they exhibit peculiar behaviours,
including overconfidence when stating incorrect recommendations, which
increases the risk of spreading medical misinformation.","['Francois Barnard', 'Marlize Van Sittert', 'Sirisha Rambhatla']",6,0.6536333
"This monograph provides an overview of the mathematical theories and
computational algorithm design for contagion source detection in large
networks. By leveraging network centrality as a tool for statistical inference,
we can accurately identify the source of contagions, trace their spread, and
predict future trajectories. This approach provides fundamental insights into
surveillance capability and asymptotic behavior of contagion spreading in
networks. Mathematical theory and computational algorithms are vital to
understanding contagion dynamics, improving surveillance capabilities, and
developing effective strategies to prevent the spread of infectious diseases
and misinformation.","['Chee Wei Tan', 'Pei-Duo Yu']",2,0.66704094
"With social media, the flow of uncertified information is constantly
increasing, with the risk that more people will trust low-credible information
sources. To design effective strategies against this phenomenon, it is of
paramount importance to understand how people end up believing one source
rather than another. To this end, we propose a realistic and cognitively
affordable heuristic mechanism for opinion formation inspired by the well-known
belief propagation algorithm. In our model, an individual observing a network
of information sources must infer which of them are reliable and which are not.
We study how the individual's ability to identify credible sources, and hence
to form correct opinions, is affected by the noise in the system, intended as
the amount of disorder in the relationships between the information sources in
the network. We find numerically and analytically that there is a critical
noise level above which it is impossible for the individual to detect the
nature of the sources. Moreover, by comparing our opinion formation model with
existing ones in the literature, we show under what conditions people's
opinions can be reliable. Overall, our findings imply that the increasing
complexity of the information environment is a catalyst for misinformation
channels.","['Enrico Maria Fenoaltea', 'Alejandro Lage-Castellanos']",0,0.71693885
"Generative AI has made significant strides, yet concerns about the accuracy
and reliability of its outputs continue to grow. Such inaccuracies can have
serious consequences such as inaccurate decision-making, the spread of false
information, privacy violations, legal liabilities, and more. Although efforts
to address these risks are underway, including explainable AI and responsible
AI practices such as transparency, privacy protection, bias mitigation, and
social and environmental responsibility, misinformation caused by generative AI
will remain a significant challenge. We propose that verifying the outputs of
generative AI from a data management perspective is an emerging issue for
generative AI. This involves analyzing the underlying data from multi-modal
data lakes, including text files, tables, and knowledge graphs, and assessing
its quality and consistency. By doing so, we can establish a stronger
foundation for evaluating the outputs of generative AI models. Such an approach
can ensure the correctness of generative AI, promote transparency, and enable
decision-making with greater confidence. Our vision is to promote the
development of verifiable generative AI and contribute to a more trustworthy
and responsible use of AI.","['Nan Tang', 'Chenyu Yang', 'Ju Fan', 'Lei Cao', 'Yuyu Luo', 'Alon Halevy']",9,0.8185843
"Classification algorithms are increasingly used in areas such as housing,
credit, and law enforcement in order to make decisions affecting peoples'
lives. These algorithms can change individual behavior deliberately (a fraud
prediction algorithm deterring fraud) or inadvertently (content sorting
algorithms spreading misinformation), and they are increasingly facing public
scrutiny and regulation. Some of these regulations, like the elimination of
cash bail in some states, have focused on \textit{lowering the stakes of
certain classifications}. In this paper we characterize how optimal
classification by an algorithm designer can affect the distribution of behavior
in a population -- sometimes in surprising ways. We then look at the effect of
democratizing the rewards and punishments, or stakes, to algorithmic
classification to consider how a society can potentially stem (or facilitate!)
predatory classification. Our results speak to questions of algorithmic
fairness in settings where behavior and algorithms are interdependent, and
where typical measures of fairness focusing on statistical accuracy across
groups may not be appropriate.","['Elizabeth Maggie Penn', 'John W. Patty']",2,0.59693503
"We consider misinformation games, i.e., multi-agent interactions where the
players are misinformed with regards to the game that they play, essentially
having an \emph{incorrect} understanding of the game setting, without being
aware of their misinformation. In this paper, we introduce and study a new
family of misinformation games, called Noisy games, where misinformation is due
to structured (white) noise that affects additively the payoff values of
players. We analyse the general properties of Noisy games and derive
theoretical formulas related to ``behavioural consistency'', i.e., the
probability that the players behaviour will not be significantly affected by
the noise. We show several properties of these formulas, and present an
experimental evaluation that validates and visualises these results.","['Constantinos Varsos', 'Giorgos Flouris', 'Marina Bitsaki']",0,0.52547735
"The recent advances in natural language processing (NLP), have led to a new
trend of applying large language models (LLMs) to real-world scenarios. While
the latest LLMs are astonishingly fluent when interacting with humans, they
suffer from the misinformation problem by unintentionally generating factually
false statements. This can lead to harmful consequences, especially when
produced within sensitive contexts, such as healthcare. Yet few previous works
have focused on evaluating misinformation in the long-form (LF) generation of
LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have
been shown to perform well in different languages, misinformation evaluation
has been mostly conducted in English. To this end, we present a benchmark,
CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,
specifically the maternity and infant care domain; and 2) a language other than
English, namely Chinese. Most importantly, we provide an innovative paradigm
for building LF generation evaluation benchmarks that can be transferred to
other knowledge-intensive domains and low-resourced languages. Our proposed
benchmark fills the gap between the extensive usage of LLMs and the lack of
datasets for assessing the misinformation generated by these models. It
contains 1,612 expert-checked questions, accompanied with human-selected
references. Using our benchmark, we conduct extensive experiments and found
that current Chinese LLMs are far from perfect in the topic of maternity and
infant care. In an effort to minimize the reliance on human resources for
performance evaluation, we offer off-the-shelf judgment models for
automatically assessing the LF output of LLMs given benchmark questions.
Moreover, we compare potential solutions for LF generation evaluation and
provide insights for building better automated metrics.","['Tong Xiang', 'Liangzhi Li', 'Wangyue Li', 'Mingbai Bai', 'Lu Wei', 'Bowen Wang', 'Noa Garcia']",6,0.82320744
"Social media use data is widely being used in health, psychology, and
marketing research to analyze human behavior. However, we have very limited
knowledge on social media use among American Indians. In this context, this
study was designed to assess preferences and perceptions of social media use
among American Indians during COVID-19. We collected data from American Indians
in South Dakota using online survey. Results show that Facebook, YouTube,
TikTok, Instagram and Snapchat are the most preferred social media platforms.
Most of the participants reported that the use of social media increased
tremendously during COVID-19 and had perceptions of more negative effects than
positive effects. Hate/harassment/extremism, misinformation/made up news, and
people getting one point of view were the top reasons for negative effects.","['Deepthi Kolady', 'Amrit Dumre', 'Weiwei Zhang', 'Kaiqun Fu', ""Marcia O'Leary"", 'Laura Rose']",3,0.6555871
"The advent of ChatGPT, a large language model-powered chatbot, has prompted
questions about its potential implications for traditional search engines. In
this study, we investigate the differences in user behavior when employing
search engines and chatbot tools for information-seeking tasks. We carry out a
randomized online experiment, dividing participants into two groups: one using
a ChatGPT-like tool and the other using a Google Search-like tool. Our findings
reveal that the ChatGPT group consistently spends less time on all tasks, with
no significant difference in overall task performance between the groups.
Notably, ChatGPT levels user search performance across different education
levels and excels in answering straightforward questions and providing general
solutions but falls short in fact-checking tasks. Users perceive ChatGPT's
responses as having higher information quality compared to Google Search,
despite displaying a similar level of trust in both tools. Furthermore,
participants using ChatGPT report significantly better user experiences in
terms of usefulness, enjoyment, and satisfaction, while perceived ease of use
remains comparable between the two tools. However, ChatGPT may also lead to
overreliance and generate or replicate misinformation, yielding inconsistent
results. Our study offers valuable insights for search engine management and
highlights opportunities for integrating chatbot technologies into search
engine designs.","['Ruiyun Xu', 'Yue Feng', 'Hailiang Chen']",0,0.5208391
"Opinion dynamics is an important and very active area of research that delves
into the complex processes through which individuals form and modify their
opinions within a social context. The ability to comprehend and unravel the
mechanisms that drive opinion formation is of great significance for predicting
a wide range of social phenomena such as political polarization, the diffusion
of misinformation, the formation of public consensus, and the emergence of
collective behaviors. In this paper, we aim to contribute to that field by
introducing a novel mathematical model that specifically accounts for the
influence of social media networks on opinion dynamics. With the rise of
platforms such as Twitter, Facebook, and Instagram and many others, social
networks have become significant arenas where opinions are shared, discussed,
and potentially altered. To this aim after an analytical construction of our
new model and through incorporation of real-life data from Twitter, we
calibrate the model parameters to accurately reflect the dynamics that unfold
in social media, showing in particular the role played by the so-called
influencers in driving individual opinions towards predetermined directions.","['Giacomo Albi', 'Elisa Calzola', 'Giacomo Dimarco']",0,0.6871077
"The emergence of new public forums in the shape of online social media has
introduced unprecedented challenges to public discourse, including
polarization, misinformation, and the emergence of echo chambers. While
existing research has extensively studied the behavior of active users within
echo chambers, little attention has been given to the hidden audience, also
known as lurkers, who passively consume content without actively engaging. This
study aims to estimate the share of the hidden audience and investigate their
interplay with the echo chamber effect. Using Twitter as a case study, we
analyze a polarized political debate to understand the engagement patterns and
factors influencing the hidden audience's presence. Our findings reveal a
relevant fraction of users that consume content without active interaction,
which underscores the importance of considering their presence in online
debates. Notably, our results indicate that the engagement of the hidden
audience is primarily influenced by factors such as the reliability of media
sources mentioned in tweets rather than the ideological stance of the user that
produced the content. These findings highlight the need for a comprehensive
understanding of the hidden audience's role in online debates and how they may
influence public opinion.","['Anees Baqir', 'Yijing Chen', 'Fernando Diaz-Diaz', 'Sercan Kiyak', 'Thomas Louf', 'Virginia Morini', 'Valentina Pansanella', 'Maddalena Torricelli', 'Alessandro Galeazzi']",3,0.7908075
"As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.","['Zijian Cai', 'Zhaoxuan Tan', 'Zhenyu Lei', 'Zifeng Zhu', 'Hongrui Wang', 'Qinghua Zheng', 'Minnan Luo']",13,0.739496
"Information disorders on social media can have a significant impact on
citizens' participation in democratic processes. To better understand the
spread of false and inaccurate information online, this research analyzed data
from Twitter, Facebook, and Instagram. The data was collected and verified by
professional fact-checkers in Chile between October 2019 and October 2021, a
period marked by political and health crises. The study found that false
information spreads faster and reaches more users than true information on
Twitter and Facebook. Instagram, on the other hand, seemed to be less affected
by this phenomenon. False information was also more likely to be shared by
users with lower reading comprehension skills. True information, on the other
hand, tended to be less verbose and generate less interest among audiences.
This research provides valuable insights into the characteristics of
misinformation and how it spreads online. By recognizing the patterns of how
false information diffuses and how users interact with it, we can identify the
circumstances in which false and inaccurate messages are prone to become
widespread. This knowledge can help us develop strategies to counter the spread
of misinformation and protect the integrity of democratic processes.","['Marcelo Mendoza', 'Sebasti√°n Valenzuela', 'Enrique N√∫√±ez-Mussa', 'Fabi√°n Padilla', 'Eliana Providel', 'Sebasti√°n Campos', 'Renato Bassi', 'Andrea Riquelme', 'Valeria Aldana', 'Claudia L√≥pez']",3,0.74726427
"The COVID-19 Infodemic had an unprecedented impact on health behaviors and
outcomes at a global scale. While many studies have focused on a qualitative
and quantitative understanding of misinformation, including sentiment analysis,
there is a gap in understanding the emotion-carriers of misinformation and
their differences across geographies. In this study, we characterized emotion
carriers and their impact on vaccination rates in India and the United States.
A manually labelled dataset was created from 2.3 million tweets and collated
with three publicly available datasets (CoAID, AntiVax, CMU) to train deep
learning models for misinformation classification. Misinformation labelled
tweets were further analyzed for behavioral aspects by leveraging Plutchik
Transformers to determine the emotion for each tweet. Time series analysis was
conducted to study the impact of misinformation on spatial and temporal
characteristics. Further, categorical classification was performed using
transformer models to assign categories for the misinformation tweets.
Word2Vec+BiLSTM was the best model for misinformation classification, with an
F1-score of 0.92. The US had the highest proportion of misinformation tweets
(58.02%), followed by the UK (10.38%) and India (7.33%). Disgust, anticipation,
and anger were associated with an increased prevalence of misinformation
tweets. Disgust was the predominant emotion associated with misinformation
tweets in the US, while anticipation was the predominant emotion in India. For
India, the misinformation rate exhibited a lead relationship with vaccination,
while in the US it lagged behind vaccination. Our study deciphered that
emotions acted as differential carriers of misinformation across geography and
time. These carriers can be monitored to develop strategic interventions for
countering misinformation, leading to improved public health.","['Ridam Pal', 'Sanjana S', 'Deepak Mahto', 'Kriti Agrawal', 'Gopal Mengi', 'Sargun Nagpal', 'Akshaya Devadiga', 'Tavpritesh Sethi']",5,0.7541465
"Social media platforms have facilitated the rapid spread of dis- and
mis-information. Parler, a US-based fringe social media platform that positions
itself as a champion of free-speech, has had substantial information integrity
issues. In this study, we seek to characterize temporal misinformation trends
on Parler. Comparing a dataset of 189 million posts and comments from Parler
against 1591 rated claims (false, barely true, half true, mostly true, pants on
fire, true) from Politifact, we identified 231,881 accuracy-labeled posts on
Parler. We used BERT-Topic to thematically analyze the Poltifact claims, and
then compared trends in these categories to real world events to contextualize
their distribution. We identified three distinct categories of misinformation
circulating on Parler: COVID-19, the 2020 presidential election, and the Black
Lives Matter movement. Our results are significant, with a surprising 69.2% of
posts in our dataset found to be 'false' and 7.6% 'barely true'. We also found
that when Parler posts ('parleys') containing misinformation were posted
increased around major events (e.g., George Floyd's murder).","['Eliana Norton', 'Tha√Øs Thomas', 'Akaash Kolluri', 'Torie Hyunsik Kim', 'Dhiraj Murthy']",3,0.6912436
"Generative Artificial Intelligence (GenAI) has emerged as a powerful
technology capable of autonomously producing highly realistic content in
various domains, such as text, images, audio, and videos. With its potential
for positive applications in creative arts, content generation, virtual
assistants, and data synthesis, GenAI has garnered significant attention and
adoption. However, the increasing adoption of GenAI raises concerns about its
potential misuse for crafting convincing phishing emails, generating
disinformation through deepfake videos, and spreading misinformation via
authentic-looking social media posts, posing a new set of challenges and risks
in the realm of cybersecurity. To combat the threats posed by GenAI, we propose
leveraging the Cyber Kill Chain (CKC) to understand the lifecycle of
cyberattacks, as a foundational model for cyber defense. This paper aims to
provide a comprehensive analysis of the risk areas introduced by the offensive
use of GenAI techniques in each phase of the CKC framework. We also analyze the
strategies employed by threat actors and examine their utilization throughout
different phases of the CKC, highlighting the implications for cyber defense.
Additionally, we propose GenAI-enabled defense strategies that are both
attack-aware and adaptive. These strategies encompass various techniques such
as detection, deception, and adversarial training, among others, aiming to
effectively mitigate the risks posed by GenAI-induced cyber threats.","['Subash Neupane', 'Ivan A. Fernandez', 'Sudip Mittal', 'Shahram Rahimi']",9,0.7335419
"Twitter bot detection has become an increasingly important and challenging
task to combat online misinformation, facilitate social content moderation, and
safeguard the integrity of social platforms. Though existing graph-based
Twitter bot detection methods achieved state-of-the-art performance, they are
all based on the homophily assumption, which assumes users with the same label
are more likely to be connected, making it easy for Twitter bots to disguise
themselves by following a large number of genuine users. To address this issue,
we proposed HOFA, a novel graph-based Twitter bot detection framework that
combats the heterophilous disguise challenge with a homophily-oriented graph
augmentation module (Homo-Aug) and a frequency adaptive attention module
(FaAt). Specifically, the Homo-Aug extracts user representations and computes a
k-NN graph using an MLP and improves Twitter's homophily by injecting the k-NN
graph. For the FaAt, we propose an attention mechanism that adaptively serves
as a low-pass filter along a homophilic edge and a high-pass filter along a
heterophilic edge, preventing user features from being over-smoothed by their
neighborhood. We also introduce a weight guidance loss to guide the frequency
adaptive attention module. Our experiments demonstrate that HOFA achieves
state-of-the-art performance on three widely-acknowledged Twitter bot detection
benchmarks, which significantly outperforms vanilla graph-based bot detection
techniques and strong heterophilic baselines. Furthermore, extensive studies
confirm the effectiveness of our Homo-Aug and FaAt module, and HOFA's ability
to demystify the heterophilous disguise challenge.","['Sen Ye', 'Zhaoxuan Tan', 'Zhenyu Lei', 'Ruijie He', 'Hongrui Wang', 'Qinghua Zheng', 'Minnan Luo']",13,0.6860688
"We are in the midst of a transformation of the digital news ecosystem. The
expansion of online social networks, the influence of recommender systems,
increased automation, and new generative artificial intelligence tools are
rapidly changing the speed and the way misinformation about climate change and
sustainability issues moves around the world. Policymakers, researchers and the
public need to combine forces to address the dangerous combination of opaque
social media algorithms, polarizing social bots, and a new generation of
AI-generated content. This synthesis brief is the result of a collaboration
between Stockholm Resilience Centre at Stockholm University, the Beijer
Institute of Ecological Economics at the Royal Swedish Academy of Sciences, the
Complexity Science Hub Vienna, and Karolinska Institutet. It has been put
together as an independent contribution to the Nobel Prize Summit 2023, Truth,
Trust and Hope, Washington D.C., 24th to 26th of May 2023.","['Victor Galaz', 'Hannah Metzler', 'Stefan Daume', 'Andreas Olsson', 'Bj√∂rn Lindstr√∂m', 'Arvid Marklund']",9,0.5746323
"Social media feed algorithms are designed to optimize online social
engagements for the purpose of maximizing advertising profits, and therefore
have an incentive to promote controversial posts including misinformation. By
thinking about misinformation as information pollution, we can draw parallels
with environmental policy for countering pollution such as carbon taxes.
Similar to pollution, a Pigouvian tax on misinformation provides economic
incentives for social media companies to control the spread of misinformation
more effectively to avoid or reduce their misinformation tax, while preserving
some degree of freedom in platforms' response. In this paper, we highlight a
bird's eye view of a Pigouvian misinformation tax and discuss the key questions
and next steps for implementing such a taxing scheme.","['Ashkan Kazemi', 'Rada Mihalcea']",3,0.5776224
"While social media plays a vital role in communication nowadays,
misinformation and trolls can easily take over the conversation and steer
public opinion on these platforms. We saw the effect of misinformation during
the COVID-19 pandemic when public health officials faced significant push-back
while trying to motivate the public to vaccinate. To tackle the current and any
future threats in emergencies and motivate the public towards a common goal, it
is essential to understand how public motivation shifts and which topics
resonate among the general population. In this study, we proposed an
interactive visualization tool to inspect and analyze the topics that resonated
among Twitter-sphere during the COVID-19 pandemic and understand the key
factors that shifted public stance for vaccination. This tool can easily be
generalized for any scenario for visual analysis and to increase the
transparency of social media data for researchers and the general population
alike.","['Ashiqur Rahman', 'Hamed Alhoori']",5,0.787822
"The technology of Conversational AI has made significant advancements over
the last eighteen months. As a consequence, conversational agents are likely to
be deployed in the near future that are designed to pursue targeted influence
objectives. Sometimes referred to as the ""AI Manipulation Problem,"" the
emerging risk is that consumers will unwittingly engage in real-time dialog
with predatory AI agents that can skillfully persuade them to buy particular
products, believe particular pieces of misinformation, or fool them into
revealing sensitive personal data. For many users, current systems like ChatGPT
and LaMDA feel safe because they are primarily text-based, but the industry is
already shifting towards real-time voice and photorealistic digital personas
that look, move, and express like real people. This will enable the deployment
of agenda-driven Virtual Spokespeople (VSPs) that will be highly persuasive
through real-time adaptive influence. This paper explores the manipulative
tactics that are likely to be deployed through conversational AI agents, the
unique threats such agents pose to the epistemic agency of human users, and the
emerging need for policymakers to protect against the most likely predatory
practices.",['Louis Rosenberg'],9,0.77849865
"The advent of ChatGPT by OpenAI has prompted extensive discourse on its
potential implications for science and higher education. While the impact on
education has been a primary focus, there is limited empirical research on the
effects of large language models (LLMs) and LLM-based chatbots on science and
scientific practice. To investigate this further, we conducted a Delphi study
involving 72 experts specialising in research and AI. The study focused on
applications and limitations of LLMs, their effects on the science system,
ethical and legal considerations, and the required competencies for their
effective use. Our findings highlight the transformative potential of LLMs in
science, particularly in administrative, creative, and analytical tasks.
However, risks related to bias, misinformation, and quality assurance need to
be addressed through proactive regulation and science education. This research
contributes to informed discussions on the impact of generative AI in science
and helps identify areas for future action.","['Benedikt Fecher', 'Marcel Hebing', 'Melissa Laufer', 'J√∂rg Pohle', 'Fabian Sofsky']",9,0.62315047
"The pervasive influence of misinformation has far-reaching and detrimental
effects on both individuals and society. The COVID-19 pandemic has witnessed an
alarming surge in the dissemination of medical misinformation. However,
existing datasets pertaining to misinformation predominantly focus on textual
information, neglecting the inclusion of visual elements, and tend to center
solely on COVID-19-related misinformation, overlooking misinformation
surrounding other diseases. Furthermore, the potential of Large Language Models
(LLMs), such as the ChatGPT developed in late 2022, in generating
misinformation has been overlooked in previous works. To overcome these
limitations, we present Med-MMHL, a novel multi-modal misinformation detection
dataset in a general medical domain encompassing multiple diseases. Med-MMHL
not only incorporates human-generated misinformation but also includes
misinformation generated by LLMs like ChatGPT. Our dataset aims to facilitate
comprehensive research and development of methodologies for detecting
misinformation across diverse diseases and various scenarios, including human
and LLM-generated misinformation detection at the sentence, document, and
multi-modal levels. To access our dataset and code, visit our GitHub
repository: \url{https://github.com/styxsys0927/Med-MMHL}.","['Yanshen Sun', 'Jianfeng He', 'Shuo Lei', 'Limeng Cui', 'Chang-Tien Lu']",5,0.74704146
"Screenshots are prevalent on social media as a common approach for
information sharing. Users rarely verify before sharing a screenshot whether
the post it contains is fake or real. Information sharing through fake
screenshots can be highly responsible for misinformation and disinformation
spread on social media. Our ultimate goal is to develop a tool that could take
a screenshot of a tweet and provide a probability that the tweet is real, using
resources found on the live web and in web archives. This paper provides
methods for extracting the tweet text, timestamp, and Twitter handle from a
screenshot of a tweet.","['Tarannum Zaki', 'Michael L. Nelson', 'Michele C. Weigle']",10,0.5513552
"The detection of political fake statements is crucial for maintaining
information integrity and preventing the spread of misinformation in society.
Historically, state-of-the-art machine learning models employed various methods
for detecting deceptive statements. These methods include the use of metadata
(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic
(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent
advancements in large language models, such as GPT-3 (Brown et al., 2020) have
achieved state-of-the-art performance on a wide range of tasks. In this study,
we conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)
and achieved higher accuracy than state-of-the-art models without using any
additional meta or linguistic features. Additionally, we experimented with
zero-shot learning using a carefully designed prompt and achieved near
state-of-the-art performance. An advantage of this approach is that the model
provided evidence for its decision, which adds transparency to the model's
decision-making and offers a chance for users to verify the validity of the
evidence provided.",['Mars Gokturk Buchholz'],8,0.6369593
"With the rapid growth and spread of online misinformation, people need tools
to help them evaluate the credibility and accuracy of online information.
Lateral reading, a strategy that involves cross-referencing information with
multiple sources, may be an effective approach to achieving this goal. In this
paper, we present ReadProbe, a tool to support lateral reading, powered by
generative large language models from OpenAI and the Bing search engine. Our
tool is able to generate useful questions for lateral reading, scour the web
for relevant documents, and generate well-attributed answers to help people
better evaluate online information. We made a web-based application to
demonstrate how ReadProbe can help reduce the risk of being misled by false
information. The code is available at
https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won
the first prize in a national AI misinformation hackathon.","['Dake Zhang', 'Ronak Pradeep']",0,0.635615
"Graph generative models become increasingly effective for data distribution
approximation and data augmentation. While they have aroused public concerns
about their malicious misuses or misinformation broadcasts, just as what
Deepfake visual and auditory media has been delivering to society. Hence it is
essential to regulate the prevalence of generated graphs. To tackle this
problem, we pioneer the formulation of the generated graph detection problem to
distinguish generated graphs from real ones. We propose the first framework to
systematically investigate a set of sophisticated models and their performance
in four classification scenarios. Each scenario switches between seen and
unseen datasets/generators during testing to get closer to real-world settings
and progressively challenge the classifiers. Extensive experiments evidence
that all the models are qualified for generated graph detection, with specific
models having advantages in specific scenarios. Resulting from the validated
generality and oblivion of the classifiers to unseen datasets/generators, we
draw a safe conclusion that our solution can sustain for a decent while to curb
generated graph misuses.","['Yihan Ma', 'Zhikun Zhang', 'Ning Yu', 'Xinlei He', 'Michael Backes', 'Yun Shen', 'Yang Zhang']",7,0.7180288
"False information can spread quickly on social media, negatively influencing
the citizens' behaviors and responses to social events. To better detect all of
the fake news, especially long texts which are harder to find completely, a
Long-Text Chinese Rumor detection dataset named LTCR is proposed. The LTCR
dataset provides a valuable resource for accurately detecting misinformation,
especially in the context of complex fake news related to COVID-19. The dataset
consists of 1,729 and 500 pieces of real and fake news, respectively. The
average lengths of real and fake news are approximately 230 and 152 characters.
We also propose \method, Salience-aware Fake News Detection Model, which
achieves the highest accuracy (95.85%), fake news recall (90.91%) and F-score
(90.60%) on the dataset. (https://github.com/Enderfga/DoubleCheck)","['Ziyang Ma', 'Mengsha Liu', 'Guian Fang', 'Ying Shen']",4,0.72907686
"The most surprising observation reported by the study in (arXiv:2208.13523),
involving stance detection of COVID-19 vaccine related tweets during the first
year of pandemic, is the presence of a significant number of users (~2 million)
who posted tweets with both anti-vax and pro-vax stances. This is a sizable
cohort even when the stance detection noise is considered. In this paper, we
tried to get deeper understanding of this 'dual-stance' group. Out of this
group, 60% of users have more pro-vax tweets than anti-vax tweets and 17% have
the same number of tweets in both classes. The rest have more anti-vax tweets,
and they were highly active in expressing concerns about mandate and safety of
a fast-tracked vaccine, while also tweeted some updates about vaccine
development. The leaning pro-vax group have opposite composition: more vaccine
updates and some posts about concerns. It is important to note that vaccine
concerns were not always genuine and had a large dose of misinformation. 43% of
the balanced group have only tweeted one tweet of each type during our study
period and are the less active participants in the vaccine discourse. Our
temporal study also shows that the change-of-stance behaviour became really
significant once the trial results of COVID-19 vaccine were announced to the
public, and it appears as the change of stance towards pro-vax is a reaction to
people changing their opinion towards anti-vax. Our study finished at Mar 23,
2021 when the conundrum was still going strong. The dilemma might be a
reflection of the uncertain and stressful times, but it also highlights the
importance of building public trust to combat prevalent misinformation.","['Zainab Zaidi', 'Mengbin Ye', 'Shanika Karunasekera', 'Yoshihisa Kashima']",12,0.8036022
"The abundance of information on social media has increased the necessity of
accurate real-time rumour detection. Manual techniques of identifying and
verifying fake news generated by AI tools are impracticable and time-consuming
given the enormous volume of information generated every day. This has sparked
an increase in interest in creating automated systems to find fake news on the
Internet. The studies in this research demonstrate that the BERT and RobertA
models with fine-tuning had the best success in detecting AI generated news.
With a score of 98%, tweaked RobertA in particular showed excellent precision.
In conclusion, this study has shown that neural networks can be used to
identify bogus news AI generation news created by ChatGPT. The RobertA and BERT
models' excellent performance indicates that these models can play a critical
role in the fight against misinformation.","['Zecong Wang', 'Jiaxi Cheng', 'Chen Cui', 'Chenhao Yu']",4,0.73116386
"Synthetic realities are digital creations or augmentations that are
contextually generated through the use of Artificial Intelligence (AI) methods,
leveraging extensive amounts of data to construct new narratives or realities,
regardless of the intent to deceive. In this paper, we delve into the concept
of synthetic realities and their implications for Digital Forensics and society
at large within the rapidly advancing field of AI. We highlight the crucial
need for the development of forensic techniques capable of identifying harmful
synthetic creations and distinguishing them from reality. This is especially
important in scenarios involving the creation and dissemination of fake news,
disinformation, and misinformation. Our focus extends to various forms of
media, such as images, videos, audio, and text, as we examine how synthetic
realities are crafted and explore approaches to detecting these malicious
creations. Additionally, we shed light on the key research challenges that lie
ahead in this area. This study is of paramount importance due to the rapid
progress of AI generative techniques and their impact on the fundamental
principles of Forensic Science.","['Jo√£o Phillipe Cardenuto', 'Jing Yang', 'Rafael Padilha', 'Renjie Wan', 'Daniel Moreira', 'Haoliang Li', 'Shiqi Wang', 'Fernanda Andal√≥', 'S√©bastien Marcel', 'Anderson Rocha']",11,0.69525915
"The rapid advancement of generative models, facilitating the creation of
hyper-realistic images from textual descriptions, has concurrently escalated
critical societal concerns such as misinformation. Although providing some
mitigation, traditional fingerprinting mechanisms fall short in attributing
responsibility for the malicious use of synthetic images. This paper introduces
a novel approach to model fingerprinting that assigns responsibility for the
generated images, thereby serving as a potential countermeasure to model
misuse. Our method modifies generative models based on each user's unique
digital fingerprint, imprinting a unique identifier onto the resultant content
that can be traced back to the user. This approach, incorporating fine-tuning
into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates
near-perfect attribution accuracy with a minimal impact on output quality.
Through extensive evaluation, we show that our method outperforms baseline
methods with an average improvement of 11\% in handling image post-processes.
Our method presents a promising and novel avenue for accountable model
distribution and responsible use. Our code is available in
\url{https://github.com/kylemin/WOUAF}.","['Changhoon Kim', 'Kyle Min', 'Maitreya Patel', 'Sheng Cheng', 'Yezhou Yang']",7,0.81707466
"This paper aims to build an actionable framework for permissible online
content moderation to combat misinformation. Often strong content moderation
policies are invoked when misinformation causes harm. By adopting Mill's
ethical framework, I show the complexities involved in permissible content
moderation. The conclusion will be that, besides invoking the notion of harm,
we should also introduce the idea of cognitive autonomy and adopt useful tools,
such as cognitive nudging, to promote a healthier epistemic environment online.",['Marianna Ganapini'],0,0.5485828
"Internet immunity doctrine is broken. Under Section 230 of the Communications
Decency Act of 1996, online entities are absolutely immune from lawsuits
related to content authored by third parties. The law has been essential to the
internet's development over the last twenty years, but it has not kept pace
with the times and is now deeply flawed. Democrats demand accountability for
online misinformation. Republicans decry politically motivated censorship. And
Congress, President Biden, the Department of Justice, and the Federal
Communications Commission all have their own plans for reform. Absent from the
fray, however -- until now -- has been the Supreme Court, which has never
issued a decision interpreting Section 230. That appears poised to change,
however, following Justice Thomas's statement in Malwarebytes v. Enigma in
which he urges the Court to prune back decades of lower-court precedent to
craft a more limited immunity doctrine. This Essay discusses how courts'
zealous enforcement of the early internet's free-information ethos gave birth
to an expansive immunity doctrine, warns of potential pitfalls to reform, and
explores what a narrower, text-focused doctrine might mean for the tech
industry.",['Gregory M. Dickinson'],10,0.47992676
"Generative deep learning models are able to create realistic audio and video.
This technology has been used to impersonate the faces and voices of
individuals. These ``deepfakes'' are being used to spread misinformation,
enable scams, perform fraud, and blackmail the innocent. The technology
continues to advance and today attackers have the ability to generate deepfakes
in real-time. This new capability poses a significant threat to society as
attackers begin to exploit the technology in advances social engineering
attacks. In this paper, we discuss the implications of this emerging threat,
identify the challenges with preventing these attacks and suggest a better
direction for researching stronger defences.","['Guy Frankovits', 'Yisroel Mirsky']",11,0.7269218
"The problem of community-level information pathway prediction (CLIPP) aims at
predicting the transmission trajectory of content across online communities. A
successful solution to CLIPP holds significance as it facilitates the
distribution of valuable information to a larger audience and prevents the
proliferation of misinformation. Notably, solving CLIPP is non-trivial as
inter-community relationships and influence are unknown, information spread is
multi-modal, and new content and new communities appear over time. In this
work, we address CLIPP by collecting large-scale, multi-modal datasets to
examine the diffusion of online YouTube videos on Reddit. We analyze these
datasets to construct community influence graphs (CIGs) and develop a novel
dynamic graph framework, INPAC (Information Pathway Across Online Communities),
which incorporates CIGs to capture the temporal variability and multi-modal
nature of video propagation across communities. Experimental results in both
warm-start and cold-start scenarios show that INPAC outperforms seven baselines
in CLIPP.","['Yiqiao Jin', 'Yeon-Chang Lee', 'Kartik Sharma', 'Meng Ye', 'Karan Sikka', 'Ajay Divakaran', 'Srijan Kumar']",2,0.6641946
"With the prevalence of graphs for modeling complex relationships among
objects, the topic of graph mining has attracted a great deal of attention from
both academic and industrial communities in recent years. As one of the most
fundamental problems in graph mining, the densest subgraph discovery (DSD)
problem has found a wide spectrum of real applications, such as discovery of
filter bubbles in social media, finding groups of actors propagating
misinformation in social media, social network community detection, graph index
construction, regulatory motif discovery in DNA, fake follower detection, and
so on. Theoretically, DSD closely relates to other fundamental graph problems,
such as network flow and bipartite matching. Triggered by these applications
and connections, DSD has garnered much attention from the database, data
mining, theory, and network communities.
  In this survey, we first highlight the importance of DSD in various
real-world applications and the unique challenges that need to be addressed.
Subsequently, we classify existing DSD solutions into several groups, which
cover around 50 research papers published in many well-known venues (e.g.,
SIGMOD, PVLDB, TODS, WWW), and conduct a thorough review of these solutions in
each group. Afterwards, we analyze and compare the models and solutions in
these works. Finally, we point out a list of promising future research
directions. It is our hope that this survey not only helps researchers have a
better understanding of existing densest subgraph models and solutions, but
also provides insights and identifies directions for future study.","['Wensheng Luo', 'Chenhao Ma', 'Yixiang Fang', 'Laks V. S. Lakshmanan']",2,0.69017065
"Encrypted messaging services like WhatsApp, Facebook Messenger, and Signal
provide secure and deniable communication for billions across the world, but
these exact properties prevent holding users accountable for sending messages
that are abusive, misinformative, or otherwise harmful to society. Previous
works have addressed this concern by allowing a moderator to verify the
identity of a message's sender if a message is reported; if not reported,
messages maintain all security guarantees. Using primitives from threshold
cryptography, this work extends the message-reporting protocol Hecate from
Issa, Alhaddad, and Varia to a setting in which consensus among a group of
moderators is required to reveal and verify the identity of a message's sender.","['Alistair Pattison', 'Nicholas Hopper']",4,0.4095599
"The proliferation of fake news poses a serious threat to society, as it can
misinform and manipulate the public, erode trust in institutions, and undermine
democratic processes. To address this issue, we present FakeSwarm, a fake news
identification system that leverages the swarming characteristics of fake news.
To extract the swarm behavior, we propose a novel concept of fake news swarming
characteristics and design three types of swarm features, including principal
component analysis, metric representation, and position encoding. We evaluate
our system on a public dataset and demonstrate the effectiveness of
incorporating swarm features in fake news identification, achieving an f1-score
and accuracy of over 97% by combining all three types of swarm features.
Furthermore, we design an online learning pipeline based on the hypothesis of
the temporal distribution pattern of fake news emergence, validated on a topic
with early emerging fake news and a shortage of text samples, showing that
swarm features can significantly improve recall rates in such cases. Our work
provides a new perspective and approach to fake news detection and highlights
the importance of considering swarming characteristics in detecting fake news.","['Jun Wu', 'Xuesong Ye']",4,0.79738057
"The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for fake news
detection. However, state-of-the-art approaches are usually trained on datasets
of smaller size or with a limited set of specific topics. As a consequence,
these models lack generalization capabilities and are not applicable to
real-world data. In this paper, we propose three models that adopt and
fine-tune state-of-the-art multimodal transformers for multimodal fake news
detection. We conduct an in-depth analysis by manipulating the input data aimed
to explore models performance in realistic use cases on social media. Our study
across multiple models demonstrates that these systems suffer significant
performance drops against manipulated data. To reduce the bias and improve
model generalization, we suggest training data augmentation to conduct more
meaningful experiments for fake news detection on social media. The proposed
data augmentation techniques enable models to generalize better and yield
improved state-of-the-art results.","['Sahar Tahmasebi', 'Sherzod Hakimov', 'Ralph Ewerth', 'Eric M√ºller-Budack']",4,0.70518625
"State-of-the-art language models (LMs) are notoriously susceptible to
generating hallucinated information. Such inaccurate outputs not only undermine
the reliability of these models but also limit their use and raise serious
concerns about misinformation and propaganda. In this work, we focus on
hallucinated book and article references and present them as the ""model
organism"" of language model hallucination research, due to their frequent and
easy-to-discern nature. We posit that if a language model cites a particular
reference in its output, then it should ideally possess sufficient information
about its authors and content, among other relevant details. Using this basic
insight, we illustrate that one can identify hallucinated references without
ever consulting any external resources, by asking a set of direct or indirect
queries to the language model about the references. These queries can be
considered as ""consistency checks."" Our findings highlight that while LMs,
including GPT-4, often produce inconsistent author lists for hallucinated
references, they also often accurately recall the authors of real references.
In this sense, the LM can be said to ""know"" when it is hallucinating
references. Furthermore, these findings show how hallucinated references can be
dissected to shed light on their nature. Replication code and results can be
found at https://github.com/microsoft/hallucinated-references.","['Ayush Agrawal', 'Mirac Suzgun', 'Lester Mackey', 'Adam Tauman Kalai']",6,0.56852734
"The task of fact-checking deals with assessing the veracity of factual claims
based on credible evidence and background knowledge. In particular, scientific
fact-checking is the variation of the task concerned with verifying claims
rooted in scientific knowledge. This task has received significant attention
due to the growing importance of scientific and health discussions on online
platforms. Automated scientific fact-checking methods based on NLP can help
combat the spread of misinformation, assist researchers in knowledge discovery,
and help individuals understand new scientific breakthroughs. In this paper, we
present a comprehensive survey of existing research in this emerging field and
its related tasks. We provide a task description, discuss the construction
process of existing datasets, and analyze proposed models and approaches. Based
on our findings, we identify intriguing challenges and outline potential future
directions to advance the field.","['Juraj Vladika', 'Florian Matthes']",0,0.67643344
"While there exists a large amount of literature on the general challenges of
and best practices for trustworthy online A/B testing, there are limited
studies on sample size estimation, which plays a crucial role in trustworthy
and efficient A/B testing that ensures the resulting inference has a sufficient
power and type I error control. For example, when sample size is
under-estimated, the statistical inference, even with the correct analysis
methods, will not be able to detect the true significant improvement leading to
misinformed and costly decisions. This paper addresses this fundamental gap by
developing new sample size calculation methods for correlated data, as well as
absolute vs. relative treatment effects, both ubiquitous in online experiments.
Additionally, we address a practical question of the minimal observed
difference that will be statistically significant and how it relates to average
treatment effect and sample size calculation. All proposed methods are
accompanied by mathematical proofs, illustrative examples, and simulations. We
end by sharing some best practices on various practical topics on sample size
calculation and experimental design.","['Jing Zhou', 'Jiannan Lu', 'Anas Shallah']",2,0.62550545
"Developing tools to automatically detect check-worthy claims in political
debates and speeches can greatly help moderators of debates, journalists, and
fact-checkers. While previous work on this problem has focused exclusively on
the text modality, here we explore the utility of the audio modality as an
additional input. We create a new multimodal dataset (text and audio in
English) containing 48 hours of speech from past political debates in the USA.
We then experimentally demonstrate that, in the case of multiple speakers,
adding the audio modality yields sizable improvements over using the text
modality alone; moreover, an audio-only model could outperform a text-only one
for a single speaker. With the aim to enable future research, we make all our
data and code publicly available at
https://github.com/petar-iv/audio-checkworthiness-detection.","['Petar Ivanov', 'Ivan Koychev', 'Momchil Hardalov', 'Preslav Nakov']",8,0.606108
"Misinformation poses a critical societal challenge, and current approaches
have yet to produce an effective solution. We propose focusing on
generalization, uncertainty, and how to leverage recent large language models,
in order to create more practical tools to evaluate information veracity in
contexts where perfect classification is impossible. We first demonstrate that
GPT-4 can outperform prior methods in multiple settings and languages. Next, we
explore generalization, revealing that GPT-4 and RoBERTa-large exhibit
differences in failure modes. Third, we propose techniques to handle
uncertainty that can detect impossible examples and strongly improve outcomes.
We also discuss results on other language models, temperature, prompting,
versioning, explainability, and web retrieval, each one providing practical
insights and directions for future research. Finally, we publish the LIAR-New
dataset with novel paired English and French misinformation data and
Possibility labels that indicate if there is sufficient context for veracity
evaluation. Overall, this research lays the groundwork for future tools that
can drive real-world progress to combat misinformation.","['Kellin Pelrine', 'Anne Imouza', 'Camille Thibault', 'Meilina Reksoprodjo', 'Caleb Gupta', 'Joel Christoph', 'Jean-Fran√ßois Godbout', 'Reihaneh Rabbany']",8,0.7535932
"With the recent appearance of LLMs in practical settings, having methods that
can effectively detect factual inconsistencies is crucial to reduce the
propagation of misinformation and improve trust in model outputs. When testing
on existing factual consistency benchmarks, we find that a few large language
models (LLMs) perform competitively on classification benchmarks for factual
inconsistency detection compared to traditional non-LLM methods. However, a
closer analysis reveals that most LLMs fail on more complex formulations of the
task and exposes issues with existing evaluation benchmarks, affecting
evaluation precision. To address this, we propose a new protocol for
inconsistency detection benchmark creation and implement it in a 10-domain
benchmark called SummEdits. This new benchmark is 20 times more cost-effective
per sample than previous benchmarks and highly reproducible, as we estimate
inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with
performance close to random chance. The best-performing model, GPT-4, is still
8\% below estimated human performance, highlighting the gaps in LLMs' ability
to reason about facts and detect inconsistencies when they occur.","['Philippe Laban', 'Wojciech Kry≈õci≈Ñski', 'Divyansh Agarwal', 'Alexander R. Fabbri', 'Caiming Xiong', 'Shafiq Joty', 'Chien-Sheng Wu']",6,0.71585476
"With the rapid progress of large language models (LLMs) and the huge amount
of text they generated, it becomes more and more impractical to manually
distinguish whether a text is machine-generated. Given the growing use of LLMs
in social media and education, it prompts us to develop methods to detect
machine-generated text, preventing malicious usage such as plagiarism,
misinformation, and propaganda. Previous work has studied several zero-shot
methods, which require no training data. These methods achieve good
performance, but there is still a lot of room for improvement. In this paper,
we introduce two novel zero-shot methods for detecting machine-generated text
by leveraging the log rank information. One is called DetectLLM-LRR, which is
fast and efficient, and the other is called DetectLLM-NPR, which is more
accurate, but slower due to the need for perturbations. Our experiments on
three datasets and seven language models show that our proposed methods improve
over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,
DetectLLM-NPR needs fewer perturbations than previous work to achieve the same
level of performance, which makes it more practical for real-world use. We also
investigate the efficiency--performance trade-off based on users preference on
these two measures and we provide intuition for using them in practice
effectively. We release the data and the code of both methods in
https://github.com/mbzuai-nlp/DetectLLM","['Jinyan Su', 'Terry Yue Zhuo', 'Di Wang', 'Preslav Nakov']",1,0.6710132
"In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.","['Yikang Pan', 'Liangming Pan', 'Wenhu Chen', 'Preslav Nakov', 'Min-Yen Kan', 'William Yang Wang']",6,0.8600253
"Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned
image. Multimodal misinformation is perceived as more credible by humans, and
spreads faster than its text-only counterparts. While an increasing body of
research investigates automated fact-checking (AFC), previous surveys mostly
focus on text. In this survey, we conceptualise a framework for AFC including
subtasks unique to multimodal misinformation. Furthermore, we discuss related
terms used in different communities and map them to our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research","['Mubashara Akhtar', 'Michael Schlichtkrull', 'Zhijiang Guo', 'Oana Cocarascu', 'Elena Simperl', 'Andreas Vlachos']",0,0.6476395
"With emerging topics (e.g., COVID-19) on social media as a source for the
spreading misinformation, overcoming the distributional shifts between the
original training domain (i.e., source domain) and such target domains remains
a non-trivial task for misinformation detection. This presents an elusive
challenge for early-stage misinformation detection, where a good amount of data
and annotations from the target domain is not available for training. To
address the data scarcity issue, we propose MetaAdapt, a meta learning based
approach for domain adaptive few-shot misinformation detection. MetaAdapt
leverages limited target examples to provide feedback and guide the knowledge
transfer from the source to the target domain (i.e., learn to adapt). In
particular, we train the initial model with multiple source tasks and compute
their similarity scores to the meta task. Based on the similarity scores, we
rescale the meta gradients to adaptively learn from the source tasks. As such,
MetaAdapt can learn how to adapt the misinformation detection model and exploit
the source data for improved performance in the target domain. To demonstrate
the efficiency and effectiveness of our method, we perform extensive
experiments to compare MetaAdapt with state-of-the-art baselines and large
language models (LLMs) such as LLaMA, where MetaAdapt achieves better
performance in domain adaptive few-shot misinformation detection with
substantially reduced parameters on real-world datasets.","['Zhenrui Yue', 'Huimin Zeng', 'Yang Zhang', 'Lanyu Shang', 'Dong Wang']",1,0.65759885
"Crowdsourced investigations shore up democratic institutions by debunking
misinformation and uncovering human rights abuses. However, current
crowdsourcing approaches rely on simplistic collaborative or competitive models
and lack technological support, limiting their collective impact. Prior
research has shown that blending elements of competition and collaboration can
lead to greater performance and creativity, but crowdsourced investigations
pose unique analytical and ethical challenges. In this paper, we employed a
four-month-long Research through Design process to design and evaluate a novel
interaction style called collaborative capture the flag competitions (CoCTFs).
We instantiated this interaction style through CoSINT, a platform that enables
a trained crowd to work with professional investigators to identify and
investigate social media misinformation. Our mixed-methods evaluation showed
that CoSINT leverages the complementary strengths of competition and
collaboration, allowing a crowd to quickly identify and debunk misinformation.
We also highlight tensions between competition versus collaboration and discuss
implications for the design of crowdsourced investigations.","['Sukrit Venkatagiri', 'Anirban Mukhopadhyay', 'David Hicks', 'Aaron Brantly', 'Kurt Luther']",0,0.5780479
"Automatic assessment of the quality of arguments has been recognized as a
challenging task with significant implications for misinformation and targeted
speech. While real-world arguments are tightly anchored in context, existing
computational methods analyze their quality in isolation, which affects their
accuracy and generalizability. We propose SPARK: a novel method for scoring
argument quality based on contextualization via relevant knowledge. We devise
four augmentations that leverage large language models to provide feedback,
infer hidden assumptions, supply a similar-quality argument, or give a
counter-argument. SPARK uses a dual-encoder Transformer architecture to enable
the original argument and its augmentation to be considered jointly. Our
experiments in both in-domain and zero-shot setups show that SPARK consistently
outperforms existing techniques across multiple metrics.","['Darshan Deshpande', 'Zhivar Sourati', 'Filip Ilievski', 'Fred Morstatter']",1,0.6828027
"With the advent of fluent generative language models that can produce
convincing utterances very similar to those written by humans, distinguishing
whether a piece of text is machine-generated or human-written becomes more
challenging and more important, as such models could be used to spread
misinformation, fake news, fake reviews and to mimic certain authors and
figures. To this end, there have been a slew of methods proposed to detect
machine-generated text. Most of these methods need access to the logits of the
target model or need the ability to sample from the target. One such black-box
detection method relies on the observation that generated text is locally
optimal under the likelihood function of the generator, while human-written
text is not. We find that overall, smaller and partially-trained models are
better universal text detectors: they can more precisely detect text generated
from both small and larger models. Interestingly, we find that whether the
detector and generator were trained on the same data is not critically
important to the detection success. For instance the OPT-125M model has an AUC
of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT
family, GPTJ-6B, has AUC of 0.45.","['Niloofar Mireshghallah', 'Justus Mattern', 'Sicun Gao', 'Reza Shokri', 'Taylor Berg-Kirkpatrick']",8,0.6394303
"As large language models (LLMs) like ChatGPT have gained traction, an
increasing number of news websites have begun utilizing them to generate
articles. However, not only can these language models produce factually
inaccurate articles on reputable websites but disreputable news sites can
utilize LLMs to mass produce misinformation. To begin to understand this
phenomenon, we present one of the first large-scale studies of the prevalence
of synthetic articles within online news media. To do this, we train a
DeBERTa-based synthetic news detector and classify over 15.46 million articles
from 3,074 misinformation and mainstream news websites. We find that between
January 1, 2022, and May 1, 2023, the relative number of synthetic news
articles increased by 57.3% on mainstream websites while increasing by 474% on
misinformation sites. We find that this increase is largely driven by smaller
less popular websites. Analyzing the impact of the release of ChatGPT using an
interrupted-time-series, we show that while its release resulted in a marked
increase in synthetic articles on small sites as well as misinformation news
websites, there was not a corresponding increase on large mainstream news
websites.","['Hans W. A. Hanley', 'Zakir Durumeric']",4,0.67304623
"Social media platforms provide users with a profile description field,
commonly known as a ``bio,"" where they can present themselves to the world. A
growing literature shows that text in these bios can improve our understanding
of online self-presentation and behavior, but existing work relies exclusively
on keyword-based approaches to do so. We here propose and evaluate a suite of
\hl{simple, effective, and theoretically motivated} approaches to embed bios in
spaces that capture salient dimensions of social meaning, such as age and
partisanship. We \hl{evaluate our methods on four tasks, showing that the
strongest one out-performs several practical baselines.} We then show the
utility of our method in helping understand associations between
self-presentation and the sharing of URLs from low-quality news sites on
Twitter\hl{, with a particular focus on explore the interactions between age
and partisanship, and exploring the effects of self-presentations of
religiosity}. Our work provides new tools to help computational social
scientists make use of information in bios, and provides new insights into how
misinformation sharing may be perceived on Twitter.","['Navid Madani', 'Rabiraj Bandyopadhyay', 'Briony Swire-Thompson', 'Michael Miller Yoder', 'Kenneth Joseph']",0,0.6905552
"Deploying links to fact-checking websites (so-called ""snoping"") is a common
intervention that can be used by social media users to refute misleading
claims. However, its real-world effect may be limited as it suffers from low
visibility and distrust towards professional fact-checkers. As a remedy,
Twitter launched its community-based fact-checking system Community Notes on
which fact-checks are carried out by actual Twitter users and directly shown on
the fact-checked tweets. Yet, an understanding of how fact-checking via
Community Notes differs from snoping is absent. In this study, we analyze
differences in how contributors to Community Notes and Snopers select their
targets when fact-checking social media posts. For this purpose, we analyze two
unique datasets from Twitter: (a) 25,912 community-created fact-checks from
Twitter's Community Notes platform; and (b) 52,505 ""snopes"" that debunk tweets
via fact-checking replies linking to professional fact-checking websites. We
find that Notes contributors and Snopers focus on different targets when
fact-checking social media content. For instance, Notes contributors tend to
fact-check posts from larger accounts with higher social influence and are
relatively less likely to endorse/emphasize the accuracy of not misleading
posts. Fact-checking targets of Notes contributors and Snopers rarely overlap;
however, those overlapping exhibit a high level of agreement in the
fact-checking assessment. Moreover, we demonstrate that Snopers fact-check
social media posts at a higher speed. Altogether, our findings imply that
different fact-checking approaches -- carried out on the same social media
platform -- can result in vastly different social media posts getting
fact-checked. This has important implications for future research on
misinformation, which should not rely on a single fact-checking approach when
compiling misinformation datasets.","['Moritz Pilarski', 'Kirill Solovev', 'Nicolas Pr√∂llochs']",3,0.7582097
"Misinformation has become a significant issue in today's society, with the
proliferation of false information through various mediums such as social media
and traditional news sources. The rapid spread of misinformation has made it
increasingly difficult for people to separate truth from fiction, and this has
the potential to cause significant harm to individuals and society as a whole.
In addition, there currently exists an information gap with regard to internet
education, with many schools across America not having the teaching personnel
nor resources to adequately educate their students about the dangers of the
internet, specifically with regard to misinformation in the political sphere.
To address the dangers of misinformation, some game developers have created
video games that aim to educate players on the issue and help them develop
critical thinking skills. These games can be used to raise awareness about the
importance of verifying information before sharing it. By doing so, they can
help reduce the spread of misinformation and promote a more informed and
discerning public. They can also provide players with a safe and controlled
environment to practice these skills and build confidence in their ability to
evaluate information. However, these existing games often suffer from various
shortcomings such as failing to adequately address how misinformation
specifically exploits the biases within people to be effective and rarely
covering how evolving modern technologies like sophisticated chatbots and deep
fakes have made individuals even more vulnerable to misinformation. The purpose
of this study is to create an educational misinformation game to address this
information gap and investigate its efficacy as an educational tool while also
iterating on the designs for previous games in the space.",['William Shi'],0,0.70881987
"Language models (LMs) are pretrained on diverse data sources, including news,
discussion forums, books, and online encyclopedias. A significant portion of
this data includes opinions and perspectives which, on one hand, celebrate
democracy and diversity of ideas, and on the other hand are inherently socially
biased. Our work develops new methods to (1) measure political biases in LMs
trained on such corpora, along social and economic axes, and (2) measure the
fairness of downstream NLP models trained on top of politically biased LMs. We
focus on hate speech and misinformation detection, aiming to empirically
quantify the effects of political (social, economic) biases in pretraining data
on the fairness of high-stakes social-oriented tasks. Our findings reveal that
pretrained LMs do have political leanings that reinforce the polarization
present in pretraining corpora, propagating social biases into hate speech
predictions and misinformation detectors. We discuss the implications of our
findings for NLP research and propose future directions to mitigate unfairness.","['Shangbin Feng', 'Chan Young Park', 'Yuhan Liu', 'Yulia Tsvetkov']",0,0.6233126
"Coordinated inauthentic behavior is used as a tool on social media to shape
public opinion by elevating or suppressing topics using systematic engagements
-- e.g. through *likes* or similar reactions. In an honest world, reactions may
be informative to users when selecting on what to spend their attention:
through the wisdom of crowds, summed reactions may help identifying relevant
and high-quality content. This is nullified by coordinated inauthentic liking.
To restore wisdom-of-crowds effects, it is therefore desirable to separate the
inauthentic agents from the wise crowd, and use only the latter as a voting
*jury* on the relevance of a post. To this end, we design two *jury selection
procedures* (JSPs) that discard agents classified as inauthentic. Using machine
learning techniques, both cluster on binary vote data -- one using a Gaussian
Mixture Model (GMM JSP), one the k-means algorithm (KM JSP) -- and label agents
by logistic regression. We evaluate the jury selection procedures with an
agent-based model, and show that the GMM JSP detects more inauthentic agents,
but both JSPs select juries with vastly increased correctness of vote by
majority. This proof of concept provides an argument for the release of
reactions data from social media platforms through a direct use-case in the
fight against online misinformation.","['Laura Jahn', 'Rasmus K. Rendsvig', 'Jacob St√¶rk-√òstergaard']",1,0.6194706
"Local news has become increasingly important in the news industry due to its
various benefits. It offers local audiences information that helps them
participate in their communities and interests. It also serves as a reliable
source of factual reporting that can prevent misinformation. Moreover, it can
influence national audiences as some local stories may have wider implications
for politics, environment or crime. Hence, detecting the exact geolocation and
impact scope of local news is crucial for news recommendation systems. There
are two fundamental things required in this process, (1) classify whether an
article belongs to local news, and (2) identify the geolocation of the article
and its scope of influence to recommend it to appropriate users. In this paper,
we focus on the second step and propose (1) an efficient approach to determine
the location and radius of local news articles, (2) a method to reconcile the
user's location with the article's location, and (3) a metric to evaluate the
quality of the local news feed. We demonstrate that our technique is scalable
and effective in serving hyperlocal news to users worldwide.","['Deven Santosh Shah', 'Gosuddin Kamaruddin Siddiqi', 'Shiying He', 'Radhika Bansal']",4,0.6530969
"Artificial intelligence (AI)-powered recommender systems play a crucial role
in determining the content that users are exposed to on social media platforms.
However, the behavioural patterns of these systems are often opaque,
complicating the evaluation of their impact on the dissemination and
consumption of disinformation and misinformation. To begin addressing this
evidence gap, this study presents a measurement approach that uses observed
digital traces to infer the status of algorithmic amplification of
low-credibility content on Twitter over a 14-day period in January 2023. Using
an original dataset of 2.7 million posts on COVID-19 and climate change
published on the platform, this study identifies tweets sharing information
from low-credibility domains, and uses a bootstrapping model with two
stratifications, a tweet's engagement level and a user's followers level, to
compare any differences in impressions generated between low-credibility and
high-credibility samples. Additional stratification variables of toxicity,
political bias, and verified status are also examined. This analysis provides
valuable observational evidence on whether the Twitter algorithm favours the
visibility of low-credibility content, with results indicating that tweets
containing low-credibility URL domains perform significantly better than tweets
that do not across both datasets. Furthermore, high toxicity tweets and those
with right-leaning bias see heightened amplification, as do low-credibility
tweets from verified accounts. This suggests that Twitter s recommender system
may have facilitated the diffusion of false content, even when originating from
notoriously low-credibility sources.",['Giulio Corsi'],13,0.7210796
"Multimodal misinformation on online social platforms is becoming a critical
concern due to increasing credibility and easier dissemination brought by
multimedia content, compared to traditional text-only information. While
existing multimodal detection approaches have achieved high performance, the
lack of interpretability hinders these systems' reliability and practical
deployment. Inspired by NeuralSymbolic AI which combines the learning ability
of neural networks with the explainability of symbolic learning, we propose a
novel logic-based neural model for multimodal misinformation detection which
integrates interpretable logic clauses to express the reasoning process of the
target task. To make learning effective, we parameterize symbolic logical
elements using neural representations, which facilitate the automatic
generation and evaluation of meaningful logic clauses. Additionally, to make
our framework generalizable across diverse misinformation sources, we introduce
five meta-predicates that can be instantiated with different correlations.
Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the
feasibility and versatility of our model.","['Hui Liu', 'Wenya Wang', 'Haoliang Li']",2,0.6884638
"We consider a network of $n$ user nodes that receives updates from a source
and employs an age-based gossip protocol for faster dissemination of version
updates to all nodes. When a node forwards its packet to another node, the
packet information gets mutated with probability $p$ during transmission,
creating misinformation. The receiver node does not know whether an incoming
packet information is different from the packet information originally at the
sender node. We assume that truth prevails over misinformation, and therefore,
when a receiver encounters both accurate information and misinformation
corresponding to the same version, the accurate information gets chosen for
storage at the node. We study the expected fraction of nodes with correct
information in the network and version age at the nodes in this setting using
stochastic hybrid systems (SHS) modelling and study their properties. We
observe that very high or very low gossiping rates help curb misinformation,
and misinformation spread is higher with moderate gossiping rates. We support
our theoretical findings with simulation results which shed further light on
the behavior of above quantities.","['Priyanka Kaswan', 'Sennur Ulukus']",2,0.6553145
"Social media platforms empower us in several ways, from information
dissemination to consumption. While these platforms are useful in promoting
citizen journalism, public awareness etc., they have misuse potentials.
Malicious users use them to disseminate hate-speech, offensive content, rumor
etc. to gain social and political agendas or to harm individuals, entities and
organizations. Often times, general users unconsciously share information
without verifying it, or unintentionally post harmful messages. Some of such
content often get deleted either by the platform due to the violation of terms
and policies, or users themselves for different reasons, e.g., regrets. There
is a wide range of studies in characterizing, understanding and predicting
deleted content. However, studies which aims to identify the fine-grained
reasons (e.g., posts are offensive, hate speech or no identifiable reason)
behind deleted content, are limited. In this study we address this gap, by
identifying deleted tweets, particularly within the Arabic context, and
labeling them with a corresponding fine-grained disinformation category. We
then develop models that can predict the potentiality of tweets getting
deleted, as well as the potential reasons behind deletion. Such models can help
in moderating social media posts before even posting.","['Hamdy Mubarak', 'Samir Abdaljalil', 'Azza Nassar', 'Firoj Alam']",3,0.691517
"Misinformation spreading in mainstream and social media has been misleading
users in different ways. Manual detection and verification efforts by
journalists and fact-checkers can no longer cope with the great scale and quick
spread of misleading information. This motivated research and industry efforts
to develop systems for analyzing and verifying news spreading online. The
SemEval-2023 Task 3 is an attempt to address several subtasks under this
overarching problem, targeting writing techniques used in news articles to
affect readers' opinions. The task addressed three subtasks with six languages,
in addition to three ``surprise'' test languages, resulting in 27 different
test setups. This paper describes our participating system to this task. Our
team is one of the 6 teams that successfully submitted runs for all setups. The
official results show that our system is ranked among the top 3 systems for 10
out of the 27 setups.","['Maram Hasanain', 'Ahmed Oumar El-Shangiti', 'Rabindra Nath Nandi', 'Preslav Nakov', 'Firoj Alam']",4,0.6179719
"Social media and user-generated content (UGC) have become increasingly
important features of journalistic work in a number of different ways. However,
the growth of misinformation means that news organisations have had devote more
and more resources to determining its veracity and to publishing corrections if
it is found to be misleading. In this work, we present the results of
interviews with eight members of fact-checking teams from two organisations.
Team members described their fact-checking processes and the challenges they
currently face in completing a fact-check in a robust and timely way. The
former reveals, inter alia, significant differences in fact-checking practices
and the role played by collaboration between team members. We conclude with a
discussion of the implications for the development and application of
computational tools, including where computational tool support is currently
lacking and the importance of being able to accommodate different fact-checking
practices.","['Rob Procter', 'Miguel Arana-Catania', 'Yulan He', 'Maria Liakata', 'Arkaitz Zubiaga', 'Elena Kochkina', 'Runcong Zhao']",0,0.6909174
"With the ever-increasing spread of misinformation on online social networks,
it has become very important to identify the spreaders of misinformation
(unintentional), disinformation (intentional), and misinformation refutation.
It can help in educating the first, stopping the second, and soliciting the
help of the third category, respectively, in the overall effort to counter
misinformation spread. Existing research to identify spreaders is limited to
binary classification (true vs false information spreaders). However, people's
intention (whether naive or malicious) behind sharing misinformation can only
be understood after observing their behavior after exposure to both the
misinformation and its refutation which the existing literature lacks to
consider. In this paper, we propose a labeling mechanism to label people as one
of the five defined categories based on the behavioral actions they exhibit
when exposed to misinformation and its refutation. However, everyone does not
show behavioral actions but is part of a network. Therefore, we use their
network features, extracted through deep learning-based graph embedding models,
to train a machine learning model for the prediction of the classes. We name
our approach behavioral forensics since it is an evidence-based investigation
of suspicious behavior which is spreading misinformation and disinformation in
our case. After evaluating our proposed model on a real-world Twitter dataset,
we achieved 77.45% precision and 75.80% recall in detecting the malicious
actors, who shared the misinformation even after receiving its refutation. Such
behavior shows intention, and hence these actors can rightfully be called
agents of disinformation spread.","['Euna Mehnaz Khan', 'Ayush Ram', 'Bhavtosh Rath', 'Emily Vraga', 'Jaideep Srivastava']",0,0.721171
"This paper examines how the European press dealt with the no-vax reactions
against the Covid-19 vaccine and the dis- and misinformation associated with
this movement. Using a curated dataset of 1786 articles from 19 European
newspapers on the anti-vaccine movement over a period of 22 months in
2020-2021, we used Natural Language Processing techniques including topic
modeling, sentiment analysis, semantic relationship with word embeddings,
political analysis, named entity recognition, and semantic networks, to
understand the specific role of the European traditional press in the
disinformation ecosystem. The results of this multi-angle analysis demonstrate
that the European well-established press actively opposed a variety of hoaxes
mainly spread on social media, and was critical of the anti-vax trend,
regardless of the political orientation of the newspaper. This confirms the
relevance of studying the role of high-quality press in the disinformation
ecosystem.","['David Alonso del Barrio', 'Daniel Gatica-Perez']",12,0.67773855
"Class incremental learning approaches are useful as they help the model to
learn new information (classes) sequentially, while also retaining the
previously acquired information (classes). However, it has been shown that such
approaches are extremely vulnerable to the adversarial backdoor attacks, where
an intelligent adversary can introduce small amount of misinformation to the
model in the form of imperceptible backdoor pattern during training to cause
deliberate forgetting of a specific task or class at test time. In this work,
we propose a novel defensive framework to counter such an insidious attack
where, we use the attacker's primary strength-hiding the backdoor pattern by
making it imperceptible to humans-against it, and propose to learn a
perceptible (stronger) pattern (also during the training) that can overpower
the attacker's imperceptible (weaker) pattern. We demonstrate the effectiveness
of the proposed defensive mechanism through various commonly used Replay-based
(both generative and exact replay-based) class incremental learning algorithms
using continual learning benchmark variants of CIFAR-10, CIFAR-100, and MNIST
datasets. Most noteworthy, our proposed defensive framework does not assume
that the attacker's target task and target class is known to the defender. The
defender is also unaware of the shape, size, and location of the attacker's
pattern. We show that our proposed defensive framework considerably improves
the performance of class incremental learning algorithms with no knowledge of
the attacker's target task, attacker's target class, and attacker's
imperceptible pattern. We term our defensive framework as Adversary Aware
Continual Learning (AACL).","['Muhammad Umer', 'Robi Polikar']",7,0.5711394
"Automated fact-checking is often presented as an epistemic tool that
fact-checkers, social media consumers, and other stakeholders can use to fight
misinformation. Nevertheless, few papers thoroughly discuss how. We document
this by analysing 100 highly-cited papers, and annotating epistemic elements
related to intended use, i.e., means, ends, and stakeholders. We find that
narratives leaving out some of these aspects are common, that many papers
propose inconsistent means and ends, and that the feasibility of suggested
strategies rarely has empirical backing. We argue that this vagueness actively
hinders the technology from reaching its goals, as it encourages overclaiming,
limits criticism, and prevents stakeholder feedback. Accordingly, we provide
several recommendations for thinking and writing about the use of fact-checking
artefacts.","['Michael Schlichtkrull', 'Nedjma Ousidhoum', 'Andreas Vlachos']",0,0.64701223
"Multimedia content has become ubiquitous on social media platforms, leading
to the rise of multimodal misinformation (MM) and the urgent need for effective
strategies to detect and prevent its spread. In recent years, the challenge of
multimodal misinformation detection (MMD) has garnered significant attention by
researchers and has mainly involved the creation of annotated, weakly
annotated, or synthetically generated training datasets, along with the
development of various deep learning MMD models. However, the problem of
unimodal bias has been overlooked, where specific patterns and biases in MMD
benchmarks can result in biased or unimodal models outperforming their
multimodal counterparts on an inherently multimodal task; making it difficult
to assess progress. In this study, we systematically investigate and identify
the presence of unimodal bias in widely-used MMD benchmarks, namely VMU-Twitter
and COSMOS. To address this issue, we introduce the ""VERification of Image-TExt
pairs"" (VERITE) benchmark for MMD which incorporates real-world data, excludes
""asymmetric multimodal misinformation"" and utilizes ""modality balancing"". We
conduct an extensive comparative study with a Transformer-based architecture
that shows the ability of VERITE to effectively address unimodal bias,
rendering it a robust evaluation framework for MMD. Furthermore, we introduce a
new method -- termed Crossmodal HArd Synthetic MisAlignment (CHASMA) -- for
generating realistic synthetic training data that preserve crossmodal relations
between legitimate images and false human-written captions. By leveraging
CHASMA in the training process, we observe consistent and notable improvements
in predictive performance on VERITE; with a 9.2% increase in accuracy. We
release our code at: https://github.com/stevejpapad/image-text-verification","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Symeon Papadopoulos', 'Panagiotis C. Petrantonakis']",2,0.70202357
"In online forums like Reddit, users share their experiences with medical
conditions and treatments, including making claims, asking questions, and
discussing the effects of treatments on their health. Building systems to
understand this information can effectively monitor the spread of
misinformation and verify user claims. The Task-8 of the 2023 International
Workshop on Semantic Evaluation focused on medical applications, specifically
extracting patient experience- and medical condition-related entities from user
posts on social media. The Reddit Health Online Talk (RedHot) corpus contains
posts from medical condition-related subreddits with annotations characterizing
the patient experience and medical conditions. In Subtask-1, patient experience
is characterized by personal experience, questions, and claims. In Subtask-2,
medical conditions are characterized by population, intervention, and outcome.
For the automatic extraction of patient experiences and medical condition
information, as a part of the challenge, we proposed language-model-based
extraction systems that ranked $3^{rd}$ on both subtasks' leaderboards. In this
work, we describe our approach and, in addition, explore the automatic
extraction of this information using domain-specific language models and the
inclusion of external knowledge.","['Giridhar Kaushik Ramachandran', 'Haritha Gangavarapu', 'Kevin Lybarger', 'Ozlem Uzuner']",0,0.53227174
"The surge of research on fake news and misinformation in the aftermath of the
2016 election has led to a significant increase in publicly available source
code repositories. Our study aims to systematically analyze and evaluate the
most relevant repositories and their Python source code in this area to improve
awareness, quality, and understanding of these resources within the research
community. Additionally, our work aims to measure the quality and complexity
metrics of these repositories and identify their fundamental features to aid
researchers in advancing the fields knowledge in understanding and preventing
the spread of misinformation on social media. As a result, we found that more
popular fake news repositories and associated papers with higher citation
counts tend to have more maintainable code measures, more complex code paths, a
larger number of lines of code, a higher Halstead effort, and fewer comments.","['Jason Duran', 'Mostofa Sakib', 'Nasir Eisty', 'Francesca Spezzano']",0,0.61419195
"The emerging social network platforms enable users to share their own
opinions, as well as to exchange opinions with others. However, adversarial
network perturbation, where malicious users intentionally spread their extreme
opinions, rumors, and misinformation to others, is ubiquitous in social
networks. Such adversarial network perturbation greatly influences the opinion
formation of the public and threatens our societies. Thus, it is critical to
study and control the influence of adversarial network perturbation. Although
tremendous efforts have been made in both academia and industry to guide and
control the public opinion dynamics, most of these works assume that the
network is static, and ignore such adversarial network perturbation. In this
work, based on the well-accepted Friedkin-Johnsen opinion dynamics model, we
model the adversarial network perturbation and analyze its impact on the
networks' opinion. Then, from the adversary's perspective, we analyze its
optimal network perturbation, which maximally changes the network's opinion.
Next, from the network defender's perspective, we formulate a Stackelberg game
and aim to control the network's opinion even under such adversarial network
perturbation. We devise a projected subgradient algorithm to solve the
formulated Stackelberg game. Extensive simulations on real social networks
validate our analysis of the adversarial network perturbation's influence and
the effectiveness of the proposed opinion control algorithm.","['Yuejiang Li', 'Zhanjiang Chen', 'H. Vicky Zhao']",2,0.588135
"Human societies are organized and developed through collective cooperative
behaviors, in which interactions between individuals are governed by the
underlying social connections. It is well known that, based on the information
in their environment, individuals can form collective cooperation by
strategically imitating superior behaviors and changing unfavorable
surroundings in self-organizing ways. However, facing the tough situation that
some humans and social bots keep spreading misinformation, we still lack the
systematic investigation on the impact of such proliferation of misinformation
on the evolution of social cooperation. Here we study this problem by virtue of
classical evolutionary game theory. We find that misinformation generally
impedes the emergence of collective cooperation compared to scenarios with
completely true information, although the level of cooperation is slightly
higher when the benefits provided by cooperators are reduced below a proven
threshold. We further show that this possible advantage shrinks as social
connections become denser, suggesting that misinformation is more detrimental
to the formation of collective cooperation when 'social viscosity' is low. Our
results uncover the quantitative effect of misinformation on the social
cooperative behavior in the complex networked society, and pave the way for
designing possible interventions to improve collective cooperation.","['Yao Meng', 'Mark Broom', 'Aming Li']",0,0.5446259
"The spread of misinformation in social media outlets has become a prevalent
societal problem and is the cause of many kinds of social unrest. Curtailing
its prevalence is of great importance and machine learning has shown
significant promise. However, there are two main challenges when applying
machine learning to this problem. First, while much too prevalent in one
respect, misinformation, actually, represents only a minor proportion of all
the postings seen on social media. Second, labeling the massive amount of data
necessary to train a useful classifier becomes impractical. Considering these
challenges, we propose a simple semi-supervised learning framework in order to
deal with extreme class imbalances that has the advantage, over other
approaches, of using actual rather than simulated data to inflate the minority
class. We tested our framework on two sets of Covid-related Twitter data and
obtained significant improvement in F1-measure on extremely imbalanced
scenarios, as compared to simple classical and deep-learning data generation
methods such as SMOTE, ADASYN, or GAN-based data generation.","['Yueyang Liu', 'Zois Boukouvalas', 'Nathalie Japkowicz']",2,0.68459815
"The illusion of consensus occurs when people believe there is consensus
across multiple sources, but the sources are the same and thus there is no
""true"" consensus. We explore this phenomenon in the context of an AI-based
intelligent agent designed to augment metacognition on social media.
Misinformation, especially on platforms like Twitter, is a global problem for
which there is currently no good solution. As an explainable AI (XAI) system,
the agent provides explanations for its decisions on the misinformed nature of
social media content. In this late-breaking study, we explored the roles of
trust (attitude) and reliance (behaviour) as key elements of XAI user
experience (UX) and whether these influenced the illusion of consensus.
Findings show no effect of trust, but an effect of reliance on consensus-based
explanations. This work may guide the design of anti-misinformation systems
that use XAI, especially the user-centred design of explanations.","['Takane Ueno', 'Yeongdae Kim', 'Hiroki Oura', 'Katie Seaborn']",9,0.6406292
"The growing reliance of society on social media for authentic information has
done nothing but increase over the past years. This has only raised the
potential consequences of the spread of misinformation. One of the growing
methods in popularity is to deceive users using a deepfake. A deepfake is an
invention that has come with the latest technological advancements, which
enables nefarious online users to replace their face with a computer generated,
synthetic face of numerous powerful members of society. Deepfake images and
videos now provide the means to mimic important political and cultural figures
to spread massive amounts of false information. Models that can detect these
deepfakes to prevent the spread of misinformation are now of tremendous
necessity. In this paper, we propose a new deepfake detection schema utilizing
two deep learning algorithms: long short term memory and multilayer perceptron.
We evaluate our model using a publicly available dataset named 140k Real and
Fake Faces to detect images altered by a deepfake with accuracies achieved as
high as 74.7%","['Jacob Mallet', 'Natalie Krueger', 'Mounika Vanamala', 'Rushit Dave']",11,0.79683435
"In the era of social media, people frequently share their own opinions online
on various issues and also in the way, get exposed to others' opinions. Be it
for selective exposure of news feed recommendation algorithms or our own
inclination to listen to opinions that support ours, the result is that we get
more and more exposed to opinions closer to ours. Further, any population is
inherently heterogeneous i.e. people will hold a varied range of opinions
regarding a topic and showcase a varied range of openness to get influenced by
others. In this paper, we demonstrate the different behavior put forward by
open- and close-minded agents towards an issue, when allowed to freely intermix
and communicate.
  We have shown that the intermixing among people leads to formation of opinion
echo chambers i.e. a small closed network of people who hold similar opinions
and are not affected by opinions of people outside the network. Echo chambers
are evidently harmful for a society because it inhibits free healthy
communication among all and thus, prevents exchange of opinions, spreads
misinformation and increases extremist beliefs. This calls for reduction in
echo chambers, because a total consensus of opinion is neither possible nor is
welcome. We show that the number of echo chambers depends on the number of
close-minded agents and cannot be lessened by increasing the number of
open-minded agents. We identify certain 'moderate'-minded agents, who possess
the capability of manipulating and reducing the number of echo chambers. The
paper proposes an algorithm for intelligent placement of moderate-minded agents
in the opinion-time spectrum by which the opinion echo chambers can be
maximally reduced. With various experimental setups, we demonstrate that the
proposed algorithm fares well when compared to placement of other agents (open-
or close-minded) and random placement of 'moderate'-minded agents.","['Prithwish Jana', 'Romit Roy Choudhury', 'Niloy Ganguly']",3,0.6367346
"As growing usage of social media websites in the recent decades, the amount
of news articles spreading online rapidly, resulting in an unprecedented scale
of potentially fraudulent information. Although a plenty of studies have
applied the supervised machine learning approaches to detect such content, the
lack of gold standard training data has hindered the development. Analysing the
single data format, either fake text description or fake image, is the
mainstream direction for the current research. However, the misinformation in
real-world scenario is commonly formed as a text-image pair where the news
article/news title is described as text content, and usually followed by the
related image. Given the strong ability of learning features without labelled
data, contrastive learning, as a self-learning approach, has emerged and
achieved success on the computer vision. In this paper, our goal is to explore
the constrastive learning in the domain of misinformation identification. We
developed a self-learning model and carried out the comprehensive experiments
on a public data set named COSMOS. Comparing to the baseline classifier, our
model shows the superior performance of non-matched image-text pair detection
(approximately 10%) when the training data is insufficient. In addition, we
observed the stability for contrsative learning and suggested the use of it
offers large reductions in the number of training data, whilst maintaining
comparable classification results.","['Hao Chen', 'Peng Zheng', 'Xin Wang', 'Shu Hu', 'Bin Zhu', 'Jinrong Hu', 'Xi Wu', 'Siwei Lyu']",0,0.6913653
"Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns.","['Abeer Alessa', 'Hend Al-Khalifa']",9,0.5443438
"Misinformation is a global problem in modern social media platforms with few
solutions known to be effective. Social media platforms have offered tools to
raise awareness of information, but these are closed systems that have not been
empirically evaluated. Others have developed novel tools and strategies, but
most have been studied out of context using static stimuli, researcher prompts,
or low fidelity prototypes. We offer a new anti-misinformation agent grounded
in theories of metacognition that was evaluated within Twitter. We report on a
pilot study (n=17) and multi-part experimental study (n=57, n=49) where
participants experienced three versions of the agent, each deploying a
different strategy. We found that no single strategy was superior over the
control. We also confirmed the necessity of transparency and clarity about the
agent's underlying logic, as well as concerns about repeated exposure to
misinformation and lack of user engagement.","['Yeongdae Kim', 'Takane Ueno', 'Katie Seaborn', 'Hiroki Oura', 'Jacqueline Urakami', 'Yuto Sawa']",0,0.70282805
"Misinformation has become a growing issue on online social platforms (OSPs),
especially during elections or pandemics. To combat this, OSPs have implemented
various policies, such as tagging, to notify users about potentially misleading
information. However, these policies are often transparent and therefore
susceptible to being exploited by content creators, who may not be willing to
invest effort into producing authentic content, causing the viral spread of
misinformation. Instead of mitigating the reach of existing misinformation,
this work focuses on a solution of prevention, aiming to stop the spread of
misinformation before it has a chance to gain momentum. We propose a Bayesian
persuaded branching process ($\operatorname{BP}^2$) to model the strategic
interactions among the OSP, the content creator, and the user. The
misinformation spread on OSP is modeled by a multi-type branching process,
where users' positive and negative comments influence the misinformation
spreading. Using a Lagrangian induced by Bayesian plausibility, we characterize
the OSP's optimal policy under the perfect Bayesian equilibrium. The convexity
of the Lagrangian implies that the OSP's optimal policy is simply the fully
informative tagging policy: revealing the content's accuracy to the user. Such
a tagging policy solicits the best effort from the content creator in reducing
misinformation, even though the OSP exerts no direct control over the content
creator. We corroborate our findings using numerical simulations.","['Ya-Ting Yang', 'Tao Li', 'Quanyan Zhu']",2,0.59832895
"Fake news detection algorithms apply machine learning to various news
attributes and their relationships. However, their success is usually evaluated
based on how the algorithm performs on a static benchmark, independent of real
users. On the other hand, studies of user trust in fake news has identified
relevant factors such as the user's previous beliefs, the article format, and
the source's reputation. We present a user study (n=40) evaluating how warnings
issued by fake news detection algorithms affect the user's ability to detect
misinformation. We find that such warnings strongly influence users' perception
of the truth, that even a moderately accurate classifier can improve overall
user accuracy, and that users tend to be biased towards agreeing with the
algorithm, even when it is incorrect.","['Bruno Tafur', 'Advait Sarkar']",4,0.7415726
"With the current shift in the mass media landscape from journalistic rigor to
social media, personalized social media is becoming the new norm. Although the
digitalization progress of the media brings many advantages, it also increases
the risk of spreading disinformation, misinformation, and malformation through
the use of fake news. The emergence of this harmful phenomenon has managed to
polarize society and manipulate public opinion on particular topics, e.g.,
elections, vaccinations, etc. Such information propagated on social media can
distort public perceptions and generate social unrest while lacking the rigor
of traditional journalism. Natural Language Processing and Machine Learning
techniques are essential for developing efficient tools that can detect fake
news. Models that use the context of textual data are essential for resolving
the fake news detection problem, as they manage to encode linguistic features
within the vector representation of words. In this paper, we propose a new
approach that uses document embeddings to build multiple models that accurately
label news articles as reliable or fake. We also present a benchmark on
different architectures that detect fake news using binary or multi-labeled
classification. We evaluated the models on five large news corpora using
accuracy, precision, and recall. We obtained better results than more complex
state-of-the-art Deep Neural Network models. We observe that the most important
factor for obtaining high accuracy is the document encoding, not the
classification model's complexity.","['Ciprian-Octavian TruicƒÉ', 'Elena-Simona Apostol']",4,0.8496065
"Misinformation is considered a threat to our democratic values and
principles. The spread of such content on social media polarizes society and
undermines public discourse by distorting public perceptions and generating
social unrest while lacking the rigor of traditional journalism. Transformers
and transfer learning proved to be state-of-the-art methods for multiple
well-known natural language processing tasks. In this paper, we propose
MisRoB{\AE}RTa, a novel transformer-based deep neural ensemble architecture for
misinformation detection. MisRoB{\AE}RTa takes advantage of two transformers
(BART \& RoBERTa) to improve the classification performance. We also
benchmarked and evaluated the performances of multiple transformers on the task
of misinformation detection. For training and testing, we used a large
real-world news articles dataset labeled with 10 classes, addressing two
shortcomings in the current research: increasing the size of the dataset from
small to large, and moving the focus of fake news detection from binary
classification to multi-class classification. For this dataset, we manually
verified the content of the news articles to ensure that they were correctly
labeled. The experimental results show that the accuracy of transformers on the
misinformation detection problem was significantly influenced by the method
employed to learn the context, dataset size, and vocabulary dimension. We
observe empirically that the best accuracy performance among the classification
models that use only one transformer is obtained by BART, while DistilRoBERTa
obtains the best accuracy in the least amount of time required for fine-tuning
and training. The proposed MisRoB{\AE}RTa outperforms the other transformer
models in the task of misinformation detection. To arrive at this conclusion,
we performed ample ablation and sensitivity testing with MisRoB{\AE}RTa on two
datasets.","['Ciprian-Octavian TruicƒÉ', 'Elena-Simona Apostol']",4,0.7160661
"Recent years have witnessed the sustained evolution of misinformation that
aims at manipulating public opinions. Unlike traditional rumors or fake news
editors who mainly rely on generated and/or counterfeited images, text and
videos, current misinformation creators now more tend to use out-of-context
multimedia contents (e.g. mismatched images and captions) to deceive the public
and fake news detection systems. This new type of misinformation increases the
difficulty of not only detection but also clarification, because every
individual modality is close enough to true information. To address this
challenge, in this paper we explore how to achieve interpretable cross-modal
de-contextualization detection that simultaneously identifies the mismatched
pairs and the cross-modal contradictions, which is helpful for fact-check
websites to document clarifications. The proposed model first symbolically
disassembles the text-modality information to a set of fact queries based on
the Abstract Meaning Representation of the caption and then forwards the
query-image pairs into a pre-trained large vision-language model select the
``evidences"" that are helpful for us to detect misinformation. Extensive
experiments indicate that the proposed methodology can provide us with much
more interpretable predictions while maintaining the accuracy same as the
state-of-the-art model on this task.","['Yizhou Zhang', 'Loc Trinh', 'Defu Cao', 'Zijun Cui', 'Yan Liu']",1,0.76336664
"Despite the ever-strong demand for mental health care globally, access to
traditional mental health services remains severely limited expensive, and
stifled by stigma and systemic barriers. Thus, over the last few years, young
people are increasingly turning to content on video-sharing platforms (VSPs)
like TikTok and YouTube to help them navigate their mental health journey.
However, navigating towards trustworthy information relating to mental health
on these platforms is challenging, given the uncontrollable and unregulated
growth of dedicated mental health content and content creators catering to a
wide array of mental health conditions on these platforms. In this paper, we
attempt to define what constitutes as ""mental health misinformation"" through
examples. In addition, we also suggest some open questions to answer and
challenges to tackle regarding this important and timely research topic","['Viet Cuong Nguyen', 'Michael Birnbaum', 'Munmun De Choudhury']",5,0.49705076
"Vaccine hesitancy continues to be a main challenge for public health
officials during the COVID-19 pandemic. As this hesitancy undermines vaccine
campaigns, many researchers have sought to identify its root causes, finding
that the increasing volume of anti-vaccine misinformation on social media
platforms is a key element of this problem. We explored Twitter as a source of
misleading content with the goal of extracting overlapping cultural and
political beliefs that motivate the spread of vaccine misinformation. To do
this, we have collected a data set of vaccine-related Tweets and annotated them
with the help of a team of annotators with a background in communications and
journalism. Ultimately we hope this can lead to effective and targeted public
health communication strategies for reaching individuals with anti-vaccine
beliefs. Moreover, this information helps with developing Machine Learning
models to automatically detect vaccine misinformation posts and combat their
negative impacts. In this paper, we present Vax-Culture, a novel Twitter
COVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an
extensive set of human-provided annotations including vaccine-hesitancy stance,
indication of any misinformation in tweets, the entities criticized and
supported in each tweet and the communicated message of each tweet. Moreover,
we define five baseline tasks including four classification and one sequence
generation tasks, and report the results of a set of recent transformer-based
models for them. The dataset and code are publicly available at
https://github.com/mrzarei5/Vax-Culture.","['Mohammad Reza Zarei', 'Michael Christensen', 'Sarah Everts', 'Majid Komeili']",12,0.87255025
"Twitter bot detection has become a crucial task in efforts to combat online
misinformation, mitigate election interference, and curb malicious propaganda.
However, advanced Twitter bots often attempt to mimic the characteristics of
genuine users through feature manipulation and disguise themselves to fit in
diverse user communities, posing challenges for existing Twitter bot detection
models. To this end, we propose BotMoE, a Twitter bot detection framework that
jointly utilizes multiple user information modalities (metadata, textual
content, network structure) to improve the detection of deceptive bots.
Furthermore, BotMoE incorporates a community-aware Mixture-of-Experts (MoE)
layer to improve domain generalization and adapt to different Twitter
communities. Specifically, BotMoE constructs modal-specific encoders for
metadata features, textual content, and graphical structure, which jointly
model Twitter users from three modal-specific perspectives. We then employ a
community-aware MoE layer to automatically assign users to different
communities and leverage the corresponding expert networks. Finally, user
representations from metadata, text, and graph perspectives are fused with an
expert fusion layer, combining all three modalities while measuring the
consistency of user information. Extensive experiments demonstrate that BotMoE
significantly advances the state-of-the-art on three Twitter bot detection
benchmarks. Studies also confirm that BotMoE captures advanced and evasive
bots, alleviates the reliance on training data, and better generalizes to new
and previously unseen user communities.","['Yuhan Liu', 'Zhaoxuan Tan', 'Heng Wang', 'Shangbin Feng', 'Qinghua Zheng', 'Minnan Luo']",13,0.7848879
"The COVID-19 pandemic led to an infodemic where an overwhelming amount of
COVID-19 related content was being disseminated at high velocity through social
media. This made it challenging for citizens to differentiate between accurate
and inaccurate information about COVID-19. This motivated us to carry out a
comparative study of the characteristics of COVID-19 misinformation versus
those of accurate COVID-19 information through a large-scale computational
analysis of over 242 million tweets. The study makes comparisons alongside four
key aspects: 1) the distribution of topics, 2) the live status of tweets, 3)
language analysis and 4) the spreading power over time. An added contribution
of this study is the creation of a COVID-19 misinformation classification
dataset. Finally, we demonstrate that this new dataset helps improve
misinformation classification by more than 9\% based on average F1 measure.","['Yida Mu', 'Ye Jiang', 'Freddy Heppell', 'Iknoor Singh', 'Carolina Scarton', 'Kalina Bontcheva', 'Xingyi Song']",5,0.7234313
"Despite recent concerns about undesirable behaviors generated by large
language models (LLMs), including non-factual, biased, and hateful language, we
find LLMs are inherent multi-task language checkers based on their latent
representations of natural and social knowledge. We present an interpretable,
unified, language checking (UniLC) method for both human and machine-generated
language that aims to check if language input is factual and fair. While
fairness and fact-checking tasks have been handled separately with dedicated
models, we find that LLMs can achieve high performance on a combination of
fact-checking, stereotype detection, and hate speech detection tasks with a
simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task
language checking method proposed in this work, the GPT3.5-turbo model
outperforms fully supervised baselines on several language tasks. The simple
approach and results suggest that based on strong latent knowledge
representations, an LLM can be an adaptive and explainable tool for detecting
misinformation, stereotypes, and hate speech.","['Tianhua Zhang', 'Hongyin Luo', 'Yung-Sung Chuang', 'Wei Fang', 'Luc Gaitskell', 'Thomas Hartvigsen', 'Xixin Wu', 'Danny Fox', 'Helen Meng', 'James Glass']",6,0.75223637
"The outbreak of COVID-19 has led to a global surge of Sinophobia partly
because of the spread of misinformation, disinformation, and fake news on
China. In this paper, we report on the creation of a novel classifier that
detects whether Chinese-language social media posts from Twitter are related to
fake news about China. The classifier achieves an F1 score of 0.64 and an
accuracy rate of 93%. We provide the final model and a new training dataset
with 18,425 tweets for researchers to study fake news in the Chinese language
during the COVID-19 pandemic. We also introduce a new dataset generated by our
classifier that tracks the dynamics of fake news in the Chinese language during
the early pandemic.","['Yongjun Zhang', 'Sijia Liu', 'Yi Wang', 'Xinguang Fan']",4,0.67216855
"Detecting misinformation threads is crucial to guarantee a healthy
environment on social media. We address the problem using the data set created
during the COVID-19 pandemic. It contains cascades of tweets discussing
information weakly labeled as reliable or unreliable, based on a previous
evaluation of the information source. The models identifying unreliable threads
usually rely on textual features. But reliability is not just what is said, but
by whom and to whom. We additionally leverage on network information. Following
the homophily principle, we hypothesize that users who interact are generally
interested in similar topics and spreading similar kind of news, which in turn
is generally reliable or not. We test several methods to learn representations
of the social interactions within the cascades, combining them with deep neural
language models in a Multi-Input (MI) framework. Keeping track of the sequence
of the interactions during the time, we improve over previous state-of-the-art
models.","['Tommaso Fornaciari', 'Luca Luceri', 'Emilio Ferrara', 'Dirk Hovy']",4,0.6966403
"One of the new developments in chit-chat bots is a long-term memory mechanism
that remembers information from past conversations for increasing engagement
and consistency of responses. The bot is designed to extract knowledge of
personal nature from their conversation partner, e.g., stating preference for a
particular color. In this paper, we show that this memory mechanism can result
in unintended behavior. In particular, we found that one can combine a personal
statement with an informative statement that would lead the bot to remember the
informative statement alongside personal knowledge in its long term memory.
This means that the bot can be tricked into remembering misinformation which it
would regurgitate as statements of fact when recalling information relevant to
the topic of conversation. We demonstrate this vulnerability on the BlenderBot
2 framework implemented on the ParlAI platform and provide examples on the more
recent and significantly larger BlenderBot 3 model. We generate 150 examples of
misinformation, of which 114 (76%) were remembered by BlenderBot 2 when
combined with a personal statement. We further assessed the risk of this
misinformation being recalled after intervening innocuous conversation and in
response to multiple questions relevant to the injected memory. Our evaluation
was performed on both the memory-only and the combination of memory and
internet search modes of BlenderBot 2. From the combinations of these
variables, we generated 12,890 conversations and analyzed recalled
misinformation in the responses. We found that when the chat bot is questioned
on the misinformation topic, it was 328% more likely to respond with the
misinformation as fact when the misinformation was in the long-term memory.","['Conor Atkins', 'Benjamin Zi Hao Zhao', 'Hassan Jameel Asghar', 'Ian Wood', 'Mohamed Ali Kaafar']",13,0.6713575
"Misinformation has become a pressing issue. Fake media, in both visual and
textual forms, is widespread on the web. While various deepfake detection and
text fake news detection methods have been proposed, they are only designed for
single-modality forgery based on binary classification, let alone analyzing and
reasoning subtle forgery traces across different modalities. In this paper, we
highlight a new research problem for multi-modal fake media, namely Detecting
and Grounding Multi-Modal Media Manipulation (DGM^4). DGM^4 aims to not only
detect the authenticity of multi-modal media, but also ground the manipulated
content (i.e., image bounding boxes and text tokens), which requires deeper
reasoning of multi-modal media manipulation. To support a large-scale
investigation, we construct the first DGM^4 dataset, where image-text pairs are
manipulated by various approaches, with rich annotation of diverse
manipulations. Moreover, we propose a novel HierArchical Multi-modal
Manipulation rEasoning tRansformer (HAMMER) to fully capture the fine-grained
interaction between different modalities. HAMMER performs 1) manipulation-aware
contrastive learning between two uni-modal encoders as shallow manipulation
reasoning, and 2) modality-aware cross-attention by multi-modal aggregator as
deep manipulation reasoning. Dedicated manipulation detection and grounding
heads are integrated from shallow to deep levels based on the interacted
multi-modal information. Finally, we build an extensive benchmark and set up
rigorous evaluation metrics for this new research problem. Comprehensive
experiments demonstrate the superiority of our model; several valuable
observations are also revealed to facilitate future research in multi-modal
media manipulation.","['Rui Shao', 'Tianxing Wu', 'Ziwei Liu']",11,0.67404497
"Recent years have witnessed a significant increase in cyber crimes and system
failures caused by misinformation. Many of these instances can be classified as
gaslighting, which involves manipulating the perceptions of others through the
use of information. In this paper, we propose a dynamic game-theoretic
framework built on a partially observed stochastic control system to study
gaslighting. The decision-maker (DM) in the game only accesses partial
observations, and she determines the controls by constructing information
states that capture her perceptions of the system. The gaslighter in the game
influences the system indirectly by designing the observations to manipulate
the DM's perceptions and decisions. We analyze the impact of the gaslighter's
efforts using robustness analysis of the information states and optimal value
to deviations in the observations. A stealthiness constraint is introduced to
restrict the power of the gaslighter and to help him stay undetected. We
consider approximate feedback Stackelberg equilibrium as the solution concept
and estimate the cost of gaslighting.","['Shutian Liu', 'Quanyan Zhu']",0,0.592673
"The explosive growth of online misinformation, such as false claims, has
affected the social behavior of online users. In order to be persuasive and
mislead the audience, false claims are made to trigger emotions in their
audience. This paper contributes to understanding how misinformation in social
media is shaped by investigating the emotional framing that authors of the
claims try to create for their audience. We investigate how, firstly, the
existence of emotional framing in the claims depends on the topic and
credibility of the claims. Secondly, we explore how emotionally framed content
triggers emotional response posts by social media users, and how emotions
expressed in claims and corresponding users' response posts affect their
sharing behavior on social media. Analysis of four data sets covering different
topics (politics, health, Syrian war, and COVID-19) reveals that authors shape
their claims depending on the topic area to pass targeted emotions to their
audience. By analysing responses to claims, we show that the credibility of the
claim influences the distribution of emotions that the claim incites in its
audience. Moreover, our analysis shows that emotions expressed in the claims
are repeated in the users' responses. Finally, the analysis of users' sharing
behavior shows that negative emotional framing such as anger, fear, and sadness
of false claims leads to more interaction among users than positive emotions.
This analysis also reveals that in the claims that trigger happy responses,
true claims result in more sharing compared to false claims.","['Akram Sadat Hosseini', 'Steffen Staab']",3,0.69618
"Retracted scientific articles about COVID-19 vaccines have proliferated false
claims about vaccination harms and discouraged vaccine acceptance. Our study
analyzed the topical content of 4,876 English-language tweets about retracted
COVID-19 vaccine research and found that 27.4% of tweets contained
retraction-related misinformation. Misinformed tweets either ignored the
retraction, or less commonly, politicized the retraction using conspiratorial
rhetoric. To address this, Twitter and other social media platforms should
expand their efforts to address retraction-related misinformation.","['Rod Abhari', 'Esteban Villa-Turek', 'Nicholas Vincent', 'Henry Dambanemuya', 'Em≈ëke-√Ågnes Horv√°t']",12,0.82085055
"COVID-19 misinformation on social media platforms such as twitter is a threat
to effective pandemic management. Prior works on tweet COVID-19 misinformation
negates the role of semantic features common to twitter such as charged
emotions. Thus, we present a novel COVID-19 misinformation model, which uses
both a tweet emotion encoder and COVID-19 misinformation encoder to predict
whether a tweet contains COVID-19 misinformation. Our emotion encoder was
fine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder
was fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show
superior results using the combination of emotion and misinformation encoders
as opposed to a misinformation classifier alone. Furthermore, extensive result
analysis was conducted, highlighting low quality labels and mismatched label
distributions as key limitations to our study.","['Gabriel Asher', 'Phil Bohlman', 'Karsten Kleyensteuber']",5,0.6335986
"Mis- and disinformation can spread rapidly on video-sharing platforms (VSPs).
Despite the growing use of VSPs, there has not been a proportional increase in
our ability to understand this medium and the messages conveyed through it. In
this work, we draw on our prior experiences to outline three core challenges
faced in studying VSPs in high-stakes and fast-paced settings: (1) navigating
the unique affordances of VSPs, (2) understanding VSP content and determining
its authenticity, and (3) novel user behaviors on VSPs for spreading
misinformation. By highlighting these challenges, we hope that researchers can
reflect on how to adapt existing research methods and tools to these new
contexts, or develop entirely new ones.","['Sukrit Venkatagiri', 'Joseph S. Schafer', 'Stephen Prochaska']",0,0.5969699
"We propose VADER, a spatio-temporal matching, alignment, and change
summarization method to help fight misinformation spread via manipulated
videos. VADER matches and coarsely aligns partial video fragments to candidate
videos using a robust visual descriptor and scalable search over adaptively
chunked video content. A transformer-based alignment module then refines the
temporal localization of the query fragment within the matched video. A
space-time comparator module identifies regions of manipulation between aligned
content, invariant to any changes due to any residual temporal misalignments or
artifacts arising from non-editorial changes of the content. Robustly matching
video to a trusted source enables conclusions to be drawn on video provenance,
enabling informed trust decisions on content encountered.","['Alexander Black', 'Simon Jenni', 'Tu Bui', 'Md. Mehrab Tanjim', 'Stefano Petrangeli', 'Ritwik Sinha', 'Viswanathan Swaminathan', 'John Collomosse']",1,0.47443542
"With the spread of high-speed Internet and portable smart devices, the way
people access and consume information has drastically changed. However, this
presents many challenges, including information overload, personal data
leakage, and misinformation diffusion. Across the spectrum of risks that
Internet users face nowadays, this work focuses on understanding how young
people perceive and deal with false information. Within an experimental
campaign involving 183 students, we presented six different news items to the
participants and invited them to browse the Internet to assess the veracity of
the presented information. Our results suggest that online search is more
likely to lead students to validate true news than to refute false ones. We
found that students change their opinion about a specific piece of information
more often than their global idea about a broader topic. Also, our experiment
reflected that most participants rely on online sources to obtain information
and access the news, and those getting information from books and Internet
browsing are the most accurate in assessing the veracity of a news item. This
work provides a principled understanding of how young people perceive and
distinguish true and false pieces of information, identifying strengths and
weaknesses amidst young subjects and contributing to building tailored digital
information literacy strategies for youth.","['Azza Bouleimen', 'Luca Luceri', 'Felipe Cardoso', 'Luca Botturi', 'Martin Hermida', 'Loredana Addimando', 'Chiara Beretta', 'Marzia Galloni', 'Silvia Giordano']",0,0.7558227
"Adaptive design optimization (ADO) is a state-of-the-art technique for
experimental design (Cavagnaro, Myung, Pitt, & Kujala, 2010). ADO dynamically
identifies stimuli that, in expectation, yield the most information about a
hypothetical construct of interest (e.g., parameters of a cognitive model). To
calculate this expectation, ADO leverages the modeler's existing knowledge,
specified in the form of a prior distribution. Informative priors align with
the distribution of the focal construct in the participant population. This
alignment is assumed by ADO's internal assessment of expected information gain.
If the prior is instead misinformative, i.e., does not align with the
participant population, ADO's estimates of expected information gain could be
inaccurate. In many cases, the true distribution that characterizes the
participant population is unknown, and experimenters rely on heuristics in
their choice of prior and without an understanding of how this choice affects
ADO's behavior.
  Our work introduces a mathematical framework that facilitates investigation
of the consequences of the choice of prior distribution on the efficiency of
experiments designed using ADO. Through theoretical and empirical results, we
show that, in the context of prior misinformation, measures of expected
information gain are distinct from the correctness of the corresponding
inference. Through a series of simulation experiments, we show that, in the
case of parameter estimation, ADO nevertheless outperforms other design
methods. Conversely, in the case of model selection, misinformative priors can
lead inference to favor the wrong model, and rather than mitigating this
pitfall, ADO exacerbates it.","['Sabina J. Sloman', 'Daniel Cavagnaro', 'Stephen B. Broomell']",2,0.60251844
"For more than a decade scholars have been investigating the disinformation
flow on social media contextually to societal events, like, e.g., elections. In
this paper, we analyze the Twitter traffic related to the US 2020 pre-election
debate and ask whether it mirrors the electoral system. The U.S. electoral
system provides that, regardless of the actual vote gap, the premier candidate
who received more votes in one state `takes' that state. Criticisms of this
system have pointed out that election campaigns can be more intense in
particular key states to achieve victory, so-called {\it swing states}. Our
intuition is that election debate may cause more traffic on Twitter-and
probably be more plagued by misinformation-when associated with swing states.
The results mostly confirm the intuition. About 88\% of the entire traffic can
be associated with swing states, and links to non-trustworthy news are shared
far more in swing-related traffic than the same type of news in safe-related
traffic. Considering traffic origin instead, non-trustworthy tweets generated
by automated accounts, so-called social bots, are mostly associated with swing
states. Our work sheds light on the role an electoral system plays in the
evolution of online debates, with, in the spotlight, disinformation and social
bots.","['Manuel Pratelli', 'Marinella Petrocchi', 'Fabio Saracco', 'Rocco De Nicola']",10,0.70745564
"In recent years, industry leaders and researchers have proposed to use
technical provenance standards to address visual misinformation spread through
digitally altered media. By adding immutable and secure provenance information
such as authorship and edit date to media metadata, social media users could
potentially better assess the validity of the media they encounter. However, it
is unclear how end users would respond to provenance information, or how to
best design provenance indicators to be understandable to laypeople. We
conducted an online experiment with 595 participants from the US and UK to
investigate how provenance information altered users' accuracy perceptions and
trust in visual content shared on social media. We found that provenance
information often lowered trust and caused users to doubt deceptive media,
particularly when it revealed that the media was composited. We additionally
tested conditions where the provenance information itself was shown to be
incomplete or invalid, and found that these states have a significant impact on
participants' accuracy perceptions and trust in media, leading them, in some
cases, to disbelieve honest media. Our findings show that provenance, although
enlightening, is still not a concept well-understood by users, who confuse
media credibility with the orthogonal (albeit related) concept of provenance
credibility. We discuss how design choices may contribute to provenance
(mis)understanding, and conclude with implications for usable provenance
systems, including clearer interfaces and user education.","['K. J. Kevin Feng', 'Nick Ritchie', 'Pia Blumenthal', 'Andy Parsons', 'Amy X. Zhang']",3,0.6912974
"Misinformation propagation in online social networks has become an
increasingly challenging problem. Although many studies exist to solve the
problem computationally, a permanent and robust solution is yet to be
discovered. In this study, we propose and demonstrate the effectiveness of a
blockchain-machine learning hybrid approach for addressing the issue of
misinformation in a crowdsourced environment. First, we motivate the use of
blockchain for this problem by finding the crucial parts contributing to the
dissemination of misinformation and how blockchain can be useful, respectively.
Second, we propose a method that combines the wisdom of the crowd with a
behavioral classifier to classify the news stories in terms of their
truthfulness while reducing the effects of the actions performed by malicious
users. We conduct experiments and simulations under different scenarios and
attacks to assess the performance of this approach. Finally, we provide a case
study involving a comparison with an existing approach using Twitter Birdwatch
data. Our results suggest that this solution holds promise and warrants further
investigation.","['Tolga Yilmaz', '√ñzg√ºr Ulusoy']",2,0.7164402
"An essential topic in online social network security is how to accurately
detect bot accounts and relieve their harmful impacts (e.g., misinformation,
rumor, and spam) on genuine users. Based on a real-world data set, we construct
behavioral sequences from raw event logs. After extracting critical
characteristics from behavioral time series, we observe differences between
bots and genuine users and similar patterns among bot accounts. We present a
novel social bot detection system BotShape, to automatically catch behavioral
sequences and characteristics as features for classifiers to detect bots. We
evaluate the detection performance of our system in ground-truth instances,
showing an average accuracy of 98.52% and an average f1-score of 96.65% on
various types of classifiers. After comparing it with other research, we
conclude that BotShape is a novel approach to profiling an account, which could
improve performance for most methods by providing significant behavioral
features.","['Jun Wu', 'Xuesong Ye', 'Chengjie Mou']",13,0.79794204
"The large language model called ChatGPT has drawn extensively attention
because of its human-like expression and reasoning abilities. In this study, we
investigate the feasibility of using ChatGPT in experiments on using ChatGPT to
translate radiology reports into plain language for patients and healthcare
providers so that they are educated for improved healthcare. Radiology reports
from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI
metastases screening scans were collected in the first half of February for
this study. According to the evaluation by radiologists, ChatGPT can
successfully translate radiology reports into plain language with an average
score of 4.27 in the five-point system with 0.08 places of information missing
and 0.07 places of misinformation. In terms of the suggestions provided by
ChatGPT, they are general relevant such as keeping following-up with doctors
and closely monitoring any symptoms, and for about 37% of 138 cases in total
ChatGPT offers specific suggestions based on findings in the report. ChatGPT
also presents some randomness in its responses with occasionally
over-simplified or neglected information, which can be mitigated using a more
detailed prompt. Furthermore, ChatGPT results are compared with a newly
released large model GPT-4, showing that GPT-4 can significantly improve the
quality of translated reports. Our results show that it is feasible to utilize
large language models in clinical education, and further efforts are needed to
address limitations and maximize their potential.","['Qing Lyu', 'Josh Tan', 'Michael E. Zapadka', 'Janardhana Ponnatapura', 'Chuang Niu', 'Kyle J. Myers', 'Ge Wang', 'Christopher T. Whitlow']",8,0.52724344
"Online misinformation has been a serious threat to public health and society.
Social media users are known to reply to misinformation posts with
counter-misinformation messages, which have been shown to be effective in
curbing the spread of misinformation. This is called social correction.
However, the characteristics of tweets that attract social correction versus
those that do not remain unknown. To close the gap, we focus on answering the
following two research questions: (1) ``Given a tweet, will it be countered by
other users?'', and (2) ``If yes, what will be the magnitude of countering
it?''. This exploration will help develop mechanisms to guide users'
misinformation correction efforts and to measure disparity across users who get
corrected. In this work, we first create a novel dataset with 690,047 pairs of
misinformation tweets and counter-misinformation replies. Then, stratified
analysis of tweet linguistic and engagement features as well as tweet posters'
user attributes are conducted to illustrate the factors that are significant in
determining whether a tweet will get countered. Finally, predictive classifiers
are created to predict the likelihood of a misinformation tweet to get
countered and the degree to which that tweet will be countered. The code and
data is accessible on https://github.com/claws-lab/social-correction-twitter.","['Yingchen Ma', 'Bing He', 'Nathan Subrahmanian', 'Srijan Kumar']",10,0.675435
"Persuasion is a key aspect of what it means to be human, and is central to
business, politics, and other endeavors. Advancements in artificial
intelligence (AI) have produced AI systems that are capable of persuading
humans to buy products, watch videos, click on search results, and more. Even
systems that are not explicitly designed to persuade may do so in practice. In
the future, increasingly anthropomorphic AI systems may form ongoing
relationships with users, increasing their persuasive power. This paper
investigates the uncertain future of persuasive AI systems. We examine ways
that AI could qualitatively alter our relationship to and views regarding
persuasion by shifting the balance of persuasive power, allowing personalized
persuasion to be deployed at scale, powering misinformation campaigns, and
changing the way humans can shape their own discourse. We consider ways
AI-driven persuasion could differ from human-driven persuasion. We warn that
ubiquitous highlypersuasive AI systems could alter our information environment
so significantly so as to contribute to a loss of human control of our own
future. In response, we examine several potential responses to AI-driven
persuasion: prohibition, identification of AI agents, truthful AI, and legal
remedies. We conclude that none of these solutions will be airtight, and that
individuals and governments will need to take active steps to guard against the
most pernicious effects of persuasive AI.","['Matthew Burtell', 'Thomas Woodside']",9,0.7932663
"Deep neural retrieval models have amply demonstrated their power but
estimating the reliability of their predictions remains challenging. Most
dialog response retrieval models output a single score for a response on how
relevant it is to a given question. However, the bad calibration of deep neural
network results in various uncertainty for the single score such that the
unreliable predictions always misinform user decisions. To investigate these
issues, we present an efficient calibration and uncertainty estimation
framework PG-DRR for dialog response retrieval models which adds a Gaussian
Process layer to a deterministic deep neural network and recovers conjugacy for
tractable posterior inference by P\'{o}lya-Gamma augmentation. Finally, PG-DRR
achieves the lowest empirical calibration error (ECE) in the in-domain datasets
and the distributional shift task while keeping $R_{10}@1$ and MAP performance.","['Tong Ye', 'Shijing Si', 'Jianzong Wang', 'Ning Cheng', 'Zhitao Li', 'Jing Xiao']",2,0.6921399
"Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we systematically test the robustness of popular text classifiers
against available attacking techniques and discover that, indeed, in some cases
insignificant changes in input text can mislead the models. We also introduce
BODEGA: a benchmark for testing both victim models and attack methods on four
misinformation detection tasks in an evaluation framework designed to simulate
real use-cases of content moderation. Finally, we manually analyse a subset
adversarial examples and check what kinds of modifications are used in
successful attacks. The BODEGA code and data is openly shared in hope of
enhancing the comparability and replicability of further research in this area","['Piotr Przyby≈Ça', 'Alexander Shvets', 'Horacio Saggion']",1,0.73806703
"State-sponsored trolls are the main actors of influence campaigns on social
media and automatic troll detection is important to combat misinformation at
scale. Existing troll detection models are developed based on training data for
known campaigns (e.g.\ the influence campaign by Russia's Internet Research
Agency on the 2016 US Election), and they fall short when dealing with {\em
novel} campaigns with new targets. We propose MetaTroll, a text-based troll
detection model based on the meta-learning framework that enables high
portability and parameter-efficient adaptation to new campaigns using only a
handful of labelled samples for few-shot transfer. We introduce
\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''
campaign-specific knowledge so as to tackle catastrophic forgetting, where a
model ``forgets'' how to detect trolls from older campaigns due to continual
adaptation. Our experiments demonstrate that MetaTroll substantially
outperforms baselines and state-of-the-art few-shot text classification models.
Lastly, we explore simple approaches to extend MetaTroll to multilingual and
multimodal detection. Source code for MetaTroll is available at:
https://github.com/ltian678/metatroll-code.git.","['Lin Tian', 'Xiuzhen Zhang', 'Jey Han Lau']",8,0.60780287
"This study presents a secondary data analysis of the survey data collected as
part of the American Trends Panel series by the Pew Research Center. A logistic
regression was performed to ascertain the effects of the perceived risk of
sharing, perceived problems on Twitter, and motivation of using Twitter on the
likelihood that participants regret sharing on Twitter. The logistic regression
model was statistically significant, \c{hi}2(15) = 102.5, p < .001. The model
correctly classified 78.5 percent of cases. Whether or not Twitter users regret
sharing on Twitter depends on different motivations for using Twitter. We
observe that ""A way to express my opinion"" is statistically significant in the
mod-el, indicating that the odds of Twitter users regretting sharing for this
motivation is 2.1 times higher than that of entertainment. Perceived risks of
potential hostility and visibility were negatively associated with an increased
likelihood of regret sharing. In contrast, perceived problems on Twitter
concerning misinformation were negatively associated with the likelihood of
regret sharing.",['Kijung Lee'],10,0.71181047
"The spread of online misinformation threatens public health, democracy, and
the broader society. While professional fact-checkers form the first line of
defense by fact-checking popular false claims, they do not engage directly in
conversations with misinformation spreaders. On the other hand, non-expert
ordinary users act as eyes-on-the-ground who proactively counter misinformation
-- recent research has shown that 96% counter-misinformation responses are made
by ordinary users. However, research also found that 2/3 times, these responses
are rude and lack evidence. This work seeks to create a counter-misinformation
response generation model to empower users to effectively correct
misinformation. This objective is challenging due to the absence of datasets
containing ground-truth of ideal counter-misinformation responses, and the lack
of models that can generate responses backed by communication theories. In this
work, we create two novel datasets of misinformation and counter-misinformation
response pairs from in-the-wild social media and crowdsourcing from
college-educated students. We annotate the collected data to distinguish poor
from ideal responses that are factual, polite, and refute misinformation. We
propose MisinfoCorrect, a reinforcement learning-based framework that learns to
generate counter-misinformation responses for an input misinformation post. The
model rewards the generator to increase the politeness, factuality, and
refutation attitude while retaining text fluency and relevancy. Quantitative
and qualitative evaluation shows that our model outperforms several baselines
by generating high-quality counter-responses. This work illustrates the promise
of generative text models for social good -- here, to help create a safe and
reliable information ecosystem. The code and data is accessible on
https://github.com/claws-lab/MisinfoCorrect.","['Bing He', 'Mustaque Ahamad', 'Srijan Kumar']",1,0.75291884
"Popular messaging applications now enable end-to-end-encryption (E2EE) by
default, and E2EE data storage is becoming common. These important advances for
security and privacy create new content moderation challenges for online
services, because services can no longer directly access plaintext content.
While ongoing public policy debates about E2EE and content moderation in the
United States and European Union emphasize child sexual abuse material and
misinformation in messaging and storage, we identify and synthesize a wealth of
scholarship that goes far beyond those topics. We bridge literature that is
diverse in both content moderation subject matter, such as malware, spam, hate
speech, terrorist content, and enterprise policy compliance, as well as
intended deployments, including not only privacy-preserving content moderation
for messaging, email, and cloud storage, but also private introspection of
encrypted web traffic by middleboxes. In this work, we systematize the study of
content moderation in E2EE settings. We set out a process pipeline for content
moderation, drawing on a broad interdisciplinary literature that is not
specific to E2EE. We examine cryptography and policy design choices at all
stages of this pipeline, and we suggest areas of future research to fill gaps
in literature and better understand possible paths forward.","['Sarah Scheffler', 'Jonathan Mayer']",0,0.48807597
"We assess the situation of our elementary Linear Algebra classes in the US
holistically and through personal history recollections. Possible remedies for
our elementary Linear Algebra's teaching problems are discussed and a change
from abstract algebraic taught classes to a concrete matrix based first course
is considered. The challenges of such modernization attempts for this course
are laid out in light of our increased after-Covid use of e-books and
e-primers. We specifically address the useless and needless, but ubiquitous use
of determinants, characteristic polynomials and polynomial root finding methods
that are propagated in our elementary text books and are used in the majority
of our elementary Linear Algebra classes for the matrix eigenvalue problem but
that have no practical use whatsoever and offer no solution for finding matrix
eigenvalues. This paper challenges all mathematicians as we have misinformed
and miseducated our students badly for decades in elementary Linear Algebra now
and urges a switch to a new, fully matrix theoretical approach that covers all
classical subjects in a practical and computable way.",['Frank Uhlig'],2,0.4608811
"In this paper we proposed a Graph-Based conspiracy source detection method
for the MediaEval task 2022 FakeNews: Corona Virus and Conspiracies Multimedia
Analysis Task. The goal of this study was to apply SOTA graph neural network
methods to the problem of misinformation spreading in online social networks.
We explore three different Graph Neural Network models: GCN, GraphSAGE and
DGCNN. Experimental results demonstrate that DGCNN outperforms in terms of
accuracy.","['Atta Ullah', 'Rabeeh Ayaz Abbasi', 'Akmal Saeed Khattak', 'Anwar Said']",2,0.5991581
"Recent advancements in pre-trained language models have enabled convenient
methods for generating human-like text at a large scale. Though these
generation capabilities hold great potential for breakthrough applications, it
can also be a tool for an adversary to generate misinformation. In particular,
social media platforms like Twitter are highly susceptible to AI-generated
misinformation. A potential threat scenario is when an adversary hijacks a
credible user account and incorporates a natural language generator to generate
misinformation. Such threats necessitate automated detectors for AI-generated
tweets in a given user's Twitter timeline. However, tweets are inherently
short, thus making it difficult for current state-of-the-art pre-trained
language model-based detectors to accurately detect at what point the AI starts
to generate tweets in a given Twitter timeline. In this paper, we present a
novel algorithm using stylometric signals to aid detecting AI-generated tweets.
We propose models corresponding to quantifying stylistic changes in human and
AI tweets in two related tasks: Task 1 - discriminate between human and
AI-generated tweets, and Task 2 - detect if and when an AI starts to generate
tweets in a given Twitter timeline. Our extensive experiments demonstrate that
the stylometric features are effective in augmenting the state-of-the-art
AI-generated text detectors.","['Tharindu Kumarage', 'Joshua Garland', 'Amrita Bhattacharjee', 'Kirill Trapeznikov', 'Scott Ruston', 'Huan Liu']",13,0.6690368
"Is it possible for machines to think like humans? And if it is, how should we
go about teaching them to do so? As early as 1950, Alan Turing stated that we
ought to teach machines in the way of teaching a child. Reinforcement learning
with human feedback (RLHF) has emerged as a strong candidate toward allowing
agents to learn from human feedback in a naturalistic manner. RLHF is distinct
from traditional reinforcement learning as it provides feedback from a human
teacher in addition to a reward signal. It has been catapulted into public view
by multiple high-profile AI applications, including OpenAI's ChatGPT,
DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are
already overturning our understanding of how AI interacts with humanity. The
wide applicability and burgeoning success of RLHF strongly motivate the need to
evaluate its social impacts. In light of recent developments, this paper
considers an important question: can RLHF be developed and used without
negatively affecting human societies? Our objectives are threefold: to provide
a systematic study of the social effects of RLHF; to identify key social and
ethical issues of RLHF; and to discuss social impacts for stakeholders.
Although text-based applications of RLHF have received much attention, it is
crucial to consider when evaluating its social implications the diverse range
of areas to which it may be deployed. We describe seven primary ways in which
RLHF-based technologies will affect society by positively transforming human
experiences with AI. This paper ultimately proposes that RLHF has potential to
net positively impact areas of misinformation, AI value-alignment, bias, AI
access, cross-cultural dialogue, industry, and workforce. As RLHF raises
concerns that echo those of existing AI technologies, it will be important for
all to be aware and intentional in the adoption of RLHF.",['Gabrielle Kaili-May Liu'],9,0.6848097
"AI Generated Content (AIGC) has received tremendous attention within the past
few years, with content generated in the format of image, text, audio, video,
etc. Meanwhile, AIGC has become a double-edged sword and recently received much
criticism regarding its responsible usage. In this article, we focus on 8 main
concerns that may hinder the healthy development and deployment of AIGC in
practice, including risks from (1) privacy; (2) bias, toxicity, misinformation;
(3) intellectual property (IP); (4) robustness; (5) open source and
explanation; (6) technology abuse; (7) consent, credit, and compensation; (8)
environment. Additionally, we provide insights into the promising directions
for tackling these risks while constructing generative models, enabling AIGC to
be used more responsibly to truly benefit society.","['Chen Chen', 'Jie Fu', 'Lingjuan Lyu']",9,0.7026112
"With the expansion of social media and the increasing dissemination of
multimedia content, the spread of misinformation has become a major concern.
This necessitates effective strategies for multimodal misinformation detection
(MMD) that detect whether the combination of an image and its accompanying text
could mislead or misinform. Due to the data-intensive nature of deep neural
networks and the labor-intensive process of manual annotation, researchers have
been exploring various methods for automatically generating synthetic
multimodal misinformation - which we refer to as Synthetic Misinformers - in
order to train MMD models. However, limited evaluation on real-world
misinformation and a lack of comparisons with other Synthetic Misinformers
makes difficult to assess progress in the field. To address this, we perform a
comparative study on existing and new Synthetic Misinformers that involves (1)
out-of-context (OOC) image-caption pairs, (2) cross-modal named entity
inconsistency (NEI) as well as (3) hybrid approaches and we evaluate them
against real-world misinformation; using the COSMOS benchmark. The comparative
study showed that our proposed CLIP-based Named Entity Swapping can lead to MMD
models that surpass other OOC and NEI Misinformers in terms of multimodal
accuracy and that hybrid approaches can lead to even higher detection accuracy.
Nevertheless, after alleviating information leakage from the COSMOS evaluation
protocol, low Sensitivity scores indicate that the task is significantly more
challenging than previous studies suggested. Finally, our findings showed that
NEI-based Synthetic Misinformers tend to suffer from a unimodal bias, where
text-only MMDs can outperform multimodal ones.","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Symeon Papadopoulos', 'Panagiotis C. Petrantonakis']",2,0.7033639
"In this demo, we introduce a web-based misinformation detection system
PANACEA on COVID-19 related claims, which has two modules, fact-checking and
rumour detection. Our fact-checking module, which is supported by novel natural
language inference methods with a self-attention network, outperforms
state-of-the-art approaches. It is also able to give automated veracity
assessment and ranked supporting evidence with the stance towards the claim to
be checked. In addition, PANACEA adapts the bi-directional graph convolutional
networks model, which is able to detect rumours based on comment networks of
related tweets, instead of relying on the knowledge base. This rumour detection
module assists by warning the users in the early stages when a knowledge base
may not be available.","['Runcong Zhao', 'Miguel Arana-Catania', 'Lixing Zhu', 'Elena Kochkina', 'Lin Gui', 'Arkaitz Zubiaga', 'Rob Procter', 'Maria Liakata', 'Yulan He']",1,0.70340025
"The emergence of toxic information on social networking sites, such as
Twitter, Parler, and Reddit, has become a growing concern. Consequently, this
study aims to assess the level of toxicity in COVID-19 discussions on Twitter,
Parler, and Reddit. Using data analysis from January 1 through December 31,
2020, we examine the development of toxicity over time and compare the findings
across the three platforms. The results indicate that Parler had lower toxicity
levels than both Twitter and Reddit in discussions related to COVID-19. In
contrast, Reddit showed the highest levels of toxicity, largely due to various
anti-vaccine forums that spread misinformation about COVID-19 vaccines.
Notably, our analysis of COVID-19 vaccination conversations on Twitter also
revealed a significant presence of conspiracy theories among individuals with
highly toxic attitudes. Our computational approach provides decision-makers
with useful information about reducing the spread of toxicity within online
communities. The study's findings highlight the importance of taking action to
encourage more uplifting and productive online discourse across all platforms.","['Nahiyan Bin Noor', 'Niloofar Yousefi', 'Billy Spann', 'Nitin Agarwal']",12,0.7696127
"Information cascade in online social networks can be rather negative, e.g.,
the spread of rumors may trigger panic. To limit the influence of
misinformation in an effective and efficient manner, the influence minimization
(IMIN) problem is studied in the literature: given a graph G and a seed set S,
blocking at most b vertices such that the influence spread of the seed set is
minimized. In this paper, we are the first to prove the IMIN problem is NP-hard
and hard to approximate. Due to the hardness of the problem, existing works
resort to greedy solutions and use Monte-Carlo Simulations to solve the
problem. However, they are cost-prohibitive on large graphs since they have to
enumerate all the candidate blockers and compute the decrease of expected
spread when blocking each of them. To improve the efficiency, we propose the
AdvancedGreedy algorithm (AG) based on a new graph sampling technique that
applies the dominator tree structure, which can compute the decrease of the
expected spread of all candidate blockers at once. Besides, we further propose
the GreedyReplace algorithm (GR) by considering the relationships among
candidate blockers. Extensive experiments on 8 real-life graphs demonstrate
that our AG and GR algorithms are significantly faster than the
state-of-the-art by up to 6 orders of magnitude, and GR can achieve better
effectiveness with its time cost close to AG.","['Jiadong Xie', 'Fan Zhang', 'Kai Wang', 'Xuemin Lin', 'Wenjie Zhang']",2,0.71317637
"Recent rapid advancements in deepfake technology have allowed the creation of
highly realistic fake media, such as video, image, and audio. These materials
pose significant challenges to human authentication, such as impersonation,
misinformation, or even a threat to national security. To keep pace with these
rapid advancements, several deepfake detection algorithms have been proposed,
leading to an ongoing arms race between deepfake creators and deepfake
detectors. Nevertheless, these detectors are often unreliable and frequently
fail to detect deepfakes. This study highlights the challenges they face in
detecting deepfakes, including (1) the pre-processing pipeline of artifacts and
(2) the fact that generators of new, unseen deepfake samples have not been
considered when building the defense models. Our work sheds light on the need
for further research and development in this field to create more robust and
reliable detectors.","['Binh Le', 'Shahroz Tariq', 'Alsharif Abuadbba', 'Kristen Moore', 'Simon Woo']",11,0.8098241
"Natural language processing based on large language models (LLMs) is a
booming field of AI research. After neural networks have proven to outperform
humans in games and practical domains based on pattern recognition, we might
stand now at a road junction where artificial entities might eventually enter
the realm of human communication. However, this comes with serious risks. Due
to the inherent limitations regarding the reliability of neural networks,
overreliance on LLMs can have disruptive consequences. Since it will be
increasingly difficult to distinguish between human-written and
machine-generated text, one is confronted with new ethical challenges. This
begins with the no longer undoubtedly verifiable human authorship and continues
with various types of fraud, such as a new form of plagiarism. This also
concerns the violation of privacy rights, the possibility of circulating
counterfeits of humans, and, last but not least, it makes a massive spread of
misinformation possible.",['Anna Strasser'],9,0.696193
"In this study, we tested users' perception of accuracy and engagement with
TikTok videos in which ChatGPT responded to prompts about ""at-home"" abortion
remedies. The chatbot's responses, though somewhat vague and confusing,
nonetheless recommended consulting with health professionals before attempting
an ""at-home"" abortion. We used ChatGPT to create two TikTok video variants -
one where users can see ChatGPT explicitly typing back a response, and one
where the text response is presented without any notion to the chatbot. We
randomly exposed 100 participants to each variant and found that the group of
participants unaware of ChatGPT's text synthetization was more inclined to
believe the responses were misinformation. Under the same impression, TikTok
itself attached misinformation warning labels (""Get the facts about abortion"")
to all videos after we collected our initial results. We then decided to test
the videos again with another set of 50 participants and found that the labels
did not affect the perceptions of abortion misinformation except in the case
where ChatGPT explicitly responded to a prompt for a lyrical output. We also
found that more than 60% of the participants expressed negative or hesitant
opinions about chatbots as sources of credible health information.","['Filipo Sharevski', 'Jennifer Vander Loop', 'Peter Jachim', 'Amy Devine', 'Emma Pieroni']",13,0.46776268
"Deep learning constitutes a pivotal component within the realm of machine
learning, offering remarkable capabilities in tasks ranging from image
recognition to natural language processing. However, this very strength also
renders deep learning models susceptible to adversarial examples, a phenomenon
pervasive across a diverse array of applications. These adversarial examples
are characterized by subtle perturbations artfully injected into clean images
or videos, thereby causing deep learning algorithms to misclassify or produce
erroneous outputs. This susceptibility extends beyond the confines of digital
domains, as adversarial examples can also be strategically designed to target
human cognition, leading to the creation of deceptive media, such as deepfakes.
Deepfakes, in particular, have emerged as a potent tool to manipulate public
opinion and tarnish the reputations of public figures, underscoring the urgent
need to address the security and ethical implications associated with
adversarial examples. This article delves into the multifaceted world of
adversarial examples, elucidating the underlying principles behind their
capacity to deceive deep learning algorithms. We explore the various
manifestations of this phenomenon, from their insidious role in compromising
model reliability to their impact in shaping the contemporary landscape of
disinformation and misinformation. To illustrate progress in combating
adversarial examples, we showcase the development of a tailored Convolutional
Neural Network (CNN) designed explicitly to detect deepfakes, a pivotal step
towards enhancing model robustness in the face of adversarial threats.
Impressively, this custom CNN has achieved a precision rate of 76.2% on the
DFDC dataset.","['Saminder Dhesi', 'Laura Fontes', 'Pedro Machado', 'Isibor Kennedy Ihianle', 'Farhad Fassihi Tash', 'David Ada Adama']",7,0.78722167
"Misinformation is an important topic in the Information Retrieval (IR)
context and has implications for both system-centered and user-centered IR.
While it has been established that the performance in discerning misinformation
is affected by a person's cognitive load, the variation in cognitive load in
judging the veracity of news is less understood. To understand the variation in
cognitive load imposed by reading news headlines related to COVID-19 claims,
within the context of a fact-checking system, we conducted a within-subject,
lab-based, quasi-experiment (N=40) with eye-tracking. Our results suggest that
examining true claims imposed a higher cognitive load on participants when news
headlines provided incorrect evidence for a claim and were inconsistent with
the person's prior beliefs. In contrast, checking false claims imposed a higher
cognitive load when the news headlines provided correct evidence for a claim
and were consistent with the participants' prior beliefs. However, changing
beliefs after examining a claim did not have a significant relationship with
cognitive load while reading the news headlines. The results illustrate that
reading news headlines related to true and false claims in the fact-checking
context impose different levels of cognitive load. Our findings suggest that
user engagement with tools for discerning misinformation needs to account for
the possible variation in the mental effort involved in different information
contexts.","['Li Shi', 'Nilavra Bhattacharya', 'Anubrata Das', 'Jacek Gwizdka']",3,0.67191887
"With the 2022 US midterm elections approaching, conspiratorial claims about
the 2020 presidential elections continue to threaten users' trust in the
electoral process. To regulate election misinformation, YouTube introduced
policies to remove such content from its searches and recommendations. In this
paper, we conduct a 9-day crowd-sourced audit on YouTube to assess the extent
of enactment of such policies. We recruited 99 users who installed a browser
extension that enabled us to collect up-next recommendation trails and search
results for 45 videos and 88 search queries about the 2020 elections. We find
that YouTube's search results, irrespective of search query bias, contain more
videos that oppose rather than support election misinformation. However,
watching misinformative election videos still lead users to a small number of
misinformative videos in the up-next trails. Our results imply that while
YouTube largely seems successful in regulating election misinformation, there
is still room for improvement.","['Prerna Juneja', 'Md Momen Bhuiyan', 'Tanushree Mitra']",10,0.5936737
"In this study, we interviewed 22 prominent hacktivists to learn their take on
the increased proliferation of misinformation on social media. We found that
none of them welcomes the nefarious appropriation of trolling and memes for the
purpose of political (counter)argumentation and dissemination of propaganda.
True to the original hacker ethos, misinformation is seen as a threat to the
democratic vision of the Internet, and as such, it must be confronted on the
face with tried hacktivists' methods like deplatforming the ""misinformers"" and
doxing or leaking data about their funding and recruitment. The majority of the
hacktivists also recommended interventions for raising misinformation literacy
in addition to targeted hacking campaigns. We discuss the implications of these
findings relative to the emergent recasting of hacktivism in defense of a
constructive and factual social media discourse.","['Filipo Sharevski', 'Benjamin Kessell']",10,0.6609557
"Social media is one of the main sources for news consumption, especially
among the younger generation. With the increasing popularity of news
consumption on various social media platforms, there has been a surge of
misinformation which includes false information or unfounded claims. As various
text- and social context-based fake news detectors are proposed to detect
misinformation on social media, recent works start to focus on the
vulnerabilities of fake news detectors. In this paper, we present the first
adversarial attack framework against Graph Neural Network (GNN)-based fake news
detectors to probe their robustness. Specifically, we leverage a multi-agent
reinforcement learning (MARL) framework to simulate the adversarial behavior of
fraudsters on social media. Research has shown that in real-world settings,
fraudsters coordinate with each other to share different news in order to evade
the detection of fake news detectors. Therefore, we modeled our MARL framework
as a Markov Game with bot, cyborg, and crowd worker agents, which have their
own distinctive cost, budget, and influence. We then use deep Q-learning to
search for the optimal policy that maximizes the rewards. Extensive
experimental results on two real-world fake news propagation datasets
demonstrate that our proposed framework can effectively sabotage the GNN-based
fake news detector performance. We hope this paper can provide insights for
future research on fake news detection.","['Haoran Wang', 'Yingtong Dou', 'Canyu Chen', 'Lichao Sun', 'Philip S. Yu', 'Kai Shu']",4,0.86773014
"Multi-modal fact verification has become an important but challenging issue
on social media due to the mismatch between the text and images in the
misinformation of news content, which has been addressed by considering
cross-modalities to identify the veracity of the news in recent years. In this
paper, we propose the Pre-CoFactv2 framework with new parameter-efficient
foundation models for modeling fine-grained text and input embeddings with
lightening parameters, multi-modal multi-type fusion for not only capturing
relations for the same and different modalities but also for different types
(i.e., claim and document), and feature representations for explicitly
providing metadata for each sample. In addition, we introduce a unified
ensemble method to boost model performance by adjusting the importance of each
trained model with not only the weights but also the powers. Extensive
experiments show that Pre-CoFactv2 outperforms Pre-CoFact by a large margin and
achieved new state-of-the-art results at the Factify challenge at AAAI 2023. We
further illustrate model variations to verify the relative contributions of
different components. Our team won the first prize (F1-score: 81.82%) and we
made our code publicly available at
https://github.com/wwweiwei/Pre-CoFactv2-AAAI-2023.","['Wei-Wei Du', 'Hong-Wei Wu', 'Wei-Yao Wang', 'Wen-Chih Peng']",1,0.6581708
"Misinformation on social media presents a major threat to modern societies.
While previous research has analyzed the virality across true and false social
media posts, not every misleading post is necessarily equally viral. Rather,
misinformation has different characteristics and varies in terms of its
believability and harmfulness - which might influence its spread. In this work,
we study how the perceived believability and harmfulness of misleading posts
are associated with their virality on social media. Specifically, we analyze
(and validate) a large sample of crowd-annotated social media posts from
Twitter's Birdwatch platform, on which users can rate the believability and
harmfulness of misleading tweets. To address our research questions, we
implement an explanatory regression model and link the crowd ratings for
believability and harmfulness to the virality of misleading posts on Twitter.
Our findings imply that misinformation that is (i) easily believable and (ii)
not particularly harmful is associated with more viral resharing cascades.
These results offer insights into how different kinds of crowd fact-checked
misinformation spreads and suggest that the most viral misleading posts are
often not the ones that are particularly concerning from the perspective of
public safety. From a practical view, our findings may help platforms to
develop more effective strategies to curb the proliferation of misleading posts
on social media.","['Chiara Drolsbach', 'Nicolas Pr√∂llochs']",3,0.7773608
"In recent years, algorithms have been incorporated into fact-checking
pipelines. They are used not only to flag previously fact-checked
misinformation, but also to provide suggestions about which trending claims
should be prioritized for fact-checking - a paradigm called `check-worthiness.'
While several studies have examined the accuracy of these algorithms, none have
investigated how the benefits from these algorithms (via reduction in exposure
to misinformation) are distributed amongst various online communities. In this
paper, we investigate how diverse representation across multiple stages of the
AI development pipeline affects the distribution of benefits from AI-assisted
fact-checking for different online communities. We simulate information
propagation through the network using our novel Topic-Aware, Community-Impacted
Twitter (TACIT) simulator on a large Twitter followers network, tuned to
produce realistic cascades of true and false information across multiple
topics. Finally, using simulated data as a test bed, we implement numerous
algorithmic fact-checking interventions that explicitly account for notions of
diversity. We find that both representative and egalitarian methods for
sampling and labeling check-worthiness model training data can lead to
network-wide benefit concentrated in majority communities, while incorporating
diversity into how fact-checkers use algorithmic recommendations can actively
reduce inequalities in benefits between majority and minority communities.
These findings contribute to an important conversation around the responsible
implementation of AI-assisted fact-checking by social media platforms and
fact-checking organizations.","['Terrence Neumann', 'Nicholas Wolczynski']",2,0.74668527
"Fake news and misinformation spread rapidly on the Internet. How to identify
it and how to interpret the identification results have become important
issues. In this paper, we propose a Dual Co-Attention Network (Dual-CAN) for
fake news detection, which takes news content, social media replies, and
external knowledge into consideration. Our experimental results support that
the proposed Dual-CAN outperforms current representative models in two
benchmark datasets. We further make in-depth discussions by comparing how
models work in both datasets with empirical analysis of attention weights.","['Sin-Han Yang', 'Chung-Chi Chen', 'Hen-Hsen Huang', 'Hsin-Hsi Chen']",4,0.73922336
"With information consumption via online video streaming becoming increasingly
popular, misinformation video poses a new threat to the health of the online
information ecosystem. Though previous studies have made much progress in
detecting misinformation in text and image formats, video-based misinformation
brings new and unique challenges to automatic detection systems: 1) high
information heterogeneity brought by various modalities, 2) blurred distinction
between misleading video manipulation and nonmalicious artistic video editing,
and 3) new patterns of misinformation propagation due to the dominant role of
recommendation systems on online video platforms. To facilitate research on
this challenging task, we conduct this survey to present advances in
misinformation video detection. We first analyze and characterize the
misinformation video from three levels including signals, semantics, and
intents. Based on the characterization, we systematically review existing works
for detection from features of various modalities to techniques for clue
integration. We also introduce existing resources including representative
datasets and useful tools. Besides summarizing existing studies, we discuss
related areas and outline open issues and future directions to encourage and
guide more research on misinformation video detection. The corresponding
repository is at https://github.com/ICTMCG/Awesome-Misinfo-Video-Detection.","['Yuyan Bu', 'Qiang Sheng', 'Juan Cao', 'Peng Qi', 'Danding Wang', 'Jintao Li']",0,0.7197027
"Human data labeling is an important and expensive task at the heart of
supervised learning systems. Hierarchies help humans understand and organize
concepts. We ask whether and how concept hierarchies can inform the design of
annotation interfaces to improve labeling quality and efficiency. We study this
question through annotation of vaccine misinformation, where the labeling task
is difficult and highly subjective. We investigate 6 user interface designs for
crowdsourcing hierarchical labels by collecting over 18,000 individual
annotations. Under a fixed budget, integrating hierarchies into the design
improves crowdsource workers' F1 scores. We attribute this to (1) Grouping
similar concepts, improving F1 scores by +0.16 over random groupings, (2)
Strong relative performance on high-difficulty examples (relative F1 score
difference of +0.40), and (3) Filtering out obvious negatives, increasing
precision by +0.07. Ultimately, labeling schemes integrating the hierarchy
outperform those that do not - achieving mean F1 of 0.70.","['Rickard Stureborg', 'Bhuwan Dhingra', 'Jun Yang']",2,0.54242855
"The emergence of large language models (LLMs) has resulted in the production
of LLM-generated texts that is highly sophisticated and almost
indistinguishable from texts written by humans. However, this has also sparked
concerns about the potential misuse of such texts, such as spreading
misinformation and causing disruptions in the education system. Although many
detection approaches have been proposed, a comprehensive understanding of the
achievements and challenges is still lacking. This survey aims to provide an
overview of existing LLM-generated text detection techniques and enhance the
control and regulation of language generation models. Furthermore, we emphasize
crucial considerations for future research, including the development of
comprehensive evaluation metrics and the threat posed by open-source LLMs, to
drive progress in the area of LLM-generated text detection.","['Ruixiang Tang', 'Yu-Neng Chuang', 'Xia Hu']",6,0.7351294
"Spreading processes, e.g. epidemics, wildfires and rumors, are often modeled
on static networks. However, their underlying network structures, e.g. changing
contacts in social networks, different weather forecasts for wildfires, are due
to ever-changing circumstances inherently time-varying in nature. In this
paper, we therefore, propose an optimization framework for sparse resource
allocation for control of spreading processes over temporal networks with known
connectivity patterns. We use convex optimization, in particular exponential
cone programming, and dynamic programming techniques to bound and minimize the
risk of an undetected outbreak by allocating budgeted resources each time step.
We demonstrate with misinformation, epidemic and wildfire examples how the
method can provide targeted allocation of resources.","['Vera L. J. Somers', 'Ian R. Manchester']",2,0.6997106
"Twitter bot detection is vital in combating misinformation and safeguarding
the integrity of social media discourse. While malicious bots are becoming more
and more sophisticated and personalized, standard bot detection approaches are
still agnostic to social environments (henceforth, communities) the bots
operate at. In this work, we introduce community-specific bot detection,
estimating the percentage of bots given the context of a community. Our method
-- BotPercent -- is an amalgamation of Twitter bot detection datasets and
feature-, text-, and graph-based models, adjusted to a particular community on
Twitter. We introduce an approach that performs confidence calibration across
bot detection models, which addresses generalization issues in existing
community-agnostic models targeting individual bots and leads to more accurate
community-level bot estimations. Experiments demonstrate that BotPercent
achieves state-of-the-art performance in community-level Twitter bot detection
across both balanced and imbalanced class distribution settings, %outperforming
existing approaches and presenting a less biased estimator of Twitter bot
populations within the communities we analyze. We then analyze bot rates in
several Twitter groups, including users who engage with partisan news media,
political communities in different countries, and more. Our results reveal that
the presence of Twitter bots is not homogeneous, but exhibiting a
spatial-temporal distribution with considerable heterogeneity that should be
taken into account for content moderation and social media policy making. The
implementation of BotPercent is available at
https://github.com/TamSiuhin/BotPercent.","['Zhaoxuan Tan', 'Shangbin Feng', 'Melanie Sclar', 'Herun Wan', 'Minnan Luo', 'Yejin Choi', 'Yulia Tsvetkov']",13,0.8592253
"Manipulated news online is a growing problem which necessitates the use of
automated systems to curtail its spread. We argue that while misinformation and
disinformation detection have been studied, there has been a lack of investment
in the important open challenge of detecting harmful agendas in news articles;
identifying harmful agendas is critical to flag news campaigns with the
greatest potential for real world harm. Moreover, due to real concerns around
censorship, harmful agenda detectors must be interpretable to be effective. In
this work, we propose this new task and release a dataset, NewsAgendas, of
annotated news articles for agenda identification. We show how interpretable
systems can be effective on this task and demonstrate that they can perform
comparably to black-box models.","['Melanie Subbiah', 'Amrita Bhattacharjee', 'Yilun Hua', 'Tharindu Kumarage', 'Huan Liu', 'Kathleen McKeown']",4,0.7048352
"Real-time social media data can provide useful information on evolving
hazards. Alongside traditional methods of disaster detection, the integration
of social media data can considerably enhance disaster management. In this
paper, we investigate the problem of detecting geolocation-content communities
on Twitter and propose a novel distributed system that provides in near
real-time information on hazard-related events and their evolution. We show
that content-based community analysis leads to better and faster dissemination
of reports on hazards. Our distributed disaster reporting system analyzes the
social relationship among worldwide geolocated tweets, and applies topic
modeling to group tweets by topics. Considering for each tweet the following
information: user, timestamp, geolocation, retweets, and replies, we create a
publisher-subscriber distribution model for topics. We use content similarity
and the proximity of nodes to create a new model for geolocation-content based
communities. Users can subscribe to different topics in specific geographical
areas or worldwide and receive real-time reports regarding these topics. As
misinformation can lead to increase damage if propagated in hazards related
tweets, we propose a new deep learning model to detect fake news. The
misinformed tweets are then removed from display. We also show empirically the
scalability capabilities of the proposed system.","['Elena-Simona Apostol', 'Ciprian-Octavian TruicƒÉ', 'Adrian Paschke']",4,0.67446834
"The ease and speed of spreading misinformation and propaganda on the Web
motivate the need to develop trustworthy technology for detecting fallacies in
natural language arguments. However, state-of-the-art language modeling methods
exhibit a lack of robustness on tasks like logical fallacy classification that
require complex reasoning. In this paper, we propose a Case-Based Reasoning
method that classifies new cases of logical fallacy by language-modeling-driven
retrieval and adaptation of historical cases. We design four complementary
strategies to enrich input representation for our model, based on external
information about goals, explanations, counterarguments, and argument
structure. Our experiments in in-domain and out-of-domain settings indicate
that Case-Based Reasoning improves the accuracy and generalizability of
language models. Our ablation studies suggest that representations of similar
cases have a strong impact on the model performance, that models perform well
with fewer retrieved cases, and that the size of the case database has a
negligible effect on the performance. Finally, we dive deeper into the
relationship between the properties of the retrieved cases and the model
performance.","['Zhivar Sourati', 'Filip Ilievski', 'H√¥ng-√Çn Sandlin', 'Alain Mermoud']",1,0.7127469
"In this work, we examine the influence of unreliable information on political
incivility and toxicity on the social media platform Reddit. We show that
comments on articles from unreliable news websites are posted more often in
right-leaning subreddits and that within individual subreddits, comments, on
average, are 32% more likely to be toxic compared to comments on reliable news
articles. Using a regression model, we show that these results hold after
accounting for partisanship and baseline toxicity rates within individual
subreddits. Utilizing a zero-inflated negative binomial regression, we further
show that as the toxicity of subreddits increases, users are more likely to
comment on posts from known unreliable websites. Finally, modeling user
interactions with an exponential random graph model, we show that when reacting
to a Reddit submission that links to a website known for spreading unreliable
information, users are more likely to be toxic to users of different political
beliefs. Our results collectively illustrate that low-quality/unreliable
information not only predicts increased toxicity but also polarizing
interactions between users of different political orientations.","['Hans W. A. Hanley', 'Zakir Durumeric']",3,0.682957
"Social media is currently being used by many individuals online as a major
source of information. However, not all information shared online is true, even
photos and videos can be doctored. Deepfakes have recently risen with the rise
of technological advancement and have allowed nefarious online users to replace
one face with a computer generated face of anyone they would like, including
important political and cultural figures. Deepfakes are now a tool to be able
to spread mass misinformation. There is now an immense need to create models
that are able to detect deepfakes and keep them from being spread as seemingly
real images or videos. In this paper, we propose a new deepfake detection
schema using two popular machine learning algorithms.","['Jacob mallet', 'Laura Pryor', 'Rushit Dave', 'Mounika Vanamala']",11,0.7288158
"Do we live in a ""Golden Age of Conspiracy Theories?"" In the last few decades,
conspiracy theories have proliferated on the Internet with some having
dangerous real-world consequences. A large contingent of those who participated
in the January 6th attack on the US Capitol fervently believed in the QAnon
conspiracy theory. In this work, we study the relationships amongst five
prominent conspiracy theories (QAnon, COVID, UFO/Aliens, 9/11, and Flat-Earth)
and each of their respective relationships to the news media, both authentic
news and misinformation. Identifying and publishing a set of 755 different
conspiracy theory websites dedicated to our five conspiracy theories, we find
that each set often hyperlinks to the same external domains, with COVID and
QAnon conspiracy theory websites having the largest amount of shared
connections. Examining the role of news media, we further find that not only do
outlets known for spreading misinformation hyperlink to our set of conspiracy
theory websites more often than authentic news websites but also that this
hyperlinking increased dramatically between 2018 and 2021, with the advent of
QAnon and the start of COVID-19 pandemic. Using partial Granger-causality, we
uncover several positive correlative relationships between the hyperlinks from
misinformation websites and the popularity of conspiracy theory websites,
suggesting the prominent role that misinformation news outlets play in
popularizing many conspiracy theories.","['Hans W. A. Hanley', 'Deepak Kumar', 'Zakir Durumeric']",10,0.58810383
"During a public health crisis like the COVID-19 pandemic, a credible and
easy-to-access information portal is highly desirable. It helps with disease
prevention, public health planning, and misinformation mitigation. However,
creating such an information portal is challenging because 1) domain expertise
is required to identify and curate credible and intelligible content, 2) the
information needs to be updated promptly in response to the fast-changing
environment, and 3) the information should be easily accessible by the general
public; which is particularly difficult when most people do not have the domain
expertise about the crisis. In this paper, we presented an expert-sourcing
framework and created Jennifer, an AI chatbot, which serves as a credible and
easy-to-access information portal for individuals during the COVID-19 pandemic.
Jennifer was created by a team of over 150 scientists and health professionals
around the world, deployed in the real world and answered thousands of user
questions about COVID-19. We evaluated Jennifer from two key stakeholders'
perspectives, expert volunteers and information seekers. We first interviewed
experts who contributed to the collaborative creation of Jennifer to learn
about the challenges in the process and opportunities for future improvement.
We then conducted an online experiment that examined Jennifer's effectiveness
in supporting information seekers in locating COVID-19 information and gaining
their trust. We share the key lessons learned and discuss design implications
for building expert-sourced and AI-powered information portals, along with the
risks and opportunities of misinformation mitigation and beyond.","['Ziang Xiao', 'Q. Vera Liao', 'Michelle X. Zhou', 'Tyrone Grandison', 'Yunyao Li']",5,0.62209606
"The full-scale conflict between the Russian Federation and Ukraine generated
an unprecedented amount of news articles and social media data reflecting
opposing ideologies and narratives. These polarized campaigns have led to
mutual accusations of misinformation and fake news, shaping an atmosphere of
confusion and mistrust for readers worldwide. This study analyses how the media
affected and mirrored public opinion during the first month of the war using
news articles and Telegram news channels in Ukrainian, Russian, Romanian and
English. We propose and compare two methods of multilingual automated
pro-Kremlin propaganda identification, based on Transformers and linguistic
features. We analyse the advantages and disadvantages of both methods, their
adaptability to new genres and languages, and ethical considerations of their
usage for content moderation. With this work, we aim to lay the foundation for
further development of moderation tools tailored to the current conflict.","['Veronika Solopova', 'Oana-Iuliana Popescu', 'Christoph Benzm√ºller', 'Tim Landgraf']",4,0.68596005
"With the advent of deep learning, text generation language models have
improved dramatically, with text at a similar level as human-written text. This
can lead to rampant misinformation because content can now be created cheaply
and distributed quickly. Automated claim verification methods exist to validate
claims, but they lack foundational data and often use mainstream news as
evidence sources that are strongly biased towards a specific agenda. Current
claim verification methods use deep neural network models and complex
algorithms for a high classification accuracy but it is at the expense of model
explainability. The models are black-boxes and their decision-making process
and the steps it took to arrive at a final prediction are obfuscated from the
user. We introduce a novel claim verification approach, namely: ExClaim, that
attempts to provide an explainable claim verification system with foundational
evidence. Inspired by the legal system, ExClaim leverages rationalization to
provide a verdict for the claim and justifies the verdict through a natural
language explanation (rationale) to describe the model's decision-making
process. ExClaim treats the verdict classification task as a question-answer
problem and achieves a performance of 0.93 F1 score. It provides subtasks
explanations to also justify the intermediate outcomes. Statistical and
Explainable AI (XAI) evaluations are conducted to ensure valid and trustworthy
outcomes. Ensuring claim verification systems are assured, rational, and
explainable is an essential step toward improving Human-AI trust and the
accessibility of black-box systems.","['Sai Gurrapu', 'Lifu Huang', 'Feras A. Batarseh']",1,0.8321103
"Toxic misinformation campaigns have caused significant societal harm, e.g.,
affecting elections and COVID-19 information awareness. Unfortunately, despite
successes of (gold standard) retrospective studies of misinformation that
confirmed their harmful effects after the fact, they arrive too late for timely
intervention and reduction of such harm. By design, misinformation evades
retrospective classifiers by exploiting two properties we call new-normal: (1)
never-seen-before novelty that cause inescapable generalization challenges for
previous classifiers, and (2) massive but short campaigns that end before they
can be manually annotated for new classifier training. To tackle these
challenges, we propose UFIT, which combines two techniques: semantic masking of
strong signal keywords to reduce overfitting, and intra-proxy smoothness
regularization of high-density regions in the latent space to improve
reliability and maintain accuracy. Evaluation of UFIT on public new-normal
misinformation data shows over 30% improvement over existing approaches on
future (and unseen) campaigns. To the best of our knowledge, UFIT is the first
successful effort to achieve such high level of generalization on new-normal
misinformation data with minimal concession (1 to 5%) of accuracy compared to
oracles trained with full knowledge of all campaigns.","['Abhijit Suprem', 'Joao Eduardo Ferreira', 'Calton Pu']",0,0.6101709
"Social media platforms provide actionable information during crises and
pandemic outbreaks. The COVID-19 pandemic has imposed a chronic public health
crisis worldwide, with experts considering vaccines as the ultimate prevention
to achieve herd immunity against the virus. A proportion of people may turn to
social media platforms to oppose vaccines and vaccination, hindering government
efforts to eradicate the virus. This paper presents the COVID-19 vaccines and
vaccination-specific global geotagged tweets dataset, GeoCovaxTweets, that
contains more than 1.8 million tweets, with location information and longer
temporal coverage, originating from 233 countries and territories between
January 2020 and November 2022. The paper discusses the dataset's curation
method and how it can be re-created locally, and later explores the dataset
through multiple tweets distributions and briefly discusses its potential use
cases. We anticipate that the dataset will assist the researchers in the crisis
computing domain to explore the conversational dynamics of COVID-19 vaccines
and vaccination Twitter discourse through numerous spatial and temporal
dimensions concerning trends, shifts in opinions, misinformation, and
anti-vaccination campaigns.","['Pardeep Singh', 'Rabindra Lamsal', 'Monika', 'Satish Chand', 'Bhawna Shishodia']",12,0.7976152
"Assessing the trustworthiness of information online is complicated.
Literacy-based paradigms are both widely used to help and widely critiqued. We
conducted a study with 35 Gen Zers from across the U.S. to understand how they
assess information online. We found that they tended to encounter -- rather
than search for -- information, and that those encounters were shaped more by
social motivations than by truth-seeking queries. For them, information
processing is fundamentally a social practice. Gen Zers interpreted online
information together, as aspirational members of social groups. Our
participants sought information sensibility: a socially-informed awareness of
the value of information encountered online. We outline key challenges they
faced and practices they used to make sense of information. Our findings
suggest that like their information sensibility practices, solutions and
strategies to address misinformation should be embedded in social contexts
online.","['Amelia Hassoun', 'Ian Beacock', 'Sunny Consolvo', 'Beth Goldberg', 'Patrick Gage Kelley', 'Daniel M. Russell']",0,0.76320505
"Accurate bot detection is necessary for the safety and integrity of online
platforms. It is also crucial for research on the influence of bots in
elections, the spread of misinformation, and financial market manipulation.
Platforms deploy infrastructure to flag or remove automated accounts, but their
tools and data are not publicly available. Thus, the public must rely on
third-party bot detection. These tools employ machine learning and often
achieve near perfect performance for classification on existing datasets,
suggesting bot detection is accurate, reliable and fit for use in downstream
applications. We provide evidence that this is not the case and show that high
performance is attributable to limitations in dataset collection and labeling
rather than sophistication of the tools. Specifically, we show that simple
decision rules -- shallow decision trees trained on a small number of features
-- achieve near-state-of-the-art performance on most available datasets and
that bot detection datasets, even when combined together, do not generalize
well to out-of-sample datasets. Our findings reveal that predictions are highly
dependent on each dataset's collection and labeling procedures rather than
fundamental differences between bots and humans. These results have important
implications for both transparency in sampling and labeling procedures and
potential biases in research using existing bot detection tools for
pre-processing.","['Chris Hays', 'Zachary Schutzman', 'Manish Raghavan', 'Erin Walk', 'Philipp Zimmer']",13,0.7353544
"Misinformation is one of the key challenges facing society today.
User-centered misinformation interventions as digital countermeasures that
exert a direct influence on users represent a promising means to deal with the
large amounts of information available. While an extensive body of research on
this topic exists, researchers are confronted with a diverse research landscape
spanning multiple disciplines. This review systematizes the landscape of
user-centered misinformation interventions to facilitate knowledge transfer,
identify trends, and enable informed decision-making. Over 5,700 scholarly
publications were screened and a systematic literature review (N=163) was
conducted. A taxonomy was derived regarding intervention design (e.g., (binary)
label), user interaction (active or passive), and timing (e.g., post exposure
to misinformation). We provide a structured overview of approaches across
multiple disciplines, and derive six overarching challenges for future
research.","['Katrin Hartwig', 'Frederic Doell', 'Christian Reuter']",0,0.7991362
"We share the largest dataset for the Pakistani Twittersphere consisting of
over 49 million tweets, collected during one of the most politically active
periods in the country. We collect the data after the deposition of the
government by a No Confidence Vote in April 2022. This large-scale dataset can
be used for several downstream tasks such as political bias, bots detection,
trolling behavior, (dis)misinformation, and censorship related to Pakistani
Twitter users. In addition, this dataset provides a large collection of tweets
in Urdu and Roman Urdu that can be used for optimizing language processing
tasks.","['Ehsan-Ul Haq', 'Haris Bin Zia', 'Reza Hadi Mogavi', 'Gareth Tyson', 'Yang K. Lu', 'Tristan Braud', 'Pan Hui']",10,0.5764003
"The scientific effort devoted to health misinformation mostly focuses on the
implications of misleading vaccines and communicable disease claims with
respect to public health. However, the proliferation of abortion misinformation
following the Supreme Court's decision to overturn Roe v. Wade banning legal
abortion in the US highlighted a gap in scientific attention to individual
health-related misinformation. To address this gap, we conducted a study with
60 TikTok users to uncover their experiences with abortion misinformation and
the way they conceptualize, assess, and respond to misleading video content on
this platform. Our findings indicate that users mostly encounter short-term
videos suggesting herbal ""at-home"" remedies for pregnancy termination. While
many of the participants were cautious about scientifically debunked ""abortion
alternatives,"" roughly 30% of the entire sample believed in their safety and
efficacy. Even an explicit debunking label attached to a misleading abortion
video about the harms of ""at-home"" did not help a third of the participants to
dismiss a video about self-administering abortion as misinformation. We discuss
the implications of our findings for future participation on TikTok and other
polarizing topics debated on social media.","['Filipo Sharevski', 'Jennifer Vander Loop', 'Peter Jachim', 'Amy Devine', 'Emma Pieroni']",12,0.5476867
"Over the past decade, the media landscape has seen a radical shift. As more
of the public stay informed of current events via online sources, competition
has grown as outlets vie for attention. This competition has prompted some
online outlets to publish sensationalist and alarmist content to grab readers'
attention. Such practices may threaten democracy by distorting the truth and
misleading readers about the nature of events. This paper proposes a novel
system for detecting, processing, and warning users about misleading content
online to combat the threats posed by misinformation. By training a machine
learning model on an existing dataset of 32,000 clickbait news article
headlines, the model predicts how sensationalist a headline is and then
interfaces with a web browser extension which constructs a unique content
warning notification based on existing design principles and incorporates the
models' prediction. This research makes a novel contribution to machine
learning and human-centred security with promising findings for future
research. By warning users when they may be viewing misinformation, it is
possible to prevent spontaneous reactions, helping users to take a deep breath
and approach online media with a clear mind.","['Marc Kydd', 'Lynsay A. Shepherd']",4,0.7568804
"Misinformation threatens modern society by promoting distrust in science,
changing narratives in public health, heightening social polarization, and
disrupting democratic elections and financial markets, among a myriad of other
societal harms. To address this, a growing cadre of professional fact-checkers
and journalists provide high-quality investigations into purported facts.
However, these largely manual efforts have struggled to match the enormous
scale of the problem. In response, a growing body of Natural Language
Processing (NLP) technologies have been proposed for more scalable
fact-checking. Despite tremendous growth in such research, however, practical
adoption of NLP technologies for fact-checking still remains in its infancy
today.
  In this work, we review the capabilities and limitations of the current NLP
technologies for fact-checking. Our particular focus is to further chart the
design space for how these technologies can be harnessed and refined in order
to better meet the needs of human fact-checkers. To do so, we review key
aspects of NLP-based fact-checking: task formulation, dataset construction,
modeling, and human-centered strategies, such as explainable models and
human-in-the-loop approaches. Next, we review the efficacy of applying
NLP-based fact-checking tools to assist human fact-checkers. We recommend that
future research include collaboration with fact-checker stakeholders early on
in NLP research, as well as incorporation of human-centered design practices in
model development, in order to further guide technology development for human
use and practical adoption. Finally, we advocate for more research on benchmark
development supporting extrinsic evaluation of human-centered fact-checking
technologies.","['Anubrata Das', 'Houjiang Liu', 'Venelin Kovatchev', 'Matthew Lease']",0,0.60294855
"Networked discrete dynamical systems are often used to model the spread of
contagions and decision-making by agents in coordination games. Fixed points of
such dynamical systems represent configurations to which the system converges.
In the dissemination of undesirable contagions (such as rumors and
misinformation), convergence to fixed points with a small number of affected
nodes is a desirable goal. Motivated by such considerations, we formulate a
novel optimization problem of finding a nontrivial fixed point of the system
with the minimum number of affected nodes. We establish that, unless P = NP,
there is no polynomial time algorithm for approximating a solution to this
problem to within the factor n^1-\epsilon for any constant epsilon > 0. To cope
with this computational intractability, we identify several special cases for
which the problem can be solved efficiently. Further, we introduce an integer
linear program to address the problem for networks of reasonable sizes. For
solving the problem on larger networks, we propose a general heuristic
framework along with greedy selection methods. Extensive experimental results
on real-world networks demonstrate the effectiveness of the proposed
heuristics.","['Zirou Qiu', 'Chen Chen', 'Madhav V. Marathe', 'S. S. Ravi', 'Daniel J. Rosenkrantz', 'Richard E. Stearns', 'Anil Vullikanti']",2,0.80103636
"The increasing popularity of online social networks (OSNs) attracted growing
interest in modeling social interactions. On online social platforms, a few
individuals, commonly referred to as influencers, produce the majority of
content consumed by users and hegemonize the landscape of the social debate.
However, classical opinion models do not capture this communication asymmetry.
We develop an opinion model inspired by observations on social media platforms
{with two main objectives: first, to describe this inherent communication
asymmetry in OSNs, and second, to model the effects of content personalization.
We derive a Fokker-Planck equation for the temporal evolution of users' opinion
distribution and analytically characterize the stationary system behavior.
Analytical results, confirmed by Monte-Carlo simulations, show how strict forms
of content personalization tend to radicalize user opinion, leading to the
emergence of echo chambers, and favor structurally advantaged influencers. As
an example application, we apply our model to Facebook data during the Italian
government crisis in the summer of 2019. Our work provides a flexible framework
to evaluate the impact of content personalization on the opinion formation
process, focusing on the interaction between influential individuals and
regular users. This framework is interesting in the context of marketing and
advertising, misinformation spreading, politics and activism.","['Franco Galante', 'Luca Vassio', 'Michele Garetto', 'Emilio Leonardi']",3,0.6734321
"Although the effects of the social norm on mitigating misinformation are
identified, scant knowledge exists about the patterns of social norm emergence,
such as the patterns and variations of social tipping in online communities
with diverse characteristics. Accordingly, this study investigates the features
of social tipping in online communities and examines the correlations between
the tipping features and characteristics of online communities. Taking the side
effects of COVID-19 vaccination as the case topic, we first track the patterns
of tipping features in 100 online communities, which are detected using Louvain
Algorithm from the aggregated communication network on Twitter between May 2020
and April 2021. Then, we use multi-variant linear regression to explore the
correlations between tipping features and community characteristics. We find
that social tipping in online communities can sustain for two to four months
and lead to a 50% increase in populations who accept the normative belief in
online communities. The regression indicates that the duration of social
tipping is positively related to the community populations and original
acceptance of social norms, while the correlation between the tipping duration
and the degrees among community members is negative. Additionally, the network
modularity and original acceptance of social norms have negative relationships
with the extent of social tipping, while the degree and betweenness centrality
can have significant positive relationships with the extent of tipping. Our
findings shed light on more precise normative interventions on misinformation
in digital environments as it offers preliminary evidence about the timing and
mechanism of social norm emergence.","['Shangde Gao', 'Yan Wang', 'My T. Thai']",2,0.5089576
"How can we induce social media users to be discerning when sharing
information during a pandemic? An experiment on Facebook Messenger with users
from Kenya (n = 7,498) and Nigeria (n = 7,794) tested interventions designed to
decrease intentions to share COVID-19 misinformation without decreasing
intentions to share factual posts. The initial stage of the study incorporated:
(i) a factorial design with 40 intervention combinations; and (ii) a contextual
adaptive design, increasing the probability of assignment to treatments that
worked better for previous subjects with similar characteristics. The second
stage evaluated the best-performing treatments and a targeted treatment
assignment policy estimated from the data. We precisely estimate null effects
from warning flags and related article suggestions, tactics used by social
media platforms. However, nudges to consider information's accuracy reduced
misinformation sharing relative to control by 4.9% (estimate = -2.3 pp, s.e. =
1.0 , Z = -2.31, p = 0.021, 95% CI = [-4.2 , -0.35]). Such low-cost scalable
interventions may improve the quality of information circulating online.","['Molly Offer-Westort', 'Leah R. Rosenzweig', 'Susan Athey']",5,0.62791103
"The spread of misinformation is a pressing global problem that has elicited a
range of responses from researchers, policymakers, civil society and industry.
Over the past decade, these stakeholders have developed many interventions to
tackle misinformation that vary across factors such as which effects of
misinformation they hope to target, at what stage in the misinformation
lifecycle they are aimed at, and who they are implemented by. These
interventions also differ in how effective they are at reducing susceptibility
to (and curbing the spread of) misinformation. In recent years, a vast amount
of scholarly work on misinformation has become available, which extends across
multiple disciplines and methodologies. It has become increasingly difficult to
comprehensively map all of the available interventions, assess their efficacy,
and understand the challenges, opportunities and tradeoffs associated with
using them. Few papers have systematically assessed and compared the various
interventions, which has led to a lack of understanding in civic and
policymaking discourses. With this in mind, we develop a new hierarchical
framework for understanding interventions against misinformation online. The
framework comprises three key elements: Interventions that Prepare people to be
less susceptible; Interventions that Curb the spread and effects of
misinformation; and Interventions that Respond to misinformation. We outline
how different interventions are thought to work, categorise them, and summarise
the available evidence on their efficacy; offering researchers, policymakers
and practitioners working to combat online misinformation both an analytical
framework that they can use to understand and evaluate different interventions
(and which could be extended to address new interventions that we do not
describe here) and a summary of the range of interventions that have been
proposed to date.","['Pica Johansson', 'Florence Enock', 'Scott Hale', 'Bertie Vidgen', 'Cassidy Bereskin', 'Helen Margetts', 'Jonathan Bright']",0,0.7799537
"Quotable signature schemes are digital signature schemes with the additional
property that from the signature for a message, any party can extract
signatures for (allowable) quotes from the message, without knowing the secret
key or interacting with the signer of the original message. Crucially, the
extracted signatures are still signed with the original secret key. We define a
notion of security for quotable signature schemes and construct a concrete
example of a quotable signature scheme, using Merkle trees and classical
digital signature schemes. The scheme is shown to be secure, with respect to
the aforementioned notion of security. Additionally, we prove bounds on the
complexity of the constructed scheme and provide algorithms for signing,
quoting, and verifying. Finally, concrete use cases of quotable signatures are
considered, using them to combat misinformation by bolstering authentic content
on social media. We consider both how quotable signatures can be used, and why
using them could help mitigate the effects of fake news.","['Joan Boyar', 'Simon Erfurth', 'Kim S. Larsen', 'Ruben Niederhagen']",7,0.39464706
"Recently, online social media has become a primary source for new information
and misinformation or rumours. In the absence of an automatic rumour detection
system the propagation of rumours has increased manifold leading to serious
societal damages. In this work, we propose a novel method for building
automatic rumour detection system by focusing on oversampling to alleviating
the fundamental challenges of class imbalance in rumour detection task. Our
oversampling method relies on contextualised data augmentation to generate
synthetic samples for underrepresented classes in the dataset. The key idea
exploits selection of tweets in a thread for augmentation which can be achieved
by introducing a non-random selection criteria to focus the augmentation
process on relevant tweets. Furthermore, we propose two graph neural
networks(GNN) to model non-linear conversations on a thread. To enhance the
tweet representations in our method we employed a custom feature selection
technique based on state-of-the-art BERTweet model. Experiments of three
publicly available datasets confirm that 1) our GNN models outperform the the
current state-of-the-art classifiers by more than 20%(F1-score); 2) our
oversampling technique increases the model performance by more than
9%;(F1-score) 3) focusing on relevant tweets for data augmentation via
non-random selection criteria can further improve the results; and 4) our
method has superior capabilities to detect rumours at very early stage.","['Shaswat Patel', 'Prince Bansal', 'Preeti Kaur']",1,0.6833676
"We present a human-in-the-loop evaluation framework for fact-checking novel
misinformation claims and identifying social media messages that support them.
Our approach extracts check-worthy claims, which are aggregated and ranked for
review. Stance classifiers are then used to identify tweets supporting novel
misinformation claims, which are further reviewed to determine whether they
violate relevant policies. To demonstrate the feasibility of our approach, we
develop a baseline system based on modern NLP methods for human-in-the-loop
fact-checking in the domain of COVID-19 treatments. We make our data and
detailed annotation guidelines available to support the evaluation of
human-in-the-loop systems that identify novel misinformation directly from raw
user-generated content.","['Ethan Mendes', 'Yang Chen', 'Wei Xu', 'Alan Ritter']",1,0.70390797
"Despite tremendous advancements in dialogue systems, stable evaluation still
requires human judgments producing notoriously high-variance metrics due to
their inherent subjectivity. Moreover, methods and labels in dialogue
evaluation are not fully standardized, especially for open-domain chats, with a
lack of work to compare and assess the validity of those approaches. The use of
inconsistent evaluation can misinform the performance of a dialogue system,
which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of
chat-oriented open-domain dialogue systems that reliably measures several
aspects of dialogue capabilities is desired. This paper presents a novel human
evaluation method to estimate the rates of many dialogue system behaviors. Our
method is used to evaluate four state-of-the-art open-domain dialogue systems
and compared with existing approaches. The analysis demonstrates that our
behavior method is more suitable than alternative Likert-style or comparative
approaches for dimensional evaluation of these systems.","['Sarah E. Finch', 'James D. Finch', 'Jinho D. Choi']",1,0.4466368
"During the COVID-19 pandemic, the World Health Organization provided a
checklist to help people distinguish between accurate and misinformation. In
controlled experiments in the United States and Germany, we investigated the
utility of this ordered checklist and designed an interactive version to lower
the cost of acting on checklist items. Across interventions, we observe
non-trivial differences in participants' performance in distinguishing accurate
and misinformation between the two countries and discuss some possible reasons
that may predict the future helpfulness of the checklist in different
environments. The checklist item that provides source labels was most
frequently followed and was considered most helpful. Based on our empirical
findings, we recommend practitioners focus on providing source labels rather
than interventions that support readers performing their own fact-checks, even
though this recommendation may be influenced by the WHO's chosen order. We
discuss the complexity of providing such source labels and provide design
recommendations.","['Hendrik Heuer', 'Elena Leah Glassman']",1,0.5532252
"Automatic fake news detection is a challenging problem in misinformation
spreading, and it has tremendous real-world political and social impacts. Past
studies have proposed machine learning-based methods for detecting such fake
news, focusing on different properties of the published news articles, such as
linguistic characteristics of the actual content, which however have
limitations due to the apparent language barriers. Departing from such efforts,
we propose FNDaaS, the first automatic, content-agnostic fake news detection
method, that considers new and unstudied features such as network and
structural characteristics per news website. This method can be enforced
as-a-Service, either at the ISP-side for easier scalability and maintenance, or
user-side for better end-user privacy. We demonstrate the efficacy of our
method using data crawled from existing lists of 637 fake and 1183 real news
websites, and by building and testing a proof of concept system that
materializes our proposal. Our analysis of data collected from these websites
shows that the vast majority of fake news domains are very young and appear to
have lower time periods of an IP associated with their domain than real news
ones. By conducting various experiments with machine learning classifiers, we
demonstrate that FNDaaS can achieve an AUC score of up to 0.967 on past sites,
and up to 77-92% accuracy on newly-flagged ones.","['Panagiotis Papadopoulos', 'Dimitris Spithouris', 'Evangelos P. Markatos', 'Nicolas Kourtellis']",4,0.8446334
"The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We employ three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods combine language
models with background knowledge and explainable mechanisms. Moreover, we
address data sparsity with strategies for data augmentation and curriculum
learning. Our three-stage framework natively consolidates prior datasets and
methods from existing tasks, like propaganda detection, serving as an
overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.","['Zhivar Sourati', 'Vishnu Priya Prasanna Venkatesh', 'Darshan Deshpande', 'Himanshu Rawlani', 'Filip Ilievski', 'H√¥ng-√Çn Sandlin', 'Alain Mermoud']",1,0.75285923
"The widespread of false information is a rising concern worldwide with
critical social impact, inspiring the emergence of fact-checking organizations
to mitigate misinformation dissemination. However, human-driven verification
leads to a time-consuming task and a bottleneck to have checked trustworthy
information at the same pace they emerge. Since misinformation relates not only
to the content itself but also to other social features, this paper addresses
automatic misinformation checking in social networks from a multimodal
perspective. Moreover, as simply naming a piece of news as incorrect may not
convince the citizen and, even worse, strengthen confirmation bias, the
proposal is a modality-level explainable-prone misinformation classifier
framework. Our framework comprises a misinformation classifier assisted by
explainable methods to generate modality-oriented explainable inferences.
Preliminary findings show that the misinformation classifier does benefit from
multimodal information encoding and the modality-oriented explainable mechanism
increases both inferences' interpretability and completeness.","['V√≠tor Louren√ßo', 'Aline Paes']",0,0.7564558
"The social web has linked people on a global scale, transforming how we
communicate and interact. The massive interconnectedness has created new
vulnerabilities in the form of social manipulation and misinformation. As the
social web matures, we are entering a new phase, where people share their
private feelings and emotions. This so-called social emotional web creates new
opportunities for human flourishing, but also exposes new vulnerabilities. To
reap the benefits of the social emotional web, and reduce potential harms, we
must anticipate how it will evolve and create policies that minimize risks.",['Kristina Lerman'],9,0.51333296
"Video synthesis methods rapidly improved in recent years, allowing easy
creation of synthetic humans. This poses a problem, especially in the era of
social media, as synthetic videos of speaking humans can be used to spread
misinformation in a convincing manner. Thus, there is a pressing need for
accurate and robust deepfake detection methods, that can detect forgery
techniques not seen during training. In this work, we explore whether this can
be done by leveraging a multi-modal, out-of-domain backbone trained in a
self-supervised manner, adapted to the video deepfake domain. We propose
FakeOut; a novel approach that relies on multi-modal data throughout both the
pre-training phase and the adaption phase. We demonstrate the efficacy and
robustness of FakeOut in detecting various types of deepfakes, especially
manipulations which were not seen during training. Our method achieves
state-of-the-art results in cross-dataset generalization on audio-visual
datasets. This study shows that, perhaps surprisingly, training on
out-of-domain videos (i.e., not especially featuring speaking humans), can lead
to better deepfake detection systems. Code is available on GitHub.","['Gil Knafo', 'Ohad Fried']",11,0.82919955
"Online social media represent an oftentimes unique source of information, and
having access to reliable and unbiased content is crucial, especially during
crises and contentious events. We study the spread of propaganda and
misinformation that circulated on Facebook and Twitter during the first few
months of the Russia-Ukraine conflict. By leveraging two large datasets of
millions of social media posts, we estimate the prevalence of Russian
propaganda and low-credibility content on the two platforms, describing
temporal patterns and highlighting the disproportionate role played by
superspreaders in amplifying unreliable content. We infer the political leaning
of Facebook pages and Twitter users sharing propaganda and misinformation, and
observe they tend to be more right-leaning than the average. By estimating the
amount of content moderated by the two platforms, we show that only about 8-15%
of the posts and tweets sharing links to Russian propaganda or untrustworthy
sources were removed. Overall, our findings show that Facebook and Twitter are
still vulnerable to abuse, especially during crises: we highlight the need to
urgently address this issue to preserve the integrity of online conversations.","['Francesco Pierri', 'Luca Luceri', 'Nikhil Jindal', 'Emilio Ferrara']",10,0.7853696
"Fake videos represent an important misinformation threat. While existing
forensic networks have demonstrated strong performance on image forgeries,
recent results reported on the Adobe VideoSham dataset show that these networks
fail to identify fake content in videos. In this paper, we show that this is
due to video coding, which introduces local variation into forensic traces. In
response, we propose VideoFACT - a new network that is able to detect and
localize a wide variety of video forgeries and manipulations. To overcome
challenges that existing networks face when analyzing videos, our network
utilizes both forensic embeddings to capture traces left by manipulation,
context embeddings to control for variation in forensic traces introduced by
video coding, and a deep self-attention mechanism to estimate the quality and
relative importance of local forensic embeddings. We create several new video
forgery datasets and use these, along with publicly available data, to
experimentally evaluate our network's performance. These results show that our
proposed network is able to identify a diverse set of video forgeries,
including those not encountered during training. Furthermore, we show that our
network can be fine-tuned to achieve even stronger performance on challenging
AI-based manipulations.","['Tai D. Nguyen', 'Shengbang Fang', 'Matthew C. Stamm']",11,0.6583233
"Approaches to disease control are influenced by and reflected in public
opinion, and the two are intrinsically entwined. Bovine tuberculosis (bTB) in
British cattle and badgers is one example where there is a high degree of
polarisation in opinion. Bovine viral diarrhoea (BVD), on the other hand, does
not have the same controversy.
  In this paper we examine how language subjectivity on Twitter differs when
comparing the discourses surrounding bTB and BVD, using a combination of
network analysis and language and sentiment analysis. That data used for this
study was collected from the Twitter public API over a two-year period. We
investigated the network structure, language content, and user profiles of
tweets featuring both diseases.
  While analysing network structure showed little difference between the two
disease topics, elements of the structure allowed us to better investigate the
language structure and profile of users. We found distinct differences between
the language and sentiment used in tweets about each disease, and in the
profile of the users who were doing the tweeting. We hope that this will guide
further investigation and potential avenues for surveillance or the control of
misinformation.","['Christopher J. Banks', 'Jessica Enright', 'Sibylle Mohr', 'Rowland R. Kao']",5,0.6501105
"Systems for large scale deliberation have resolved polarized issues and
shifted agenda setting into the public's hands. These systems integrate
bridging-based ranking algorithms - including group informed consensus
implemented in Polis and the continuous matrix factorization approach
implemented by Twitter Birdwatch - making it possible to highlight statements
which enjoy broad support from a diversity of opinion groups.
  Polis has been productively employed to foster more constructive political
deliberation at nation scale in law making exercises. Twitter Birdwatch is
implemented with the intention of addressing misinformation in the global
public square. From one perspective, Twitter Birdwatch can be viewed as an
anti-misinformation system which has deliberative aspects. But it can also be
viewed as a first step towards a generalized deliberative system, using
Twitter's misinformation problem as a proving ground.
  In this paper, we propose that Twitter could adapt Birdwatch to produce maps
of public opinion. We describe a system in five parts for generalizing
Birdwatch: activation of a deliberative system and topic selection, population
sampling and the role of expert networks, deliberation, reporting interpretable
results and finally distribution of the results to the public and those in
power.","['Colin Megill', 'Elizabeth Barry', 'Christopher Small']",3,0.595589
"Over the last couple of decades, Social Networks have connected people on the
web from across the globe and have become a crucial part of our daily life.
These networks have also rapidly grown as platforms for propagating products,
ideas, and opinions to target a wider audience. This calls for the need to find
influential nodes in a network for a variety of reasons, including the curb of
misinformation being spread across the networks, advertising products
efficiently, finding prominent protein structures in biological networks, etc.
In this paper, we propose Modified Community Diversity (MCD), a novel method
for finding influential nodes in a network by exploiting community detection
and a modified community diversity approach. We extend the concept of community
diversity to a two-hop scenario. This helps us evaluate a node's possible
influence over a network more accurately and also avoids the selection of seed
nodes with an overlapping scope of influence. Experimental results verify that
MCD outperforms various other state-of-the-art approaches on eight datasets
cumulatively across three performance metrics.","['Aaryan Gupta', 'Inder Khatri', 'Arjun Choudhry', 'Sanjay Kumar']",2,0.69369745
"Recent advances in text classification and knowledge capture in language
models have relied on availability of large-scale text datasets. However,
language models are trained on static snapshots of knowledge and are limited
when that knowledge evolves. This is especially critical for misinformation
detection, where new types of misinformation continuously appear, replacing old
campaigns. We propose time-aware misinformation datasets to capture
time-critical phenomena. In this paper, we first present evidence of evolving
misinformation and show that incorporating even simple time-awareness
significantly improves classifier accuracy. Second, we present COVID-TAD, a
large-scale COVID-19 misinformation da-taset spanning 25 months. It is the
first large-scale misinformation dataset that contains multiple snapshots of a
datastream and is orders of magnitude bigger than related misinformation
datasets. We describe the collection and labeling pro-cess, as well as
preliminary experiments.","['Abhijit Suprem', 'Sanjyot Vaidya', 'Joao Eduardo Ferreira', 'Calton Pu']",8,0.7013402
"There is growing concern about misinformation and the role online media plays
in social polarization. Analyzing belief dynamics is one way to enhance our
understanding of these problems. Existing analytical tools, such as survey
research or stance detection, lack the power to correlate contextual factors
with population-level changes in belief dynamics. In this exploratory study, I
present the Belief Landscape Framework, which uses data about people's
professed beliefs in an online setting to measure belief dynamics with high
resolution. I provide initial validation of the approach by comparing the
method's output to a set of hypotheses drawn from the literature and by
inspecting the ""belief landscape"" generated by the method. My analysis
indicates that the method is relatively robust to different parameter settings,
and results suggest that 1) there are many stable configurations of belief, or
attractors, on the polarizing issue of climate change and 2) that people move
in predictable ways around these attractors. The method paves the way for more
powerful tools that can be used to understand how the modern digital media
ecosystem impacts collective belief dynamics and what role misinformation plays
in that process.",['Joshua Introne'],0,0.66646326
"Anti-vaccination views pervade online social media, fueling distrust in
scientific expertise and increasing vaccine-hesitant individuals. While
previous studies focused on specific countries, the COVID-19 pandemic brought
the vaccination discourse worldwide, underpinning the need to tackle
low-credible information flows on a global scale to design effective
countermeasures. Here, we leverage 316 million vaccine-related Twitter messages
in 18 languages, from October 2019 to March 2021, to quantify misinformation
flows between users exposed to anti-vaccination (no-vax) content. We find that,
during the pandemic, no-vax communities became more central in the
country-specific debates and their cross-border connections strengthened,
revealing a global Twitter anti-vaccination network. U.S. users are central in
this network, while Russian users also become net exporters of misinformation
during vaccination roll-out. Interestingly, we find that Twitter's content
moderation efforts, and in particular the suspension of users following the
January 6th U.S. Capitol attack, had a worldwide impact in reducing
misinformation spread about vaccines. These findings may help public health
institutions and social media platforms to mitigate the spread of
health-related, low-credible information by revealing vulnerable online
communities.","['Jacopo Lenti', 'Kyriaki Kalimeri', 'Andr√© Panisson', 'Daniela Paolotti', 'Michele Tizzani', 'Yelena Mejova', 'Michele Starnini']",12,0.85524225
"Propaganda is the expression of an opinion or an action by an individual or a
group deliberately designed to influence the opinions or the actions of other
individuals or groups with reference to predetermined ends, which is achieved
by means of well-defined rhetorical and psychological devices. Propaganda
techniques are commonly used in social media to manipulate or to mislead users.
Thus, there has been a lot of recent research on automatic detection of
propaganda techniques in text as well as in memes. However, so far the focus
has been primarily on English. With the aim to bridge this language gap, we ran
a shared task on detecting propaganda techniques in Arabic tweets as part of
the WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to
identify the set of propaganda techniques used in a tweet, which is a
multilabel classification problem, while Subtask~2 asks to detect the
propaganda techniques used in a tweet together with the exact span(s) of text
in which each propaganda technique appears. The task attracted 63 team
registrations, and eventually 14 and 3 teams made submissions for subtask 1 and
2, respectively. Finally, 11 teams submitted system description papers.","['Firoj Alam', 'Hamdy Mubarak', 'Wajdi Zaghouani', 'Giovanni Da San Martino', 'Preslav Nakov']",3,0.5497644
"Online forums that allow for participatory engagement between users have been
transformative for the public discussion of many important issues. However,
such conversations can sometimes escalate into full-blown exchanges of hate and
misinformation. Existing approaches in natural language processing (NLP), such
as deep learning models for classification tasks, use as inputs only a single
comment or a pair of comments depending upon whether the task concerns the
inference of properties of the individual comments or the replies between pairs
of comments, respectively. But in online conversations, comments and replies
may be based on external context beyond the immediately relevant information
that is input to the model. Therefore, being aware of the conversations'
surrounding contexts should improve the model's performance for the inference
task at hand.
  We propose GraphNLI, a novel graph-based deep learning architecture that uses
graph walks to incorporate the wider context of a conversation in a principled
manner. Specifically, a graph walk starts from a given comment and samples
""nearby"" comments in the same or parallel conversation threads, which results
in additional embeddings that are aggregated together with the initial
comment's embedding. We then use these enriched embeddings for downstream NLP
prediction tasks that are important for online conversations. We evaluate
GraphNLI on two such tasks - polarity prediction and misogynistic hate speech
detection - and found that our model consistently outperforms all relevant
baselines for both tasks. Specifically, GraphNLI with a biased root-seeking
random walk performs with a macro-F1 score of 3 and 6 percentage points better
than the best-performing BERT-based baselines for the polarity prediction and
hate speech detection tasks, respectively.","['Vibhor Agarwal', 'Anthony P. Young', 'Sagar Joglekar', 'Nishanth Sastry']",1,0.64530474
"It's been controversial whether re-opening school will facilitate viral
spread among household communities with mitigation strategies such as
mask-wearing in place. In this work, we propose an epidemiological model that
explores the viral transmission over the multi-layer contact network composed
of the school layer and community layer with population heterogeneity on
mask-wearing behavior. We derive analytical expressions for three key
epidemiological quantities: the probability of emergence, the epidemic
threshold, and the expected epidemic size. In particular, we show how the
aforementioned quantities depend on the structure of the multi-layer contact
network, viral transmission dynamics, and the distribution of the different
types of masks within the population. Through extensive simulations, our
analytical results show near-perfect agreement with the simulation results with
a limited number of nodes. Utilizing the model, we study the impact of the
opening/closure of the school layer on the viral transmission dynamics with
various mask-wearing scenarios. Interestingly, we found that it's safe to open
the school layer with the proper proportion of good-quality masks in the
population. Moreover, we validate the theory of the trade-off between
source-control and self-protection over a single layer by Tian et al on our
multi-layer setting. We conclude that even on a multi-layer network, it's of
great significance to treat the spreading process as two distinct phases in
mind when considering mitigation strategies. Besides, we would like to remark
that our model of spreading process over multi-layer networks with population
heterogeneity can also be applied to various other domains, such as
misinformation control.","['Yurun Tian', 'Osman Yagan']",2,0.6997614
"Nowadays, the spread of misinformation is a prominent problem in society. Our
research focuses on aiding the automatic identification of misinformation by
analyzing the persuasive strategies employed in textual documents. We introduce
a novel annotation scheme encompassing common persuasive writing tactics to
achieve our objective. Additionally, we provide a dataset on health
misinformation, thoroughly annotated by experts utilizing our proposed scheme.
Our contribution includes proposing a new task of annotating pieces of text
with their persuasive writing strategy types. We evaluate fine-tuning and
prompt-engineering techniques with pre-trained language models of the BERT
family and the generative large language models of the GPT family using
persuasive strategies as an additional source of information. We evaluate the
effects of employing persuasive strategies as intermediate labels in the
context of misinformation detection. Our results show that those strategies
enhance accuracy and improve the explainability of misinformation detection
models. The persuasive strategies can serve as valuable insights and
explanations, enabling other models or even humans to make more informed
decisions regarding the trustworthiness of the information.","['Danial Kamali', 'Joseph Romain', 'Huiyi Liu', 'Wei Peng', 'Jingbo Meng', 'Parisa Kordjamshidi']",0,0.73895967
"The issue of quantifying and characterizing various forms of social media
manipulation and abuse has been at the forefront of the computational social
science research community for over a decade. In this paper, I provide a
(non-comprehensive) survey of research efforts aimed at estimating the
prevalence of spam and false accounts on Twitter, as well as characterizing
their use, activity, and behavior. I propose a taxonomy of spam and false
accounts, enumerating known techniques used to create and detect them. Then, I
summarize studies estimating the prevalence of spam and false accounts on
Twitter. Finally, I report on research that illustrates how spam and false
accounts are used for scams and frauds, stock market manipulation, political
disinformation and deception, conspiracy amplification, coordinated influence,
public health misinformation campaigns, radical propaganda and recruitment, and
more. I will conclude with a set of recommendations aimed at charting the path
forward to combat these problems.",['Emilio Ferrara'],3,0.6215032
"Progress on many Natural Language Processing (NLP) tasks, such as text
classification, is driven by objective, reproducible and scalable evaluation
via publicly available benchmarks. However, these are not always representative
of real-world scenarios where text classifiers are employed, such as sentiment
analysis or misinformation detection. In this position paper, we put forward
two points that aim to alleviate this problem. First, we propose to extend text
classification benchmarks to evaluate the explainability of text classifiers.
We review challenges associated with objectively evaluating the capabilities to
produce valid explanations which leads us to the second main point: We propose
to ground these benchmarks in human-centred applications, for example by using
social media, gamification or to learn explainability metrics from human
judgements.","['Viktor Schlegel', 'Erick Mendez-Guzman', 'Riza Batista-Navarro']",1,0.6582301
"Social media has been one of the main information consumption sources for the
public, allowing people to seek and spread information more quickly and easily.
However, the rise of various social media platforms also enables the
proliferation of online misinformation. In particular, misinformation in the
health domain has significant impacts on our society such as the COVID-19
infodemic. Therefore, health misinformation in social media has become an
emerging research direction that attracts increasing attention from researchers
of different disciplines. Compared to misinformation in other domains, the key
differences of health misinformation include the potential of causing actual
harm to humans' bodies and even lives, the hardness to identify for normal
people, and the deep connection with medical science. In addition, health
misinformation on social media has distinct characteristics from conventional
channels such as television on multiple dimensions including the generation,
dissemination, and consumption paradigms. Because of the uniqueness and
importance of combating health misinformation in social media, we conduct this
survey to further facilitate interdisciplinary research on this problem. In
this survey, we present a comprehensive review of existing research about
online health misinformation in different disciplines. Furthermore, we also
systematically organize the related literature from three perspectives:
characterization, detection, and intervention. Lastly, we conduct a deep
discussion on the pressing open issues of combating health misinformation in
social media and provide future directions for multidisciplinary researchers.","['Canyu Chen', 'Haoran Wang', 'Matthew Shapiro', 'Yunyu Xiao', 'Fei Wang', 'Kai Shu']",0,0.7391509
"Misinformation spread over social media has become an undeniable infodemic.
However, not all spreading claims are made equal. If propagated, some claims
can be destructive, not only on the individual level, but to organizations and
even countries. Detecting claims that should be prioritized for fact-checking
is considered the first step to fight against spread of fake news. With
training data limited to a handful of languages, developing supervised models
to tackle the problem over lower-resource languages is currently infeasible.
Therefore, our work aims to investigate whether we can use existing datasets to
train models for predicting worthiness of verification of claims in tweets in
other languages. We present a systematic comparative study of six approaches
for cross-lingual check-worthiness estimation across pairs of five diverse
languages with the help of Multilingual BERT (mBERT) model. We run our
experiments using a state-of-the-art multilingual Twitter dataset. Our results
show that for some language pairs, zero-shot cross-lingual transfer is possible
and can perform as good as monolingual models that are trained on the target
language. We also show that in some languages, this approach outperforms (or at
least is comparable to) state-of-the-art models.","['Maram Hasanain', 'Tamer Elsayed']",8,0.8646428
"Over the past few years, AI methods of generating images have been increasing
in capabilities, with recent breakthroughs enabling high-resolution,
photorealistic ""deepfakes"" (artificially generated images with the purpose of
misinformation or harm). The rise of deepfakes has potential for social
disruption. Recent work has proposed using ZK-SNARKs (zero-knowledge succinct
non-interactive argument of knowledge) and attested cameras to verify that
images were taken by a camera. ZK-SNARKs allow verification of image
transformations non-interactively (i.e., post-hoc) with only standard
cryptographic hardness assumptions. Unfortunately, this work does not preserve
input privacy, is impractically slow (working only on 128$\times$128 images),
and/or requires custom cryptographic arguments.
  To address these issues, we present zk-img, a library for attesting to image
transformations while hiding the pre-transformed image. zk-img allows
application developers to specify high level image transformations. Then,
zk-img will transparently compile these specifications to ZK-SNARKs. To hide
the input or output images, zk-img will compute the hash of the images inside
the ZK-SNARK. We further propose methods of chaining image transformations
securely and privately, which allows for arbitrarily many transformations. By
combining these optimizations, zk-img is the first system to be able to
transform HD images on commodity hardware, securely and privately.","['Daniel Kang', 'Tatsunori Hashimoto', 'Ion Stoica', 'Yi Sun']",7,0.67460465
"We restrict the propagation of misinformation in a social-media-like
environment while preserving the spread of correct information. We model the
environment as a random network of users in which each news item propagates in
the network in consecutive cascades. Existing studies suggest that the cascade
behaviors of misinformation and correct information are affected differently by
user polarization and reflexivity. We show that this difference can be used to
alter network dynamics in a way that selectively hinders the spread of
misinformation content. To implement these alterations, we introduce an
optimization-based probabilistic dropout method that randomly removes
connections between users to achieve minimal propagation of misinformation. We
use disciplined convex programming to optimize these removal probabilities over
a reduced space of possible network alterations. We test the algorithm's
effectiveness using simulated social networks. In our tests, we use both
synthetic network structures based on stochastic block models, and natural
network structures that are generated using random sampling of a dataset
collected from Twitter. The results show that on average the algorithm
decreases the cascade size of misinformation content by up to $70\%$ in
synthetic network tests and up to $45\%$ in natural network tests while
maintaining a branching ratio of at least $1.5$ for correct information.","['Yigit E. Bayiz', 'Ufuk Topcu']",2,0.83190733
"Fact verification has attracted a lot of research attention recently, e.g.,
in journalism, marketing, and policymaking, as misinformation and
disinformation online can sway one's opinion and affect one's actions. While
fact-checking is a hard task in general, in many cases, false statements can be
easily debunked based on analytics over tables with reliable information.
Hence, table-based fact verification has recently emerged as an important and
growing research area. Yet, progress has been limited due to the lack of
datasets that can be used to pre-train language models (LMs) to be aware of
common table operations, such as aggregating a column or comparing tuples. To
bridge this gap, in this paper we introduce PASTA, a novel state-of-the-art
framework for table-based fact verification via pre-training with synthesized
sentence-table cloze questions. In particular, we design six types of common
sentence-table cloze tasks, including Filter, Aggregation, Superlative,
Comparative, Ordinal, and Unique, based on which we synthesize a large corpus
consisting of 1.2 million sentence-table pairs from WikiTables. PASTA uses a
recent pre-trained LM, DeBERTaV3, and further pretrains it on our corpus. Our
experimental results show that PASTA achieves new state-of-the-art performance
on two table-based fact verification benchmarks: TabFact and SEM-TAB-FACTS. In
particular, on the complex set of TabFact, which contains multiple operations,
PASTA largely outperforms the previous state of the art by 4.7 points (85.6%
vs. 80.9%), and the gap between PASTA and human performance on the small
TabFact test set is narrowed to just 1.5 points (90.6% vs. 92.1%).","['Zihui Gu', 'Ju Fan', 'Nan Tang', 'Preslav Nakov', 'Xiaoman Zhao', 'Xiaoyong Du']",8,0.59365714
"The increasing reliance on online communities for healthcare information by
patients and caregivers has led to the increase in the spread of
misinformation, or subjective, anecdotal and inaccurate or non-specific
recommendations, which, if acted on, could cause serious harm to the patients.
Hence, there is an urgent need to connect users with accurate and tailored
health information in a timely manner to prevent such harm. This paper proposes
an innovative approach to suggesting reliable information to participants in
online communities as they move through different stages in their disease or
treatment. We hypothesize that patients with similar histories of disease
progression or course of treatment would have similar information needs at
comparable stages. Specifically, we pose the problem of predicting topic tags
or keywords that describe the future information needs of users based on their
profiles, traces of their online interactions within the community (past posts,
replies) and the profiles and traces of online interactions of other users with
similar profiles and similar traces of past interaction with the target users.
The result is a variant of the collaborative information filtering or
recommendation system tailored to the needs of users of online health
communities. We report results of our experiments on an expert curated data set
which demonstrate the superiority of the proposed approach over the state of
the art baselines with respect to accurate and timely prediction of topic tags
(and hence information sources of interest).","['Amogh Subbakrishna Adishesha', 'Lily Jakielaszek', 'Fariha Azhar', 'Peixuan Zhang', 'Vasant Honavar', 'Fenglong Ma', 'Chandra Belani', 'Prasenjit Mitra', 'Sharon Xiaolei Huang']",0,0.6850959
"Malicious actors exploit social media to inflate stock prices, sway
elections, spread misinformation, and sow discord. To these ends, they employ
tactics that include the use of inauthentic accounts and campaigns. Methods to
detect these abuses currently rely on features specifically designed to target
suspicious behaviors. However, the effectiveness of these methods decays as
malicious behaviors evolve. To address this challenge, we propose a general
language for modeling social media account behavior. Words in this language,
called BLOC, consist of symbols drawn from distinct alphabets representing user
actions and content. The language is highly flexible and can be applied to
model a broad spectrum of legitimate and suspicious online behaviors without
extensive fine-tuning. Using BLOC to represent the behaviors of Twitter
accounts, we achieve performance comparable to or better than state-of-the-art
methods in the detection of social bots and coordinated inauthentic behavior.","['Alexander C. Nwala', 'Alessandro Flammini', 'Filippo Menczer']",13,0.6725551
"Social media users who report content are key allies in the management of
online misinformation, however, no research has been conducted yet to
understand their role and the different trends underlying their reporting
activity. We suggest an original approach to studying misinformation: examining
it from the reporting users perspective at the content-level and comparatively
across regions and platforms. We propose the first classification of reported
content pieces, resulting from a review of c. 9,000 items reported on Facebook
and Instagram in France, the UK, and the US in June 2020. This allows us to
observe meaningful distinctions regarding reporting content between countries
and platforms as it significantly varies in volume, type, topic, and
manipulation technique. Examining six of these techniques, we identify a novel
one that is specific to Instagram US and significantly more sophisticated than
others, potentially presenting a concrete challenge for algorithmic detection
and human moderation. We also identify four reporting behaviours, from which we
derive four types of noise capable of explaining half of the inaccuracy found
in content reported as misinformation. We finally show that breaking down the
user reporting signal into a plurality of behaviours allows to train a simple,
although competitive, classifier on a small dataset with a combination of basic
users-reports to classify the different types of reported content pieces.","['Hubert Etienne', 'Onur √áelebi']",4,0.692732
"We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi,
and Arabic annotated with stance towards 251 misinformation claims. As far as
we are aware, it is the largest corpus annotated with stance towards
misinformation claims. The claims in Stanceosaurus originate from 15
fact-checking sources that cover diverse geographical regions and cultures.
Unlike existing stance datasets, we introduce a more fine-grained 5-class
labeling strategy with additional subcategories to distinguish implicit stance.
Pre-trained transformer-based stance classifiers that are fine-tuned on our
corpus show good generalization on unseen claims and regional claims from
countries outside the training data. Cross-lingual experiments demonstrate
Stanceosaurus' capability of training multi-lingual models, achieving 53.1 F1
on Hindi and 50.4 F1 on Arabic without any target-language fine-tuning.
Finally, we show how a domain adaptation method can be used to improve
performance on Stanceosaurus using additional RumourEval-2019 data. We make
Stanceosaurus publicly available to the research community and hope it will
encourage further work on misinformation identification across languages and
cultures.","['Jonathan Zheng', 'Ashutosh Baheti', 'Tarek Naous', 'Wei Xu', 'Alan Ritter']",8,0.6869448
"We present an approach for selecting objectively informative and subjectively
helpful annotations to social media posts. We draw on data from on an online
environment where contributors annotate misinformation and simultaneously rate
the contributions of others. Our algorithm uses a matrix-factorization (MF)
based approach to identify annotations that appeal broadly across heterogeneous
user groups - sometimes referred to as ""bridging-based ranking."" We pair these
data with a survey experiment in which individuals are randomly assigned to see
annotations to posts. We find that annotations selected by the algorithm
improve key indicators compared with overall average and crowd-generated
baselines. Further, when deployed on Twitter, people who saw annotations
selected through this bridging-based approach were significantly less likely to
reshare social media posts than those who did not see the annotations.","['Stefan Wojcik', 'Sophie Hilgard', 'Nick Judd', 'Delia Mocanu', 'Stephen Ragain', 'M. B. Fallin Hunzaker', 'Keith Coleman', 'Jay Baxter']",2,0.6078855
"With recent advancements in diffusion models, users can generate high-quality
images by writing text prompts in natural language. However, generating images
with desired details requires proper prompts, and it is often unclear how a
model reacts to different prompts or what the best prompts are. To help
researchers tackle these critical challenges, we introduce DiffusionDB, the
first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14
million images generated by Stable Diffusion, 1.8 million unique prompts, and
hyperparameters specified by real users. We analyze the syntactic and semantic
characteristics of prompts. We pinpoint specific hyperparameter values and
prompt styles that can lead to model errors and present evidence of potentially
harmful model usage, such as the generation of misinformation. The
unprecedented scale and diversity of this human-actuated dataset provide
exciting research opportunities in understanding the interplay between prompts
and generative models, detecting deepfakes, and designing human-AI interaction
tools to help users more easily use these models. DiffusionDB is publicly
available at: https://poloclub.github.io/diffusiondb.","['Zijie J. Wang', 'Evan Montoya', 'David Munechika', 'Haoyang Yang', 'Benjamin Hoover', 'Duen Horng Chau']",7,0.6587721
"Misinformation emerges in times of uncertainty when credible information is
limited. This is challenging for NLP-based fact-checking as it relies on
counter-evidence, which may not yet be available. Despite increasing interest
in automatic fact-checking, it is still unclear if automated approaches can
realistically refute harmful real-world misinformation. Here, we contrast and
compare NLP fact-checking with how professional fact-checkers combat
misinformation in the absence of counter-evidence. In our analysis, we show
that, by design, existing NLP task definitions for fact-checking cannot refute
misinformation as professional fact-checkers do for the majority of claims. We
then define two requirements that the evidence in datasets must fulfill for
realistic fact-checking: It must be (1) sufficient to refute the claim and (2)
not leaked from existing fact-checking articles. We survey existing
fact-checking datasets and find that all of them fail to satisfy both criteria.
Finally, we perform experiments to demonstrate that models trained on a
large-scale fact-checking dataset rely on leaked evidence, which makes them
unsuitable in real-world scenarios. Taken together, we show that current NLP
fact-checking cannot realistically combat real-world misinformation because it
depends on unrealistic assumptions about counter-evidence in the data.","['Max Glockner', 'Yufang Hou', 'Iryna Gurevych']",1,0.71005243
"This paper seeks to address the classification of misinformation in news
articles using a Long Short Term Memory Recurrent Neural Network. Articles were
taken from 2018; a year that was filled with reporters writing about President
Donald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.
The model presented successfully classifies these articles with an accuracy
score of 0.779944. We consider this to be successful because the model was
trained on articles that included languages other than English as well as
incomplete, or fragmented, articles.","['Brendan Cunha', 'Lydia Manikonda']",4,0.513446
"Given the prevalence of online misinformation and our scarce cognitive
capacity, Internet users have been shown to frequently fall victim to such
information. As some studies have investigated psychological factors that make
people susceptible to believe or share misinformation, some ongoing research
further put these findings into practice by objectively identifying when and
which users are vulnerable to misinformation. In this position paper, we
highlight two ongoing avenues of research to identify vulnerable users:
detecting cognitive biases and exploring misinformation spreaders. We also
discuss the potential implications of these objective approaches: discovering
more cohorts of vulnerable users and prompting interventions to more
effectively address the right group of users. Lastly, we point out two of the
understudied contexts for misinformation vulnerability research as
opportunities for future research.","['Nattapat Boonprakong', 'Benjamin Tag', 'Tilman Dingler']",0,0.7666321
"In this paper, we present results of an auditing study performed over YouTube
aimed at investigating how fast a user can get into a misinformation filter
bubble, but also what it takes to ""burst the bubble"", i.e., revert the bubble
enclosure. We employ a sock puppet audit methodology, in which pre-programmed
agents (acting as YouTube users) delve into misinformation filter bubbles by
watching misinformation promoting content. Then they try to burst the bubbles
and reach more balanced recommendations by watching misinformation debunking
content. We record search results, home page results, and recommendations for
the watched videos. Overall, we recorded 17,405 unique videos, out of which we
manually annotated 2,914 for the presence of misinformation. The labeled data
was used to train a machine learning model classifying videos into three
classes (promoting, debunking, neutral) with the accuracy of 0.82. We use the
trained model to classify the remaining videos that would not be feasible to
annotate manually.
  Using both the manually and automatically annotated data, we observe the
misinformation bubble dynamics for a range of audited topics. Our key finding
is that even though filter bubbles do not appear in some situations, when they
do, it is possible to burst them by watching misinformation debunking content
(albeit it manifests differently from topic to topic). We also observe a sudden
decrease of misinformation filter bubble effect when misinformation debunking
videos are watched after misinformation promoting videos, suggesting a strong
contextuality of recommendations. Finally, when comparing our results with a
previous similar study, we do not observe significant improvements in the
overall quantity of recommended misinformation content.","['Ivan Srba', 'Robert Moro', 'Matus Tomlein', 'Branislav Pecher', 'Jakub Simko', 'Elena Stefancova', 'Michal Kompan', 'Andrea Hrckova', 'Juraj Podrouzek', 'Adrian Gavornik', 'Maria Bielikova']",3,0.58172995
"Social media platforms often assume that users can self-correct against
misinformation. However, social media users are not equally susceptible to all
misinformation as their biases influence what types of misinformation might
thrive and who might be at risk. We call ""diverse misinformation"" the complex
relationships between human biases and demographics represented in
misinformation. To investigate how users' biases impact their susceptibility
and their ability to correct each other, we analyze classification of deepfakes
as a type of diverse misinformation. We chose deepfakes as a case study for
three reasons: 1) their classification as misinformation is more objective; 2)
we can control the demographics of the personas presented; 3) deepfakes are a
real-world concern with associated harms that must be better understood. Our
paper presents an observational survey (N=2,016) where participants are exposed
to videos and asked questions about their attributes, not knowing some might be
deepfakes. Our analysis investigates the extent to which different users are
duped and which perceived demographics of deepfake personas tend to mislead. We
find that accuracy varies by demographics, and participants are generally
better at classifying videos that match them. We extrapolate from these results
to understand the potential population-level impacts of these biases using a
mathematical model of the interplay between diverse misinformation and crowd
correction. Our model suggests that diverse contacts might provide ""herd
correction"" where friends can protect each other. Altogether, human biases and
the attributes of misinformation matter greatly, but having a diverse social
group may help reduce susceptibility to misinformation.","['Juniper Lovato', 'Laurent H√©bert-Dufresne', 'Jonathan St-Onge', 'Randall Harp', 'Gabriela Salazar Lopez', 'Sean P. Rogers', 'Ijaz Ul Haq', 'Jeremiah Onaolapo']",3,0.75454015
"Question generation has recently gained a lot of research interest,
especially with the advent of large language models. In and of itself, question
generation can be considered 'AI-hard', as there is a lack of unanimously
agreed sense of what makes a question 'good' or 'bad'. In this paper, we tackle
two fundamental problems in parallel: on one hand, we try to solve the scaling
problem, where question-generation and answering applications have to be
applied to a massive amount of text without ground truth labeling. The usual
approach to solve this problem is to either downsample or summarize. However,
there are critical risks of misinformation with these approaches. On the other
hand, and related to the misinformation problem, we try to solve the 'safety'
problem, as many public institutions rely on a much higher level of accuracy
for the content they provide. We introduce an adversarial approach to tackle
the question generation safety problem with scale. Specifically, we designed a
question-answering system that specifically prunes out unanswerable questions
that may be generated, and further increases the quality of the answers that
are generated. We build a production-ready, easily-plugged pipeline that can be
used on any given body of text, that is scalable and immune from generating any
hate speech, profanity, or misinformation. Based on the results, we are able to
generate more than six times the number of quality questions generated by the
abstractive approach, with a perceived quality being 44% higher, according to a
survey of 168 participants.","['Sreehari Sankar', 'Zhihang Dong']",9,0.61653745
"This study investigated and compared public sentiment related to COVID-19
vaccines expressed on two popular social media platforms, Reddit and Twitter,
harvested from January 1, 2020, to March 1, 2022. To accomplish this task, we
created a fine-tuned DistilRoBERTa model to predict sentiments of approximately
9.5 million Tweets and 70 thousand Reddit comments. To fine-tune our model, our
team manually labeled the sentiment of 3600 Tweets and then augmented our
dataset by the method of back-translation. Text sentiment for each social media
platform was then classified with our fine-tuned model using Python and the
Huggingface sentiment analysis pipeline. Our results determined that the
average sentiment expressed on Twitter was more negative (52% positive) than
positive and the sentiment expressed on Reddit was more positive than negative
(53% positive). Though average sentiment was found to vary between these social
media platforms, both displayed similar behavior related to sentiment shared at
key vaccine-related developments during the pandemic. Considering this similar
trend in shared sentiment demonstrated across social media platforms, Twitter
and Reddit continue to be valuable data sources that public health officials
can utilize to strengthen vaccine confidence and combat misinformation. As the
spread of misinformation poses a range of psychological and psychosocial risks
(anxiety, fear, etc.), there is an urgency in understanding the public
perspective and attitude toward shared falsities. Comprehensive educational
delivery systems tailored to the population's expressed sentiments that
facilitate digital literacy, health information-seeking behavior, and precision
health promotion could aid in clarifying such misinformation.","['Chad A Melton', 'Brianna M White', 'Robert L Davis', 'Robert A Bednarczyk', 'Arash Shaban-Nejad']",12,0.7337498
"Speech synthesis methods can create realistic-sounding speech, which may be
used for fraud, spoofing, and misinformation campaigns. Forensic methods that
detect synthesized speech are important for protection against such attacks.
Forensic attribution methods provide even more information about the nature of
synthesized speech signals because they identify the specific speech synthesis
method (i.e., speech synthesizer) used to create a speech signal. Due to the
increasing number of realistic-sounding speech synthesizers, we propose a
speech attribution method that generalizes to new synthesizers not seen during
training. To do so, we investigate speech synthesizer attribution in both a
closed set scenario and an open set scenario. In other words, we consider some
speech synthesizers to be ""known"" synthesizers (i.e., part of the closed set)
and others to be ""unknown"" synthesizers (i.e., part of the open set). We
represent speech signals as spectrograms and train our proposed method, known
as compact attribution transformer (CAT), on the closed set for multi-class
classification. Then, we extend our analysis to the open set to attribute
synthesized speech signals to both known and unknown synthesizers. We utilize a
t-distributed stochastic neighbor embedding (tSNE) on the latent space of the
trained CAT to differentiate between each unknown synthesizer. Additionally, we
explore poly-1 loss formulations to improve attribution results. Our proposed
approach successfully attributes synthesized speech signals to their respective
speech synthesizers in both closed and open set scenarios.","['Emily R. Bartusiak', 'Edward J. Delp']",11,0.5807086
"Recent years have witnessed the rise of misinformation campaigns that spread
specific narratives on social media to manipulate public opinions on different
areas, such as politics and healthcare. Consequently, an effective and
efficient automatic methodology to estimate the influence of the misinformation
on user beliefs and activities is needed. However, existing works on
misinformation impact estimation either rely on small-scale psychological
experiments or can only discover the correlation between user behaviour and
misinformation. To address these issues, in this paper, we build up a causal
framework that model the causal effect of misinformation from the perspective
of temporal point process. To adapt the large-scale data, we design an
efficient yet precise way to estimate the Individual Treatment Effect(ITE) via
neural temporal point process and gaussian mixture models. Extensive
experiments on synthetic dataset verify the effectiveness and efficiency of our
model. We further apply our model on a real-world dataset of social media posts
and engagements about COVID-19 vaccines. The experimental results indicate that
our model recognized identifiable causal effect of misinformation that hurts
people's subjective emotions toward the vaccines.","['Yizhou Zhang', 'Defu Cao', 'Yan Liu']",0,0.676214
"We propose a novel system to help fact-checkers formulate search queries for
known misinformation claims and effectively search across multiple social media
platforms. We introduce an adaptable rewriting strategy, where editing actions
for queries containing claims (e.g., swap a word with its synonym; change verb
tense into present simple) are automatically learned through offline
reinforcement learning. Our model uses a decision transformer to learn a
sequence of editing actions that maximizes query retrieval metrics such as mean
average precision. We conduct a series of experiments showing that our query
rewriting system achieves a relative increase in the effectiveness of the
queries of up to 42%, while producing editing action sequences that are human
interpretable.","['Ashkan Kazemi', 'Artem Abzaliev', 'Naihao Deng', 'Rui Hou', 'Scott A. Hale', 'Ver√≥nica P√©rez-Rosas', 'Rada Mihalcea']",1,0.68967116
"Ad-tech enables publishers to programmatically sell their ad inventory to
millions of demand partners through a complex supply chain. Bogus or low
quality publishers can exploit the opaque nature of the ad-tech to deceptively
monetize their ad inventory. In this paper, we investigate for the first time
how misinformation sites subvert the ad-tech transparency standards and pool
their ad inventory with unrelated sites to circumvent brand safety protections.
We find that a few major ad exchanges are disproportionately responsible for
the dark pools that are exploited by misinformation websites. We further find
evidence that dark pooling allows misinformation sites to deceptively sell
their ad inventory to reputable brands. We conclude with a discussion of
potential countermeasures such as better vetting of ad exchange partners,
adoption of new ad-tech transparency standards that enable end-to-end
validation of the ad-tech supply chain, as well as widespread deployment of
independent audits like ours.","['Yash Vekaria', 'Rishab Nithyanand', 'Zubair Shafiq']",9,0.47126588
"The rapid dissemination of misinformation through online social networks
poses a pressing issue with harmful consequences jeopardizing human health,
public safety, democracy, and the economy; therefore, urgent action is required
to address this problem. In this study, we construct a new human-annotated
dataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with
their misinformation labels for several recent events between 2020 and 2022,
including the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset
includes user engagements with the tweets in terms of likes, replies, retweets,
and quotes. We also provide a detailed data analysis with descriptive
statistics and the experimental results of a benchmark evaluation for
misinformation detection.","['Cagri Toraman', 'Oguzhan Ozcelik', 'Furkan ≈ûahinu√ß', 'Fazli Can']",5,0.6795796
"The widespread diffusion of medical and political claims in the wake of
COVID-19 has led to a voluminous rise in misinformation and fake news. The
current vogue is to employ manual fact-checkers to efficiently classify and
verify such data to combat this avalanche of claim-ridden misinformation.
However, the rate of information dissemination is such that it vastly outpaces
the fact-checkers' strength. Therefore, to aid manual fact-checkers in
eliminating the superfluous content, it becomes imperative to automatically
identify and extract the snippets of claim-worthy (mis)information present in a
post. In this work, we introduce the novel task of Claim Span Identification
(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim
spans on more than 7.5k tweets. Furthermore, along with the standard token
classification baselines, we benchmark our dataset with DABERTa, an
adapter-based variation of RoBERTa. The experimental results attest that
DABERTa outperforms the baseline systems across several evaluation metrics,
improving by about 1.5 points. We also report detailed error analysis to
validate the model's performance along with the ablation studies. Lastly, we
release our comprehensive span annotation guidelines for public use.","['Megha Sundriyal', 'Atharva Kulkarni', 'Vaibhav Pulastya', 'Md Shad Akhtar', 'Tanmoy Chakraborty']",1,0.7437062
"In the real-world application of COVID-19 misinformation detection, a
fundamental challenge is the lack of the labeled COVID data to enable
supervised end-to-end training of the models, especially at the early stage of
the pandemic. To address this challenge, we propose an unsupervised domain
adaptation framework using contrastive learning and adversarial domain mixup to
transfer the knowledge from an existing source data domain to the target
COVID-19 data domain. In particular, to bridge the gap between the source
domain and the target domain, our method reduces a radial basis function (RBF)
based discrepancy between these two domains. Moreover, we leverage the power of
domain adversarial examples to establish an intermediate domain mixup, where
the latent representations of the input text from both domains could be mixed
during the training process. Extensive experiments on multiple real-world
datasets suggest that our method can effectively adapt misinformation detection
systems to the unseen COVID-19 target domain with significant improvements
compared to the state-of-the-art baselines.","['Huimin Zeng', 'Zhenrui Yue', 'Ziyi Kou', 'Lanyu Shang', 'Yang Zhang', 'Dong Wang']",2,0.6349206
"Machine learning (ML) enabled classification models are becoming increasingly
popular for tackling the sheer volume and speed of online misinformation and
other content that could be identified as harmful. In building these models,
data scientists need to take a stance on the legitimacy, authoritativeness and
objectivity of the sources of ``truth"" used for model training and testing.
This has political, ethical and epistemic implications which are rarely
addressed in technical papers. Despite (and due to) their reported high
accuracy and performance, ML-driven moderation systems have the potential to
shape online public debate and create downstream negative impacts such as undue
censorship and the reinforcing of false beliefs. Using collaborative
ethnography and theoretical insights from social studies of science and
expertise, we offer a critical analysis of the process of building ML models
for (mis)information classification: we identify a series of algorithmic
contingencies--key moments during model development that could lead to
different future outcomes, uncertainty and harmful effects as these tools are
deployed by social media platforms. We conclude by offering a tentative path
toward reflexive and responsible development of ML tools for moderating
misinformation and other harmful content online.","['Andr√©s Dom√≠nguez Hern√°ndez', 'Richard Owen', 'Dan Saattrup Nielsen', 'Ryan McConville']",0,0.7448141
"This paper provides a simple theoretical framework to evaluate the effect of
key parameters of ranking algorithms, namely popularity and personalization
parameters, on measures of platform engagement, misinformation and
polarization. The results show that an increase in the weight assigned to
online social interactions (e.g., likes and shares) and to personalized content
may increase engagement on the social media platform, while at the same time
increasing misinformation and/or polarization. By exploiting Facebook's 2018
""Meaningful Social Interactions"" algorithmic ranking update, we also provide
direct empirical support for some of the main predictions of the model.","['Fabrizio Germano', 'Vicen√ß G√≥mez', 'Francesco Sobbrio']",0,0.5839567
"Manipulation tools that realistically edit images are widely available,
making it easy for anyone to create and spread misinformation. In an attempt to
fight fake news, forgery detection and localization methods were designed.
However, existing methods struggle to accurately reveal manipulations found in
images on the internet, i.e., in the wild. That is because the type of forgery
is typically unknown, in addition to the tampering traces being damaged by
recompression. This paper presents Comprint, a novel forgery detection and
localization method based on the compression fingerprint or comprint. It is
trained on pristine data only, providing generalization to detect different
types of manipulation. Additionally, we propose a fusion of Comprint with the
state-of-the-art Noiseprint, which utilizes a complementary camera model
fingerprint. We carry out an extensive experimental analysis and demonstrate
that Comprint has a high level of accuracy on five evaluation datasets that
represent a wide range of manipulation types, mimicking in-the-wild
circumstances. Most notably, the proposed fusion significantly outperforms
state-of-the-art reference methods. As such, Comprint and the fusion
Comprint+Noiseprint represent a promising forensics tool to analyze in-the-wild
tampered images.","['Hannes Mareen', 'Dante Vanden Bussche', 'Fabrizio Guillaro', 'Davide Cozzolino', 'Glenn Van Wallendael', 'Peter Lambert', 'Luisa Verdoliva']",7,0.7688403
"Epidemics and outbreaks present arduous challenges requiring both individual
and communal efforts. Social media offer significant amounts of data that can
be leveraged for bio-surveillance. They also provide a platform to quickly and
efficiently reach a sizeable percentage of the population, hence their
potential impact on various aspects of epidemic mitigation. The general
objective of this systematic literature review is to provide a methodical
overview of the integration of social media in different epidemic-related
contexts. Three research questions were conceptualized for this review,
resulting in over 10000 publications collected in the first PRISMA stage, 129
of which were selected for inclusion. A thematic method-oriented synthesis was
undertaken and identified 5 main themes related to social media enabled
epidemic surveillance, misinformation management, and mental health. Findings
uncover a need for more robust applications of the lessons learned from
epidemic post-mortem documentation. A vast gap exists between retrospective
analysis of epidemic management and result integration in prospective studies.
Harnessing the full potential of social media in epidemic related tasks
requires streamlining the results of epidemic forecasting, public opinion
understanding and misinformation propagation, all while keeping abreast of
potential mental health implications. Pro-active prevention has thus become
vital for epidemic curtailment and containment.","['Chaimae Asaad', 'Imane Khaouja', 'Mounir Ghogho', 'Karim Ba√Øna']",5,0.6666142
"Deepfakes pose severe threats of visual misinformation to our society. One
representative deepfake application is face manipulation that modifies a
victim's facial attributes in an image, e.g., changing her age or hair color.
The state-of-the-art face manipulation techniques rely on Generative
Adversarial Networks (GANs). In this paper, we propose the first defense
system, namely UnGANable, against GAN-inversion-based face manipulation. In
specific, UnGANable focuses on defending GAN inversion, an essential step for
face manipulation. Its core technique is to search for alternative images
(called cloaked images) around the original images (called target images) in
image space. When posted online, these cloaked images can jeopardize the GAN
inversion process. We consider two state-of-the-art inversion techniques
including optimization-based inversion and hybrid inversion, and design five
different defenses under five scenarios depending on the defender's background
knowledge. Extensive experiments on four popular GAN models trained on two
benchmark face datasets show that UnGANable achieves remarkable effectiveness
and utility performance, and outperforms multiple baseline methods. We further
investigate four adaptive adversaries to bypass UnGANable and show that some of
them are slightly effective.","['Zheng Li', 'Ning Yu', 'Ahmed Salem', 'Michael Backes', 'Mario Fritz', 'Yang Zhang']",7,0.72047055
"Malicious actors may seek to use different voice-spoofing attacks to fool ASV
systems and even use them for spreading misinformation. Various countermeasures
have been proposed to detect these spoofing attacks. Due to the extensive work
done on spoofing detection in automated speaker verification (ASV) systems in
the last 6-7 years, there is a need to classify the research and perform
qualitative and quantitative comparisons on state-of-the-art countermeasures.
Additionally, no existing survey paper has reviewed integrated solutions to
voice spoofing evaluation and speaker verification, adversarial/antiforensics
attacks on spoofing countermeasures, and ASV itself, or unified solutions to
detect multiple attacks using a single model. Further, no work has been done to
provide an apples-to-apples comparison of published countermeasures in order to
assess their generalizability by evaluating them across corpora. In this work,
we conduct a review of the literature on spoofing detection using hand-crafted
features, deep learning, end-to-end, and universal spoofing countermeasure
solutions to detect speech synthesis (SS), voice conversion (VC), and replay
attacks. Additionally, we also review integrated solutions to voice spoofing
evaluation and speaker verification, adversarial and anti-forensics attacks on
voice countermeasures, and ASV. The limitations and challenges of the existing
spoofing countermeasures are also presented. We report the performance of these
countermeasures on several datasets and evaluate them across corpora. For the
experiments, we employ the ASVspoof2019 and VSDC datasets along with GMM, SVM,
CNN, and CNN-GRU classifiers. (For reproduceability of the results, the code of
the test bed can be found in our GitHub Repository.","['Awais Khan', 'Khalid Mahmood Malik', 'James Ryan', 'Mikul Saravanan']",1,0.66077566
"As the information on the Internet continues growing exponentially,
understanding and assessing the reliability of a website is becoming
increasingly important. Misinformation has far-ranging repercussions, from
sowing mistrust in media to undermining democratic elections. While some
research investigates how to alert people to misinformation on the web, much
less research has been conducted on explaining how websites engage in spreading
false information. To fill the research gap, we present MisVis, a web-based
interactive visualization tool that helps users assess a website's reliability
by understanding how it engages in spreading false information on the World
Wide Web. MisVis visualizes the hyperlink connectivity of the website and
summarizes key characteristics of the Twitter accounts that mention the site. A
large-scale user study with 139 participants demonstrates that MisVis
facilitates users to assess and understand false information on the web and
node-link diagrams can be used to communicate with non-experts. MisVis is
available at the public demo link: https://poloclub.github.io/MisVis.","['Seongmin Lee', 'Sadia Afroz', 'Haekyu Park', 'Zijie J. Wang', 'Omar Shaikh', 'Vibhor Sehgal', 'Ankit Peshin', 'Duen Horng Chau']",0,0.75538933
"Social media platforms have had considerable impact on the real world
especially during the Covid-19 pandemic. Misinformation related to Covid-19
might have caused significant impact on the population specifically due to its
association with dangerous beliefs such as anti-vaccination and Covid denial.
In this work, we study a unique dataset of Facebook posts by users who shared
and believed in Covid-19 misinformation before succumbing to Covid-19 often
resulting in death. We aim to characterize the dominant themes and sources
present in the victim's posts along with identifying the role of the platform
in handling deadly narratives. Our analysis reveals the overwhelming
politicization of Covid-19 through the prevalence of anti-government themes
propagated by right-wing political and media ecosystem. Furthermore, we
highlight the failures of Facebook's implementation and completeness of soft
moderation actions intended to warn users of misinformation. Results from this
study bring insights into the responsibility of political elites in shaping
public discourse and the platform's role in dampening the reach of harmful
misinformation.","['Hussam Habib', 'Rishab Nithyanand']",5,0.772635
"Due to the reduction of technological costs and the increase of satellites
launches, satellite images are becoming more popular and easier to obtain.
Besides serving benevolent purposes, satellite data can also be used for
malicious reasons such as misinformation. As a matter of fact, satellite images
can be easily manipulated relying on general image editing tools. Moreover,
with the surge of Deep Neural Networks (DNNs) that can generate realistic
synthetic imagery belonging to various domains, additional threats related to
the diffusion of synthetically generated satellite images are emerging. In this
paper, we review the State of the Art (SOTA) on the generation and manipulation
of satellite images. In particular, we focus on both the generation of
synthetic satellite imagery from scratch, and the semantic manipulation of
satellite images by means of image-transfer technologies, including the
transformation of images obtained from one type of sensor to another one. We
also describe forensic detection techniques that have been researched so far to
classify and detect synthetic image forgeries. While we focus mostly on
forensic techniques explicitly tailored to the detection of AI-generated
synthetic contents, we also review some methods designed for general splicing
detection, which can in principle also be used to spot AI manipulate images","['Lydia Abady', 'Edoardo Daniele Cannas', 'Paolo Bestagini', 'Benedetta Tondi', 'Stefano Tubaro', 'Mauro Barni']",7,0.7147882
"Both real and fake news in various domains, such as politics, health, and
entertainment are spread via online social media every day, necessitating fake
news detection for multiple domains. Among them, fake news in specific domains
like politics and health has more serious potential negative impacts on the
real world (e.g., the infodemic led by COVID-19 misinformation). Previous
studies focus on multi-domain fake news detection, by equally mining and
modeling the correlation between domains. However, these multi-domain methods
suffer from a seesaw problem: the performance of some domains is often improved
at the cost of hurting the performance of other domains, which could lead to an
unsatisfying performance in specific domains. To address this issue, we propose
a Domain- and Instance-level Transfer Framework for Fake News Detection
(DITFEND), which could improve the performance of specific target domains. To
transfer coarse-grained domain-level knowledge, we train a general model with
data of all domains from the meta-learning perspective. To transfer
fine-grained instance-level knowledge and adapt the general model to a target
domain, we train a language model on the target domain to evaluate the
transferability of each data instance in source domains and re-weigh each
instance's contribution. Offline experiments on two datasets demonstrate the
effectiveness of DITFEND. Online experiments show that DITFEND brings
additional improvements over the base models in a real-world scenario.","['Qiong Nan', 'Danding Wang', 'Yongchun Zhu', 'Qiang Sheng', 'Yuhui Shi', 'Juan Cao', 'Jintao Li']",4,0.61479455
"As social media becomes a hotbed for the spread of misinformation, the
crucial task of rumor detection has witnessed promising advances fostered by
open-source benchmark datasets. Despite being widely used, we find that these
datasets suffer from spurious correlations, which are ignored by existing
studies and lead to severe overestimation of existing rumor detection
performance. The spurious correlations stem from three causes: (1) event-based
data collection and labeling schemes assign the same veracity label to multiple
highly similar posts from the same underlying event; (2) merging multiple data
sources spuriously relates source identities to veracity labels; and (3)
labeling bias. In this paper, we closely investigate three of the most popular
rumor detection benchmark datasets (i.e., Twitter15, Twitter16 and PHEME), and
propose event-separated rumor detection as a solution to eliminate spurious
cues. Under the event-separated setting, we observe that the accuracy of
existing state-of-the-art models drops significantly by over 40%, becoming only
comparable to a simple neural classifier. To better address this task, we
propose Publisher Style Aggregation (PSA), a generalizable approach that
aggregates publisher posting records to learn writing style and veracity
stance. Extensive experiments demonstrate that our method outperforms existing
baselines in terms of effectiveness, efficiency and generalizability.","['Jiaying Wu', 'Bryan Hooi']",1,0.7182648
"Recent advances in artificial speech and audio technologies have improved the
abilities of deep-fake operators to falsify media and spread malicious
misinformation. Anyone with limited coding skills can use freely available
speech synthesis tools to create convincing simulations of influential
speakers' voices with the malicious intent to distort the original message.
With the latest technology, malicious operators do not have to generate an
entire audio clip; instead, they can insert a partial manipulation or a segment
of synthetic speech into a genuine audio recording to change the entire context
and meaning of the original message. Detecting these insertions is especially
challenging because partially manipulated audio can more easily avoid synthetic
speech detectors than entirely fake messages can. This paper describes a
potential partial synthetic speech detection system based on the x-ResNet
architecture with a probabilistic linear discriminant analysis (PLDA) backend
and interleaved aware score processing. Experimental results suggest that the
PLDA backend results in a 25% average error reduction among partially
synthesized datasets over a non-PLDA baseline.","['Md Hafizur Rahman', 'Martin Graciarena', 'Diego Castan', 'Chris Cobo-Kroenke', 'Mitchell McLaren', 'Aaron Lawson']",11,0.692102
"Multiple-objective optimization (MOO) aims to simultaneously optimize
multiple conflicting objectives and has found important applications in machine
learning, such as minimizing classification loss and discrepancy in treating
different populations for fairness. At optimality, further optimizing one
objective will necessarily harm at least another objective, and decision-makers
need to comprehensively explore multiple optima (called Pareto front) to
pinpoint one final solution. We address the efficiency of finding the Pareto
front. First, finding the front from scratch using stochastic multi-gradient
descent (SMGD) is expensive with large neural networks and datasets. We propose
to explore the Pareto front as a manifold from a few initial optima, based on a
predictor-corrector method. Second, for each exploration step, the predictor
solves a large-scale linear system that scales quadratically in the number of
model parameters and requires one backpropagation to evaluate a second-order
Hessian-vector product per iteration of the solver. We propose a Gauss-Newton
approximation that only scales linearly, and that requires only first-order
inner-product per iteration. This also allows for a choice between the MINRES
and conjugate gradient methods when approximately solving the linear system.
The innovations make predictor-corrector possible for large networks.
Experiments on multi-objective (fairness and accuracy) misinformation detection
tasks show that 1) the predictor-corrector method can find Pareto fronts better
than or similar to SMGD with less time; and 2) the proposed first-order method
does not harm the quality of the Pareto front identified by the second-order
method, while further reduce running time.","['Eric Enouen', 'Katja Mathesius', 'Sean Wang', 'Arielle Carr', 'Sihong Xie']",2,0.71829045
"Online news and information sources are convenient and accessible ways to
learn about current issues. For instance, more than 300 million people engage
with posts on Twitter globally, which provides the possibility to disseminate
misleading information. There are numerous cases where violent crimes have been
committed due to fake news. This research presents the CovidMis20 dataset
(COVID-19 Misinformation 2020 dataset), which consists of 1,375,592 tweets
collected from February to July 2020. CovidMis20 can be automatically updated
to fetch the latest news and is publicly available at:
https://github.com/everythingguy/CovidMis20. This research was conducted using
Bi-LSTM deep learning and an ensemble CNN+Bi-GRU for fake news detection. The
results showed that, with testing accuracy of 92.23% and 90.56%, respectively,
the ensemble CNN+Bi-GRU model consistently provided higher accuracy than the
Bi-LSTM model.","['Aos Mulahuwaish', 'Manish Osti', 'Kevin Gyorick', 'Majdi Maabreh', 'Ajay Gupta', 'Basheer Qolomany']",4,0.66284204
"Vaccination represents a major public health intervention intended to protect
against COVID-19 infections and hospitalizations. However, vaccine hesitancy
due to misinformation/disinformation, especially among ethnic minority groups,
negatively impacts the effectiveness of such an intervention. The aim of the
study is to provide an understanding of how information gleaned from social
media can be used to improve attitudes towards vaccination and decrease vaccine
hesitancy. This work focused on Spanish-language posts and will highlight the
relationship between vaccination rates across different Texas counties and the
sentiment and emotional content of Facebook data, the most popular platform
among the Hispanic population. The analysis of this valuable dataset indicates
that vaccination rates among this minority group are negatively correlated with
negative sentiment and fear, meaning that the higher prevalence of negative and
fearful posts reveals lower vaccination rates in these counties. This first
study investigating vaccine hesitancy in the Hispanic population suggests that
social media listening can be a valuable tool for measuring attitudes toward
public health interventions.","['Ana Aleksandric', 'Henry Isaac Anderson', 'Sarah Melcher', 'Shirin Nilizadeh', 'Gabriela Mustata Wilson']",12,0.8227257
"Social media platforms have become new battlegrounds for anti-social
elements, with misinformation being the weapon of choice. Fact-checking
organizations try to debunk as many claims as possible while staying true to
their journalistic processes but cannot cope with its rapid dissemination. We
believe that the solution lies in partial automation of the fact-checking life
cycle, saving human time for tasks which require high cognition. We propose a
new workflow for efficiently detecting previously fact-checked claims that uses
abstractive summarization to generate crisp queries. These queries can then be
executed on a general-purpose retrieval system associated with a collection of
previously fact-checked claims. We curate an abstractive text summarization
dataset comprising noisy claims from Twitter and their gold summaries. It is
shown that retrieval performance improves 2x by using popular out-of-the-box
summarization models and 3x by fine-tuning them on the accompanying dataset
compared to verbatim querying. Our approach achieves Recall@5 and MRR of 35%
and 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset,
code, and models are available publicly:
https://github.com/varadhbhatnagar/FC-Claim-Det/","['Varad Bhatnagar', 'Diptesh Kanojia', 'Kameswari Chebrolu']",1,0.71515954
"Following recent outbreaks, monkeypox-related misinformation continues to
rapidly spread online. This negatively impacts response strategies and
disproportionately harms LGBTQ+ communities in the short-term, and ultimately
undermines the overall effectiveness of public health responses. In an attempt
to combat monkeypox-related misinformation, we present PoxVerifi, an
open-source, extensible tool that provides a comprehensive approach to
assessing the accuracy of monkeypox related claims. Leveraging information from
existing fact checking sources and published World Health Organization (WHO)
information, we created an open-sourced corpus of 225 rated monkeypox claims.
Additionally, we trained an open-sourced BERT-based machine learning model for
specifically classifying monkeypox information, which achieved 96%
cross-validation accuracy. PoxVerifi is a Google Chrome browser extension
designed to empower users to navigate through monkeypox-related misinformation.
Specifically, PoxVerifi provides users with a comprehensive toolkit to assess
the veracity of headlines on any webpage across the Internet without having to
visit an external site. Users can view an automated accuracy review from our
trained machine learning model, a user-generated accuracy review based on
community-member votes, and have the ability to see similar, vetted, claims.
Besides PoxVerifi's comprehensive approach to claim-testing, our platform
provides an efficient and accessible method to crowdsource accuracy ratings on
monkeypox related-claims, which can be aggregated to create new labeled
misinformation datasets.","['Akaash Kolluri', 'Kami Vinton', 'Dhiraj Murthy']",1,0.6504267
"COVID-19 impacted every part of the world, although the misinformation about
the outbreak traveled faster than the virus. Misinformation spread through
online social networks (OSN) often misled people from following correct medical
practices. In particular, OSN bots have been a primary source of disseminating
false information and initiating cyber propaganda. Existing work neglects the
presence of bots that act as a catalyst in the spread and focuses on fake news
detection in 'articles shared in posts' rather than the post (textual) content.
Most work on misinformation detection uses manually labeled datasets that are
hard to scale for building their predictive models. In this research, we
overcome this challenge of data scarcity by proposing an automated approach for
labeling data using verified fact-checked statements on a Twitter dataset. In
addition, we combine textual features with user-level features (such as
followers count and friends count) and tweet-level features (such as number of
mentions, hashtags and urls in a tweet) to act as additional indicators to
detect misinformation. Moreover, we analyzed the presence of bots in tweets and
show that bots change their behavior over time and are most active during the
misinformation campaign. We collected 10.22 Million COVID-19 related tweets and
used our annotation model to build an extensive and original ground truth
dataset for classification purposes. We utilize various machine learning models
to accurately detect misinformation and our best classification model achieves
precision (82%), recall (96%), and false positive rate (3.58%). Also, our bot
analysis indicates that bots generated approximately 10% of misinformation
tweets. Our methodology results in substantial exposure of false information,
thus improving the trustworthiness of information disseminated through social
media platforms.","['Mohammad Majid Akhtar', 'Bibhas Sharma', 'Ishan Karunanayake', 'Rahat Masood', 'Muhammad Ikram', 'Salil S. Kanhere']",13,0.86374307
"Mis- and disinformation are a substantial global threat to our security and
safety. To cope with the scale of online misinformation, researchers have been
working on automating fact-checking by retrieving and verifying against
relevant evidence. However, despite many advances, a comprehensive evaluation
of the possible attack vectors against such systems is still lacking.
Particularly, the automated fact-verification process might be vulnerable to
the exact disinformation campaigns it is trying to combat. In this work, we
assume an adversary that automatically tampers with the online evidence in
order to disrupt the fact-checking model via camouflaging the relevant evidence
or planting a misleading one. We first propose an exploratory taxonomy that
spans these two targets and the different threat model dimensions. Guided by
this, we design and propose several potential attack methods. We show that it
is possible to subtly modify claim-salient snippets in the evidence and
generate diverse and claim-aligned evidence. Thus, we highly degrade the
fact-checking performance under many different permutations of the taxonomy's
dimensions. The attacks are also robust against post-hoc modifications of the
claim. Our analysis further hints at potential limitations in models' inference
when faced with contradicting evidence. We emphasize that these attacks can
have harmful implications on the inspectable and human-in-the-loop usage
scenarios of such models, and we conclude by discussing challenges and
directions for future defenses.","['Sahar Abdelnabi', 'Mario Fritz']",1,0.7178159
"Today, participating in discussions on online forums is extremely commonplace
and these discussions have started rendering a strong influence on the overall
opinion of online users. Naturally, twisting the flow of the argument can have
a strong impact on the minds of naive users, which in the long run might have
socio-political ramifications, for example, winning an election or spreading
targeted misinformation. Thus, these platforms are potentially highly
vulnerable to malicious players who might act individually or as a cohort to
breed fallacious arguments with a motive to sway public opinion. Ad hominem
arguments are one of the most effective forms of such fallacies. Although a
simple fallacy, it is effective enough to sway public debates in offline world
and can be used as a precursor to shutting down the voice of opposition by
slander.
  In this work, we take a first step in shedding light on the usage of ad
hominem fallacies in the wild. First, we build a powerful ad hominem detector
with high accuracy (F1 more than 83%, showing a significant improvement over
prior work), even for datasets for which annotated instances constitute a very
small fraction. We then used our detector on 265k arguments collected from the
online debate forum - CreateDebate. Our crowdsourced surveys validate our
in-the-wild predictions on CreateDebate data (94% match with manual
annotation). Our analysis revealed that a surprising 31.23% of CreateDebate
content contains ad hominem fallacy, and a cohort of highly active users post
significantly more ad hominem to suppress opposing views. Then, our temporal
analysis revealed that ad hominem argument usage increased significantly since
the 2016 US Presidential election, not only for topics like Politics, but also
for Science and Law. We conclude by discussing important implications of our
work to detect and defend against ad hominem fallacies.","['Utkarsh Patel', 'Animesh Mukherjee', 'Mainack Mondal']",10,0.5766531
"Images are powerful. Visual information can attract attention, improve
persuasion, trigger stronger emotions, and is easy to share and spread. We
examine the characteristics of the popular images shared on Twitter as part of
""Stop the Steal"", the widespread misinformation campaign during the 2020 U.S.
election. We analyze the spread of the forty most popular images shared on
Twitter as part of this campaign. Using a coding process, we categorize and
label the images according to their type, content, origin, and role, and
perform a mixed-method analysis of these images' spread on Twitter. Our results
show that popular images include both photographs and text rendered as image.
Only very few of these popular images included alleged photographic evidence of
fraud; and none of the popular photographs had been manipulated. Most images
reached a significant portion of their total spread within several hours from
their first appearance, and both popular- and less-popular accounts were
involved in various stages of their spread.","['Hana Matatov', 'Mor Naaman', 'Ofra Amir']",10,0.61378956
"Vaccinations play a critical role in mitigating the impact of COVID-19 and
other diseases. This study explores COVID-19 vaccine misinformation circulating
on Twitter during 2021, when vaccines were being released to the public in an
effort to mitigate the global pandemic. Our findings show a low prevalence of
low-credibility information compared to mainstream news. However, most popular
low-credibility sources had larger reshare volumes than authoritative sources
such as the CDC and WHO. We observed an increasing trend in the prevalence of
low-credibility news relative to mainstream news about vaccines. We also
observed a considerable amount of suspicious YouTube videos shared on Twitter.
Tweets by a small group of about 800 ""superspreaders"" verified by Twitter
accounted for approximately 35% of all reshares of misinformation on the
average day, with the top superspreader (@RobertKennedyJr) responsible for over
13% of retweets. Low-credibility news and suspicious YouTube videos were more
likely to be shared by automated accounts. Our findings are consistent with the
hypothesis that superspreaders are driven by financial incentives that allow
them to profit from health misinformation. Despite high-profile cases of
deplatformed misinformation superspreaders, our results show that in 2021 a few
individuals still played an outsize role in the spread of low-credibility
vaccine content. Social media policies should consider revoking the verified
status of repeat-spreaders of harmful content, especially during public health
crises.","['Francesco Pierri', 'Matthew R. DeVerna', 'Kai-Cheng Yang', 'David Axelrod', 'John Bryden', 'Filippo Menczer']",12,0.8069687
"Though graph representation learning (GRL) has made significant progress, it
is still a challenge to extract and embed the rich topological structure and
feature information in an adequate way. Most existing methods focus on local
structure and fail to fully incorporate the global topological structure. To
this end, we propose a novel Structure-Preserving Graph Representation Learning
(SPGRL) method, to fully capture the structure information of graphs.
Specifically, to reduce the uncertainty and misinformation of the original
graph, we construct a feature graph as a complementary view via k-Nearest
Neighbor method. The feature graph can be used to contrast at node-level to
capture the local relation. Besides, we retain the global topological structure
information by maximizing the mutual information (MI) of the whole graph and
feature embeddings, which is theoretically reduced to exchanging the feature
embeddings of the feature and the original graphs to reconstruct themselves.
Extensive experiments show that our method has quite superior performance on
semi-supervised node classification task and excellent robustness under noise
perturbation on graph structure or node features.","['Ruiyi Fang', 'Liangjian Wen', 'Zhao Kang', 'Jianzhuang Liu']",2,0.628888
"""Emergence"", the phenomenon where a complex system displays properties,
behaviours, or dynamics not trivially reducible to its constituent elements, is
one of the defining properties of complex systems. Recently, there has been a
concerted effort to formally define emergence using the mathematical framework
of information theory, which proposes that emergence can be understood in terms
of how the states of wholes and parts collectively disclose information about
the system's collective future. In this paper, we show how a common,
foundational component of information-theoretic approaches to emergence implies
an inherent instability to emergent properties, which we call flickering
emergence. A system may on average display a meaningful emergent property (be
it an informative coarse-graining, or higher-order synergy), but for particular
configurations, that emergent property falls apart and becomes misinformative.
We show existence proofs that flickering emergence occurs in two different
frameworks (one based on coarse-graining and another based on multivariate
information decomposition) and argue that any approach based on temporal mutual
information will display it. Finally, we argue that flickering emergence should
not be a disqualifying property of any model of emergence, but that it should
be accounted for when attempting to theorize about how emergence relates to
practical models of the natural world.",['Thomas F. Varley'],2,0.65997624
"While most social media companies have attempted to address the challenge of
COVID-19 misinformation, the success of those policies is difficult to assess,
especially when focusing on individual platforms. This study explores the
relationship between Twitter and YouTube in spreading COVID-19 vaccine-related
misinformation through a mixed-methods approach to analyzing a collection of
tweets in 2021 sharing YouTube videos where those Twitter accounts had also
linked to deleted YouTube videos. Principal components, cluster and network
analyses are used to group the videos and tweets into interpretable groups by
shared tweet dates, terms and sharing patterns; content analysis is employed to
assess the orientation of tweets and videos to COVID-19 messages. From this we
observe that a preponderance of anti-vaccine messaging remains among users who
previously shared suspect information, in which a dissident political framing
dominates, and which suggests moderation policy inefficacy where the platforms
interact.","['David S. Axelrod', 'Brian P. Harper', 'John C. Paolillo']",3,0.73191905
"The wide spread of false information online including misinformation and
disinformation has become a major problem for our highly digitised and
globalised society. A lot of research has been done to better understand
different aspects of false information online such as behaviours of different
actors and patterns of spreading, and also on better detection and prevention
of such information using technical and socio-technical means. One major
approach to detect and debunk false information online is to use human
fact-checkers, who can be helped by automated tools. Despite a lot of research
done, we noticed a significant gap on the lack of conceptual models describing
the complicated ecosystems of false information and fact checking. In this
paper, we report the first graphical models of such ecosystems, focusing on
false information online in multiple contexts, including traditional media
outlets and user-generated content. The proposed models cover a wide range of
entity types and relationships, and can be a new useful tool for researchers
and practitioners to study false information online and the effects of fact
checking.","['Haiyue Yuan', 'Enes Altuncu', 'Shujun Li', 'Can Baskent']",0,0.8000579
"The Russian Invasion of Ukraine in early 2022 resulted in a rapidly changing
(cyber) threat environment. This changing environment incentivized the sharing
of security advice on social media, both for the Ukrainian population, as well
as against Russian cyber attacks at large. Previous research found a
significant influence of online security advice on end users.
  We collected 8,920 tweets posted after the Russian Invasion of Ukraine and
examined 1,228 in detail, including qualitatively coding 232 relevant tweets
and 140 linked documents for security and privacy advice. We identified 221
unique pieces of advice which we divided into seven categories and 21
subcategories, and advice targeted at individuals or organizations. We then
compared our findings to those of prior studies, finding noteworthy
similarities. Our results confirm a lack of advice prioritization found by
prior work, which seems especially detrimental during times of crisis. In
addition, we find offers for individual support to be a valuable tool and
identify misinformation as a rising threat in general and for security advice
specifically.","['Juliane Schm√ºser', 'Noah W√∂hler', 'Harshini Sri Ramulu', 'Christian Stransky', 'Dominik Wermke', 'Sascha Fahl', 'Yasemin Acar']",5,0.54560924
"Some political strategies to win elections over the last years were based
heavily on fomenting general distrust in information institutions and favoring
distrustful sources. The misinformation pandemic has the straightforward
consequence that people do not believe any information unless it is compatible
with their own beliefs. We present a simple model to study the emergence of
consensus in opinion pools of uncertain agents that trust (couple to) their
neighbors (information sources) with strength K. We focus the studies on
regular lattices and linear coupling. Depending on the coupling constant, K,
and the propensity to choose an opinion (the probability to manifest a given
opinion in solitude), p_0, we get regions where consensus is surely reached
even in infinity systems (K greater than or equal to K_c), regions where
not-consensus is the only steady state (K lesser than K_c), and a region where
consensus on any opinion is transitory with each agent presenting periods of
strong oscillation of opinions before changing polarization (p_0 equal to p_c
and K greater than or equal K_c. The first model in this last region presents
transition probabilities identical to the voter model (p_0 equal to p_c and K
equal to K_c). Different upbringings, exposition to education biased to a
single political view, and previous coexistence with opinion polarized people
can change the opinion of agents in isolation. We model such characteristics
with heterogeneous populations (p^i_0 not equal to p^j_0 for some pairs of
nodes i,j). Such systems present regions where the coexistence of local
consensus (bulk-stable clusters of like-minded opinions), weak consensus
(bulk-unstable temporary clusters where contrary opinions emerge inside the
cluster), and distrust (random orientations that do not form clusters) are
possible.","['Ricardo Sim√£o', 'Lucas Wardil']",2,0.5732063
"The spread of online misinformation on social media is increasingly perceived
as a problem for societal cohesion and democracy. The role of political leaders
in this process has attracted less research attention, even though politicians
who ""speak their mind"" are perceived by segments of the public as authentic and
honest even if their statements are unsupported by evidence. Analyzing
communications by members of the U.S. Congress on Twitter between 2011 and
2022, we show that politicians' conception of honesty has undergone a distinct
shift, with authentic belief-speaking that may be decoupled from evidence
becoming more prominent and more differentiated from explicitly evidence-based
truth seeking. We show that for Republicans - but not Democrats - an increase
of belief-speaking of 10% is associated with a decrease of 12.8 points of
quality (NewsGuard scoring system) in the sources shared in a tweet.
Conversely, an increase in truth-seeking language is associated with an
increase in quality of sources for both parties. The results support the
hypothesis that the current dissemination of misinformation in political
discourse is in part driven by an alternative understanding of truth and
honesty that emphasizes invocation of subjective belief at the expense of
reliance on evidence.","['Jana Lasser', 'Segun Taofeek Aroyehun', 'Fabio Carrella', 'Almog Simchon', 'David Garcia', 'Stephan Lewandowsky']",3,0.68053675
"Hate speech and misinformation, spread over social networking services (SNS)
such as Facebook and Twitter, have inflamed ethnic and political violence in
countries across the globe. We argue that there is limited research on this
problem within the context of the Global South and present an approach for
tackling them. Prior works have shown how machine learning models built with
user-level interaction features can effectively identify users who spread
inflammatory content. While this technique is beneficial in low-resource
language settings where linguistic resources such as ground truth data and
processing capabilities are lacking, it is still unclear how these interaction
features contribute to model performance. In this work, we investigate and show
significant differences in interaction features between users who spread
inflammatory content and others who do not, applying explainability tools to
understand our trained model. We find that features with higher interaction
significance (such as account age and activity count) show higher explanatory
power than features with lower interaction significance (such as name length
and if the user has a location on their bio). Our work extends research
directions that aim to understand the nature of inflammatory content in
low-resource, high-risk contexts as the growth of social media use in the
Global South outstrips moderation efforts.","['Cuong Nguyen', 'Daniel Nkemelu', 'Ankit Mehta', 'Michael Best']",0,0.66309917
"Despite recent progress in improving the performance of misinformation
detection systems, classifying misinformation in an unseen domain remains an
elusive challenge. To address this issue, a common approach is to introduce a
domain critic and encourage domain-invariant input features. However, early
misinformation often demonstrates both conditional and label shifts against
existing misinformation data (e.g., class imbalance in COVID-19 datasets),
rendering such methods less effective for detecting early misinformation. In
this paper, we propose contrastive adaptation network for early misinformation
detection (CANMD). Specifically, we leverage pseudo labeling to generate
high-confidence target examples for joint training with source data. We
additionally design a label correction component to estimate and correct the
label shifts (i.e., class priors) between the source and target domains.
Moreover, a contrastive adaptation loss is integrated in the objective function
to reduce the intra-class discrepancy and enlarge the inter-class discrepancy.
As such, the adapted model learns corrected class priors and an invariant
conditional distribution across both domains for improved estimation of the
target data distribution. To demonstrate the effectiveness of the proposed
CANMD, we study the case of COVID-19 early misinformation detection and perform
extensive experiments using multiple real-world datasets. The results suggest
that CANMD can effectively adapt misinformation detection systems to the unseen
COVID-19 target domain with significant improvements compared to the
state-of-the-art baselines.","['Zhenrui Yue', 'Huimin Zeng', 'Ziyi Kou', 'Lanyu Shang', 'Dong Wang']",2,0.72174835
"Fact-checking is one of the effective solutions in fighting online
misinformation. However, traditional fact-checking is a process requiring
scarce expert human resources, and thus does not scale well on social media
because of the continuous flow of new content to be checked. Methods based on
crowdsourcing have been proposed to tackle this challenge, as they can scale
with a smaller cost, but, while they have shown to be feasible, have always
been studied in controlled environments. In this work, we study the first
large-scale effort of crowdsourced fact-checking deployed in practice, started
by Twitter with the Birdwatch program. Our analysis shows that crowdsourcing
may be an effective fact-checking strategy in some settings, even comparable to
results obtained by human experts, but does not lead to consistent, actionable
results in others. We processed 11.9k tweets verified by the Birdwatch program
and report empirical evidence of i) differences in how the crowd and experts
select content to be fact-checked, ii) how the crowd and the experts retrieve
different resources to fact-check, and iii) the edge the crowd shows in
fact-checking scalability and efficiency as compared to expert checkers.","['Mohammed Saeed', 'Nicolas Traub', 'Maelle Nicolas', 'Gianluca Demartini', 'Paolo Papotti']",1,0.6010778
"Twitter bots are automatic programs operated by malicious actors to
manipulate public opinion and spread misinformation. Research efforts have been
made to automatically identify bots based on texts and networks on social
media. Existing methods only leverage texts or networks alone, and while few
works explored the shallow combination of the two modalities, we hypothesize
that the interaction and information exchange between texts and graphs could be
crucial for holistically evaluating bot activities on social media. In
addition, according to a recent survey (Cresci, 2020), Twitter bots are
constantly evolving while advanced bots steal genuine users' tweets and dilute
their malicious content to evade detection. This results in greater
inconsistency across the timeline of novel Twitter bots, which warrants more
attention. In light of these challenges, we propose BIC, a Twitter Bot
detection framework with text-graph Interaction and semantic Consistency.
Specifically, in addition to separately modeling the two modalities on social
media, BIC employs a text-graph interaction module to enable information
exchange across modalities in the learning process. In addition, given the
stealing behavior of novel Twitter bots, BIC proposes to model semantic
consistency in tweets based on attention weights while using it to augment the
decision process. Extensive experiments demonstrate that BIC consistently
outperforms state-of-the-art baselines on two widely adopted datasets. Further
analyses reveal that text-graph interactions and modeling semantic consistency
are essential improvements and help combat bot evolution.","['Zhenyu Lei', 'Herun Wan', 'Wenqian Zhang', 'Shangbin Feng', 'Zilong Chen', 'Jundong Li', 'Qinghua Zheng', 'Minnan Luo']",13,0.8474762
"Online social networks facilitate the diffusion of misinformation. Some
theorists construe the problem of misinformation as a problem of knowledge,
hence of ignorance. This assumption leads to solutions in which misinformation
(false belief) is resisted by good information (true belief). We argue that
information is better understood as gossip. We believe that gossip spreads as
part of an economy of social capital that has a specific discursive grammar
that mimics ordinary human gossip. But there are some critical differences.
These differences have immense and divisive social and political effects. If we
shift our focus from the truth or falsity of information, and instead focus on
the social dynamics of gossip we can more effectively respond to the challenges
and dangers of online social networks.
  Our argument has three parts. (1) We briefly critique epistemological and
truth-centered accounts of misinformation. (2) We describe a basic discursive
grammar of gossip as a social practice. (3) We, then, match the properties of
online information with this discursive grammar of gossip. While gossip has a
particular discursive form, its online modes involve a number of unique social
features that will have immense and divisive social and political effects. Our
goal is not to replace current accounts of information diffusion but to augment
these accounts with a descriptive model of gossip. Information diffusion models
should be understood as tools with which to explore the sociology of evolving
online communities in conjunction with offline communities.","['Brett Bourbon', 'Renita Murimi']",3,0.67683995
"In recent years there has been substantial growth in the capabilities of
systems designed to generate text that mimics the fluency and coherence of
human language. From this, there has been considerable research aimed at
examining the potential uses of these natural language generators (NLG) towards
a wide number of tasks. The increasing capabilities of powerful text generators
to mimic human writing convincingly raises the potential for deception and
other forms of dangerous misuse. As these systems improve, and it becomes ever
harder to distinguish between human-written and machine-generated text,
malicious actors could leverage these powerful NLG systems to a wide variety of
ends, including the creation of fake news and misinformation, the generation of
fake online product reviews, or via chatbots as means of convincing users to
divulge private information. In this paper, we provide an overview of the NLG
field via the identification and examination of 119 survey-like papers focused
on NLG research. From these identified papers, we outline a proposed high-level
taxonomy of the central concepts that constitute NLG, including the methods
used to develop generalised NLG systems, the means by which these systems are
evaluated, and the popular NLG tasks and subtasks that exist. In turn, we
provide an overview and discussion of each of these items with respect to
current research and offer an examination of the potential roles of NLG in
deception and detection systems to counteract these threats. Moreover, we
discuss the broader challenges of NLG, including the risks of bias that are
often exhibited by existing text generation systems. This work offers a broad
overview of the field of NLG with respect to its potential for misuse, aiming
to provide a high-level understanding of this rapidly developing area of
research.","['Keenan Jones', 'Enes Altuncu', 'Virginia N. L. Franqueira', 'Yichao Wang', 'Shujun Li']",9,0.61987203
"Social movements are dominated by storytelling, as narratives play a key role
in how communities involved in these movements shape their identities. Thus,
recognizing the accepted narratives of different communities is central to
understanding social movements. In this context, journalists face the challenge
of making sense of these emerging narratives in social media when they seek to
report social protests. Thus, they would benefit from support tools that allow
them to identify and explore such narratives. In this work, we propose a
narrative extraction algorithm from social media that incorporates the concept
of community acceptance. Using our method, we study the 2021 Cuban protests and
characterize five relevant communities. The extracted narratives differ in both
structure and content across communities. Our work has implications in the
study of social movements, intelligence analysis, computational journalism, and
misinformation research.","['Brian Felipe Keith Norambuena', 'Tanushree Mitra', 'Chris North']",3,0.64288926
"With the onset of the COVID-19 pandemic, news outlets and social media have
become central tools for disseminating and consuming information. Because of
their ease of access, users seek COVID-19-related information from online
social media (i.e., online news) and news outlets (i.e., offline news). Online
and offline news are often connected, sharing common topics while each has
unique, different topics. A gap between these two news sources can lead to
misinformation propagation. For instance, according to the Guardian, most
COVID-19 misinformation comes from users on social media. Without fact-checking
social media news, misinformation can lead to health threats. In this paper, we
focus on the novel problem of bridging the gap between online and offline data
by monitoring their common and distinct topics generated over time. We employ
Twitter (online) and local news (offline) data for a time span of two years.
Using online matrix factorization, we analyze and study online and offline
COVID-19-related data differences and commonalities. We design experiments to
show how online and offline data are linked together and what trends they
follow.","['Nayoung Kim', 'Ahmadreza Mosallanezhad', 'Lu Cheng', 'Baoxin Li', 'Huan Li']",5,0.70738304
"In the context of COVID-19 pandemic, social networks such as Twitter and
YouTube stand out as important sources of information. YouTube, as the largest
and most engaging online media consumption platform, has a large influence in
the spread of information and misinformation, which makes it important to study
how it deals with the problems that arise from disinformation, as well as how
its users interact with different types of content. Considering that United
States (USA) and Brazil (BR) are two countries with the highest COVID-19 death
tolls, we asked the following question: What are the nuances of vaccination
campaigns in the two countries? With that in mind, we engage in a comparative
analysis of pro and anti-vaccine movements on YouTube. We also investigate the
role of YouTube in countering online vaccine misinformation in USA and BR. For
this means, we monitored the removal of vaccine related content on the platform
and also applied various techniques to analyze the differences in discourse and
engagement in pro and anti-vaccine ""comment sections"". We found that American
anti-vaccine content tend to lead to considerably more toxic and negative
discussion than their pro-vaccine counterparts while also leading to 18% higher
user-user engagement, while Brazilian anti-vaccine content was significantly
less engaging. We also found that pro-vaccine and anti-vaccine discourses are
considerably different as the former is associated with conspiracy theories
(e.g. ccp), misinformation and alternative medicine (e.g. hydroxychloroquine),
while the latter is associated with protective measures. Finally, it was
observed that YouTube content removals are still insufficient, with only
approximately 16% of the anti-vaccine content being removed by the end of the
studied period, with the USA registering the highest percentage of removed
anti-vaccine content(34%) and BR registering the lowest(9.8%).","['Marcelo Sartori Locatelli', 'Josemar Caetano', 'Wagner Meira Jr.', 'Virgilio Almeida']",12,0.76368237
"Freely available and easy-to-use audio editing tools make it straightforward
to perform audio splicing. Convincing forgeries can be created by combining
various speech samples from the same person. Detection of such splices is
important both in the public sector when considering misinformation, and in a
legal context to verify the integrity of evidence. Unfortunately, most existing
detection algorithms for audio splicing use handcrafted features and make
specific assumptions. However, criminal investigators are often faced with
audio samples from unconstrained sources with unknown characteristics, which
raises the need for more generally applicable methods.
  With this work, we aim to take a first step towards unconstrained audio
splicing detection to address this need. We simulate various attack scenarios
in the form of post-processing operations that may disguise splicing. We
propose a Transformer sequence-to-sequence (seq2seq) network for splicing
detection and localization. Our extensive evaluation shows that the proposed
method outperforms existing dedicated approaches for splicing detection [3, 10]
as well as the general-purpose networks EfficientNet [28] and RegNet [25].","['Denise Moussa', 'Germans Hirsch', 'Christian Riess']",11,0.6567651
"In the recent years, social media has grown to become a major source of
information for many online users. This has given rise to the spread of
misinformation through deepfakes. Deepfakes are videos or images that replace
one persons face with another computer-generated face, often a more
recognizable person in society. With the recent advances in technology, a
person with little technological experience can generate these videos. This
enables them to mimic a power figure in society, such as a president or
celebrity, creating the potential danger of spreading misinformation and other
nefarious uses of deepfakes. To combat this online threat, researchers have
developed models that are designed to detect deepfakes. This study looks at
various deepfake detection models that use deep learning algorithms to combat
this looming threat. This survey focuses on providing a comprehensive overview
of the current state of deepfake detection models and the unique approaches
many researchers take to solving this problem. The benefits, limitations, and
suggestions for future work will be thoroughly discussed throughout this paper.","['Jacob Mallet', 'Rushit Dave', 'Naeem Seliya', 'Mounika Vanamala']",11,0.8403803
"As tools for content editing mature, and artificial intelligence (AI) based
algorithms for synthesizing media grow, the presence of manipulated content
across online media is increasing. This phenomenon causes the spread of
misinformation, creating a greater need to distinguish between ``real'' and
``manipulated'' content. To this end, we present VideoSham, a dataset
consisting of 826 videos (413 real and 413 manipulated). Many of the existing
deepfake datasets focus exclusively on two types of facial manipulations --
swapping with a different subject's face or altering the existing face.
VideoSham, on the other hand, contains more diverse, context-rich, and
human-centric, high-resolution videos manipulated using a combination of 6
different spatial and temporal attacks. Our analysis shows that
state-of-the-art manipulation detection algorithms only work for a few specific
attacks and do not scale well on VideoSham. We performed a user study on Amazon
Mechanical Turk with 1200 participants to understand if they can differentiate
between the real and manipulated videos in VideoSham. Finally, we dig deeper
into the strengths and weaknesses of performances by humans and SOTA-algorithms
to identify gaps that need to be filled with better AI algorithms. We present
the dataset at https://github.com/adobe-research/VideoSham-dataset.","['Trisha Mittal', 'Ritwik Sinha', 'Viswanathan Swaminathan', 'John Collomosse', 'Dinesh Manocha']",11,0.7025486
"In this paper we investigate what folk models of misinformation exist through
semi-structured interviews with a sample of 235 social media users. Work on
social media misinformation does not investigate how ordinary users - the
target of misinformation - deal with it; rather, the focus is mostly on the
anxiety, tensions, or divisions misinformation creates. Studying the aspects of
creation, diffusion and amplification also overlooks how misinformation is
internalized by users on social media and thus is quick to prescribe
""inoculation"" strategies for the presumed lack of immunity to misinformation.
How users grapple with social media content to develop ""natural immunity"" as a
precursor to misinformation resilience remains an open question. We have
identified at least five folk models that conceptualize misinformation as
either: political (counter)argumentation, out-of-context narratives, inherently
fallacious information, external propaganda, or simply entertainment. We use
the rich conceptualizations embodied in these folk models to uncover how social
media users minimize adverse reactions to misinformation encounters in their
everyday lives.","['Filipo Sharevski', 'Amy Devine', 'Emma Pieroni', 'Peter Jachim']",3,0.76471305
"The Covid-19 pandemic has sparked renewed attention on the prevalence of
misinformation online, whether intentional or not, underscoring the potential
risks posed to individuals' quality of life associated with the dissemination
of misconceptions and enduring myths on health-related subjects. In this study,
we analyze 6 years (2016-2021) of Italian vaccine debate across diverse social
media platforms (Facebook, Instagram, Twitter, YouTube), encompassing all major
news sources - both questionable and reliable. We first use the symbolic
transfer entropy analysis of news production time-series to dynamically
determine which category of sources, questionable or reliable, causally drives
the agenda on vaccines. Then, leveraging deep learning models capable to
accurately classify vaccine-related content based on the conveyed stance and
discussed topic, respectively, we evaluate the focus on various topics by news
sources promoting opposing views and compare the resulting user engagement.
Aside from providing valuable resources for further investigation of
vaccine-related misinformation, particularly in a language (Italian) that
receives less attention in scientific research compared to languages like
English, our study uncovers misinformation not as a parasite of the news
ecosystem that merely opposes the perspectives offered by mainstream media, but
as an autonomous force capable of even overwhelming the production of
vaccine-related content from the latter. While the pervasiveness of
misinformation is evident in the significantly higher engagement of
questionable sources compared to reliable ones, our findings underscore the
importance of consistent and thorough pro-vax coverage. This is especially
crucial in addressing the most sensitive topics where the risk of
misinformation spreading and potentially exacerbating negative attitudes toward
vaccines among the users involved is higher.","['Emanuele Brugnoli', 'Marco Delmastro']",12,0.8305241
"Online social network platforms have a problem with misinformation. One
popular way of addressing this problem is via the use of machine learning based
automated misinformation detection systems to classify if a post is
misinformation. Instead of post hoc detection, we propose to predict if a user
will engage with misinformation in advance and design an effective graph neural
network classifier based on ego-graphs for this task. However, social networks
are highly dynamic, reflecting continual changes in user behaviour, as well as
the content being posted. This is problematic for machine learning models which
are typically trained on a static training dataset, and can thus become
outdated when the social network changes. Inspired by the success of continual
learning on such problems, we propose an ego-graphs replay strategy in
continual learning (EgoCL) using graph neural networks to effectively address
this issue. We have evaluated the performance of our method on user engagement
with misinformation on two Twitter datasets across nineteen misinformation and
conspiracy topics. Our experimental results show that our approach EgoCL has
better performance in terms of predictive accuracy and computational resources
than the state of the art.","['Hongbo Bo', 'Ryan McConville', 'Jun Hong', 'Weiru Liu']",2,0.6739161
"Advancements in generative models, like Deepfake allows users to imitate a
targeted person and manipulate online interactions. It has been recognized that
disinformation may cause disturbance in society and ruin the foundation of
trust. This article presents DeFakePro, a decentralized consensus
mechanism-based Deepfake detection technique in online video conferencing
tools. Leveraging Electrical Network Frequency (ENF), an environmental
fingerprint embedded in digital media recording, affords a consensus mechanism
design called Proof-of-ENF (PoENF) algorithm. The similarity in ENF signal
fluctuations is utilized in the PoENF algorithm to authenticate the media
broadcasted in conferencing tools. By utilizing the video conferencing setup
with malicious participants to broadcast deep fake video recordings to other
participants, the DeFakePro system verifies the authenticity of the incoming
media in both audio and video channels.","['Deeraj Nagothu', 'Ronghua Xu', 'Yu Chen', 'Erik Blasch', 'Alexander Aved']",11,0.701
"Facial forgery by deepfakes has raised severe societal concerns. Several
solutions have been proposed by the vision community to effectively combat the
misinformation on the internet via automated deepfake detection systems. Recent
studies have demonstrated that facial analysis-based deep learning models can
discriminate based on protected attributes. For the commercial adoption and
massive roll-out of the deepfake detection technology, it is vital to evaluate
and understand the fairness (the absence of any prejudice or favoritism) of
deepfake detectors across demographic variations such as gender and race. As
the performance differential of deepfake detectors between demographic
subgroups would impact millions of people of the deprived sub-group. This paper
aims to evaluate the fairness of the deepfake detectors across males and
females. However, existing deepfake datasets are not annotated with demographic
labels to facilitate fairness analysis. To this aim, we manually annotated
existing popular deepfake datasets with gender labels and evaluated the
performance differential of current deepfake detectors across gender. Our
analysis on the gender-labeled version of the datasets suggests (a) current
deepfake datasets have skewed distribution across gender, and (b) commonly
adopted deepfake detectors obtain unequal performance across gender with mostly
males outperforming females. Finally, we contributed a gender-balanced and
annotated deepfake dataset, GBDF, to mitigate the performance differential and
to promote research and development towards fairness-aware deep fake detectors.
The GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF","['Aakash Varma Nadimpalli', 'Ajita Rattani']",11,0.71236056
"The world's digital information ecosystem continues to struggle with the
spread of misinformation. Prior work has suggested that users who consistently
disseminate a disproportionate amount of low-credibility content -- so-called
superspreaders -- are at the center of this problem. We quantitatively confirm
this hypothesis and introduce simple metrics to predict the top superspreaders
several months into the future. We then conduct a qualitative review to
characterize the most prolific superspreaders and analyze their sharing
behaviors. Superspreaders include pundits with large followings,
low-credibility media outlets, personal accounts affiliated with those media
outlets, and a range of influencers. They are primarily political in nature and
use more toxic language than the typical user sharing misinformation. We also
find concerning evidence that suggests Twitter may be overlooking prominent
superspreaders. We hope this work will further public understanding of bad
actors and promote steps to mitigate their negative impacts on healthy digital
discourse.","['Matthew R. DeVerna', 'Rachith Aiyappa', 'Diogo Pacheco', 'John Bryden', 'Filippo Menczer']",10,0.7268069
"Realistic fake videos are a potential tool for spreading harmful
misinformation given our increasing online presence and information intake.
This paper presents a multimodal learning-based method for detection of real
and fake videos. The method combines information from three modalities - audio,
video, and physiology. We investigate two strategies for combining the video
and physiology modalities, either by augmenting the video with information from
the physiology or by novelly learning the fusion of those two modalities with a
proposed Graph Convolutional Network architecture. Both strategies for
combining the two modalities rely on a novel method for generation of visual
representations of physiological signals. The detection of real and fake videos
is then based on the dissimilarity between the audio and modified video
modalities. The proposed method is evaluated on two benchmark datasets and the
results show significant increase in detection performance compared to previous
methods.","['Kalin Stefanov', 'Bhawna Paliwal', 'Abhinav Dhall']",11,0.6728226
"Unlike traditional media, social media typically provides quantified metrics
of how many users have engaged with each piece of content. Some have argued
that the presence of these cues promotes the spread of misinformation. Here we
investigate the causal effect of social cues on users' engagement with social
media posts. We conducted an experiment with N=628 Americans on a custom-built
newsfeed interface where we systematically varied the presence and strength of
social cues. We find that when cues are shown, indicating that a larger number
of others have engaged with a post, users were more likely to share and like
that post. Furthermore, relative to a control without social cues, the presence
of social cues increased the sharing of true relative to false news. The
presence of social cues also makes it more difficult to precisely predict how
popular any given post would be. Together, our results suggest that -- instead
of distracting users or causing them to share low-quality news -- social cues
may, in certain circumstances, actually boost truth discernment and reduce the
sharing of misinformation. Our work suggests that social cues play important
roles in shaping users' attention and engagement on social media, and platforms
should understand the effects of different cues before making changes to what
cues are displayed and how.","['Ziv Epstein', 'Hause Lin', 'Gordon Pennycook', 'David Rand']",3,0.70735025
"RGB-D SOD uses depth information to handle challenging scenes and obtain
high-quality saliency maps. Existing state-of-the-art RGB-D saliency detection
methods overwhelmingly rely on the strategy of directly fusing depth
information. Although these methods improve the accuracy of saliency prediction
through various cross-modality fusion strategies, misinformation provided by
some poor-quality depth images can affect the saliency prediction result. To
address this issue, a novel RGB-D salient object detection model (SiaTrans) is
proposed in this paper, which allows training on depth image quality
classification at the same time as training on SOD. In light of the common
information between RGB and depth images on salient objects, SiaTrans uses a
Siamese transformer network with shared weight parameters as the encoder and
extracts RGB and depth features concatenated on the batch dimension, saving
space resources without compromising performance. SiaTrans uses the Class token
in the backbone network (T2T-ViT) to classify the quality of depth images
without preventing the token sequence from going on with the saliency detection
task. Transformer-based cross-modality fusion module (CMF) can effectively fuse
RGB and depth information. And in the testing process, CMF can choose to fuse
cross-modality information or enhance RGB information according to the quality
classification signal of the depth image. The greatest benefit of our designed
CMF and decoder is that they maintain the consistency of RGB and RGB-D
information decoding: SiaTrans decodes RGB-D or RGB information under the same
model parameters according to the classification signal during testing.
Comprehensive experiments on nine RGB-D SOD benchmark datasets show that
SiaTrans has the best overall performance and the least computation compared
with recent state-of-the-art methods.","['Xingzhao Jia', 'Dongye Changlei', 'Yanjun Peng']",7,0.6257539
"The COVID-19 pandemic has caused globally significant impacts since the
beginning of 2020. This brought a lot of confusion to society, especially due
to the spread of misinformation through social media. Although there were
already several studies related to the detection of misinformation in social
media data, most studies focused on the English dataset. Research on COVID-19
misinformation detection in Indonesia is still scarce. Therefore, through this
research, we collect and annotate datasets for Indonesian and build prediction
models for detecting COVID-19 misinformation by considering the tweet's
relevance. The dataset construction is carried out by a team of annotators who
labeled the relevance and misinformation of the tweet data. In this study, we
propose the two-stage classifier model using IndoBERT pre-trained language
model for the Tweet misinformation detection task. We also experiment with
several other baseline models for text classification. The experimental results
show that the combination of the BERT sequence classifier for relevance
prediction and Bi-LSTM for misinformation detection outperformed other machine
learning models with an accuracy of 87.02%. Overall, the BERT utilization
contributes to the higher performance of most prediction models. We release a
high-quality COVID-19 misinformation Tweet corpus in the Indonesian language,
indicated by the high inter-annotator agreement.","['Douglas Raevan Faisal', 'Rahmad Mahendra']",8,0.74067366
"Previous work suggests that people's preference for different kinds of
information depends on more than just accuracy. This could happen because the
messages contained within different pieces of information may either be
well-liked or repulsive. Whereas factual information must often convey
uncomfortable truths, misinformation can have little regard for veracity and
leverage psychological processes which increase its attractiveness and
proliferation on social media. In this review, we argue that when
misinformation proliferates, this happens because the social media environment
enables adherence to misinformation by reducing, rather than increasing, the
psychological cost of doing so. We cover how attention may often be shifted
away from accuracy and towards other goals, how social and individual cognition
is affected by misinformation and the cases under which debunking it is most
effective, and how the formation of online groups affects information
consumption patterns, often leading to more polarization and radicalization.
Throughout, we make the case that polarization and misinformation adherence are
closely tied. We identify ways in which the psychological cost of adhering to
misinformation can be increased when designing anti-misinformation
interventions or resilient affordances, and we outline open research questions
that the CSCW community can take up in further understanding this cost.","['Alexandros Efstratiou', 'Emiliano De Cristofaro']",0,0.75723994
"Social media platforms have been establishing content moderation guidelines
and employing various moderation policies to counter hate speech and
misinformation. The goal of this paper is to study these community guidelines
and moderation practices, as well as the relevant research publications, to
identify the research gaps, differences in moderation techniques, and
challenges that should be tackled by the social media platforms and the
research community. To this end, we study and analyze fourteen most popular
social media content moderation guidelines and practices, and consolidate them.
We then introduce three taxonomies drawn from this analysis as well as covering
over two hundred interdisciplinary research papers about moderation strategies.
We identify the differences between the content moderation employed in
mainstream and fringe social media platforms. Finally, we have in-depth applied
discussions on both research and practical challenges and solutions.","['Mohit Singhal', 'Chen Ling', 'Pujan Paudel', 'Poojitha Thota', 'Nihal Kumarswamy', 'Gianluca Stringhini', 'Shirin Nilizadeh']",0,0.64886343
"Understanding public discourse on emergency use of unproven therapeutics is
crucial for monitoring safe use and combating misinformation. We developed a
natural language processing-based pipeline to comprehend public perceptions of
and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter
over time. This retrospective study included 609,189 US-based tweets from
January 29, 2020, to November 30, 2021, about four drugs that garnered
significant public attention during the COVID-19 pandemic: (1)
Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2)
Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients.
Time-trend analysis was employed to understand popularity trends and related
events. Content and demographic analyses were conducted to explore potential
rationales behind people's stances on each drug. Time-trend analysis indicated
that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir
and Remdesivir, particularly during COVID-19 surges. Hydroxychloroquine and
Ivermectin discussions were highly politicized, related to conspiracy theories,
hearsay, and celebrity influences. The distribution of stances between the two
major US political parties was significantly different (P < .001); Republicans
were more likely to support Hydroxychloroquine (55%) and Ivermectin (30%) than
Democrats. People with healthcare backgrounds tended to oppose
Hydroxychloroquine (7%) more than the general population, while the general
population was more likely to support Ivermectin (14%). Our study found that
social media users have varying perceptions and stances on off-label versus
FDA-authorized drug use at different stages of COVID-19. This indicates that
health systems, regulatory agencies, and policymakers should design tailored
strategies to monitor and reduce misinformation to promote safe drug use.","['Yining Hua', 'Hang Jiang', 'Shixu Lin', 'Jie Yang', 'Joseph M. Plasek', 'David W. Bates', 'Li Zhou']",12,0.6700258
"Following the 2016 US presidential election and the now overwhelming evidence
of Russian interference, there has been an explosion of interest in the
phenomenon of ""fake news"". To date, research on false news has centered around
detecting content from low-credibility sources and analyzing how this content
spreads across online platforms. Misinformation poses clear risks, yet research
agendas that overemphasize veracity miss the opportunity to truly understand
the Kremlin-led disinformation campaign that shook so many Americans. In this
paper, we present a definition for disinformation - a set or sequence of
orchestrated, agenda-driven information actions with the intent to deceive -
that is useful in contextualizing Russian interference in 2016 and
disinformation campaigns more broadly. We expand on our ongoing work to
operationalize this definition and demonstrate how detecting disinformation
must extend beyond assessing the credibility of a specific publisher, user, or
story.","['Keeley Erhardt', 'Alex Pentland']",4,0.7513407
"We propose an information propagation model that captures important temporal
aspects that have been well observed in the dynamics of fake news diffusion, in
contrast with the diffusion of truth. The model accounts for differential
propagation rates of truth and misinformation and for user reaction times. We
study a time-sensitive variant of the \textit{misinformation mitigation}
problem, where $k$ seeds are to be selected to activate a truth campaign so as
to minimize the number of users that adopt misinformation propagating through a
social network. We show that the resulting objective is non-submodular and
employ a sandwiching technique by defining submodular upper and lower bounding
functions, providing data-dependent guarantees. In order to enable the use of a
reverse sampling framework, we introduce a weighted version of reverse
reachability sets that captures the associated differential propagation rates
and establish a key equivalence between weighted set coverage probabilities and
mitigation with respect to the sandwiching functions. Further, we propose an
offline reverse sampling framework that provides $(1 - 1/e -
\epsilon)$-approximate solutions to our bounding functions and introduce an
importance sampling technique to reduce the sample complexity of our solution.
Finally, we show how our framework can provide an anytime solution to the
problem. Experiments over five datasets show that our approach outperforms
previous approaches and is robust to uncertainty in the model parameters.","['Michael Simpson', 'Farnoosh Hashemi', 'Laks V. S. Lakshmanan']",2,0.80066633
"For research, this paper has included numerous literature that are covering a
variety of information on the topics of misinformation, social media and fake
news, regulation of misinformation and social media platforms, all presented
for India. Studies including thematic analysis of misinformation, brief history
on social media and its amplification of misinformation, current and past
policy interventions by the Indian government, history of self-regulations in
industries, and an analysis of regulatory approaches in the Indian context.
This paper aims at introducing a coherent reading into the context of
misinformation in the country and the subsequent social and business
disruptions that will follow. Utilizing lessons from history around industry
regulations, existing policy research and framework analysis to convince the
reader of the nature of policy intervention that will bode well for all
stakeholders involved. The literature sources have been mentioned in their
respective sections for reference. The research utilized the PASTEL framework
to analyse data collected from other research efforts covering the topic of
misinformation and regulation across academic whitepapers and news media blogs
and articles, all available freely on the public domain. Relevant secondary
data, in terms of information, previous analysis in other research efforts, and
literature work included in respective sections in the paper have been
reproduced, shared and/or indicated wherever necessary.",['Gandharv Dhruv Madan'],0,0.7019018
"Localizing the source of graph diffusion phenomena, such as misinformation
propagation, is an important yet extremely challenging task. Existing source
localization models typically are heavily dependent on the hand-crafted rules.
Unfortunately, a large portion of the graph diffusion process for many
applications is still unknown to human beings so it is important to have
expressive models for learning such underlying rules automatically. This paper
aims to establish a generic framework of invertible graph diffusion models for
source localization on graphs, namely Invertible Validity-aware Graph Diffusion
(IVGD), to handle major challenges including 1) Difficulty to leverage
knowledge in graph diffusion models for modeling their inverse processes in an
end-to-end fashion, 2) Difficulty to ensure the validity of the inferred
sources, and 3) Efficiency and scalability in source inference. Specifically,
first, to inversely infer sources of graph diffusion, we propose a graph
residual scenario to make existing graph diffusion models invertible with
theoretical guarantees; second, we develop a novel error compensation mechanism
that learns to offset the errors of the inferred sources. Finally, to ensure
the validity of the inferred sources, a new set of validity-aware layers have
been devised to project inferred sources to feasible regions by flexibly
encoding constraints with unrolled optimization techniques. A linearization
technique is proposed to strengthen the efficiency of our proposed layers. The
convergence of the proposed IVGD is proven theoretically. Extensive experiments
on nine real-world datasets demonstrate that our proposed IVGD outperforms
state-of-the-art comparison methods significantly. We have released our code at
https://github.com/xianggebenben/IVGD.","['Junxiang Wang', 'Junji Jiang', 'Liang Zhao']",2,0.69546473
"Partial information decomposition allows the joint mutual information between
an output and a set of inputs to be divided into components that are
synergistic or shared or unique to each input. We consider five different
decompositions and compare their results on data from layer 5b pyramidal cells
in two different studies. The first study was of the amplification of somatic
action potential output by apical dendritic input and its regulation by
dendritic inhibition. We find that two of the decompositions produce much
larger estimates of synergy and shared information than the others, as well as
large levels of unique misinformation. When within-neuron differences in the
components are examined, the five methods produce more similar results for all
but the shared information component, for which two methods produce a different
statistical conclusion from the others. There are some differences in the
expression of unique information asymmetry among the methods. It is
significantly larger, on average, under dendritic inhibition. Three of the
methods support a previous conclusion that apical amplification is reduced by
dendritic inhibition. The second study used a detailed compartmental model to
produce action potentials for many combinations of the numbers of basal and
apical synaptic inputs. Two analyses of decompositions are conducted on subsets
of the data. In the first, the decompositions reveal a bifurcation in unique
information asymmetry. For three of the methods this suggests that apical drive
switches to basal drive as the strength of the basal input increases, while the
other two show changing mixtures of information and misinformation.
Decompositions produced using the second set of subsets show that all five
decompositions provide support for properties of cooperative
context-sensitivity - to varying extents.","['Jim W. Kay', 'Jan M. Schulz', 'W. A. Phillips']",2,0.60513055
"The outbreak of the infectious and fatal disease COVID-19 has revealed that
pandemics assail public health in two waves: first, from the contagion itself
and second, from plagues of suspicion and stigma. Now, we have in our hands and
on our phones an outbreak of moral controversy. Modern dependency on social
medias has not only facilitated access to the locations of vaccine clinics and
testing sites but also-and more frequently-to the convoluted explanations of
how ""COVID-19 was a FIFA conspiracy""[1]. The MIT Media Lab finds that false
news ""diffuses significantly farther, faster, deeper, and more broadly than
truth, in all categories of information, and by an order of magnitude""[2]. The
question is, how does the spread of misinformation interact with a physical
epidemic disease? In this paper, we estimate the extent to which misinformation
has influenced the course of the COVID-19 pandemic using natural language
processing models and provide a strategy to combat social media posts that are
likely to cause widespread harm.","['Alexander Wang', 'Jerry Sun', 'Kaitlyn Chen', 'Kevin Zhou', 'Edward Li Gu', 'Chenxin Fang']",5,0.82590187
"We consider the problem of controlling a mutated diffusion process with an
unknown mutation time. The problem is formulated as the quickest intervention
problem with the mutation modeled by a change-point, which is a generalization
of the quickest change-point detection (QCD). Our goal is to intervene in the
mutated process as soon as possible while maintaining a low intervention cost
with optimally chosen intervention actions. This model and the proposed
algorithms can be applied to pandemic prevention (such as Covid-19) or
misinformation containment. We formulate the problem as a partially observed
Markov decision process (POMDP) and convert it to an MDP through the belief
state of the change-point. We first propose a grid approximation approach to
calculate the optimal intervention policy, whose computational complexity could
be very high when the number of grids is large. In order to reduce the
computational complexity, we further propose a low-complexity threshold-based
policy through the analysis of the first-order approximation of the value
functions in the ``local intervention'' regime. Simulation results show the
low-complexity algorithm has a similar performance as the grid approximation
and both perform much better than the QCD-based algorithms.","['Qining Zhang', 'Honghao Wei', 'Weina Wang', 'Lei Ying']",2,0.7034929
"Twitter bot detection has become an increasingly important task to combat
misinformation, facilitate social media moderation, and preserve the integrity
of the online discourse. State-of-the-art bot detection methods generally
leverage the graph structure of the Twitter network, and they exhibit promising
performance when confronting novel Twitter bots that traditional methods fail
to detect. However, very few of the existing Twitter bot detection datasets are
graph-based, and even these few graph-based datasets suffer from limited
dataset scale, incomplete graph structure, as well as low annotation quality.
In fact, the lack of a large-scale graph-based Twitter bot detection benchmark
that addresses these issues has seriously hindered the development and
evaluation of novel graph-based bot detection approaches. In this paper, we
propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark
that presents the largest dataset to date, provides diversified entities and
relations on the Twitter network, and has considerably better annotation
quality than existing datasets. In addition, we re-implement 35 representative
Twitter bot detection baselines and evaluate them on 9 datasets, including
TwiBot-22, to promote a fair comparison of model performance and a holistic
understanding of research progress. To facilitate further research, we
consolidate all implemented codes and datasets into the TwiBot-22 evaluation
framework, where researchers could consistently evaluate new models and
datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation
framework are publicly available at https://twibot22.github.io/","['Shangbin Feng', 'Zhaoxuan Tan', 'Herun Wan', 'Ningnan Wang', 'Zilong Chen', 'Binchi Zhang', 'Qinghua Zheng', 'Wenqian Zhang', 'Zhenyu Lei', 'Shujie Yang', 'Xinshun Feng', 'Qingyue Zhang', 'Hongrui Wang', 'Yuhan Liu', 'Yuyang Bai', 'Heng Wang', 'Zijian Cai', 'Yanbo Wang', 'Lijing Zheng', 'Zihan Ma', 'Jundong Li', 'Minnan Luo']",13,0.77622485
"To curb the spread of fake news on social media platforms, recent studies
have considered an online crowdsourcing fact-checking approach as one possible
intervention method to reduce misinformation. However, it remains unclear under
what conditions crowdsourcing fact-checking efforts deter the spread of
misinformation. To address this issue, we model such distributed fact-checking
as `peer policing' that will reduce the perceived payoff to share or
disseminate false information (fake news) and also reward the spread of
trustworthy information (real news). By simulating our model on synthetic
square lattices and small-world networks, we show that the presence of social
network structure enables fake news spreaders to be self-organized into echo
chambers, thereby providing a boost to the efficacy of fake news and thus its
resistance to fact-checking efforts. Additionally, to study our model in a more
realistic setting, we utilize a Twitter network dataset and study the
effectiveness of deliberately choosing specific individuals to be
fact-checkers. We find that targeted fact-checking efforts can be highly
effective, seeing the same level of success with as little as a fifth of the
number of fact-checkers, but it depends on the structure of the network in
question. In the limit of weak selection, we obtain closed-form analytical
conditions for critical threshold of crowdsourced fact-checking in terms of the
payoff values in our fact-checker/fake news game. Our work has practical
implications for developing model-based mitigation strategies for controlling
the spread of misinformation that interferes with the political discourse.","['Matthew I Jones', 'Scott D. Pauls', 'Feng Fu']",4,0.78624856
"Online social platforms have become central in the political debate. In this
context, the existence of echo chambers is a problem of primary relevance.
These clusters of like-minded individuals tend to reinforce prior beliefs,
elicit animosity towards others and aggravate the spread of misinformation. We
study this phenomenon on a Twitter dataset related to the 2017 French
presidential elections and propose a method to tackle it with content
recommendations. We use a quadratic program to find optimal recommendations
that maximise the diversity of content users are exposed to, while still
accounting for their preferences. Our method relies on a theoretical model that
can sufficiently describe how content flows through the platform. We show that
the model provides good approximations of empirical measures and demonstrate
the effectiveness of the optimisation algorithm at mitigating the echo chamber
effect on this dataset, even with limited budget for recommendations.","['Antoine Vendeville', 'Anastasios Giovanidis', 'Effrosyni Papanastasiou', 'Benjamin Guedj']",3,0.64282465
"The explosion of misinformation spreading in the media ecosystem urges for
automated fact-checking. While misinformation spans both geographic and
linguistic boundaries, most work in the field has focused on English. Datasets
and tools available in other languages, such as Chinese, are limited. In order
to bridge this gap, we construct CHEF, the first CHinese Evidence-based
Fact-checking dataset of 10K real-world claims. The dataset covers multiple
domains, ranging from politics to public health, and provides annotated
evidence retrieved from the Internet. Further, we develop established baselines
and a novel approach that is able to model the evidence retrieval as a latent
variable, allowing jointly training with the veracity prediction model in an
end-to-end fashion. Extensive experiments show that CHEF will provide a
challenging testbed for the development of fact-checking systems designed to
retrieve and reason over non-English claims.","['Xuming Hu', 'Zhijiang Guo', 'Guanyu Wu', 'Aiwei Liu', 'Lijie Wen', 'Philip S. Yu']",8,0.78665495
"With recent progress in deep generative models, the problem of identifying
synthetic data and comparing their underlying generative processes has become
an imperative task for various reasons, including fighting visual
misinformation and source attribution. Existing methods often approximate the
distance between the models via their sample distributions. In this paper, we
approach the problem of fingerprinting generative models by learning
representations that encode the residual artifacts left by the generative
models as unique signals that identify the source models. We consider these
unique traces (a.k.a. ""artificial fingerprints"") as representations of
generative models, and demonstrate their usefulness in both the discriminative
task of source attribution and the unsupervised task of defining a similarity
between the underlying models. We first extend the existing studies on
fingerprints of GANs to four representative classes of generative models (VAEs,
Flows, GANs and score-based models), and demonstrate their existence and
attributability. We then improve the stability and attributability of the
fingerprints by proposing a new learning method based on set-encoding and
contrastive training. Our set-encoder, unlike existing methods that operate on
individual images, learns fingerprints from a \textit{set} of images. We
demonstrate improvements in the stability and attributability through
comparisons to state-of-the-art fingerprint methods and ablation studies.
Further, our method employs contrastive training to learn an implicit
similarity between models. We discover latent families of generative models
using this metric in a standard hierarchical clustering algorithm.","['Hae Jin Song', 'Wael AbdAlmageed']",7,0.7699108
"Deepfakes pose a serious threat to digital well-being by fueling
misinformation. As deepfakes get harder to recognize with the naked eye, human
users become increasingly reliant on deepfake detection models to decide if a
video is real or fake. Currently, models yield a prediction for a video's
authenticity, but do not integrate a method for alerting a human user. We
introduce a framework for amplifying artifacts in deepfake videos to make them
more detectable by people. We propose a novel, semi-supervised Artifact
Attention module, which is trained on human responses to create attention maps
that highlight video artifacts. These maps make two contributions. First, they
improve the performance of our deepfake detection classifier. Second, they
allow us to generate novel ""Deepfake Caricatures"": transformations of the
deepfake that exacerbate artifacts to improve human detection. In a user study,
we demonstrate that Caricatures greatly increase human detection, across video
presentation times and user engagement levels. Overall, we demonstrate the
success of a human-centered approach to designing deepfake mitigation methods.","['Camilo Fosco', 'Emilie Josephs', 'Alex Andonian', 'Allen Lee', 'Xi Wang', 'Aude Oliva']",11,0.81384283
"We develop a model of content filtering as a game between the filter and the
content consumer, where the latter incurs information costs for examining the
content. Motivating examples include censoring misinformation, spam/phish
filtering, and recommender systems. When the attacker is exogenous, we show
that improving the filter's quality is weakly Pareto improving, but has no
impact on equilibrium payoffs until the filter becomes sufficiently accurate.
Further, if the filter does not internalize the information costs, its lack of
commitment power may render it useless and lead to inefficient outcomes. When
the attacker is also strategic, improvements to filter quality may sometimes
decrease equilibrium payoffs.","['Ian Ball', 'James Bono', 'Justin Grana', 'Nicole Immorlica', 'Brendan Lucier', 'Aleksandrs Slivkins']",1,0.37684196
"The spread of misinformation on social media is a pressing societal problem
that platforms, policymakers, and researchers continue to grapple with. As a
countermeasure, recent works have proposed to employ non-expert fact-checkers
in the crowd to fact-check social media content. While experimental studies
suggest that crowds might be able to accurately assess the veracity of social
media content, an understanding of how crowd fact-checked (mis-)information
spreads is missing. In this work, we empirically analyze the spread of
misleading vs. not misleading community fact-checked posts on social media. For
this purpose, we employ a dataset of community-created fact-checks from
Twitter's Birdwatch pilot and map them to resharing cascades on Twitter.
Different from earlier studies analyzing the spread of misinformation listed on
third-party fact-checking websites (e.g., Snopes), we find that community
fact-checked misinformation is less viral. Specifically, misleading posts are
estimated to receive 36.62% fewer retweets than not misleading posts. A partial
explanation may lie in differences in the fact-checking targets: community
fact-checkers tend to fact-check posts from influential user accounts with many
followers, while expert fact-checks tend to target posts that are shared by
less influential users. We further find that there are significant differences
in virality across different sub-types of misinformation (e.g., factual errors,
missing context, manipulated media). Moreover, we conduct a user study to
assess the perceived reliability of (real-world) community-created fact-checks.
Here, we find that users, to a large extent, agree with community-created
fact-checks. Altogether, our findings offer insights into how misleading vs.
not misleading posts spread and highlight the crucial role of sample selection
when studying misinformation on social media.","['Chiara Drolsbach', 'Nicolas Pr√∂llochs']",3,0.792948
"With the growing importance of detecting misinformation, many studies have
focused on verifying factual claims by retrieving evidence. However, canonical
fact verification tasks do not apply to catching subtle differences in
factually consistent claims, which might still bias the readers, especially on
contentious political or economic issues. Our underlying assumption is that
among the trusted sources, one's argument is not necessarily more true than the
other, requiring comparison rather than verification. In this study, we propose
ClaimDiff, a novel dataset that primarily focuses on comparing the nuance
between claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from
268 news articles. We observe that while humans are capable of detecting the
nuances between claims, strong baselines struggle to detect them, showing over
a 19% absolute gap with the humans. We hope this initial study could help
readers to gain an unbiased grasp of contentious issues through machine-aided
comparison.","['Miyoung Ko', 'Ingyu Seong', 'Hwaran Lee', 'Joonsuk Park', 'Minsuk Chang', 'Minjoon Seo']",1,0.6616224
"Misinformation has disruptive effects on our lives. Many researchers have
looked into means to identify and combat misinformation in text or data
visualization. However, there is still a lack of understanding of how
misinformation can be introduced when text and visualization are combined to
tell data stories, not to mention how to improve the lay public's awareness of
possible misperceptions about facts in narrative visualization. In this paper,
we first analyze where misinformation could possibly be injected into the
production-consumption process of data stories through a literature survey.
Then, as a first step towards combating misinformation in data stories, we
explore possible defensive design methods to enhance the reader's awareness of
information misalignment when data facts are scripted and visualized. More
specifically, we conduct a between-subjects crowdsourcing study to investigate
the impact of two design methods enhancing text-visualization integration,
i.e., explanatory annotation and interactive linking, on users' awareness of
misinformation in data stories. The study results show that although most
participants still can not find misinformation, the two design methods can
significantly lower the perceived credibility of the text or visualizations.
Our work informs the possibility of fighting an infodemic through defensive
design methods.","['Chengbo Zheng', 'Xiaojuan Ma']",0,0.77423906
"Past research has attributed the online circulation of misinformation to two
main factors - individual characteristics (e.g., a person's information
literacy) and social media effects (e.g., algorithm-mediated information
diffusion) - and has overlooked a third one: the critical mass created by the
offline self-segregation of Americans into like-minded geographical regions
such as states (a phenomenon called ""The Big Sort""). We hypothesized that this
latter factor matters for the online spreading of misinformation not least
because online interactions, despite having the potential of being global, end
up being localized: interaction probability is known to rapidly decay with
distance. Upon analysis of more than 8M Reddit comments containing news links
spanning four years, from January 2016 to December 2019, we found that Reddit
did not work as an ""hype machine"" for misinformation (as opposed to what
previous work reported for other platforms, circulation was not mainly caused
by platform-facilitated network effects) but worked as a supply-and-demand
system: misinformation news items scaled linearly with the number of users in
each state (with a scaling exponent beta=1, and a goodness of fit R2 = 0.95).
Furthermore, deviations from such a universal pattern were best explained by
state-level personality and cultural factors (R2 = {0.12, 0.39}), rather than
socioeconomic conditions (R2 = {0.15, 0.29}) or, as one would expect, political
characteristics (R2 ={0.06, 0.21}). Higher-than-expected circulation of any
type of news (including reputable news) was found in states characterised by
residents who tend to be less diligent in terms of their personality (low in
conscientiousness) and by loose cultures understating the importance of
adherence to norms (low in cultural tightness).","['Lia Bozarth', 'Daniele Quercia', 'Licia Capra', 'Sanja Scepanovic']",3,0.6883019
"COVID-19 related misinformation and fake news, coined an 'infodemic', has
dramatically increased over the past few years. This misinformation exhibits
concept drift, where the distribution of fake news changes over time, reducing
effectiveness of previously trained models for fake news detection. Given a set
of fake news models trained on multiple domains, we propose an adaptive
decision module to select the best-fit model for a new sample. We propose
MiDAS, a multi-domain adaptative approach for fake news detection that ranks
relevancy of existing models to new samples. MiDAS contains 2 components: a
doman-invariant encoder, and an adaptive model selector. MiDAS integrates
multiple pre-trained and fine-tuned models with their training data to create a
domain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of
the invariant embedding space to estimate each model's relevance to a new
sample. Higher ranked models provide predictions, and lower ranked models
abstain. We evaluate MiDAS on generalization to drifted data with 9 fake news
datasets, each obtained from different domains and modalities. MiDAS achieves
new state-of-the-art performance on multi-domain adaptation for
out-of-distribution fake news classification.","['Abhijit Suprem', 'Calton Pu']",2,0.63666654
"The COVID-19 pandemic has been accompanied by an `infodemic' -- of accurate
and inaccurate health information across social media. Detecting misinformation
amidst dynamically changing information landscape is challenging; identifying
relevant keywords and posts is arduous due to the large amount of human effort
required to inspect the content and sources of posts. We aim to reduce the
resource cost of this process by introducing a weakly-supervised iterative
graph-based approach to detect keywords, topics, and themes related to
misinformation, with a focus on COVID-19. Our approach can successfully detect
specific topics from general misinformation-related seed words in a few seed
texts. Our approach utilizes the BERT-based Word Graph Search (BWGS) algorithm
that builds on context-based neural network embeddings for retrieving
misinformation-related posts. We utilize Latent Dirichlet Allocation (LDA)
topic modeling for obtaining misinformation-related themes from the texts
returned by BWGS. Furthermore, we propose the BERT-based Multi-directional Word
Graph Search (BMDWGS) algorithm that utilizes greater starting context
information for misinformation extraction. In addition to a qualitative
analysis of our approach, our quantitative analyses show that BWGS and BMDWGS
are effective in extracting misinformation-related content compared to common
baselines in low data resource settings. Extracting such content is useful for
uncovering prevalent misconceptions and concerns and for facilitating precision
public health messaging campaigns to improve health behaviors.","['Harry Wang', 'Sharath Chandra Guntuku']",1,0.669684
"The COVID-19 pandemic has fueled the spread of misinformation on social media
and the Web as a whole. The phenomenon dubbed `infodemic' has taken the
challenges of information veracity and trust to new heights by massively
introducing seemingly scientific and technical elements into misleading
content. Despite the existing body of work on modeling and predicting
misinformation, the coverage of very complex scientific topics with inherent
uncertainty and an evolving set of findings, such as COVID-19, provides many
new challenges that are not easily solved by existing tools. To address these
issues, we introduce SciLander, a method for learning representations of news
sources reporting on science-based topics. SciLander extracts four
heterogeneous indicators for the news sources; two generic indicators that
capture (1) the copying of news stories between sources, and (2) the use of the
same terms to mean different things (i.e., the semantic shift of terms), and
two scientific indicators that capture (1) the usage of jargon and (2) the
stance towards specific citations. We use these indicators as signals of source
agreement, sampling pairs of positive (similar) and negative (dissimilar)
samples, and combine them in a unified framework to train unsupervised news
source embeddings with a triplet margin loss objective. We evaluate our method
on a novel COVID-19 dataset containing nearly 1M news articles from 500 sources
spanning a period of 18 months since the beginning of the pandemic in 2020. Our
results show that the features learned by our model outperform state-of-the-art
baseline methods on the task of news veracity classification. Furthermore, a
clustering analysis suggests that the learned representations encode
information about the reliability, political leaning, and partisanship bias of
these sources.","['Maur√≠cio Gruppi', 'Panayiotis Smeros', 'Sibel Adalƒ±', 'Carlos Castillo', 'Karl Aberer']",4,0.7263701
"The Covid-19 pandemic has caused a dramatic and parallel rise in dangerous
misinformation, denoted an `infodemic' by the CDC and WHO. Misinformation tied
to the Covid-19 infodemic changes continuously; this can lead to performance
degradation of fine-tuned models due to concept drift. Degredation can be
mitigated if models generalize well-enough to capture some cyclical aspects of
drifted data. In this paper, we explore generalizability of pre-trained and
fine-tuned fake news detectors across 9 fake news datasets. We show that
existing models often overfit on their training dataset and have poor
performance on unseen data. However, on some subsets of unseen data that
overlap with training data, models have higher accuracy. Based on this
observation, we also present KMeans-Proxy, a fast and effective method based on
K-Means clustering for quickly identifying these overlapping subsets of unseen
data. KMeans-Proxy improves generalizability on unseen fake news datasets by
0.1-0.2 f1-points across datasets. We present both our generalizability
experiments as well as KMeans-Proxy to further research in tackling the fake
news problem.","['Abhijit Suprem', 'Calton Pu']",2,0.67090905
"Deepfakes are a form of synthetic image generation used to generate fake
videos of individuals for malicious purposes. The resulting videos may be used
to spread misinformation, reduce trust in media, or as a form of blackmail.
These threats necessitate automated methods of deepfake video detection. This
paper investigates whether temporal information can improve the deepfake
detection performance of deep learning models.
  To investigate this, we propose a framework that classifies new and existing
approaches by their defining characteristics. These are the types of feature
extraction: automatic or manual, and the temporal relationship between frames:
dependent or independent. We apply this framework to investigate the effect of
temporal dependency on a model's deepfake detection performance.
  We find that temporal dependency produces a statistically significant (p <
0.05) increase in performance in classifying real images for the model using
automatic feature selection, demonstrating that spatio-temporal information can
increase the performance of deepfake video detection models.","['Will Rowan', 'Nick Pears']",7,0.7702676
"Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs.","['Marko Tesic', 'Ulrike Hahn']",9,0.62777156
"This article presents a beta-version of MEWS (Misinformation Early Warning
System). It describes the various aspects of the ingestion, manipulation
detection, and graphing algorithms employed to determine--in near
real-time--the relationships between social media images as they emerge and
spread on social media platforms. By combining these various technologies into
a single processing pipeline, MEWS can identify manipulated media items as they
arise and identify when these particular items begin trending on individual
social media platforms or even across multiple platforms. The emergence of a
novel manipulation followed by rapid diffusion of the manipulated content
suggests a disinformation campaign.","['Trenton W. Ford', 'William Theisen', 'Michael Yankoski', 'Tom Henry', 'Farah Khashman', 'Katherine R. Dearstyne', 'Tim Weninger']",0,0.65905344
"Proactively identifying misinformation spreaders is an important step towards
mitigating the impact of fake news on our society. In this paper, we introduce
a new contemporary Reddit dataset for fake news spreader analysis, called
FACTOID, monitoring political discussions on Reddit since the beginning of
2020. The dataset contains over 4K users with 3.4M Reddit posts, and includes,
beyond the users' binary labels, also their fine-grained credibility level
(very low to very high) and their political bias strength (extreme right to
extreme left). As far as we are aware, this is the first fake news spreader
dataset that simultaneously captures both the long-term context of users'
historical posts and the interactions between them. To create the first
benchmark on our data, we provide methods for identifying misinformation
spreaders by utilizing the social connections between the users along with
their psycho-linguistic features. We show that the users' social interactions
can, on their own, indicate misinformation spreading, while the
psycho-linguistic features are mostly informative in non-neural classification
settings. In a qualitative analysis, we observe that detecting affective mental
processes correlates negatively with right-biased users, and that the openness
to experience factor is lower for those who spread fake news.","['Flora Sakketou', 'Joan Plepi', 'Riccardo Cervero', 'Henri-Jacques Geiss', 'Paolo Rosso', 'Lucie Flek']",3,0.78502196
"The spread of fake news, propaganda, misinformation, disinformation, and
harmful content online raised concerns among social media platforms, government
agencies, policymakers, and society as a whole. This is because such harmful or
abusive content leads to several consequences to people such as physical,
emotional, relational, and financial. Among different harmful content
\textit{trolling-based} online content is one of them, where the idea is to
post a message that is provocative, offensive, or menacing with an intent to
mislead the audience. The content can be textual, visual, a combination of
both, or a meme. In this study, we provide a comparative analysis of
troll-based memes classification using the textual, visual, and multimodal
content. We report several interesting findings in terms of code-mixed text,
multimodal setting, and combining an additional dataset, which shows
improvements over the majority baseline.","['Rabindra Nath Nandi', 'Firoj Alam', 'Preslav Nakov']",3,0.64190906
"Harmful or abusive online content has been increasing over time, raising
concerns for social media platforms, government agencies, and policymakers.
Such harmful or abusive content can have major negative impact on society,
e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause
vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms
and deaths. The content that is posted and shared online can be textual,
visual, or a combination of both, e.g., in a meme. Here, we describe our
experiments in detecting the roles of the entities (hero, villain, victim) in
harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our
system for the task. We further provide a comparative analysis of different
experimental settings (i.e., unimodal, multimodal, attention, and
augmentation). For reproducibility, we make our experimental code publicly
available. \url{https://github.com/robi56/harmful_memes_block_fusion}","['Rabindra Nath Nandi', 'Firoj Alam', 'Preslav Nakov']",5,0.6365837
"Misinformation has become a major concern in recent last years given its
spread across our information sources. In the past years, many NLP tasks have
been introduced in this area, with some systems reaching good results on
English language datasets. Existing AI based approaches for fighting
misinformation in literature suggest automatic stance detection as an integral
first step to success. Our paper aims at utilizing this progress made for
English to transfers that knowledge into other languages, which is a
non-trivial task due to the domain gap between English and the target
languages. We propose a black-box non-intrusive method that utilizes techniques
from Domain Adaptation to reduce the domain gap, without requiring any human
expertise in the target language, by leveraging low-quality data in both a
supervised and unsupervised manner. This allows us to rapidly achieve similar
results for stance detection for the Zulu language, the target language in this
work, as are found for English. We also provide a stance detection dataset in
the Zulu language. Our experimental results show that by leveraging English
datasets and machine translation we can increase performances on both English
data along with other languages.","['Gcinizwe Dlamini', 'Imad Eddine Ibrahim Bekkouch', 'Adil Khan', 'Leon Derczynski']",8,0.80879873
"In today's era of information disorder, many organizations are moving to
verify the veracity of news published on the web and social media. In
particular, some agencies are exploring the world of online media and, through
a largely manual process, ranking the credibility and transparency of news
sources around the world. In this paper, we evaluate two procedures for
assessing the risk of online media exposing their readers to m/disinformation.
The procedures have been dictated by NewsGuard and The Global Disinformation
Index, two well-known organizations combating d/misinformation via practices of
good journalism. Specifically, considering a fixed set of media outlets, we
examine how many of them were rated equally by the two procedures, and which
aspects led to disagreement in the assessment. The result of our analysis shows
a good degree of agreement, which in our opinion has a double value: it
fortifies the correctness of the procedures and lays the groundwork for their
automation.","['Manuel Pratelli', 'Marinella Petrocchi']",4,0.7322987
"We present a comprehensive work on automated veracity assessment from dataset
creation to developing novel methods based on Natural Language Inference (NLI),
focusing on misinformation related to the COVID-19 pandemic. We first describe
the construction of the novel PANACEA dataset consisting of heterogeneous
claims on COVID-19 and their respective information sources. The dataset
construction includes work on retrieval techniques and similarity measurements
to ensure a unique set of claims. We then propose novel techniques for
automated veracity assessment based on Natural Language Inference including
graph convolutional networks and attention based approaches. We have carried
out experiments on evidence retrieval and veracity assessment on the dataset
using the proposed techniques and found them competitive with SOTA methods, and
provided a detailed discussion.","['M. Arana-Catania', 'Elena Kochkina', 'Arkaitz Zubiaga', 'Maria Liakata', 'Rob Procter', 'Yulan He']",1,0.6673944
"In recent years, the problem of misinformation on the web has become
widespread across languages, countries, and various social media platforms.
Although there has been much work on automated fake news detection, the role of
images and their variety are not well explored. In this paper, we investigate
the roles of image and text at an earlier stage of the fake news detection
pipeline, called claim detection. For this purpose, we introduce a novel
dataset, MM-Claims, which consists of tweets and corresponding images over
three topics: COVID-19, Climate Change and broadly Technology. The dataset
contains roughly 86000 tweets, out of which 3400 are labeled manually by
multiple annotators for the training and evaluation of multimodal models. We
describe the dataset in detail, evaluate strong unimodal and multimodal
baselines, and analyze the potential and drawbacks of current models.","['Gullal S. Cheema', 'Sherzod Hakimov', 'Abdul Sittar', 'Eric M√ºller-Budack', 'Christian Otto', 'Ralph Ewerth']",8,0.71109265
"Warning users about misinformation on social media is not a simple usability
task. Soft moderation has to balance between debunking falsehoods and avoiding
moderation bias while preserving the social media consumption flow. Platforms
thus employ minimally distinguishable warning tags with generic text under a
suspected misinformation content. This approach resulted in an unfavorable
outcome where the warnings ""backfired"" and users believed the misinformation
more, not less. In response, we developed enhancements to the misinformation
warnings where users are advised on the context of the information hazard and
exposed to standard warning iconography. We ran an A/B evaluation with the
Twitter's original warning tags in a 337 participant usability study. The
majority of the participants preferred the enhancements as a nudge toward
recognizing and avoiding misinformation. The enhanced warning tags were most
favored by the politically left-leaning and to a lesser degree moderate
participants, but they also appealed to roughly a third of the right-leaning
participants. The education level was the only demographic factor shaping
participants' preferences. We use our findings to propose user-tailored
improvements in the soft moderation of misinformation on social media.","['Filipo Sharevski', 'Amy Devine', 'Emma Pieroni', 'Peter Jacnim']",3,0.66860634
"The objective of this paper is to explore the opportunities for human
information behaviour research to inform and influence the field of machine
learning and the resulting machine information behaviour. Using the development
of foundation models in machine learning as an example, the paper illustrates
how human information behaviour research can bring to machine learning a more
nuanced view of information and informing, a better understanding of
information need and how that affects the communication among people and
systems, guidance on the nature of context and how to operationalize that in
models and systems, and insights into bias, misinformation, and
marginalization. Despite their clear differences, the fields of information
behaviour and machine learning share many common objectives, paradigms, and key
research questions. The example of foundation models illustrates that human
information behaviour research has much to offer in addressing some of the
challenges emerging in the nascent area of machine information behaviour.",['Michael Ridley'],0,0.7570899
"Misinformation spread through social media has become a fundamental challenge
in modern society. Recent studies have evaluated various strategies for
addressing this problem, such as by modifying social media platforms or
educating people about misinformation, to varying degrees of success. Our goal
is to develop a new strategy for countering misinformation: intelligent tools
that encourage social media users to foster metacognitive skills ""in the wild.""
As a first step, we conducted focus groups with social media users to discover
how they can be best supported in combating misinformation. Qualitative
analyses of the discussions revealed that people find it difficult to detect
misinformation. Findings also indicated a need for but lack of resources to
support cross-validation of information. Moreover, misinformation had a nuanced
emotional impact on people. Suggestions for the design of intelligent tools
that support social media users in information selection, information
engagement, and emotional response management are presented.","['Jacqueline Urakami', 'Yeongdae Kim', 'Hiroki Oura', 'Katie Seaborn']",0,0.78486913
"The integrity of elections is central to democratic systems. However, a
myriad of malicious actors aspire to influence election outcomes for financial
or political benefit. A common means to such ends is by manipulating
perceptions of the voting public about select candidates, for example, through
misinformation. We present a formal model of the impact of perception
manipulation on election outcomes in the framework of spatial voting theory, in
which the preferences of voters over candidates are generated based on their
relative distance in the space of issues. We show that controlling elections in
this model is, in general, NP-hard, whether issues are binary or real-valued.
However, we demonstrate that critical to intractability is the diversity of
opinions on issues exhibited by the voting public. When voter views lack
diversity, and we can instead group them into a small number of categories --
for example, as a result of political polarization -- the election control
problem can be solved in polynomial time in the number of issues and candidates
for arbitrary scoring rules.","['Junlin Wu', 'Andrew Estornell', 'Lecheng Kong', 'Yevgeniy Vorobeychik']",0,0.49996555
"Text data can pose a risk of harm. However, the risks are not fully
understood, and how to handle, present, and discuss harmful text in a safe way
remains an unresolved issue in the NLP community. We provide an analytical
framework categorising harms on three axes: (1) the harm type (e.g.,
misinformation, hate speech or racial stereotypes); (2) whether a harm is
\textit{sought} as a feature of the research design if explicitly studying
harmful content (e.g., training a hate speech classifier), versus
\textit{unsought} if harmful content is encountered when working on unrelated
problems (e.g., language generation or part-of-speech tagging); and (3) who it
affects, from people (mis)represented in the data to those handling the data
and those publishing on the data. We provide advice for practitioners, with
concrete steps for mitigating harm in research and in publication. To assist
implementation we introduce \textsc{HarmCheck} -- a documentation standard for
handling and presenting harmful text in research.","['Hannah Rose Kirk', 'Abeba Birhane', 'Bertie Vidgen', 'Leon Derczynski']",0,0.5840851
"Faced with the scale and surge of misinformation on social media, many
platforms and fact-checking organizations have turned to algorithms for
automating key parts of misinformation detection pipelines. While offering a
promising solution to the challenge of scale, the ethical and societal risks
associated with algorithmic misinformation detection are not well-understood.
In this paper, we employ and extend upon the notion of informational justice to
develop a framework for explicating issues of justice relating to
representation, participation, distribution of benefits and burdens, and
credibility in the misinformation detection pipeline. Drawing on the framework:
(1) we show how injustices materialize for stakeholders across three
algorithmic stages in the pipeline; (2) we suggest empirical measures for
assessing these injustices; and (3) we identify potential sources of these
harms. This framework should help researchers, policymakers, and practitioners
reason about potential harms or risks associated with these algorithms and
provide conceptual guidance for the design of algorithmic fairness audits in
this domain.","['Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour']",0,0.629357
"The paper presents the outcomes of AI-COVID19, our project aimed at better
understanding of misinformation flow about COVID-19 across social media
platforms. The specific focus of the study reported in this paper is on
collecting data from Telegram groups which are active in promotion of
COVID-related misinformation. Our corpus collected so far contains around 28
million words, from almost one million messages. Given that a substantial
portion of misinformation flow in social media is spread via multimodal means,
such as images and video, we have also developed a mechanism for utilising such
channels via producing automatic transcripts for videos and automatic
classification for images into such categories as memes, screenshots of posts
and other kinds of images. The accuracy of the image classification pipeline is
around 87%.","['Jose Sosa', 'Serge Sharoff']",5,0.6090933
"False information has a significant negative influence on individuals as well
as on the whole society. Especially in the current COVID-19 era, we witness an
unprecedented growth of medical misinformation. To help tackle this problem
with machine learning approaches, we are publishing a feature-rich dataset of
approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also
contains 573 manually and more than 51k automatically labelled mappings between
claims and articles. Mappings consist of claim presence, i.e., whether a claim
is contained in a given article, and article stance towards the claim. We
provide several baselines for these two tasks and evaluate them on the manually
labelled part of the dataset. The dataset enables a number of additional tasks
related to medical misinformation, such as misinformation characterisation
studies or studies of misinformation diffusion between sources.","['Ivan Srba', 'Branislav Pecher', 'Matus Tomlein', 'Robert Moro', 'Elena Stefancova', 'Jakub Simko', 'Maria Bielikova']",1,0.6387676
"Over the course of the COVID-19 pandemic, large volumes of biomedical
information concerning this new disease have been published on social media.
Some of this information can pose a real danger to people's health,
particularly when false information is shared, for instance recommendations on
how to treat diseases without professional medical advice. Therefore, automatic
fact-checking resources and systems developed specifically for the medical
domain are crucial. While existing fact-checking resources cover
COVID-19-related information in news or quantify the amount of misinformation
in tweets, there is no dataset providing fact-checked COVID-19-related Twitter
posts with detailed annotations for biomedical entities, relations and relevant
evidence. We contribute CoVERT, a fact-checked corpus of tweets with a focus on
the domain of biomedicine and COVID-19-related (mis)information. The corpus
consists of 300 tweets, each annotated with medical named entities and
relations. We employ a novel crowdsourcing methodology to annotate all tweets
with fact-checking labels and supporting evidence, which crowdworkers search
for online. This methodology results in moderate inter-annotator agreement.
Furthermore, we use the retrieved evidence extracts as part of a fact-checking
pipeline, finding that the real-world evidence is more useful than the
knowledge indirectly available in pretrained language models.","['Isabelle Mohr', 'Amelie W√ºhrl', 'Roman Klinger']",5,0.7420321
"Sentiment detection is an important building block for multiple information
retrieval tasks such as product recommendation, cyberbullying detection, and
misinformation detection. Unsurprisingly, multiple commercial APIs, each with
different levels of accuracy and fairness, are now available for sentiment
detection. While combining inputs from multiple modalities or black-box models
for increasing accuracy is commonly studied in multimedia computing literature,
there has been little work on combining different modalities for increasing
fairness of the resulting decision. In this work, we audit multiple commercial
sentiment detection APIs for the gender bias in two actor news headlines
settings and report on the level of bias observed. Next, we propose a ""Flexible
Fair Regression"" approach, which ensures satisfactory accuracy and fairness by
jointly learning from multiple black-box models. The results pave way for fair
yet accurate sentiment detectors for multiple applications.","['Abdulaziz A. Almuzaini', 'Vivek K. Singh']",1,0.66950226
"The phenomenon of misinformation spreading in social media has developed a
new form of active citizens who focus on tackling the problem by refuting posts
that might contain misinformation. Automatically identifying and characterizing
the behavior of such active citizens in social media is an important task in
computational social science for complementing studies in misinformation
analysis. In this paper, we study this task across different social media
platforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese)
for the first time. To this end, (1) we develop and make publicly available a
new dataset of Weibo users mapped into one of the two categories (i.e.,
misinformation posters or active citizens); (2) we evaluate a battery of
supervised models on our new Weibo dataset and an existing Twitter dataset
which we repurpose for the task; and (3) we present an extensive analysis of
the differences in language use between the two user categories.","['Yida Mu', 'Pu Niu', 'Nikolaos Aletras']",0,0.69676715
"Data visualization is powerful in persuading an audience. However, when it is
done poorly or maliciously, a visualization may become misleading or even
deceiving. Visualizations give further strength to the dissemination of
misinformation on the Internet. The visualization research community has long
been aware of visualizations that misinform the audience, mostly associated
with the terms ""lie"" and ""deceptive."" Still, these discussions have focused
only on a handful of cases. To better understand the landscape of misleading
visualizations, we open-coded over one thousand real-world visualizations that
have been reported as misleading. From these examples, we discovered 74 types
of issues and formed a taxonomy of misleading elements in visualizations. We
found four directions that the research community can follow to widen the
discussion on misleading visualizations: (1) informal fallacies in
visualizations, (2) exploiting conventions and data literacy, (3) deceptive
tricks in uncommon charts, and (4) understanding the designers' dilemma. This
work lays the groundwork for these research directions, especially in
understanding, detecting, and preventing them.","['Leo Yu-Ho Lo', 'Ayush Gupta', 'Kento Shigyo', 'Aoyu Wu', 'Enrico Bertini', 'Huamin Qu']",0,0.69181585
"The COVID-19 pandemic is accompanied by a massive ""infodemic"" that makes it
hard to identify concise and credible information for COVID-19-related
questions, like incubation time, infection rates, or the effectiveness of
vaccines. As a novel solution, our paper is concerned with designing a
question-answering system based on modern technologies from natural language
processing to overcome information overload and misinformation in pandemic
situations. To carry out our research, we followed a design science research
approach and applied Ingwersen's cognitive model of information retrieval
interaction to inform our design process from a socio-technical lens. On this
basis, we derived prescriptive design knowledge in terms of design requirements
and design principles, which we translated into the construction of a
prototypical instantiation. Our implementation is based on the comprehensive
CORD-19 dataset, and we demonstrate our artifact's usefulness by evaluating its
answer quality based on a sample of COVID-19 questions labeled by biomedical
experts.","['Johannes Graf', 'Gino Lancho', 'Patrick Zschech', 'Kai Heinrich']",1,0.57313144
"This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.","['Hyewon Choi', 'Yejun Yoon', 'Seunghyun Yoon', 'Kunwoo Park']",4,0.7672336
"As yet another alternative social network, Gettr positions itself as the
""marketplace of ideas"" where users should expect the truth to emerge without
any administrative censorship. We looked deep inside the platform by analyzing
it's structure, a sample of 6.8 million posts, and the responses from a sample
of 124 Gettr users we interviewed to see if this actually is the case.
Administratively, Gettr makes a deliberate attempt to stifle any external
evaluation of the platform as collecting data is marred with unpredictable and
abrupt changes in their API. Content-wise, Gettr notably hosts pro-Trump
content mixed with conspiracy theories and attacks on the perceived ""left.""
It's social network structure is asymmetric and centered around prominent
right-thought leaders, which is characteristic for all alt-platforms. While
right-leaning users joined Gettr as a result of a perceived freedom of speech
infringement by the mainstream platforms, left-leaning users followed them in
numbers as to ""keep up with the misinformation."" We contextualize these
findings by looking into the Gettr's user interface design to provide a
comprehensive insight into the incentive structure for joining and competing
for the truth on Gettr.","['Filipo Sharevski', 'Peter Jachim', 'Emma Pieroni', 'Amy Devine']",3,0.6705097
"The appearance of complex attention-based language models such as BERT,
Roberta or GPT-3 has allowed to address highly complex tasks in a plethora of
scenarios. However, when applied to specific domains, these models encounter
considerable difficulties. This is the case of Social Networks such as Twitter,
an ever-changing stream of information written with informal and complex
language, where each message requires careful evaluation to be understood even
by humans given the important role that context plays. Addressing tasks in this
domain through Natural Language Processing involves severe challenges. When
powerful state-of-the-art multilingual language models are applied to this
scenario, language specific nuances use to get lost in translation. To face
these challenges we present \textbf{BERTuit}, the larger transformer proposed
so far for Spanish language, pre-trained on a massive dataset of 230M Spanish
tweets using RoBERTa optimization. Our motivation is to provide a powerful
resource to better understand Spanish Twitter and to be used on applications
focused on this social network, with special emphasis on solutions devoted to
tackle the spreading of misinformation in this platform. BERTuit is evaluated
on several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very
competitive multilingual transformers. The utility of our approach is shown
with applications, in this case: a zero-shot methodology to visualize groups of
hoaxes and profiling authors spreading disinformation.
  Misinformation spreads wildly on platforms such as Twitter in languages other
than English, meaning performance of transformers may suffer when transferred
outside English speaking communities.","['Javier Huertas-Tato', 'Alejandro Martin', 'David Camacho']",8,0.7825394
"During the COVID-19 pandemic, large amounts of COVID-19 misinformation are
spreading on social media. We are interested in the stance of Twitter users
towards COVID-19 misinformation. However, due to the relative recent nature of
the pandemic, only a few stance detection datasets fit our task. We have
constructed a new stance dataset consisting of 2631 tweets annotated with the
stance towards COVID-19 misinformation. In contexts with limited labeled data,
we fine-tune our models by leveraging the MNLI dataset and two existing stance
detection datasets (RumourEval and COVIDLies), and evaluate the model
performance on our dataset. Our experimental results show that the model
performs the best when fine-tuned sequentially on the MNLI dataset and the
combination of the undersampled RumourEval and COVIDLies datasets. Our code and
dataset are publicly available at
https://github.com/yanfangh/covid-rumor-stance","['Yanfang Hou', 'Peter van der Putten', 'Suzan Verberne']",8,0.5383421
"Motivated by recent works in the communication and psychology literature, we
model and study the role social identity -- a person's sense of belonging to a
group -- plays in human information consumption. A hallmark of Social Identity
Theory (SIT) is the notion of 'status', i.e., an individual's desire to enhance
their and their 'in-group's' utility relative to that of an 'out-group'. In the
context of belief formation, this comes off as a desire to believe positive
news about the in-group and negative news about the out-group, which has been
empirically shown to support belief in misinformation and false news.
  We model this phenomenon as a Stackelberg game being played over an
information channel between a news-source (sender) and news-consumer
(receiver), with the receiver incorporating the 'status' associated with social
identity in their utility, in addition to accuracy. We characterize the
strategy that must be employed by the sender to ensure that its message is
trusted by receivers of all identities while maximizing their overall quality
of information. We show that, as a rule, this optimal quality of information at
equilibrium decreases when a receiver's sense of identity increases. We further
demonstrate how extensions of our model can be used to quantitatively estimate
the level of importance given to identity in a population.","['Vijeth Hebbar', 'Cedric Langbort']",3,0.6275879
"Deepfakes are synthetically generated media often devised with malicious
intent. They have become increasingly more convincing with large training
datasets advanced neural networks. These fakes are readily being misused for
slander, misinformation and fraud. For this reason, intensive research for
developing countermeasures is also expanding. However, recent work is almost
exclusively limited to deepfake detection - predicting if audio is real or
fake. This is despite the fact that attribution (who created which fake?) is an
essential building block of a larger defense strategy, as practiced in the
field of cybersecurity for a long time. This paper considers the problem of
deepfake attacker attribution in the domain of audio. We present several
methods for creating attacker signatures using low-level acoustic descriptors
and machine learning embeddings. We show that speech signal features are
inadequate for characterizing attacker signatures. However, we also demonstrate
that embeddings from a recurrent neural network can successfully characterize
attacks from both known and unknown attackers. Our attack signature embeddings
result in distinct clusters, both for seen and unseen audio deepfakes. We show
that these embeddings can be used in downstream-tasks to high-effect, scoring
97.10% accuracy in attacker-id classification.","['Nicolas M. M√ºller', 'Franziska Dieckmann', 'Jennifer Williams']",11,0.7548722
"As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.","['Sara Abdali', 'Sina shaham', 'Bhaskar Krishnamachari']",0,0.7258743
"The negative effects of misinformation filter bubbles in adaptive systems
have been known to researchers for some time. Several studies investigated,
most prominently on YouTube, how fast a user can get into a misinformation
filter bubble simply by selecting wrong choices from the items offered. Yet, no
studies so far have investigated what it takes to burst the bubble, i.e.,
revert the bubble enclosure. We present a study in which pre-programmed agents
(acting as YouTube users) delve into misinformation filter bubbles by watching
misinformation promoting content (for various topics). Then, by watching
misinformation debunking content, the agents try to burst the bubbles and reach
more balanced recommendation mixes. We recorded the search results and
recommendations, which the agents encountered, and analyzed them for the
presence of misinformation. Our key finding is that bursting of a filter bubble
is possible, albeit it manifests differently from topic to topic. Moreover, we
observe that filter bubbles do not truly appear in some situations. We also
draw a direct comparison with a previous study. Sadly, we did not find much
improvements in misinformation occurrences, despite recent pledges by YouTube.","['Matus Tomlein', 'Branislav Pecher', 'Jakub Simko', 'Ivan Srba', 'Robert Moro', 'Elena Stefancova', 'Michal Kompan', 'Andrea Hrckova', 'Juraj Podrouzek', 'Maria Bielikova']",3,0.5440132
"Recent social networks' misinformation mitigation approaches tend to
investigate how to reduce misinformation by considering a whole-network
statistical scale. However, unbalanced misinformation exposures among
individuals urge to study fair allocation of mitigation resources. Moreover,
the network has random dynamics which change over time. Therefore, we introduce
a stochastic and non-stationary knapsack problem, and we apply its resolution
to mitigate misinformation in social network campaigns. We further propose a
generic misinformation mitigation algorithm that is robust to different social
networks' misinformation statistics, allowing a promising impact in real-world
scenarios. A novel loss function ensures fair mitigation among users. We
achieve fairness by intelligently allocating a mitigation incentivization
budget to the knapsack, and optimizing the loss function. To this end, a team
of Learning Automata (LA) drives the budget allocation. Each LA is associated
with a user and learns to minimize its exposure to misinformation by performing
a non-stationary and stochastic walk over its state space. Our results show how
our LA-based method is robust and outperforms similar misinformation mitigation
methods in how the mitigation is fairly influencing the network users.","['Ahmed Abouzeid', 'Ole-Christoffer Granmo', 'Christian Webersik', 'Morten Goodwin']",2,0.7458949
"Our opinions and views of life can be shaped by how we perceive the opinions
of others on social media like Facebook. This dependence has increased during
COVID-19 periods when we have fewer means to connect with others. However, fake
news related to COVID-19 has become a significant problem on Facebook. Bengali
is the seventh most spoken language worldwide, yet we are aware of no previous
research that studied the prevalence of COVID-19 related fake news in Bengali
on Facebook. In this paper, we develop machine learning models to detect fake
news in Bengali automatically. The best performing model is BERT, with an
F1-score of 0.97. We apply BERT on all Facebook Bengali posts related to
COVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into
three categories: System (e.g., medical system), belief (e.g., religious
rituals), and social (e.g., scientific awareness).","['Protik Bose Pranto', 'Syed Zami-Ul-Haque Navid', 'Protik Dey', 'Gias Uddin', 'Anindya Iqbal']",5,0.5646203
"Studying misinformation and how to deal with unhealthy behaviours within
online discussions has recently become an important field of research within
social studies. With the rapid development of social media, and the increasing
amount of available information and sources, rigorous manual analysis of such
discourses has become unfeasible. Many approaches tackle the issue by studying
the semantic and syntactic properties of discussions following a supervised
approach, for example using natural language processing on a dataset labeled
for abusive, fake or bot-generated content. Solutions based on the existence of
a ground truth are limited to those domains which may have ground truth.
However, within the context of misinformation, it may be difficult or even
impossible to assign labels to instances. In this context, we consider the use
of temporal dynamic patterns as an indicator of discussion health. Working in a
domain for which ground truth was unavailable at the time (early COVID-19
pandemic discussions) we explore the characterization of discussions based on
the the volume and time of contributions. First we explore the types of
discussions in an unsupervised manner, and then characterize these types using
the concept of ephemerality, which we formalize. In the end, we discuss the
potential use of our ephemerality definition for labeling online discourses
based on how desirable, healthy and constructive they are.","['Dmitry Gnatyshak', 'Dario Garcia-Gasulla', 'Sergio Alvarez-Napagao', 'Jamie Arjona', 'Tommaso Venturini']",0,0.6531288
"Fake news detection is crucial for preventing the dissemination of
misinformation on social media. To differentiate fake news from real ones,
existing methods observe the language patterns of the news post and ""zoom in""
to verify its content with knowledge sources or check its readers' replies.
However, these methods neglect the information in the external news environment
where a fake news post is created and disseminated. The news environment
represents recent mainstream media opinion and public attention, which is an
important inspiration of fake news fabrication because fake news is often
designed to ride the wave of popular events and catch public attention with
unexpected novel content for greater exposure and spread. To capture the
environmental signals of news posts, we ""zoom out"" to observe the news
environment and propose the News Environment Perception Framework (NEP). For
each post, we construct its macro and micro news environment from recent
mainstream news. Then we design a popularity-oriented and a novelty-oriented
module to perceive useful signals and further assist final prediction.
Experiments on our newly built datasets show that the NEP can efficiently
improve the performance of basic fake news detectors.","['Qiang Sheng', 'Juan Cao', 'Xueyao Zhang', 'Rundong Li', 'Danding Wang', 'Yongchun Zhu']",4,0.7883371
"We propose that social-media users' own post histories are an underused yet
valuable resource for studying fake-news sharing. By extracting textual cues
from their prior posts, and contrasting their prevalence against random
social-media users and others (e.g., those with similar socio-demographics,
political news-sharers, and fact-check sharers), researchers can identify cues
that distinguish fake-news sharers, predict those most likely to share fake
news, and identify promising constructs to build interventions. Our research
includes studies along these lines. In Study 1, we explore the distinctive
language patterns of fake-news sharers, highlighting elements such as their
higher use of anger and power-related words. In Study 2, we show that adding
textual cues into predictive models enhances their accuracy in predicting
fake-news sharers. In Study 3, we explore the contrasting role of trait and
situational anger, and show trait anger is associated with a greater propensity
to share both true and fake news. In Study 4, we introduce a way to
authenticate Twitter accounts in surveys, before using it to explore how
crafting an ad copy that resonates with users' sense of power encourages the
adoption of fact-checking tools. We hope to encourage the use of novel research
methods for marketers and misinformation researchers.","['Verena Schoenmueller', 'Simon J. Blanchard', 'Gita V. Johar']",4,0.795995
"In this work, the integration of two machine learning approaches, namely
domain adaptation and explainable AI, is proposed to address these two issues
of generalized detection and explainability. Firstly the Domain Adversarial
Neural Network (DANN) develops a generalized misinformation detector across
multiple social media platforms DANN is employed to generate the classification
results for test domains with relevant but unseen data. The DANN-based model, a
traditional black-box model, cannot justify its outcome, i.e., the labels for
the target domain. Hence a Local Interpretable Model-Agnostic Explanations
(LIME) explainable AI model is applied to explain the outcome of the DANN mode.
To demonstrate these two approaches and their integration for effective
explainable generalized detection, COVID-19 misinformation is considered a case
study. We experimented with two datasets, namely CoAID and MiSoVac, and
compared results with and without DANN implementation. DANN significantly
improves the accuracy measure F1 classification score and increases the
accuracy and AUC performance. The results obtained show that the proposed
framework performs well in the case of domain shift and can learn
domain-invariant features while explaining the target labels with LIME
implementation enabling trustworthy information processing and extraction to
combat misinformation effectively.","['Gargi Joshi', 'Ananya Srivastava', 'Bhargav Yagnik', 'Mohammed Hasan', 'Zainuddin Saiyed', 'Lubna A Gabralla', 'Ajith Abraham', 'Rahee Walambe', 'Ketan Kotecha']",1,0.6468308
"Due to the evolution of the Web and social network platforms it becomes very
easy to disseminate the information. Peoples are creating and sharing more
information than ever before, which may be misleading, misinformation or fake
information. Fake news detection is a crucial and challenging task due to the
unstructured nature of the available information. In the recent years,
researchers have provided significant solutions to tackle with the problem of
fake news detection, but due to its nature there are still many open issues. In
this paper, we have proposed majority voting approach to detect fake news
articles. We have used different textual properties of fake and real news. We
have used publicly available fake news dataset, comprising of 20,800 news
articles among which 10,387 are real and 10,413 are fake news labeled as binary
0 and 1. For the evaluation of our approach, we have used commonly used machine
learning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random
Forest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the
aforementioned classifiers, we built a multi-model fake news detection system
using Majority Voting technique to achieve the more accurate results. The
experimental results show that, our proposed approach achieved accuracy of
96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation
confirms that, Majority Voting technique achieved more acceptable results as
compare to individual learning technique.",['Dharmaraj R. Patil'],4,0.82219774
"Databases of highly networked individuals have been indispensable in studying
narratives and influence on social media. To support studies on Twitter in
India, we present a systematically categorised database of accounts of
influence on Twitter in India, identified and annotated through an iterative
process of friends, networks, and self-described profile information, verified
manually. We built an initial set of accounts based on the friend network of a
seed set of accounts based on real-world renown in various fields, and then
snowballed ""friends of friends"" multiple times, and rank ordered individuals
based on the number of in-group connections, and overall followers. We then
manually classified identified accounts under the categories of entertainment,
sports, business, government, institutions, journalism, civil society accounts
that have independent standing outside of social media, as well as a category
of ""digital first"" referring to accounts that derive their primary influence
from online activity. Overall, we annotated 11580 unique accounts across all
categories. The database is useful studying various questions related to the
role of influencers in polarisation, misinformation, extreme speech, political
discourse etc.","['Arshia Arya', 'Soham De', 'Dibyendu Mishra', 'Gazal Shekhawat', 'Ankur Sharma', 'Anmol Panda', 'Faisal Lalani', 'Parantak Singh', 'Ramaravind Kommiya Mothilal', 'Rynaa Grover', 'Sachita Nishal', 'Saloni Dash', 'Shehla Shora', 'Syeda Zainab Akbar', 'Joyojeet Pal']",3,0.6439506
"Since recommender systems have been created and developed to automate the
recommendation process, users can easily consume their desired video content on
online platforms. In this line, several content recommendation algorithms are
introduced and employed to allow users to encounter content of their interests,
effectively. However, the recommendation systems sometimes regrettably
recommend inappropriate content, including misinformation or fake news. To make
it worse, people would unreservedly accept such content due to their cognitive
heuristic, machine heuristic, which is the rule of thumb that machines are more
accurate and trustworthy than humans. In this study, we designed and conducted
a web-based experiment where the participants are invoked machine heuristic by
experiencing the whole process of machine or human recommendation system. The
results demonstrated that participants (N = 89) showed a more positive attitude
toward a machine recommender than a human recommender, even the recommended
videos contain inappropriate content. While participants who have a high level
of trust in machines exhibited a negative attitude toward recommendations.
Based on these results, we suggest that a phenomenon known as algorithm
aversion might be simultaneously considered with machine heuristic in
investigating human interaction with a machine.","['Soyoung Oh', 'Eunil Park']",11,0.49405712
"It is challenging to control the quality of online information due to the
lack of supervision over all the information posted online. Manual checking is
almost impossible given the vast number of posts made on online media and how
quickly they spread. Therefore, there is a need for automated rumour detection
techniques to limit the adverse effects of spreading misinformation. Previous
studies mainly focused on finding and extracting the significant features of
text data. However, extracting features is time-consuming and not a highly
effective process. This study proposes the BERT- based pre-trained language
models to encode text data into vectors and utilise neural network models to
classify these vectors to detect misinformation. Furthermore, different
language models (LM) ' performance with different trainable parameters was
compared. The proposed technique is tested on different short and long text
datasets. The result of the proposed technique has been compared with the
state-of-the-art techniques on the same datasets. The results show that the
proposed technique performs better than the state-of-the-art techniques. We
also tested the proposed technique by combining the datasets. The results
demonstrated that the large data training and testing size considerably
improves the technique's performance.","['Rini Anggrainingsih', 'Ghulam Mubashar Hassan', 'Amitava Datta']",1,0.6990403
"In this paper, we present the fifth installment of the NELA-GT datasets,
NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between
January 1st, 2022 and December 31st, 2022. Just as in past releases of the
dataset, NELA-GT-2022 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H","['Maur√≠cio Gruppi', 'Benjamin D. Horne', 'Sibel Adalƒ±']",5,0.37267485
"Online social networks such as Twitter and Weibo play an important role in
how people stay informed and exchange reactions. Each crisis encompasses a new
opportunity to study the portability of models for various tasks (e.g.,
information extraction, complex event understanding, misinformation detection,
etc.), due to differences in domain, entities, and event types. We present the
Russia-Ukraine Crisis Weibo (RUW) dataset, with over 3.5M user posts and
comments in the first release. Our data is available at
https://github.com/yrf1/RussiaUkraine_weibo_dataset.","['Yi R. Fung', 'Heng Ji']",0,0.5766268
"Retracted research discussed on social media can spread misinformation. Yet
we lack an understanding of how retracted articles are mentioned by academic
and non-academic users. This is especially relevant on Twitter due to the
platform's prominent role in science communication. Here, we analyze the pre-
and post-retraction differences in Twitter attention and engagement metrics for
over 3,800 retracted English-language articles alongside comparable
non-retracted articles. We subset these findings according to five user types
detected by our supervised learning classifier: members of the public,
academics, bots, science practitioners, and science communicators. We find that
retracted articles receive greater user attention (tweet count) and engagement
(likes, retweets, and replies) than non-retracted articles, especially among
members of the public and bots, with the majority of user engagement happening
before retraction. Our results highlight the prominent role of non-experts in
discussions of retracted research and suggest an opportunity for social media
platforms to contribute towards early detection of problematic scientific
research online.","['Henry K. Dambanemuya', 'Rod Abhari', 'Nicholas Vincent', 'Em≈ëke-√Ågnes Horv√°t']",3,0.75375175
"Fighting the ongoing COVID-19 infodemic has been declared as one of the most
important focus areas by the World Health Organization since the onset of the
COVID-19 pandemic. While the information that is consumed and disseminated
consists of promoting fake cures, rumors, and conspiracy theories to spreading
xenophobia and panic, at the same time there is information (e.g., containing
advice, promoting cure) that can help different stakeholders such as
policy-makers. Social media platforms enable the infodemic and there has been
an effort to curate the content on such platforms, analyze and debunk them.
While a majority of the research efforts consider one or two aspects (e.g.,
detecting factuality) of such information, in this study we focus on a
multifaceted approach, including an
API,\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo
system,\url{https://covid19.tanbih.org}, which we made freely and publicly
available. We believe that this will facilitate researchers and different
stakeholders. A screencast of the API services and demo is
available.\url{https://youtu.be/zhbcSvxEKMk}","['Preslav Nakov', 'Firoj Alam', 'Yifan Zhang', 'Animesh Prakash', 'Fahim Dalvi']",5,0.7628478
"Online Social Networks (OSNs) play a significant role in information sharing
during a crisis. The data collected during such a crisis can reflect the large
scale public opinions and sentiment. In addition, OSN data can also be used to
study different campaigns that are employed by various entities to engineer
public opinions. Such information sharing campaigns can range from spreading
factual information to propaganda and misinformation. We provide a Twitter
dataset of the 2022 Russo-Ukrainian conflict. In the first release, we share
over 1.6 million tweets shared during the 1st week of the crisis.","['Ehsan-Ul Haq', 'Gareth Tyson', 'Lik-Hang Lee', 'Tristan Braud', 'Pan Hui']",10,0.63279855
"Health misinformation is believed to have contributed to vaccine hesitancy
during the Covid-19 pandemic, highlighting concerns about the role of social
media in polarization and social stability. While previous research has
identified a link between political partisanship and misinformation sharing
online, the interaction between partisanship and how much misinformation people
see within their social networks has not been well studied. As a result, we do
not know whether partisanship drives exposure to misinformation or people
selectively share misinformation despite being exposed to factual content. We
study Twitter discussions about the Covid-19 pandemic, classifying users
ideologically along political and factual dimensions. We find partisan
asymmetries in both sharing behaviors and exposure, with conservatives more
likely to see and share misinformation and moderate liberals seeing the most
factual content. We identify multi-dimensional echo chambers that expose users
to ideologically congruent content; however, the interaction between political
and factual dimensions creates conditions for the highly polarized users --
hardline conservatives and liberals -- to amplify misinformation. Despite this,
misinformation receives less attention than factual content and political
moderates, who represent the bulk of users in our sample, help filter out
misinformation, reducing the amount of low factuality content in the
information ecosystem. Identifying the extent of polarization and how political
ideology can exacerbate misinformation can potentially help public health
experts and policy makers improve their messaging to promote consensus.","['Ashwin Rao', 'Fred Morstatter', 'Kristina Lerman']",3,0.774356
"Hyper-partisan misinformation has become a major public concern. In order to
examine what type of misinformation label can mitigate hyper-partisan
misinformation sharing on social media, we conducted a 4 (label type:
algorithm, community, third-party fact-checker, and no label) X 2 (post
ideology: liberal vs. conservative) between-subjects online experiment (N =
1,677) in the context of COVID-19 health information. The results suggest that
for liberal users, all labels reduced the perceived accuracy and believability
of fake posts regardless of the posts' ideology. In contrast, for conservative
users, the efficacy of the labels depended on whether the posts were
ideologically consistent: algorithmic labels were more effective in reducing
the perceived accuracy and believability of fake conservative posts compared to
community labels, whereas all labels were effective in reducing their belief in
liberal posts. Our results shed light on the differing effects of various
misinformation labels dependent on people's political ideology.","['Chenyan Jia', 'Alexander Boltz', 'Angie Zhang', 'Anqing Chen', 'Min Kyung Lee']",3,0.60556626
"Reasoning is central to human intelligence. However, fallacious arguments are
common, and some exacerbate problems such as spreading misinformation about
climate change. In this paper, we propose the task of logical fallacy
detection, and provide a new dataset (Logic) of logical fallacies generally
found in text, together with an additional challenge set for detecting logical
fallacies in climate change claims (LogicClimate). Detecting logical fallacies
is a hard problem as the model must understand the underlying logical structure
of the argument. We find that existing pretrained large language models perform
poorly on this task. In contrast, we show that a simple structure-aware
classifier outperforms the best language model by 5.46% on Logic and 4.51% on
LogicClimate. We encourage future work to explore this task as (a) it can serve
as a new reasoning challenge for language models, and (b) it can have potential
applications in tackling the spread of misinformation. Our dataset and code are
available at https://github.com/causalNLP/logical-fallacy","['Zhijing Jin', 'Abhinav Lalwani', 'Tejas Vaidhya', 'Xiaoyu Shen', 'Yiwen Ding', 'Zhiheng Lyu', 'Mrinmaya Sachan', 'Rada Mihalcea', 'Bernhard Sch√∂lkopf']",1,0.56406695
"A core feature of complex systems is that the interactions between elements
in the present causally constrain each-other as the system evolves through
time. To fully model all of these interactions (between elements, as well as
ensembles of elements), we can decompose the total information flowing from
past to future into a set of non-overlapping temporal interactions that
describe all the different modes by which information can flow. To achieve
this, we propose a novel information-theoretic measure of temporal dependency
($I_{\tau sx}$) based on informative and misinformative local probability mass
exclusions. To demonstrate the utility of this framework, we apply the
decomposition to spontaneous spiking activity recorded from dissociated neural
cultures of rat cerebral cortex to show how different modes of information
processing are distributed over the system. Furthermore, being a localizable
analysis, we show that $I_{\tau sx}$ can provide insight into the computational
structure of single moments. We explore the time-resolved computational
structure of neuronal avalanches and find that different types of information
atoms have distinct profiles over the course of an avalanche, with the majority
of non-trivial information dynamics happening before the first half of the
cascade is completed. These analyses allow us to move beyond the historical
focus on single measures of dependency such as information transfer or
information integration, and explore a panoply of different relationships
between elements (and groups of elements) in complex systems.",['Thomas F. Varley'],2,0.6937376
"Recent advances in technology for hyper-realistic visual and audio effects
provoke the concern that deepfake videos of political speeches will soon be
indistinguishable from authentic video recordings. The conventional wisdom in
communication theory predicts people will fall for fake news more often when
the same version of a story is presented as a video versus text. We conduct 5
pre-registered randomized experiments with 2,215 participants to evaluate how
accurately humans distinguish real political speeches from fabrications across
base rates of misinformation, audio sources, question framings, and media
modalities. We find base rates of misinformation minimally influence
discernment and deepfakes with audio produced by the state-of-the-art
text-to-speech algorithms are harder to discern than the same deepfakes with
voice actor audio. Moreover across all experiments, we find audio and visual
information enables more accurate discernment than text alone: human
discernment relies more on how something is said, the audio-visual cues, than
what is said, the speech content.","['Matthew Groh', 'Aruna Sankaranarayanan', 'Nikhil Singh', 'Dong Young Kim', 'Andrew Lippman', 'Rosalind Picard']",4,0.66677547
"Image attribution -- matching an image back to a trusted source -- is an
emerging tool in the fight against online misinformation. Deep visual
fingerprinting models have recently been explored for this purpose. However,
they are not robust to tiny input perturbations known as adversarial examples.
First we illustrate how to generate valid adversarial images that can easily
cause incorrect image attribution. Then we describe an approach to prevent
imperceptible adversarial attacks on deep visual fingerprinting models, via
robust contrastive learning. The proposed training procedure leverages training
on $\ell_\infty$-bounded adversarial examples, it is conceptually simple and
incurs only a small computational overhead. The resulting models are
substantially more robust, are accurate even on unperturbed images, and perform
well even over a database with millions of images. In particular, we achieve
91.6% standard and 85.1% adversarial recall under $\ell_\infty$-bounded
perturbations on manipulated images compared to 80.1% and 0.0% from prior work.
We also show that robustness generalizes to other types of imperceptible
perturbations unseen during training. Finally, we show how to train an
adversarially robust image comparator model for detecting editorial changes in
matched images.","['Maksym Andriushchenko', 'Xiaoyang Rebecca Li', 'Geoffrey Oxholm', 'Thomas Gittings', 'Tu Bui', 'Nicolas Flammarion', 'John Collomosse']",7,0.73900807
"Malicious accounts spreading misinformation has led to widespread false and
misleading narratives in recent times, especially during the COVID-19 pandemic,
and social media platforms struggle to eliminate these contents rapidly. This
is because adapting to new domains requires human intensive fact-checking that
is slow and difficult to scale. To address this challenge, we propose to
leverage news-source credibility labels as weak labels for social media posts
and propose model-guided refinement of labels to construct large-scale, diverse
misinformation labeled datasets in new domains. The weak labels can be
inaccurate at the article or social media post level where the stance of the
user does not align with the news source or article credibility. We propose a
framework to use a detection model self-trained on the initial weak labels with
uncertainty sampling based on entropy in predictions of the model to identify
potentially inaccurate labels and correct for them using self-supervision or
relabeling. The framework will incorporate social context of the post in terms
of the community of its associated user for surfacing inaccurate labels towards
building a large-scale dataset with minimum human effort. To provide labeled
datasets with distinction of misleading narratives where information might be
missing significant context or has inaccurate ancillary details, the proposed
framework will use the few labeled samples as class prototypes to separate high
confidence samples into false, unproven, mixture, mostly false, mostly true,
true, and debunk information. The approach is demonstrated for providing a
large-scale misinformation dataset on COVID-19 vaccines.","['Karishma Sharma', 'Emilio Ferrara', 'Yan Liu']",4,0.6830709
"Misinformation is becoming increasingly prevalent on social media and in news
articles. It has become so widespread that we require algorithmic assistance
utilising machine learning to detect such content. Training these machine
learning models require datasets of sufficient scale, diversity and quality.
However, datasets in the field of automatic misinformation detection are
predominantly monolingual, include a limited amount of modalities and are not
of sufficient scale and quality. Addressing this, we develop a data collection
and linking system (MuMiN-trawl), to build a public misinformation graph
dataset (MuMiN), containing rich social media data (tweets, replies, users,
images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand
Twitter threads, each of which have been semantically linked to 13 thousand
fact-checked claims across dozens of topics, events and domains, in 41
different languages, spanning more than a decade. The dataset is made available
as a heterogeneous graph via a Python package (mumin). We provide baseline
results for two node classification tasks related to the veracity of a claim
involving social media, and demonstrate that these are challenging tasks, with
the highest macro-average F1-score being 62.55% and 61.45% for the two tasks,
respectively. The MuMiN ecosystem is available at
https://mumin-dataset.github.io/, including the data, documentation, tutorials
and leaderboards.","['Dan Saattrup Nielsen', 'Ryan McConville']",8,0.70365405
"Billions of COVID-19 vaccines have been administered, but many remain
hesitant. Misinformation about the COVID-19 vaccines and other vaccines,
propagating on social media, is believed to drive hesitancy towards
vaccination. The ability to automatically recognize misinformation targeting
vaccines on Twitter depends on the availability of data resources. In this
paper we present VaccineLies, a large collection of tweets propagating
misinformation about two vaccines: the COVID-19 vaccines and the Human
Papillomavirus (HPV) vaccines. Misinformation targets are organized in
vaccine-specific taxonomies, which reveal the misinformation themes and
concerns. The ontological commitments of the Misinformation taxonomies provide
an understanding of which misinformation themes and concerns dominate the
discourse about the two vaccines covered in VaccineLies. The organization into
training, testing and development sets of VaccineLies invites the development
of novel supervised methods for detecting misinformation on Twitter and
identifying the stance towards it. Furthermore, VaccineLies can be a stepping
stone for the development of datasets focusing on misinformation targeting
additional vaccines.","['Maxwell Weinzierl', 'Sanda Harabagiu']",12,0.832738
"Although billions of COVID-19 vaccines have been administered, too many
people remain hesitant. Misinformation about the COVID-19 vaccines, propagating
on social media, is believed to drive hesitancy towards vaccination. However,
exposure to misinformation does not necessarily indicate misinformation
adoption. In this paper we describe a novel framework for identifying the
stance towards misinformation, relying on attitude consistency and its
properties. The interactions between attitude consistency, adoption or
rejection of misinformation and the content of microblogs are exploited in a
novel neural architecture, where the stance towards misinformation is organized
in a knowledge graph. This new neural framework is enabling the identification
of stance towards misinformation about COVID-19 vaccines with state-of-the-art
results. The experiments are performed on a new dataset of misinformation
towards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter
discourse. Because CoVaxLies provides a taxonomy of the misinformation about
COVID-19 vaccines, we are able to show which type of misinformation is mostly
adopted and which is mostly rejected.","['Maxwell Weinzierl', 'Sanda Harabagiu']",12,0.8373405
"Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.","['Yibing Du', 'Antoine Bosselut', 'Christopher D. Manning']",1,0.8048966
"Online forums that allow participatory engagement between users have been
transformative for public discussion of important issues. However, debates on
such forums can sometimes escalate into full blown exchanges of hate or
misinformation. An important tool in understanding and tackling such problems
is to be able to infer the argumentative relation of whether a reply is
supporting or attacking the post it is replying to. This so called polarity
prediction task is difficult because replies may be based on external context
beyond a post and the reply whose polarity is being predicted. We propose
GraphNLI, a novel graph-based deep learning architecture that uses graph walk
techniques to capture the wider context of a discussion thread in a principled
fashion. Specifically, we propose methods to perform root-seeking graph walks
that start from a post and captures its surrounding context to generate
additional embeddings for the post. We then use these embeddings to predict the
polarity relation between a reply and the post it is replying to. We evaluate
the performance of our models on a curated debate dataset from Kialo, an online
debating platform. Our model outperforms relevant baselines, including S-BERT,
with an overall accuracy of 83%.","['Vibhor Agarwal', 'Sagar Joglekar', 'Anthony P. Young', 'Nishanth Sastry']",2,0.60373807
"With the growing adoption of short-form video by social media platforms,
reducing the spread of misinformation through video posts has become a critical
challenge for social media providers. In this paper, we develop methods to
detect misinformation in social media posts, exploiting modalities such as
video and text. Due to the lack of large-scale public data for misinformation
detection in multi-modal datasets, we collect 160,000 video posts from Twitter,
and leverage self-supervised learning to learn expressive representations of
joint visual and textual data. In this work, we propose two new methods for
detecting semantic inconsistencies within short-form social media video posts,
based on contrastive learning and masked language modeling. We demonstrate that
our new approaches outperform current state-of-the-art methods on both
artificial data generated by random-swapping of positive samples and in the
wild on a new manually-labeled test set for semantic misinformation.","['Kehan Wang', 'David Chan', 'Seth Z. Zhao', 'John Canny', 'Avideh Zakhor']",4,0.732627
"This paper describes the work of the Data Science for Digital Health (DS4DH)
group at the TREC Health Misinformation Track 2021. The TREC Health
Misinformation track focused on the development of retrieval methods that
provide relevant, correct and credible information for health related searches
on the Web. In our methodology, we used a two-step ranking approach that
includes i) a standard retrieval phase, based on BM25 model, and ii) a
re-ranking phase, with a pipeline of models focused on the usefulness,
supportiveness and credibility dimensions of the retrieved documents. To
estimate the usefulness, we classified the initial rank list using pre-trained
language models based on the transformers architecture fine-tuned on the MS
MARCO corpus. To assess the supportiveness, we utilized BERT-based models
fine-tuned on scientific and Wikipedia corpora. Finally, to evaluate the
credibility of the documents, we employed a random forest model trained on the
Microsoft Credibility dataset combined with a list of credible sites. The
resulting ranked lists were then combined using the Reciprocal Rank Fusion
algorithm to obtain the final list of useful, supporting and credible
documents. Our approach achieved competitive results, being top-2 in the
compatibility measurement for the automatic runs. Our findings suggest that
integrating automatic ranking models created for each information quality
dimension with transfer learning can increase the effectiveness of
health-related information retrieval.","['Boya Zhang', 'Nona Naderi', 'Fernando Jaume-Santero', 'Douglas Teodoro']",1,0.6682192
"Fake news is an age-old phenomenon, widely assumed to be associated with
political propaganda published to sway public opinion. Yet, with the growth of
social media, it has become a lucrative business for Web publishers. Despite
many studies performed and countermeasures proposed, unreliable news sites have
increased in the last years their share of engagement among the top performing
news sources. Stifling fake news impact depends on our efforts in limiting the
(economic) incentives of fake news producers.
  In this paper, we aim at enhancing the transparency around these exact
incentives, and explore: Who supports the existence of fake news websites via
paid ads, either as an advertiser or an ad seller? Who owns these websites and
what other Web business are they into? We are the first to systematize the
auditing process of fake news revenue flows. We identify the companies that
advertise in fake news websites and the intermediary companies responsible for
facilitating those ad revenues. We study more than 2,400 popular news websites
and show that well-known ad networks, such as Google and IndexExchange, have a
direct advertising relation with more than 40% of fake news websites. Using a
graph clustering approach on 114.5K sites, we show that entities who own fake
news sites, also operate other types of websites pointing to the fact that
owning a fake news website is part of a broader business operation.","['Emmanouil Papadogiannakis', 'Panagiotis Papadopoulos', 'Evangelos P. Markatos', 'Nicolas Kourtellis']",4,0.7367485
"Instant messaging platforms such as Telegram became one of the main means of
communication used by people all over the world. Most of them are home of
several groups and channels that connect thousands of people focused on
political topics. However, they have suffered with misinformation campaigns
with a direct impact on electoral processes around the world. While some
platforms, such as WhatsApp, took restrictive policies and measures to
attenuate the issues arising from the abuse of their systems, others have
emerged as alternatives, presenting little or no restrictions on content
moderation or actions in combating misinformation. Telegram is one of those
systems, which has been attracting more users and gaining popularity. In this
work, we present the ""Telegram Monitor"", a web-based system that monitors the
political debate in this environment and enables the analysis of the most
shared content in multiple channels and public groups. Our system aims to allow
journalists, researchers, and fact-checking agencies to identify trending
conspiracy theories, misinformation campaigns, or simply to monitor the
political debate in this space along the 2022 Brazilian elections. We hope our
system can assist the combat of misinformation spreading through Telegram in
Brazil.","['Manoel J√∫nior', 'Philipe Melo', 'Daniel Kansaon', 'Vitor Mafra', 'Kaio S√°', 'Fabr√≠cio Benevenuto']",10,0.7203256
"In this brief, we show that sequentially learning new information presented
to a continual (incremental) learning model introduces new security risks: an
intelligent adversary can introduce small amount of misinformation to the model
during training to cause deliberate forgetting of a specific task or class at
test time, thus creating ""false memory"" about that task. We demonstrate such an
adversary's ability to assume control of the model by injecting ""backdoor""
attack samples to commonly used generative replay and regularization based
continual learning approaches using continual learning benchmark variants of
MNIST, as well as the more challenging SVHN and CIFAR 10 datasets. Perhaps most
damaging, we show this vulnerability to be very acute and exceptionally
effective: the backdoor pattern in our attack model can be imperceptible to
human eye, can be provided at any point in time, can be added into the training
data of even a single possibly unrelated task and can be achieved with as few
as just 1\% of total training dataset of a single task.","['Muhammad Umer', 'Robi Polikar']",2,0.6188694
"While false rumors pose a threat to the successful overcoming of the COVID-19
pandemic, an understanding of how rumors diffuse in online social networks is -
even for non-crisis situations - still in its infancy. Here we analyze a large
sample consisting of COVID-19 rumor cascades from Twitter that have been
fact-checked by third-party organizations. The data comprises N=10,610 rumor
cascades that have been retweeted more than 24 million times. We investigate
whether COVID-19 misinformation spreads more viral than the truth and whether
the differences in the diffusion of true vs. false rumors can be explained by
the moral emotions they carry. We observe that, on average, COVID-19
misinformation is more likely to go viral than truthful information. However,
the veracity effect is moderated by moral emotions: false rumors are more viral
than the truth if the source tweets embed a high number of other-condemning
emotion words, whereas a higher number of self-conscious emotion words is
linked to a less viral spread. The effects are pronounced both for health
misinformation and false political rumors. These findings offer insights into
how true vs. false rumors spread and highlight the importance of considering
emotions from the moral emotion families in social media content.","['Kirill Solovev', 'Nicolas Pr√∂llochs']",5,0.6350151
"Whose labels should a machine learning (ML) algorithm learn to emulate? For
ML tasks ranging from online comment toxicity to misinformation detection to
medical diagnosis, different groups in society may have irreconcilable
disagreements about ground truth labels. Supervised ML today resolves these
label disagreements implicitly using majority vote, which overrides minority
groups' labels. We introduce jury learning, a supervised ML approach that
resolves these disagreements explicitly through the metaphor of a jury:
defining which people or groups, in what proportion, determine the classifier's
prediction. For example, a jury learning model for online toxicity might
centrally feature women and Black jurors, who are commonly targets of online
harassment. To enable jury learning, we contribute a deep learning architecture
that models every annotator in a dataset, samples from annotators' models to
populate the jury, then runs inference to classify. Our architecture enables
juries that dynamically adapt their composition, explore counterfactuals, and
visualize dissent.","['Mitchell L. Gordon', 'Michelle S. Lam', 'Joon Sung Park', 'Kayur Patel', 'Jeffrey T. Hancock', 'Tatsunori Hashimoto', 'Michael S. Bernstein']",6,0.57532096
"Exponential growth in digital information outlets and the race to publish has
made scientific misinformation more prevalent than ever. However, the task to
fact-verify a given scientific claim is not straightforward even for
researchers. Scientific claim verification requires in-depth knowledge and
great labor from domain experts to substantiate supporting and refuting
evidence from credible scientific sources. The SciFact dataset and
corresponding task provide a benchmarking leaderboard to the community to
develop automatic scientific claim verification systems via extracting and
assimilating relevant evidence rationales from source abstracts. In this work,
we propose a modular approach that sequentially carries out binary
classification for every prediction subtask as in the SciFact leaderboard. Our
simple classifier-based approach uses reduced abstract representations to
retrieve relevant abstracts. These are further used to train the relevant
rationale-selection model. Finally, we carry out two-step stance predictions
that first differentiate non-relevant rationales and then identify supporting
or refuting rationales for a given claim. Experimentally, our system RerrFact
with no fine-tuning, simple design, and a fraction of model parameters fairs
competitively on the leaderboard against large-scale, modular, and joint
modeling approaches. We make our codebase available at
https://github.com/ashishrana160796/RerrFact.","['Ashish Rana', 'Deepanshu Khanna', 'Tirthankar Ghosal', 'Muskaan Singh', 'Harpreet Singh', 'Prashant Singh Rana']",1,0.7658764
"Misinformation promotes distrust in science, undermines public health, and
may drive civil unrest. Vaccine misinformation, in particular, has stalled
efforts to overcome the COVID-19 pandemic, prompting social media platforms'
attempts to reduce it. Some have questioned whether ""soft"" content moderation
remedies -- e.g., flagging and downranking misinformation -- were successful,
suggesting that the addition of ""hard"" content remedies -- e.g., deplatforming
and content bans -- is necessary. We therefore examined whether Facebook's
vaccine misinformation content removal policies were effective. Here, we show
that Facebook's policies reduced the number of anti-vaccine posts but also
caused several perverse effects: pro-vaccine content was also removed,
engagement with remaining anti-vaccine content repeatedly recovered to
pre-policy levels, and this content became more misinformative, more
politically polarised, and more likely to be seen in users' newsfeeds. We
explain these results as an unintended consequence of Facebook's design goal:
promoting community formation. Members of communities dedicated to vaccine
refusal appear to seek out misinformation from multiple sources. Community
administrators make use of several channels afforded by the Facebook platform
to disseminate misinformation. Our findings suggest the need to address how
social media platform architecture enables community formation and mobilisation
around misinformative topics when managing the spread of online content.","['David A. Broniatowski', 'Jiayan Gu', 'Amelia M. Jamison', 'Joseph R. Simons', 'Lorien C. Abroms']",12,0.754674
"To reduce the spread of misinformation, social media platforms may take
enforcement actions against offending content, such as adding informational
warning labels, reducing distribution, or removing content entirely. However,
both their actions and their inactions have been controversial and plagued by
allegations of partisan bias. When it comes to specific content items,
surprisingly little is known about what ordinary people want the platforms to
do. We provide empirical evidence about a politically balanced panel of lay
raters' preferences for three potential platform actions on 368 news articles.
Our results confirm that on many articles there is a lack of consensus on which
actions to take. We find a clear hierarchy of perceived severity of actions
with a majority of raters wanting informational labels on the most articles and
removal on the fewest. There was no partisan difference in terms of how many
articles deserve platform actions but conservatives did prefer somewhat more
action on content from liberal sources, and vice versa. We also find that
judgments about two holistic properties, misleadingness and harm, could serve
as an effective proxy to determine what actions would be approved by a majority
of raters.","['Shubham Atreja', 'Libby Hemphill', 'Paul Resnick']",10,0.6387633
"The spread of misinformation poses a threat to the social media ecosystem.
Effective countermeasures to mitigate this threat require that social media
platforms be able to accurately detect low-credibility accounts even before the
content they share can be classified as misinformation. Here we present methods
to infer account credibility from information diffusion patterns, in particular
leveraging two networks: the reshare network, capturing an account's trust in
other accounts, and the bipartite account-source network, capturing an
account's trust in media sources. We extend network centrality measures and
graph embedding techniques, systematically comparing these algorithms on data
from diverse contexts and social media platforms. We demonstrate that both
kinds of trust networks provide useful signals for estimating account
credibility. Some of the proposed methods yield high accuracy, providing
promising solutions to promote the dissemination of reliable information in
online communities. Two kinds of homophily emerge from our results: accounts
tend to have similar credibility if they reshare each other's content or share
content from similar sources. Our methodology invites further investigation
into the relationship between accounts and news sources to better characterize
misinformation spreaders.","['Bao Tran Truong', 'Oliver Melbourne Allen', 'Filippo Menczer']",3,0.6606364
"Identifying fake news is a very difficult task, especially when considering
the multiple modes of conveying information through text, image, video and/or
audio. We attempted to tackle the problem of automated
misinformation/disinformation detection in multi-modal news sources (including
text and images) through our simple, yet effective, approach in the FACTIFY
shared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of
74.807%, which was the fourth best out of all the submissions. In this paper we
will explain our approach to undertake the shared task.","['Abhishek Dhankar', 'Osmar R. Za√Øane', 'Francois Bolduc']",4,0.6475066
"We propose a stochastic model of opinion exchange in networks. A finite set
of agents is organized in a fixed network structure. There is a binary state of
the world and each agent receives a private signal on the state. We model
beliefs as urns where red balls represent one possible value of the state and
blue balls the other value. The model revolves purely around communication and
beliefs dynamics. Communication happens in discrete time and, at each period,
agents draw and display one ball from their urn with replacement. Then, they
reinforce their urns by adding balls of the colors drawn by their neighbors. We
show that for any network structure, this process converges almost-surely to a
stable state. Futher, we show that if the communication network is connected,
this stable state is such that all urns have the same proportion of balls. This
result strengthens the main convergence properties of non-Bayesian learning
models. Yet, contrary to those models, we show that this limit proportion is a
full-support random variable. This implies that an arbitrarily small proportion
of misinformed agents can substantially change the value of the limit
consensus. We propose a set of conjectures on the distribution of this limit
proportion based on simulations. In particular, we show evidence that the limit
belief follows a beta distribution and that its average value is independent
from the network structure.",['Emilien Macault'],2,0.64396054
"A key challenge in social network analysis is understanding the position, or
stance, of people in the graph on a large set of topics. While past work has
modeled (dis)agreement in social networks using signed graphs, these approaches
have not modeled agreement patterns across a range of correlated topics. For
instance, disagreement on one topic may make disagreement(or agreement) more
likely for related topics. We propose the Stance Embeddings Model(SEM), which
jointly learns embeddings for each user and topic in signed social graphs with
distinct edge types for each topic. By jointly learning user and topic
embeddings, SEM is able to perform cold-start topic stance detection,
predicting the stance of a user on topics for which we have not observed their
engagement. We demonstrate the effectiveness of SEM using two large-scale
Twitter signed graph datasets we open-source. One dataset, TwitterSG, labels
(dis)agreements using engagements between users via tweets to derive
topic-informed, signed edges. The other, BirdwatchSG, leverages community
reports on misinformation and misleading content. On TwitterSG and BirdwatchSG,
SEM shows a 39% and 26% error reduction respectively against strong baselines.","['John Pougu√©-Biyong', 'Akshay Gupta', 'Aria Haghighi', 'Ahmed El-Kishky']",3,0.6408384
"In recent years, social media has enabled users to get exposed to a myriad of
misinformation and disinformation; thus, misinformation has attracted a great
deal of attention in research fields and as a social issue. To address the
problem, we propose a framework, Pre-CoFact, composed of two pre-trained models
for extracting features from text and images, and multiple co-attention
networks for fusing the same modality but different sources and different
modalities. Besides, we adopt the ensemble method by using different
pre-trained models in Pre-CoFact to achieve better performance. We further
illustrate the effectiveness from the ablation study and examine different
pre-trained models for comparison. Our team, Yao, won the fifth prize
(F1-score: 74.585\%) in the Factify challenge hosted by De-Factify @ AAAI 2022,
which demonstrates that our model achieved competitive performance without
using auxiliary tasks or extra information. The source code of our work is
publicly available at
https://github.com/wywyWang/Multi-Modal-Fact-Verification-2021","['Wei-Yao Wang', 'Wen-Chih Peng']",7,0.63123405
"People turn to search engines and social media to seek information during
population-level events, such as during civil unrest, disease outbreaks, fires,
or flood. They also tend to participate in discussions and disseminate
information and opinions via social media forums, and smartphone messaging
applications. COVID-19 pandemic was not any different. However, the proper
medical awareness and correct information dissemination is critical during a
pandemic. An unprecedented amount of internet traffic related to this topic was
generated during the start of the pandemic all over the world. In this work, we
have analysed the electronic data generated by users from Pakistan on Google
Search Engine and WhatsApp to understand their information-seeking behavior
during the first wave of the pandemic. The paper aims at analysing how the
Pakistani public developed their understanding about the disease, (its origin,
cures, and preventive measures to name a few) by analysing digital data. We
found that the public actively searched and discussed information at the start
of the pandemic. However, their interest waned with time and was reinvigorated
only when something novel or shocking seemed to have occurred. Understanding
this information seeking behavior will allow corrective actions to be taken by
health policymakers to better inform the public for possible future waves of a
pandemic through electronic media, as well as and the social media companies
and search engines to address misinformation among the users in the emergent
markets.","['Mehak Fatima', 'Aimal Rextin', 'Mehwish Nasim', 'Osman Yusuf']",5,0.7911087
"Contrary to expectations that the increased connectivity offered by the
internet and particularly Online Social Networks (OSNs) would result in broad
consensus on contentious issues, we instead frequently observe the formation of
polarised echo chambers, in which only one side of an argument is entertained.
These can progress to filter bubbles, actively filtering contrasting opinions,
resulting in vulnerability to misinformation and increased polarisation on
social and political issues. These have real-world effects when they spread
offline, such as vaccine hesitation and violence. This work seeks to develop a
better understanding of how echo chambers manifest in different discussions
dealing with different issues over an extended period of time. We explore the
activities of two groups of polarised accounts across three Twitter discussions
in the Australian context. We found Australian Twitter accounts arguing against
marriage equality in 2017 were more likely to support the notion that arsonists
were the primary cause of the 2019/2020 Australian bushfires, and those
supporting marriage equality argued against that arson narrative. We also found
strong evidence that the stance people took on marriage equality in 2017 did
not predict their political stance in discussions around the Australian federal
election two years later. Although mostly isolated from each other, we observe
that in certain situations the polarised groups may interact with the broader
community, which offers hope that the echo chambers may be reduced with
concerted outreach to members.","['Mehwish Nasim', 'Derek Weber', 'Tobin South', 'Jonathan Tuke', 'Nigel Bean', 'Lucia Falzon', 'Lewis Mitchell']",10,0.64958465
"The rapid influx of low-quality data visualisations is one of the main
challenges in today's communication. Misleading, unreadable, or confusing
visualisations spread misinformation, failing to fulfill their purpose. The
lack of proper tooling further heightens the problem of the quality assessment
process. Therefore, we propose VisQualdex, a systematic set of guidelines
isnpired by the Grammar of Graphics for evaluating the quality of data
visualisations. To increase the practical impact of VisQualdex, we make these
guidelines available in the form of the web server, visqual.info.","['Jan Sawicki', 'Micha≈Ç Burdukiewicz']",0,0.5302093
"Background: Vaccination programs are effective only when a significant
percentage of people are vaccinated. However, vaccine acceptance varies among
communities around the world. Social media usage is arguably one of the factors
affecting public attitudes towards vaccines. Objective: This study aims to
identify if the social media usages factors can be used to predict attitudes
and behavior towards the COVID-19 vaccines among the people in the Arab world.
Methods: An online survey was conducted in the Arab countries and 217 Arab
people participated in this study. Logistic regression was applied to identify
what demographics and social media usage factors predict public attitudes and
behavior towards the COVID-19 vaccines. Results: Of the 217 participants,
56.22% of them were willing to accept the vaccine and 41.47% of them were
hesitant. This study shows that none of the social media usages factors were
significant enough to predict the actual vaccine acceptance behavior. Whereas
the analysis showed few of the social media usage factors can predict public
attitudes towards the COVID-19 vaccines. For example, frequent social media
users were 2.85 times more likely to agree that the risk of COVID-19 is being
exaggerated (OR=2.85, 95% CI=0.86-9.45, p=0.046) than infrequent social media
users. Whereas participants having more trust in vaccine information shared by
their contacts are less likely to agree that decision-makers have verified that
vaccines are safe (OR=0.528, 95% CI= 0.276-1.012, p=0.05). Conclusion: The use
of social media and information shared on it may affect public attitudes
towards COVID-19 vaccines. Therefore, disseminating correct and validated
information about COVID-19 and other vaccines on social media is important for
increasing public trust and countering the impact of incorrect and
misinformation.","['Md. Rafiul Biswas', 'Hazrat Ali', 'Raian Ali', 'Zubair Shah']",12,0.81955516
"During the COVID-19 pandemic, health-related misinformation and harmful
content shared online had a significant adverse effect on society. To mitigate
this adverse effect, mainstream social media platforms employed soft moderation
interventions (i.e., warning labels) on potentially harmful posts. Despite the
recent popularity of these moderation interventions, we lack empirical analyses
aiming to uncover how these warning labels are used in the wild, particularly
during challenging times like the COVID-19 pandemic. In this work, we analyze
the use of warning labels on TikTok, focusing on COVID-19 videos. First, we
construct a set of 26 COVID-19 related hashtags, then we collect 41K videos
that include those hashtags in their description. Second, we perform a
quantitative analysis on the entire dataset to understand the use of warning
labels on TikTok. Then, we perform an in-depth qualitative study, using
thematic analysis, on 222 COVID-19 related videos to assess the content and the
connection between the content and the warning labels. Our analysis shows that
TikTok broadly applies warning labels on TikTok videos, likely based on
hashtags included in the description. More worrying is the addition of COVID-19
warning labels on videos where their actual content is not related to COVID-19
(23% of the cases in a sample of 143 English videos that are not related to
COVID-19). Finally, our qualitative analysis on a sample of 222 videos shows
that 7.7% of the videos share misinformation/harmful content and do not include
warning labels, 37.3% share benign information and include warning labels, and
that 35% of the videos that share misinformation/harmful content (and need a
warning label) are made for fun. Our study demonstrates the need to develop
more accurate and precise soft moderation systems, especially on a platform
like TikTok that is extremely popular among people of younger age.","['Chen Ling', 'Krishna P. Gummadi', 'Savvas Zannettou']",5,0.58268607
"The paper develops a blockchain protocol for a social media network (BE-SMN)
to mitigate the spread of misinformation. BE-SMN is derived based on the
information transmission-time distribution by modeling the misinformation
transmission as double-spend attacks on blockchain. The misinformation
distribution is then incorporated into the SIR (Susceptible, Infectious, or
Recovered) model, which substitutes the single rate parameter in the
traditional SIR model. Then, on a multi-community network, we study the
propagation of misinformation numerically and show that the proposed blockchain
enabled social media network outperforms the baseline network in flattening the
curve of the infected population.","['Rui Luo', 'Vikram Krishnamurthy', 'Erik Blasch']",2,0.7116942
"We present a minimal yet empirically-grounded theory for the spread of online
harms (e.g. misinformation, hate) across current multi-platform social media
and future Metaverses. New physics emerges from the interplay between the
intrinsic heterogeneity among online communities and platforms, their
clustering dynamics generated through user-created links and sudden moderator
shutdowns, and the contagion process. The theory provides an online `R-nought'
criterion to prevent system-wide spreading; it predicts re-entrant spreading
phases; it establishes the level of digital vaccination required for online
herd immunity; and it can be applied at multiple scales.","['Chen Xu', 'Pak Ming Hui', 'Om K. Jha', 'Chenkai Xia', 'Neil F. Johnson']",2,0.63175476
"During Australia's unprecedented bushfires in 2019-2020, misinformation
blaming arson resurfaced on Twitter using #ArsonEmergency. The extent to which
bots were responsible for disseminating and amplifying this misinformation has
received scrutiny in the media and academic research. Here we study Twitter
communities spreading this misinformation during the population-level event,
and investigate the role of online communities and bots. Our in-depth
investigation of the dynamics of the discussion uses a phased approach --
before and after reporting of bots promoting the hashtag was broadcast by the
mainstream media. Though we did not find many bots, the most bot-like accounts
were social bots, which present as genuine humans. Further, we distilled
meaningful quantitative differences between two polarised communities in the
Twitter discussion, resulting in the following insights. First, Supporters of
the arson narrative promoted misinformation by engaging others directly with
replies and mentions using hashtags and links to external sources. In response,
Opposers retweeted fact-based articles and official information. Second,
Supporters were embedded throughout their interaction networks, but Opposers
obtained high centrality more efficiently despite their peripheral positions.
By the last phase, Opposers and unaffiliated accounts appeared to coordinate,
potentially reaching a broader audience. Finally, unaffiliated accounts shared
the same URLs as Opposers over Supporters by a ratio of 9:1 in the last phase,
having shared mostly Supporter URLs in the first phase. This foiled Supporters'
efforts, highlighting the value of exposing misinformation campaigns. We
speculate that the communication strategies observed here could be discoverable
in other misinformation-related discussions and could inform
counter-strategies.","['Derek Weber', 'Lucia Falzon', 'Lewis Mitchell', 'Mehwish Nasim']",10,0.75656784
"With the proliferation of online misinformation, fake news detection has
gained importance in the artificial intelligence community. In this paper, we
propose an adversarial benchmark that tests the ability of fake news detectors
to reason about real-world facts. We formulate adversarial attacks that target
three aspects of ""understanding"": compositional semantics, lexical relations,
and sensitivity to modifiers. We test our benchmark using BERT classifiers
fine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets,
and show that both models fail to respond to changes in compositional and
lexical meaning. Our results strengthen the need for such models to be used in
conjunction with other fact checking methods.","['Lorenzo Jaime Yu Flores', 'Yiding Hao']",4,0.77295005
"Over the past decade, fake news and misinformation have turned into a major
problem that has impacted different aspects of our lives, including politics
and public health. Inspired by natural human behavior, we present an approach
that automates the detection of fake news. Natural human behavior is to
cross-check new information with reliable sources. We use Natural Language
Processing (NLP) and build a machine learning (ML) model that automates the
process of cross-checking new information with a set of predefined reliable
sources. We implement this for Twitter and build a model that flags fake
tweets. Specifically, for a given tweet, we use its text to find relevant news
from reliable news agencies. We then train a Random Forest model that checks if
the textual content of the tweet is aligned with the trusted news. If it is
not, the tweet is classified as fake. This approach can be generally applied to
any kind of information and is not limited to a specific news story or a
category of information. Our implementation of this approach gives a $70\%$
accuracy which outperforms other generic fake-news classification models. These
results pave the way towards a more sensible and natural approach to fake news
detection.","['Zahra Ghadiri', 'Milad Ranjbar', 'Fakhteh Ghanbarnejad', 'Sadegh Raeisi']",4,0.7992976
"In today's era of digital misinformation, we are increasingly faced with new
threats posed by video falsification techniques. Such falsifications range from
cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,
sophisticated AI media synthesis methods), which are becoming perceptually
indistinguishable from real videos. To tackle this challenge, we propose a
multi-modal semantic forensic approach to discover clues that go beyond
detecting discrepancies in visual quality, thereby handling both simpler
cheapfakes and visually persuasive deepfakes. In this work, our goal is to
verify that the purported person seen in the video is indeed themselves by
detecting anomalous facial movements corresponding to the spoken words. We
leverage the idea of attribution to learn person-specific biometric patterns
that distinguish a given speaker from others. We use interpretable Action Units
(AUs) to capture a person's face and head movement as opposed to deep CNN
features, and we are the first to use word-conditioned facial motion analysis.
We further demonstrate our method's effectiveness on a range of fakes not seen
in training including those without video manipulation, that were not addressed
in prior work.","['Shruti Agarwal', 'Liwen Hu', 'Evonne Ng', 'Trevor Darrell', 'Hao Li', 'Anna Rohrbach']",11,0.7985027
"Detecting out-of-context media, such as ""mis-captioned"" images on Twitter, is
a relevant problem, especially in domains of high public significance. In this
work we aim to develop defenses against such misinformation for the topics of
Climate Change, COVID-19, and Military Vehicles. We first present a large-scale
multimodal dataset with over 884k tweets relevant to these topics. Next, we
propose a detection method, based on the state-of-the-art CLIP model, that
leverages automatically generated hard image-text mismatches. While this
approach works well on our automatically constructed out-of-context tweets, we
aim to validate its usefulness on data representative of the real world. Thus,
we test it on a set of human-generated fakes created by mimicking in-the-wild
misinformation. We achieve an 11% detection improvement in a high precision
regime over a strong baseline. Finally, we share insights about our best model
design and analyze the challenges of this emerging threat.","['Giscard Biamby', 'Grace Luo', 'Trevor Darrell', 'Anna Rohrbach']",13,0.70411474
"Claim detection and verification are crucial for news understanding and have
emerged as promising technologies for mitigating misinformation and
disinformation in the news. However, most existing work has focused on claim
sentence analysis while overlooking additional crucial attributes (e.g., the
claimer and the main object associated with the claim). In this work, we
present NewsClaims, a new benchmark for attribute-aware claim detection in the
news domain. We extend the claim detection problem to include extraction of
additional attributes related to each claim and release 889 claims annotated
over 143 news articles. NewsClaims aims to benchmark claim detection systems in
emerging scenarios, comprising unseen topics with little or no training data.
To this end, we see that zero-shot and prompt-based baselines show promising
performance on this benchmark, while still considerably behind human
performance.","['Revanth Gangi Reddy', 'Sai Chetan', 'Zhenhailong Wang', 'Yi R. Fung', 'Kathryn Conger', 'Ahmed Elsayed', 'Martha Palmer', 'Preslav Nakov', 'Eduard Hovy', 'Kevin Small', 'Heng Ji']",1,0.69110775
"Sharing of anti-vaccine posts on social media, including misinformation
posts, has been shown to create confusion and reduce the publics confidence in
vaccines, leading to vaccine hesitancy and resistance. Recent years have
witnessed the fast rise of such anti-vaccine posts in a variety of linguistic
and visual forms in online networks, posing a great challenge for effective
content moderation and tracking. Extending previous work on leveraging textual
information to understand vaccine information, this paper presents Insta-VAX, a
new multi-modal dataset consisting of a sample of 64,957 Instagram posts
related to human vaccines. We applied a crowdsourced annotation procedure
verified by two trained expert judges to this dataset. We then bench-marked
several state-of-the-art NLP and computer vision classifiers to detect whether
the posts show anti-vaccine attitude and whether they contain misinformation.
Extensive experiments and analyses demonstrate the multimodal models can
classify the posts more accurately than the uni-modal models, but still need
improvement especially on visual context understanding and external knowledge
cooperation. The dataset and classifiers contribute to monitoring and tracking
of vaccine discussions for social scientific and public health efforts in
combating the problem of vaccine misinformation.","['Mingyang Zhou', 'Mahasweta Chakraborti', 'Sijia Qian', 'Zhou Yu', 'Jingwen Zhang']",12,0.83051455
"Misinformation and intergroup bias are two pathologies challenging informed
citizenship. This paper examines how identity language is used in
misinformation and debunking messages about controversial science on Chinese
digital public sphere, and their impact on how the public engage with science.
We collected an eight-year time series dataset of public discussion (N=6039) on
one of the most controversial science issues in China (GMO) from a popular Q&A
platform, Zhihu. We found that both misinformation and debunking messages use a
substantial amount of group identity languages when discussing the
controversial science issue, which we define as science factionalism --
discussion about science is divided by factions that are formed upon science
attitudes. We found that posts that use science factionalism receive more
digital votes and comments, even among the science-savvy community in China.
Science factionalism also increases the use of negativity in public discourse.
We discussed the implications of how science factionalism interacts with the
digital attention economy to affect public engagement with science
misinformation.","['Kaiping Chen', 'Yepeng Jin', 'Anqi Shao']",3,0.64693964
"This study focuses on how scientifically-correct information is disseminated
through social media, and how misinformation can be corrected. We have
identified examples on Twitter where scientific terms that have been misused
have been rectified and replaced by scientifically-correct terms through the
interaction of users. The results show that the percentage of correct terms
(""variant"" or ""COVID-19 variant"") being used instead of the incorrect terms
(""strain"") on Twitter has already increased since the end of December 2020.
This was about a month before the release of an official statement by the
Japanese Association for Infectious Diseases regarding the correct terminology,
and the use of terms on social media was faster than it was in television. Some
Twitter users who quickly started using the correct term were more likely to
retweet messages sent by leading influencers on Twitter, rather than messages
sent by traditional media or portal sites. However, a few Twitter users
continued to use wrong terms even after March 2021, even though the use of the
correct terms was widespread. Further analysis of their tweets revealed that
they were quoting sources that differed from that of other users. This study
empirically verified that self-correction occurs even on Twitter, which is
often known as a ""hotbed for spreading rumors."" The results of this study also
suggest that influencers with expertise can influence the direction of public
opinion on social media and that the media that users usually cite can also
affect the possibility of behavioral changes.","['Dongwoo Lim', 'Fujio Toriumi', 'Mitsuo Yoshida']",10,0.7112155
"Social networks have become one of the main information channels for human
beings due to the immediate and social interactivity they offer, allowing in
some cases to publish what each user considers relevant. This has brought with
it the generation of false news or Fake News, publications that only seek to
generate uncertainty, misinformation or skew the opinion of readers. It has
been shown that the human being is not capable of fully identifying whether an
article is really a fact or a Fake News, due to this it is that models arise
that seek to characterize and identify articles based on data mining and
machine learning. This article proposes a three-layer framework, the main
objective of which is to characterize the emotions present in Fake News and to
be a tool for future work that identifies the emotional state and intentional
state of the public.","['Luis Rojas Rubio', 'Claudio Meneses Villegas']",4,0.7825524
"Fake news, misinformation, and unverifiable facts on social media platforms
propagate disharmony and affect society, especially when dealing with an
epidemic like COVID-19. The task of Fake News Detection aims to tackle the
effects of such misinformation by classifying news items as fake or real. In
this paper, we propose a novel approach that improves over the current
automatic fake news detection approaches by automatically gathering evidence
for each claim. Our approach extracts supporting evidence from the web articles
and then selects appropriate text to be treated as evidence sets. We use a
pre-trained summarizer on these evidence sets and then use the extracted
summary as supporting evidence to aid the classification task. Our experiments,
using both machine learning and deep learning-based methods, help perform an
extensive evaluation of our approach. The results show that our approach
outperforms the state-of-the-art methods in fake news detection to achieve an
F1-score of 99.25 over the dataset provided for the CONSTRAINT-2021 Shared
Task. We also release the augmented dataset, our code and models for any
further research.","['Mrinal Rawat', 'Diptesh Kanojia']",4,0.8092239
"Health misinformation on search engines is a significant problem that could
negatively affect individuals or public health. To mitigate the problem, TREC
organizes a health misinformation track. This paper presents our submissions to
this track. We use a BM25 and a domain-specific semantic search engine for
retrieving initial documents. Later, we examine a health news schema for
quality assessment and apply it to re-rank documents. We merge the scores from
the different components by using reciprocal rank fusion. Finally, we discuss
the results and conclude with future works.","['Ipek Baris Schlicht', 'Angel Felipe Magnoss√£o de Paula', 'Paolo Rosso']",5,0.5762355
"Echo chambers on social media are a significant problem that can elicit a
number of negative consequences, most recently affecting the response to
COVID-19. Echo chambers promote conspiracy theories about the virus and are
found to be linked to vaccine hesitancy, less compliance with mask mandates,
and the practice of social distancing. Moreover, the problem of echo chambers
is connected to other pertinent issues like political polarization and the
spread of misinformation. An echo chamber is defined as a network of users in
which users only interact with opinions that support their pre-existing beliefs
and opinions, and they exclude and discredit other viewpoints. This survey aims
to examine the echo chamber phenomenon on social media from a social computing
perspective and provide a blueprint for possible solutions. We survey the
related literature to understand the attributes of echo chambers and how they
affect the individual and society at large. Additionally, we show the
mechanisms, both algorithmic and psychological, that lead to the formation of
echo chambers. These mechanisms could be manifested in two forms: (1) the bias
of social media's recommender systems and (2) internal biases such as
confirmation bias and homophily. While it is immensely challenging to mitigate
internal biases, there has been great efforts seeking to mitigate the bias of
recommender systems. These recommender systems take advantage of our own biases
to personalize content recommendations to keep us engaged in order to watch
more ads. Therefore, we further investigate different computational approaches
for echo chamber detection and prevention, mainly based around recommender
systems.","['Faisal Alatawi', 'Lu Cheng', 'Anique Tahir', 'Mansooreh Karami', 'Bohan Jiang', 'Tyler Black', 'Huan Liu']",3,0.7332141
"Over the last years, there has been an unprecedented proliferation of fake
news. As a consequence, we are more susceptible to the pernicious impact that
misinformation and disinformation spreading can have in different segments of
our society. Thus, the development of tools for automatic detection of fake
news plays and important role in the prevention of its negative effects. Most
attempts to detect and classify false content focus only on using textual
information. Multimodal approaches are less frequent and they typically
classify news either as true or fake. In this work, we perform a fine-grained
classification of fake news on the Fakeddit dataset, using both unimodal and
multimodal approaches. Our experiments show that the multimodal approach based
on a Convolutional Neural Network (CNN) architecture combining text and image
data achieves the best results, with an accuracy of 87%. Some fake news
categories such as Manipulated content, Satire or False connection strongly
benefit from the use of images. Using images also improves the results of the
other categories, but with less impact. Regarding the unimodal approaches using
only text, Bidirectional Encoder Representations from Transformers (BERT) is
the best model with an accuracy of 78%. Therefore, exploiting both text and
image data significantly improves the performance of fake news detection.","['Santiago Alonso-Bartolome', 'Isabel Segura-Bedmar']",4,0.8149904
"This paper aims to help structure the risk landscape associated with
large-scale Language Models (LMs). In order to foster advances in responsible
innovation, an in-depth understanding of the potential risks posed by these
models is needed. A wide range of established and anticipated risks are
analysed in detail, drawing on multidisciplinary expertise and literature from
computer science, linguistics, and social sciences.
  We outline six specific risk areas: I. Discrimination, Exclusion and
Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious
Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and
Environmental Harms. The first area concerns the perpetuation of stereotypes,
unfair discrimination, exclusionary norms, toxic language, and lower
performance by social group for LMs. The second focuses on risks from private
data leaks or LMs correctly inferring sensitive information. The third
addresses risks arising from poor, false or misleading information including in
sensitive domains, and knock-on risks such as the erosion of trust in shared
information. The fourth considers risks from actors who try to use LMs to cause
harm. The fifth focuses on risks specific to LLMs used to underpin
conversational agents that interact with human users, including unsafe use,
manipulation or deception. The sixth discusses the risk of environmental harm,
job automation, and other challenges that may have a disparate effect on
different social groups or communities.
  In total, we review 21 risks in-depth. We discuss the points of origin of
different risks and point to potential mitigation approaches. Lastly, we
discuss organisational responsibilities in implementing mitigations, and the
role of collaboration and participation. We highlight directions for further
research, particularly on expanding the toolkit for assessing and evaluating
the outlined risks in LMs.","['Laura Weidinger', 'John Mellor', 'Maribeth Rauh', 'Conor Griffin', 'Jonathan Uesato', 'Po-Sen Huang', 'Myra Cheng', 'Mia Glaese', 'Borja Balle', 'Atoosa Kasirzadeh', 'Zac Kenton', 'Sasha Brown', 'Will Hawkins', 'Tom Stepleton', 'Courtney Biles', 'Abeba Birhane', 'Julia Haas', 'Laura Rimell', 'Lisa Anne Hendricks', 'William Isaac', 'Sean Legassick', 'Geoffrey Irving', 'Iason Gabriel']",9,0.68111265
"Artificial intelligences (AI) will increasingly participate digitally and
physically in conflicts, yet there is a lack of trused communications with
humans for humanitarian purposes. In this paper we consider the integration of
a communications protocol (the 'whiteflag protocol'), distributed ledger
'blockchain' technology, and information fusion with AI, to improve conflict
communications called 'protected assurance understanding situation and
entitities' PAUSE. Such a trusted human-AI communication network could provide
accountable information exchange regarding protected entities, critical
infrastructure, humanitiarian signals and status updates for humans and
machines in conflicts. We examine several realistic potential case studies for
the integration of these technologies into a trusted human-AI network for
humanitarian benefit including mapping a conflict zone with civilians and
combatants in real time, preparation to avoid incidents and using the network
to manage misinformation. We finish with a real-world example of a PAUSE-like
network, the Human Security Information System (HSIS), being developed by
USAID, that uses blockchain technology to provide a secure means to better
understand the civilian environment.","['Susannah Kate Devitt', 'Jason Scholz', 'Timo Schless', 'Larry Lewis']",9,0.73425627
"Social media platforms are increasingly deploying complex interventions to
help users detect false news. Labeling false news using techniques that combine
crowd-sourcing with artificial intelligence (AI) offers a promising way to
inform users about potentially low-quality information without censoring
content, but also can be hard for users to understand. In this study, we
examine how users respond in their sharing intentions to information they are
provided about a hypothetical human-AI hybrid system. We ask i) if these
warnings increase discernment in social media sharing intentions and ii) if
explaining how the labeling system works can boost the effectiveness of the
warnings. To do so, we conduct a study ($N=1473$ Americans) in which
participants indicated their likelihood of sharing content. Participants were
randomly assigned to a control, a treatment where false content was labeled, or
a treatment where the warning labels came with an explanation of how they were
generated. We find clear evidence that both treatments increase sharing
discernment, and directional evidence that explanations increase the warnings'
effectiveness. Interestingly, we do not find that the explanations increase
self-reported trust in the warning labels, although we do find some evidence
that participants found the warnings with the explanations to be more
informative. Together, these results have important implications for designing
and deploying transparent misinformation warning labels, and AI-mediated
systems more broadly.","['Ziv Epstein', 'Nicol√≤ Foppiani', 'Sophie Hilgard', 'Sanjana Sharma', 'Elena Glassman', 'David Rand']",3,0.6927651
"Social media content routinely incorporates multi-modal design to covey
information and shape meanings, and sway interpretations toward desirable
implications, but the choices and outcomes of using both texts and visual
images have not been sufficiently studied. This work proposes a computational
approach to analyze the outcome of persuasive information in multi-modal
content, focusing on two aspects, popularity and reliability, in
COVID-19-related news articles shared on Twitter. The two aspects are
intertwined in the spread of misinformation: for example, an unreliable article
that aims to misinform has to attain some popularity. This work has several
contributions. First, we propose a multi-modal (image and text) approach to
effectively identify popularity and reliability of information sources
simultaneously. Second, we identify textual and visual elements that are
predictive to information popularity and reliability. Third, by modeling
cross-modal relations and similarity, we are able to uncover how unreliable
articles construct multi-modal meaning in a distorted, biased fashion. Our work
demonstrates how to use multi-modal analysis for understanding influential
content and has implications to social media literacy and engagement.","['Mesut Erhan Unal', 'Adriana Kovashka', 'Wen-Ting Chung', 'Yu-Ru Lin']",0,0.70831996
"Understanding how different online communities engage with COVID-19
misinformation is critical for public health response, as misinformation
confined to a small, isolated community of users poses a different public
health risk than misinformation being consumed by a large population spanning
many diverse communities. Here we take a longitudinal approach that leverages
tools from network science to study COVID-19 misinformation on Twitter. Our
approach provides a means to examine the breadth of misinformation engagement
using modest data needs and computational resources. We identify influential
accounts from different Twitter communities discussing COVID-19, and follow
these `sentinel nodes' longitudinally from July 2020 to January 2021. We
characterize sentinel nodes in terms of a linked-media preference score, and
use a standardized similarity score to examine alignment of tweets within and
between communities. We find that media preference is strongly correlated with
the amount of misinformation propagated by sentinel nodes. Engagement with
sensationalist misinformation topics is largely confined to a cluster of
sentinel nodes that includes influential conspiracy theorist accounts, while
misinformation relating to COVID-19 severity generated widespread engagement
across multiple communities. Our findings indicate that misinformation
downplaying COVID-19 severity is of particular concern for public health
response.","['Matthew T. Osborne', 'Samuel S. Malloy', 'Erik C. Nisbet', 'Robert M. Bond', 'Joseph H. Tien']",5,0.80257654
"Online platforms play a relevant role in the creation and diffusion of false
or misleading news. Concerningly, the COVID-19 pandemic is shaping a
communication network - barely considered in the literature - which reflects
the emergence of collective attention towards a topic that rapidly gained
universal interest. Here, we characterize the dynamics of this network on
Twitter, analyzing how unreliable content distributes among its users. We find
that a minority of accounts is responsible for the majority of the
misinformation circulating online, and identify two categories of users: a few
active ones, playing the role of ""creators"", and a majority playing the role of
""consumers"". The relative proportion of these groups ($\approx$14% creators -
86% consumers) appears stable over time: Consumers are mostly exposed to the
opinions of a vocal minority of creators, that could be mistakenly understood
as of representative of the majority of users. The corresponding pressure from
a perceived majority is identified as a potential driver of the ongoing
COVID-19 infodemic.","['Piergiorgio Castioni', 'Giulia Andrighetto', 'Riccardo Gallotti', 'Eugenia Polizzi', 'Manlio De Domenico']",3,0.6785271
"The effects of social media on critical issues, such as polarization and
misinformation, are under scrutiny due to the disruptive consequences that
these phenomena can have on our societies. Among the algorithms routinely used
by social media platforms, people-recommender systems are of special interest,
as they directly contribute to the evolution of the social network structure,
affecting the information and the opinions users are exposed to.
  In this paper, we propose a framework to assess the effect of people
recommenders on the evolution of opinions. Our proposal is based on Monte Carlo
simulations combining link recommendation and opinion-dynamics models. In order
to control initial conditions, we define a random network model to generate
graphs with opinions, with tunable amounts of modularity and homophily. We join
these elements into a methodology to study the effects of the recommender
system on echo chambers and polarization. We also show how to use our framework
to measure, by means of simulations, the impact of different intervention
strategies.
  Our thorough experimentation shows that people recommenders can in fact lead
to a significant increase in echo chambers. However, this happens only if there
is considerable initial homophily in the network. Also, we find that if the
network already contains echo chambers, the effect of the recommendation
algorithm is negligible. Such findings are robust to two very different opinion
dynamics models, a bounded confidence model and an epistemological model.","['Federico Cinus', 'Marco Minici', 'Corrado Monti', 'Francesco Bonchi']",2,0.7363945
"Misinformation is now a major problem due to its potential high risks to our
core democratic and societal values and orders. Out-of-context misinformation
is one of the easiest and effective ways used by adversaries to spread viral
false stories. In this threat, a real image is re-purposed to support other
narratives by misrepresenting its context and/or elements. The internet is
being used as the go-to way to verify information using different sources and
modalities. Our goal is an inspectable method that automates this
time-consuming and reasoning-intensive process by fact-checking the
image-caption pairing using Web evidence. To integrate evidence and cues from
both modalities, we introduce the concept of 'multi-modal cycle-consistency
check'; starting from the image/caption, we gather textual/visual evidence,
which will be compared against the other paired caption/image, respectively.
Moreover, we propose a novel architecture, Consistency-Checking Network (CCN),
that mimics the layered human reasoning across the same and different
modalities: the caption vs. textual evidence, the image vs. visual evidence,
and the image vs. caption. Our work offers the first step and benchmark for
open-domain, content-based, multi-modal fact-checking, and significantly
outperforms previous baselines that did not leverage external evidence.","['Sahar Abdelnabi', 'Rakibul Hasan', 'Mario Fritz']",7,0.6654073
"Current research in distributed Nash equilibrium (NE) seeking in the partial
information setting assumes that information is exchanged between agents that
are ""truthful"". However, in general noncooperative games agents may consider
sending misinformation to neighboring agents with the goal of further reducing
their cost. Additionally, communication networks are vulnerable to attacks from
agents outside the game as well as communication failures. In this paper, we
propose a distributed NE seeking algorithm that is robust against adversarial
agents that transmit noise, random signals, constant singles, deceitful
messages, as well as being resilient to external factors such as dropped
communication, jammed signals, and man in the middle attacks. The core issue
that makes the problem challenging is that agents have no means of verifying if
the information they receive is correct, i.e. there is no ""ground truth"". To
address this problem, we use an observation graph, that gives truthful action
information, in conjunction with a communication graph, that gives (potentially
incorrect) information. By filtering information obtained from these two
graphs, we show that our algorithm is resilient against adversarial agents and
converges to the Nash equilibrium.","['Dian Gadjov', 'Lacra Pavel']",2,0.6359022
"Fake news and misinformation are a matter of concern for people around the
globe. Users of the internet and social media sites encounter content with
false information much frequently. Fake news detection is one of the most
analyzed and prominent areas of research. These detection techniques apply
popular machine learning and deep learning algorithms. Previous work in this
domain covers fake news detection vastly among text circulating online.
Platforms that have extensively been observed and analyzed include news
websites and Twitter. Facebook, Reddit, WhatsApp, YouTube, and other social
applications are gradually gaining attention in this emerging field.
Researchers are analyzing online data based on multiple modalities composed of
text, image, video, speech, and other contributing factors. The combination of
various modalities has resulted in efficient fake news detection. At present,
there is an abundance of surveys consolidating textual fake news detection
algorithms. This review primarily deals with multi-modal fake news detection
techniques that include images, videos, and their combinations with text. We
provide a comprehensive literature survey of eighty articles presenting
state-of-the-art detection techniques, thereby identifying research gaps and
building a pathway for researchers to further advance this domain.","['Chahat Raj', 'Priyanka Meel']",4,0.8154595
"Social media is interactive, and interaction brings misinformation. With the
growing amount of user-generated data, fake news on online platforms has become
much frequent since the arrival of social networks. Now and then, an event
occurs and becomes the topic of discussion, generating and propagating false
information. Existing literature studying fake news primarily elaborates on
fake news classification models. Approaches exploring fake news characteristics
and ways to distinguish it from real news are minimal. Not many researches have
focused on statistical testing and generating new factor discoveries. This
study assumes fourteen hypotheses to identify factors exhibiting a relationship
with fake news. We perform the experiments on two real-world COVID-19 datasets
using qualitative and quantitative testing methods. This study concludes that
sentiment polarity and gender can significantly identify fake news. Dependence
on the presence of visual media is, however, inconclusive. Additionally,
Twitter-specific factors like followers count, friends count, and retweet count
significantly differ in fake and real news. Though, the contribution of status
count and favorites count is disputed. This study identifies practical factors
to be conjunctly utilized in the development of fake news detection algorithms.","['Chahat Raj', 'Priyanka Meel']",4,0.8390124
"Fake news is fabricated information that is presented as genuine, with
intention to deceive the reader. Recently, the magnitude of people relying on
social media for news consumption has increased significantly. Owing to this
rapid increase, the adverse effects of misinformation affect a wider audience.
On account of the increased vulnerability of people to such deceptive fake
news, a reliable technique to detect misinformation at its early stages is
imperative. Hence, the authors propose a novel graph-based framework SOcial
graph with Multi-head attention and Publisher information and news Statistics
Network (SOMPS-Net) comprising of two components - Social Interaction Graph
(SIG) and Publisher and News Statistics (PNS). The posited model is
experimented on the HealthStory dataset and generalizes across diverse medical
topics including Cancer, Alzheimer's, Obstetrics, and Nutrition. SOMPS-Net
significantly outperformed other state-of-the-art graph-based models
experimented on HealthStory by 17.1%. Further, experiments on early detection
demonstrated that SOMPS-Net predicted fake news articles with 79% certainty
within just 8 hours of its broadcast. Thus the contributions of this work lay
down the foundation for capturing fake health news across multiple medical
topics at its early stages.","['Prasannakumaran D', 'Harish Srinivasan', 'Sowmiya Sree S', 'Sri Gayathri Devi I', 'Saikrishnan S', 'Vineeth Vijayaraghavan']",4,0.7863305
"The COVID-19 pandemic has resulted in a slew of misinformation, often
described as an ""infodemic"". Whereas previous research has focused on the
propagation of unreliable sources as a main vehicle of misinformation, the
present study focuses on exploring the role of scientists whose views oppose
the scientific consensus. Using Nobelists in Physiology and Medicine as a proxy
for scientific consensus, we analyze two separate datasets: 15.8K tweets by
13.1K unique users on COVID-19 vaccines specifically, and 208K tweets by 151K
unique users on COVID-19 broadly which mention the Nobelist names. Our analyses
reveal that dissenting scientists are amplified by a factor of 426 relative to
true scientific consensus in the context of COVID-19 vaccines, and by a factor
of 43 in the context of COVID-19 generally. Although more popular accounts tend
to mention consensus-abiding scientists more, our results suggest that this
false consensus is driven by higher engagement with dissent-mentioning tweets.
Furthermore, false consensus mostly occurs due to traffic spikes following
highly popularized statements of dissenting scientists. We find that dissenting
voices are mainly discussed in French, English-speaking, Turkish, Brazilian,
Argentine, Indian, and Japanese misinformation clusters. This research suggests
that social media platforms should prioritize the exposure of consensus-abiding
scientists as a vehicle of reversing false consensus and addressing
misinformation stemming from seemingly credible sources.","['Alexandros Efstratiou', 'Tristan Caulfield']",5,0.67889416
"The threat posed by misinformation and disinformation is one of the defining
challenges of the 21st century. Provenance is designed to help combat this
threat by warning users when the content they are looking at may be
misinformation or disinformation. It is also designed to improve media literacy
among its users and ultimately reduce susceptibility to the threat among
vulnerable groups within society. The Provenance browser plugin checks the
content that users see on the Internet and social media and provides warnings
in their browser or social media feed. Unlike similar plugins, which require
human experts to provide evaluations and can only provide simple binary
warnings, Provenance's state of the art technology does not require human input
and it analyses seven aspects of the content users see and provides warnings
where necessary.","['Bilal Yousuf', 'M. Atif Qureshi', 'Brendan Spillane', 'Gary Munnelly', 'Oisin Carroll', 'Matthew Runswick', 'Kirsty Park', 'Eileen Culloty', 'Owen Conlan', 'Jane Suiter']",0,0.6197514
"While Wikipedia has been utilized for fact-checking and claim verification to
debunk misinformation and disinformation, it is essential to either improve
article quality and rule out noisy articles. Self-contradiction is one of the
low-quality article types in Wikipedia. In this work, we propose a task of
detecting self-contradiction articles in Wikipedia. Based on the
""self-contradictory"" template, we create a novel dataset for the
self-contradiction detection task. Conventional contradiction detection focuses
on comparing pairs of sentences or claims, but self-contradiction detection
needs to further reason the semantics of an article and simultaneously learn
the contradiction-aware comparison from all pairs of sentences. Therefore, we
present the first model, Pairwise Contradiction Neural Network (PCNN), to not
only effectively identify self-contradiction articles, but also highlight the
most contradiction pairs of contradiction sentences. The main idea of PCNN is
two-fold. First, to mitigate the effect of data scarcity on self-contradiction
articles, we pre-train the module of pairwise contradiction learning using SNLI
and MNLI benchmarks. Second, we select top-K sentence pairs with the highest
contradiction probability values and model their correlation to determine
whether the corresponding article belongs to self-contradiction. Experiments
conducted on the proposed WikiContradiction dataset exhibit that PCNN can
generate promising performance and comprehensively highlight the sentence pairs
the contradiction locates.","['Cheng Hsu', 'Cheng-Te Li', 'Diego Saez-Trumper', 'Yi-Zhan Hsu']",1,0.60030806
"A drastic rise in potentially life-threatening misinformation has been a
by-product of the COVID-19 pandemic. Computational support to identify false
information within the massive body of data on the topic is crucial to prevent
harm. Researchers proposed many methods for flagging online misinformation
related to COVID-19. However, these methods predominantly target specific
content types (e.g., news) or platforms (e.g., Twitter). The methods'
capabilities to generalize were largely unclear so far. We evaluate fifteen
Transformer-based models on five COVID-19 misinformation datasets that include
social media posts, news articles, and scientific papers to fill this gap. We
show tokenizers and models tailored to COVID-19 data do not provide a
significant advantage over general-purpose ones. Our study provides a realistic
assessment of models for detecting COVID-19 misinformation. We expect that
evaluating a broad spectrum of datasets and models will benefit future research
in developing misinformation detection systems.","['Jan Philip Wahle', 'Nischal Ashok', 'Terry Ruas', 'Norman Meuschke', 'Tirthankar Ghosal', 'Bela Gipp']",5,0.68923575
"In this paper, we present an approach for predicting trust links between
peers in social media, one that is grounded in the artificial intelligence area
of multiagent trust modeling. In particular, we propose a data-driven
multi-faceted trust modeling which incorporates many distinct features for a
comprehensive analysis. We focus on demonstrating how clustering of similar
users enables a critical new functionality: supporting more personalized, and
thus more accurate predictions for users. Illustrated in a trust-aware item
recommendation task, we evaluate the proposed framework in the context of a
large Yelp dataset. We then discuss how improving the detection of trusted
relationships in social media can assist in supporting online users in their
battle against the spread of misinformation and rumours, within a social
networking environment which has recently exploded in popularity. We conclude
with a reflection on a particularly vulnerable user base, older adults, in
order to illustrate the value of reasoning about groups of users, looking to
some future directions for integrating known preferences with insights gained
through data analysis.","['Alexandre Parmentier', 'Robin Cohen', 'Xueguang Ma', 'Gaurav Sahu', 'Queenie Chen']",2,0.662204
"This paper reports the findings of a 606-participant study where we analyzed
the perception and engagement effects of COVID-19 vaccine rumours on Twitter
pertaining to (a) vaccine efficacy; and (b) mass immunization efforts in the
United States. Misperceptions regarding vaccine efficacy were successfully
induced through simple content alterations and the addition of popular anti
COVID-19 hashtags to otherwise valid Twitter content. Twitter's misinformation
contextual tags caused a ""backfire effect"" for the skeptic, vaccine-hesitant
reinforcing their opposition stance. While the majority of the participants
staunchly refrain from engaging with the COVID-19 rumours, the skeptic,
vaccine-hesitant ones were open to comment, re-tweet, like and share the
vaccine efficacy rumors. We discuss the implications of our results in the
context of broadening the effort for dispelling rumors about COVID-19 on social
media.","['Filipo Sharevski', 'Allice Huff', 'Peter Jachim', 'Emma Pieroni']",12,0.8339882
"In recent years, the problem of rumours on online social media (OSM) has
attracted lots of attention. Researchers have started investigating from two
main directions. First is the descriptive analysis of rumours and secondly,
proposing techniques to detect (or classify) rumours. In the descriptive line
of works, where researchers have tried to analyse rumours using NLP approaches,
there isnt much emphasis on psycho-linguistics analyses of social media text.
These kinds of analyses on rumour case studies are vital for drawing meaningful
conclusions to mitigate misinformation. For our analysis, we explored the
PHEME9 rumour dataset (consisting of 9 events), including source tweets (both
rumour and non-rumour categories) and response tweets. We compared the rumour
and nonrumour source tweets and then their corresponding reply (response)
tweets to understand how they differ linguistically for every incident.
Furthermore, we also evaluated if these features can be used for classifying
rumour vs. non-rumour tweets through machine learning models. To this end, we
employed various classical and ensemble-based approaches. To filter out the
highly discriminative psycholinguistic features, we explored the SHAP AI
Explainability tool. To summarise, this research contributes by performing an
in-depth psycholinguistic analysis of rumours related to various kinds of
events.","['Sabur Butt', 'Shakshi Sharma', 'Rajesh Sharma', 'Grigori Sidorov', 'Alexander Gelbukh']",3,0.60003686
"Instagram infographics are a digital activism tool that have redefined action
frames for technology-facilitated social movements. From the 1960s through the
1980s, United States ethnic movements practiced collective action:
ideologically unified, resource-intensive traditional activism. Today,
technologically enabled movements have been categorized as practicing
connective action: individualized, low-resource online activism. Yet, we argue
that Instagram infographics are both connective and collective. This paper
juxtaposes the insights of past and present U.S. ethnic movement activists and
analyzes Black Lives Matter Instagram data over the course of 7 years
(2014-2020). We find that Instagram infographic activism bridges connective and
collective action in three ways: (1) Scope for Education: Visually enticing and
digestible infographics reduce the friction of information dissemination,
facilitating collective movement education while preserving customizability.
(2) Reconciliation for Credibility: Activists use connective features to combat
infographic misinformation and resolve internal differences, creating a trusted
collective movement front. (3) High-Resource Efforts for Transformative Change:
Instagram infographic activism has been paired with boots on the ground and
action-oriented content, curating a connective-to-collective pipeline that
expends movement resources. Our work unveils the vitality of evaluating digital
activism action frames at the movement integration level, exemplifies the
powerful coexistence of connective and collective action, and offers meaningful
design implications for activists seeking to leverage this novel tool.","['Darya Kaviani', 'Niloufar Salehi']",3,0.54106486
"Social media platforms have been exploited to disseminate misinformation in
recent years. The widespread online misinformation has been shown to affect
users' beliefs and is connected to social impact such as polarization. In this
work, we focus on misinformation's impact on specific user behavior and aim to
understand whether general Twitter users changed their behavior after being
exposed to misinformation. We compare the before and after behavior of exposed
users to determine whether the frequency of the tweets they posted, or the
sentiment of their tweets underwent any significant change. Our results
indicate that users overall exhibited statistically significant changes in
behavior across some of these metrics. Through language distance analysis, we
show that exposed users were already different from baseline users before the
exposure. We also study the characteristics of two specific user groups,
multi-exposure and extreme change groups, which were potentially highly
impacted. Finally, we study if the changes in the behavior of the users after
exposure to misinformation tweets vary based on the number of their followers
or the number of followers of the tweet authors, and find that their behavioral
changes are all similar.","['Yichen Wang', 'Richard Han', 'Tamara Lehman', 'Qin Lv', 'Shivakant Mishra']",3,0.7495218
"QAnon is an umbrella conspiracy theory that encompasses a wide spectrum of
people. The COVID-19 pandemic has helped raise the QAnon conspiracy theory to a
wide-spreading movement, especially in the US. Here, we study users' dynamics
on Twitter related to the QAnon movement (i.e., pro-/anti-QAnon and
less-leaning users) in the context of the COVID-19 infodemic and the topics
involved using a simple network-based approach. We found that pro- and
anti-leaning users show different population dynamics and that late
less-leaning users were mostly anti-QAnon. These trends might have been
affected by Twitter's suspension strategies. We also found that QAnon clusters
include many bot users. Furthermore, our results suggest that QAnon continues
to evolve amid the infodemic and does not limit itself to its original idea but
instead extends its reach to create a much larger umbrella conspiracy theory.
The network-based approach in this study is important for nowcasting the
evolution of the QAnon movement.","['Wentao Xu', 'Kazutoshi Sasahara']",10,0.56102
"Recent years have witnessed an increasing use of coordinated accounts on
social media, operated by misinformation campaigns to influence public opinion
and manipulate social outcomes. Consequently, there is an urgent need to
develop an effective methodology for coordinated group detection to combat the
misinformation on social media. However, existing works suffer from various
drawbacks, such as, either limited performance due to extreme reliance on
predefined signatures of coordination, or instead an inability to address the
natural sparsity of account activities on social media with useful prior domain
knowledge. Therefore, in this paper, we propose a coordination detection
framework incorporating neural temporal point process with prior knowledge such
as temporal logic or pre-defined filtering functions. Specifically, when
modeling the observed data from social media with neural temporal point
process, we jointly learn a Gibbs-like distribution of group assignment based
on how consistent an assignment is to (1) the account embedding space and (2)
the prior knowledge. To address the challenge that the distribution is hard to
be efficiently computed and sampled from, we design a theoretically guaranteed
variational inference approach to learn a mean-field approximation for it.
Experimental results on a real-world dataset show the effectiveness of our
proposed method compared to the SOTA model in both unsupervised and
semi-supervised settings. We further apply our model on a COVID-19 Vaccine
Tweets dataset. The detection result suggests the presence of suspicious
coordinated efforts on spreading misinformation about COVID-19 vaccines.","['Yizhou Zhang', 'Karishma Sharma', 'Yan Liu']",2,0.76818514
"Our society produces and shares overwhelming amounts of information through
Online Social Networks (OSNs). Within this environment, misinformation and
disinformation have proliferated, becoming a public safety concern in most
countries. Allowing the public and professionals to efficiently find reliable
evidences about the factual veracity of a claim is a crucial step to mitigate
this harmful spread. To this end, we propose FacTeR-Check, a multilingual
architecture for semi-automated fact-checking that can be used for either
applications designed for the general public and by fact-checking
organisations. FacTeR-Check enables retrieving fact-checked information,
unchecked claims verification and tracking dangerous information over social
media. This architectures involves several modules developed to evaluate
semantic similarity, to calculate natural language inference and to retrieve
information from Online Social Networks. The union of all these components
builds a semi-automated fact-checking tool able of verifying new claims, to
extract related evidence, and to track the evolution of a hoax on a OSN. While
individual modules are validated on related benchmarks (mainly MSTS and SICK),
the complete architecture is validated using a new dataset called NLI19-SP that
is publicly released with COVID-19 related hoaxes and tweets from Spanish
social media. Our results show state-of-the-art performance on the individual
benchmarks, as well as producing a useful analysis of the evolution over time
of 61 different hoaxes.","['Alejandro Mart√≠n', 'Javier Huertas-Tato', '√Ålvaro Huertas-Garc√≠a', 'Guillermo Villar-Rodr√≠guez', 'David Camacho']",1,0.74999964
"The growing use of social media has led to the development of several Machine
Learning (ML) and Natural Language Processing(NLP) tools to process the
unprecedented amount of social media content to make actionable decisions.
However, these MLand NLP algorithms have been widely shown to be vulnerable to
adversarial attacks. These vulnerabilities allow adversaries to launch a
diversified set of adversarial attacks on these algorithms in different
applications of social media text processing. In this paper, we provide a
comprehensive review of the main approaches for adversarial attacks and
defenses in the context of social media applications with a particular focus on
key challenges and future research directions. In detail, we cover literature
on six key applications, namely (i) rumors detection, (ii) satires detection,
(iii) clickbait & spams identification, (iv) hate speech detection,
(v)misinformation detection, and (vi) sentiment analysis. We then highlight the
concurrent and anticipated future research questions and provide
recommendations and directions for future work.","['Izzat Alsmadi', 'Kashif Ahmad', 'Mahmoud Nazzal', 'Firoj Alam', 'Ala Al-Fuqaha', 'Abdallah Khreishah', 'Abdulelah Algosaibi']",0,0.6845089
"Information sharing on social media must be accompanied by attentive behavior
so that in a distorted digital environment, users are not rushed and distracted
in deciding to share information. The spread of misinformation, especially
those related to the COVID-19, can divide and create negative effects of
falsehood in society. Individuals can also cause feelings of fear, health
anxiety, and confusion in the treatment COVID-19. Although much research has
focused on understanding human judgment from a psychological underline, few
have addressed the essential issue in the screening phase of what technology
can interfere amidst users' attention in sharing information. This research
aims to intervene in the user's attention with a visual selective attention
approach. This study uses a quantitative method through studies 1 and 2 with
pre-and post-intervention experiments. In study 1, we intervened in user
decisions and attention by stimulating ten information and misinformation using
the Visual Selective Attention System (VSAS) tool. In Study 2, we identified
associations of user tendencies in evaluating information using the Implicit
Association Test (IAT). The significant results showed that the user's
attention and decision behavior improved after using the VSAS. The IAT results
show a change in the association of user exposure, where after the intervention
using VSAS, users tend not to share misinformation about COVID-19. The results
are expected to be the basis for developing social media applications to combat
the negative impact of the infodemic COVID-19 misinformation.","['Zaid Amin', 'Nazlena Mohamad Ali', 'Alan F. Smeaton']",5,0.70259196
"This paper describes SciClops, a method to help combat online scientific
misinformation. Although automated fact-checking methods have gained
significant attention recently, they require pre-existing ground-truth
evidence, which, in the scientific context, is sparse and scattered across a
constantly-evolving scientific literature. Existing methods do not exploit this
literature, which can effectively contextualize and combat science-related
fallacies. Furthermore, these methods rarely require human intervention, which
is essential for the convoluted and critical domain of scientific
misinformation. SciClops involves three main steps to process scientific claims
found in online news articles and social media postings: extraction,
clustering, and contextualization. First, the extraction of scientific claims
takes place using a domain-specific, fine-tuned transformer model. Second,
similar claims extracted from heterogeneous sources are clustered together with
related scientific literature using a method that exploits their content and
the connections among them. Third, check-worthy claims, broadcasted by popular
yet unreliable sources, are highlighted together with an enhanced fact-checking
context that includes related verified claims, news articles, and scientific
papers. Extensive experiments show that SciClops tackles sufficiently these
three steps, and effectively assists non-expert fact-checkers in the
verification of complex scientific claims, outperforming commercial
fact-checking systems.","['Panayiotis Smeros', 'Carlos Castillo', 'Karl Aberer']",1,0.6463495
"Prior work has extensively studied misinformation related to news, politics,
and health, however, misinformation can also be about technological topics.
While less controversial, such misinformation can severely impact companies'
reputations and revenues, and users' online experiences. Recently, social media
has also been increasingly used as a novel source of knowledgebase for
extracting timely and relevant security threats, which are fed to the threat
intelligence systems for better performance. However, with possible campaigns
spreading false security threats, these systems can become vulnerable to
poisoning attacks. In this work, we proposed novel approaches for detecting
misinformation about cybersecurity and privacy threats on social media,
focusing on two topics with different types of misinformation: phishing
websites and Zoom's security & privacy threats. We developed a framework for
detecting inaccurate phishing claims on Twitter. Using this framework, we could
label about 9% of URLs and 22% of phishing reports as misinformation. We also
proposed another framework for detecting misinformation related to Zoom's
security and privacy threats on multiple platforms. Our classifiers showed
great performance with more than 98% accuracy. Employing these classifiers on
the posts from Facebook, Instagram, Reddit, and Twitter, we found respectively
that about 18%, 3%, 4%, and 3% of posts were misinformation. In addition, we
studied the characteristics of misinformation posts, their authors, and their
timelines, which helped us identify campaigns.","['Mohit Singhal', 'Nihal Kumarswamy', 'Shreyasi Kinhekar', 'Shirin Nilizadeh']",4,0.6830608
"Misinformation posting and spreading in Social Media is ignited by personal
decisions on the truthfulness of news that may cause wide and deep cascades at
a large scale in a fraction of minutes. When individuals are exposed to
information, they usually take a few seconds to decide if the content (or the
source) is reliable, and eventually to share it. Although the opportunity to
verify the rumour is often just one click away, many users fail to make a
correct evaluation. We studied this phenomenon with a web-based questionnaire
that was compiled by 7,298 different volunteers, where the participants were
asked to mark 20 news as true or false. Interestingly, false news is correctly
identified more frequently than true news, but showing the full article instead
of just the title, surprisingly, does not increase general accuracy. Also,
displaying the original source of the news may contribute to mislead the user
in some cases, while a genuine wisdom of the crowd can positively assist
individuals' ability to classify correctly. Finally, participants whose
browsing activity suggests a parallel fact-checking activity, show better
performance and declare themselves as young adults. This work highlights a
series of pitfalls that can influence human annotators when building false news
datasets, which in turn fuel the research on the automated fake news detection;
furthermore, these findings challenge the common rationale of AI that suggest
users to read the full article before re-sharing.","['Giancarlo Ruffo', 'Alfonso Semeraro']",4,0.8124787
"Socialbots are software-driven user accounts on social platforms, acting
autonomously (mimicking human behavior), with the aims to influence the
opinions of other users or spread targeted misinformation for particular goals.
As socialbots undermine the ecosystem of social platforms, they are often
considered harmful. As such, there have been several computational efforts to
auto-detect the socialbots. However, to our best knowledge, the adversarial
nature of these socialbots has not yet been studied. This begs a question ""can
adversaries, controlling socialbots, exploit AI techniques to their advantage?""
To this question, we successfully demonstrate that indeed it is possible for
adversaries to exploit computational learning mechanism such as reinforcement
learning (RL) to maximize the influence of socialbots while avoiding being
detected. We first formulate the adversarial socialbot learning as a
cooperative game between two functional hierarchical RL agents. While one agent
curates a sequence of activities that can avoid the detection, the other agent
aims to maximize network influence by selectively connecting with right users.
Our proposed policy networks train with a vast amount of synthetic graphs and
generalize better than baselines on unseen real-life graphs both in terms of
maximizing network influence (up to +18%) and sustainable stealthiness (up to
+40% undetectability) under a strong bot detector (with 90% detection
accuracy). During inference, the complexity of our approach scales linearly,
independent of a network's structure and the virality of news. This makes our
approach a practical adversarial attack when deployed in a real-life setting.","['Thai Le', 'Long Tran-Thanh', 'Dongwon Lee']",13,0.793404
"From the 2016 U.S. presidential election to the 2021 Capitol riots to the
spread of misinformation related to COVID-19, many have blamed social media for
today's deeply divided society. Recent advances in machine learning for signed
networks hold the promise to guide small interventions with the goal of
reducing polarization in social media. However, existing models are especially
ineffective in predicting conflicts (or negative links) among users. This is
due to a strong correlation between link signs and the network structure, where
negative links between polarized communities are too sparse to be predicted
even by state-of-the-art approaches. To address this problem, we first design a
partition-agnostic polarization measure for signed graphs based on the signed
random-walk and show that many real-world graphs are highly polarized. Then, we
propose POLE (POLarized Embedding for signed networks), a signed embedding
method for polarized graphs that captures both topological and signed
similarities jointly via signed autocovariance. Through extensive experiments,
we show that POLE significantly outperforms state-of-the-art methods in signed
link prediction, particularly for negative links with gains of up to one order
of magnitude.","['Zexi Huang', 'Arlei Silva', 'Ambuj Singh']",2,0.62183243
"Fact-checking is an essential tool to mitigate the spread of misinformation
and disinformation. We introduce the task of fact-checking in dialogue, which
is a relatively unexplored area. We construct DialFact, a testing benchmark
dataset of 22,245 annotated conversational claims, paired with pieces of
evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable
claim detection task distinguishes whether a response carries verifiable
factual information; 2) Evidence retrieval task retrieves the most relevant
Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue
response to be supported, refuted, or not enough information. We found that
existing fact-checking models trained on non-dialogue data like FEVER fail to
perform well on our task, and thus, we propose a simple yet data-efficient
solution to effectively improve fact-checking performance in dialogue. We point
out unique challenges in DialFact such as handling the colloquialisms,
coreferences and retrieval ambiguities in the error analysis to shed light on
future research in this direction.","['Prakhar Gupta', 'Chien-Sheng Wu', 'Wenhao Liu', 'Caiming Xiong']",1,0.7684272
"With a rise in false, inaccurate, and misleading information in propaganda,
news, and social media, real-world Question Answering (QA) systems face the
challenges of synthesizing and reasoning over misinformation-polluted contexts
to derive correct answers. This urgency gives rise to the need to make QA
systems robust to misinformation, a topic previously unexplored. We study the
risk of misinformation to QA models by investigating the sensitivity of
open-domain QA models to corpus pollution with misinformation documents. We
curate both human-written and model-generated false documents that we inject
into the evidence corpus of QA models and assess the impact on the performance
of these systems. Experiments show that QA models are vulnerable to even small
amounts of evidence contamination brought by misinformation, with large
absolute performance drops on all models. Misinformation attack brings more
threat when fake documents are produced at scale by neural models or the
attacker targets hacking specific questions of interest. To defend against such
a threat, we discuss the necessity of building a misinformation-aware QA system
that integrates question-answering and misinformation detection in a joint
fashion.","['Liangming Pan', 'Wenhu Chen', 'Min-Yen Kan', 'William Yang Wang']",1,0.74065775
"Since late 2019, COVID-19 has quickly emerged as the newest biomedical
domain, resulting in a surge of new information. As with other emergent
domains, the discussion surrounding the topic has been rapidly changing,
leading to the spread of misinformation. This has created the need for a public
space for users to ask questions and receive credible, scientific answers. To
fulfill this need, we turn to the task of open-domain question-answering, which
we can use to efficiently find answers to free-text questions from a large set
of documents. In this work, we present such a system for the emergent domain of
COVID-19. Despite the small data size available, we are able to successfully
train the system to retrieve answers from a large-scale corpus of published
COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking
and question-answering techniques, such as document diversity and multiple
answer spans. Our open-domain question-answering system can further act as a
model for the quick development of similar systems that can be adapted and
modified for other developing emergent domains.","['Sharon Levy', 'Kevin Mo', 'Wenhan Xiong', 'William Yang Wang']",1,0.5414729
"The COVID-19 pandemic poses a great threat to global public health.
Meanwhile, there is massive misinformation associated with the pandemic which
advocates unfounded or unscientific claims. Even major social media and news
outlets have made an extra effort in debunking COVID-19 misinformation, most of
the fact-checking information is in English, whereas some unmoderated COVID-19
misinformation is still circulating in other languages, threatening the health
of less-informed people in immigrant communities and developing countries. In
this paper, we make the first attempt to detect COVID-19 misinformation in a
low-resource language (Chinese) only using the fact-checked news in a
high-resource language (English). We start by curating a Chinese real&fake news
dataset according to existing fact-checking information. Then, we propose a
deep learning framework named CrossFake to jointly encode the cross-lingual
news body texts and capture the news content as much as possible. Empirical
results on our dataset demonstrate the effectiveness of CrossFake under the
cross-lingual setting and it also outperforms several monolingual and
cross-lingual fake news detectors. The dataset is available at
https://github.com/YingtongDou/CrossFake.","['Jiangshu Du', 'Yingtong Dou', 'Congying Xia', 'Limeng Cui', 'Jing Ma', 'Philip S. Yu']",4,0.72141504
"Vaccine hesitancy and other COVID-19-related concerns and complaints in the
Philippines are evident on social media. It is important to identify these
different topics and sentiments in order to gauge public opinion, use the
insights to develop policies, and make necessary adjustments or actions to
improve public image and reputation of the administering agency and the
COVID-19 vaccines themselves. This paper proposes a semi-supervised machine
learning pipeline to perform topic modeling, sentiment analysis, and an
analysis of vaccine brand reputation to obtain an in-depth understanding of
national public opinion of Filipinos on Facebook. The methodology makes use of
a multilingual version of Bidirectional Encoder Representations from
Transformers or BERT for topic modeling, hierarchical clustering, five
different classifiers for sentiment analysis, and cosine similarity of BERT
topic embeddings for vaccine brand reputation analysis. Results suggest that
any type of COVID-19 misinformation is an emergent property of COVID-19 public
opinion, and that the detection of COVID-19 misinformation can be an
unsupervised task. Sentiment analysis aided by hierarchical clustering reveal
that 21 of the 25 topics extrapolated by topic modeling are negative topics.
Such negative comments spike in count whenever the Department of Health in the
Philippines posts about the COVID-19 situation in other countries.
Additionally, the high numbers of laugh reactions on the Facebook posts by the
same agency -- without any humorous content -- suggest that the reactors of
these posts tend to react the way they do, not because of what the posts are
about but because of who posted them.","['Jasper Kyle Catapang', 'Jerome V. Cleofas']",12,0.7629861
"We introduce a novel method for analyzing person-to-person content influence
on Twitter. Using an Ego-Alter framework and Granger Causality, we examine
President Donald Trump (the Ego) and the people he retweets (Alters) as a case
study. We find that each Alter has a different scope of influence across
multiple topics, different magnitude of influence on a given topic, and the
magnitude of a single Alter's influence can vary across topics. This work is
novel in its focus on person-to-person influence and content-based influence.
Its impact is two-fold: (1) identifying ""canaries in the coal mine"" who could
be observed by misinformation researchers or platforms to identify
misinformation narratives before super-influencers spread them to large
audiences, and (2) enabling digital marketing targeted toward upstream Alters
of super-influencers.","['Richard Kuzma', 'Iain J. Cruickshank', 'Kathleen M. Carley']",10,0.65854454
"In this position paper, we discuss recent applications of simulation
approaches for recommender systems tasks. In particular, we describe how they
were used to analyze the problem of misinformation spreading and understand
which data characteristics affect the performance of recommendation algorithms
more significantly. We also present potential lines of future work where
simulation methods could advance the work in the recommendation community.","['Alejandro Bellog√≠n', 'Yashar Deldjoo']",2,0.48916024
"We examine an unexpected but significant source of positive public health
messaging during the COVID-19 pandemic -- K-pop fandoms. Leveraging more than 7
million tweets related to mask-wearing and K-pop between March 2020 and
December 2021, we analyzed the online spread of the hashtag \#WearAMask and
vaccine-related tweets amid anti-mask sentiments and public health
misinformation. Analyses reveal the South Korean boyband BTS as one of the most
significant driver of health discourse. Tweets from health agencies and
prominent figures that mentioned K-pop generate 111 times more online responses
compared to tweets that did not. These tweets also elicited strong responses
from South America, Southeast Asia, and rural States -- areas often neglected
in Twitter-based messaging by mainstream social media campaigns. Network and
temporal analysis show increased use from right-leaning elites over time.
Mechanistically, strong-levels of parasocial engagement and connectedness allow
sustained activism in the community. Our results suggest that public health
institutions may leverage pre-existing audience markets to synergistically
diffuse and target under-served communities both domestically and globally,
especially during health crises such as COVID-19.","['Ho-Chun Herbert Chang', 'Becky Pham', 'Emilio Ferrara']",5,0.69502354
"The novel coronavirus disease (COVID-19) pandemic has impacted every corner
of earth, disrupting governments and leading to socioeconomic instability. This
crisis has prompted questions surrounding how different sectors of society
interact and influence each other during times of change and stress. Given the
unprecedented economic and societal impacts of this pandemic, many new data
sources have become available, allowing us to quantitatively explore these
associations. Understanding these relationships can help us better prepare for
future disasters and mitigate the impacts. Here, we focus on the interplay
between social unrest (protests), health outcomes, public health orders, and
misinformation in eight countries of Western Europe and four regions of the
United States. We created 1-3 week forecasts of both a binary protest metric
for identifying times of high protest activity and the overall protest counts
over time. We found that for all regions, except Belgium, at least one feature
from our various data streams was predictive of protests. However, the accuracy
of the protest forecasts varied by country, that is, for roughly half of the
countries analyzed, our forecasts outperform a na\""ive model. These mixed
results demonstrate the potential of diverse data streams to predict a topic as
volatile as protests as well as the difficulties of predicting a situation that
is as rapidly evolving as a pandemic.","['Martha Barnard', 'Radhika Iyer', 'Sara Y. Del Valle', 'Ashlynn R. Daughton']",5,0.651424
"Due to their convenience and high accuracy, face recognition systems are
widely employed in governmental and personal security applications to
automatically recognise individuals. Despite recent advances, face recognition
systems have shown to be particularly vulnerable to identity attacks (i.e.,
digital manipulations and attack presentations). Identity attacks pose a big
security threat as they can be used to gain unauthorised access and spread
misinformation. In this context, most algorithms for detecting identity attacks
generalise poorly to attack types that are unknown at training time. To tackle
this problem, we introduce a differential anomaly detection framework in which
deep face embeddings are first extracted from pairs of images (i.e., reference
and probe) and then combined for identity attack detection. The experimental
evaluation conducted over several databases shows a high generalisation
capability of the proposed method for detecting unknown attacks in both the
digital and physical domains.","['Mathias Ibsen', 'L√°zaro J. Gonz√°lez-Soler', 'Christian Rathgeb', 'Pawel Drozdowski', 'Marta Gomez-Barrero', 'Christoph Busch']",11,0.6923397
"The widespread usage of social networks during mass convergence events, such
as health emergencies and disease outbreaks, provides instant access to
citizen-generated data that carry rich information about public opinions,
sentiments, urgent needs, and situational reports. Such information can help
authorities understand the emergent situation and react accordingly. Moreover,
social media plays a vital role in tackling misinformation and disinformation.
This work presents TBCOV, a large-scale Twitter dataset comprising more than
two billion multilingual tweets related to the COVID-19 pandemic collected
worldwide over a continuous period of more than one year. More importantly,
several state-of-the-art deep learning models are used to enrich the data with
important attributes, including sentiment labels, named-entities (e.g.,
mentions of persons, organizations, locations), user types, and gender
information. Last but not least, a geotagging method is proposed to assign
country, state, county, and city information to tweets, enabling a myriad of
data analysis tasks to understand real-world issues. Our sentiment and trend
analyses reveal interesting insights and confirm TBCOV's broad coverage of
important topics.","['Muhammad Imran', 'Umair Qazi', 'Ferda Ofli']",5,0.7568903
"Misinformation during pandemic situations like COVID-19 is growing rapidly on
social media and other platforms. This expeditious growth of misinformation
creates adverse effects on the people living in the society. Researchers are
trying their best to mitigate this problem using different approaches based on
Machine Learning (ML), Deep Learning (DL), and Natural Language Processing
(NLP). This survey aims to study different approaches of misinformation
detection on COVID-19 in recent literature to help the researchers in this
domain. More specifically, we review the different methods used for COVID-19
misinformation detection in their research with an overview of data
pre-processing and feature extraction methods to get a better understanding of
their work. We also summarize the existing datasets which can be used for
further research. Finally, we discuss the limitations of the existing methods
and highlight some potential future research directions along this dimension to
combat the spreading of misinformation during a pandemic.","['A. R. Sana Ullah', 'Anupam Das', 'Anik Das', 'Muhammad Ashad Kabir', 'Kai Shu']",0,0.7211406
"Digital information exchange enables quick creation and sharing of
information and thus changes existing habits. Social media is becoming the main
source of news for end-users replacing traditional media. This also enables the
proliferation of fake news, which misinforms readers and is used to serve the
interests of the creators. As a result, automated fake news detection systems
are attracting attention. However, automatic fake news detection presents a
major challenge; content evaluation is increasingly becoming the responsibility
of the end-user. Thus, in the present study we used information quality (IQ) as
an instrument to investigate how users can detect fake news. Specifically, we
examined how users perceive fake news in the form of shorter paragraphs on
individual IQ dimensions. We also investigated which user characteristics might
affect fake news detection. We performed an empirical study with 1123 users,
who evaluated randomly generated stories with statements of various level of
correctness by individual IQ dimensions. The results reveal that IQ can be used
as a tool for fake news detection. Our findings show that (1) domain knowledge
has a positive impact on fake news detection; (2) education in combination with
domain knowledge improves fake news detection; and (3) personality trait
conscientiousness contributes significantly to fake news detection in all
dimensions.","['Alja≈æ Zrnec', 'Marko Po≈æenel', 'Dejan Lavbiƒç']",4,0.8393899
"After George Floyd's death in May 2020, the volume of discussion in social
media increased dramatically. A series of protests followed this tragic event,
called as the 2020 BlackLivesMatter movement. Eventually, many user accounts
are deleted by their owners or suspended due to violating the rules of social
media platforms. In this study, we analyze what happened in Twitter before and
after the event triggers with respect to deleted and suspended users. We create
a novel dataset that includes approximately 500k users sharing 20m tweets, half
of whom actively participated in the 2020 BlackLivesMatter discussion, but some
of them were deleted or suspended later. We particularly examine the factors
for undesirable behavior in terms of spamming, negative language, hate speech,
and misinformation spread. We find that the users who participated to the 2020
BlackLivesMatter discussion have more negative and undesirable tweets, compared
to the users who did not. Furthermore, the number of new accounts in Twitter
increased significantly after the trigger event occurred, yet new users are
more oriented to have undesirable tweets, compared to old ones.","['Cagri Toraman', 'Furkan ≈ûahinu√ß', 'Eyup Halit Yilmaz']",10,0.68958473
"The rise of social networks as the primary means of communication in almost
every country in the world has simultaneously triggered an increase in the
amount of fake news circulating online. This fact became particularly evident
during the 2016 U.S. political elections and even more so with the advent of
the COVID-19 pandemic. Several research studies have shown how the effects of
fake news dissemination can be mitigated by promoting greater competence
through lifelong learning and discussion communities, and generally rigorous
training in the scientific method and broad interdisciplinary education. The
urgent need for models that can describe the growing infodemic of fake news has
been highlighted by the current pandemic. The resulting slowdown in vaccination
campaigns due to misinformation and generally the inability of individuals to
discern the reliability of information is posing enormous risks to the
governments of many countries. In this research using the tools of kinetic
theory we describe the interaction between fake news spreading and competence
of individuals through multi-population models in which fake news spreads
analogously to an infectious disease with different impact depending on the
level of competence of individuals. The level of competence, in particular, is
subject to an evolutionary dynamic due to both social interactions between
agents and external learning dynamics. The results show how the model is able
to correctly describe the dynamics of diffusion of fake news and the important
role of competence in their containment.","['Jonathan Franceschi', 'Lorenzo Pareschi']",0,0.69585586
"Modeling information cascades in a social network through the lenses of the
ideological leaning of its users can help understanding phenomena such as
misinformation propagation and confirmation bias, and devising techniques for
mitigating their toxic effects.
  In this paper we propose a stochastic model to learn the ideological leaning
of each user in a multidimensional ideological space, by analyzing the way
politically salient content propagates. In particular, our model assumes that
information propagates from one user to another if both users are interested in
the topic and ideologically aligned with each other. To infer the parameters of
our model, we devise a gradient-based optimization procedure maximizing the
likelihood of an observed set of information cascades. Our experiments on
real-world political discussions on Twitter and Reddit confirm that our model
is able to learn the political stance of the social media users in a
multidimensional ideological space.","['Corrado Monti', 'Giuseppe Manco', 'Cigdem Aslay', 'Francesco Bonchi']",0,0.64031816
"The overwhelming abundance of data has created a misinformation crisis.
Unverified sensationalism that is designed to grab the readers' short attention
span, when crafted with malice, has caused irreparable damage to our society's
structure. As a result, determining the reliability of an article has become a
crucial task. After various ablation studies, we propose a multi-input model
that can effectively leverage both tabular metadata and post content for the
task. Applying state-of-the-art finetuning techniques for the pretrained
component and training strategies for our complete model, we have achieved a
0.9462 ROC-score on the VLSP private test set.","['Hoang Viet Trinh', 'Tung Tien Bui', 'Tam Minh Nguyen', 'Huy Quang Dao', 'Quang Huu Pham', 'Ngoc N. Tran', 'Ta Minh Thanh']",1,0.623336
"The nuisance of misinformation and fake news has escalated many folds since
the advent of online social networks. Human consciousness and decision-making
capabilities are negatively influenced by manipulated, fabricated, biased or
unverified news posts. Therefore, there is a high demand for designing veracity
analysis systems to detect fake information contents in multiple data
modalities. In an attempt to find a sophisticated solution to this critical
issue, we proposed an architecture to consider both the textual and visual
attributes of the data. After the data pre-processing is done, text and image
features are extracted from the training data using separate deep learning
models. Feature extraction from text is done using BERT and ALBERT language
models that leverage the benefits of bidirectional training of transformers
using a deep self-attention mechanism. The Inception-ResNet-v2 deep neural
network model is employed for image data to perform the task. The proposed
framework focused on two independent multi-modal fusion architectures of BERT
and Inception-ResNet-v2 as well as ALBERT and Inception-ResNet-v2. Multi-modal
fusion of textual and visual branches is extensively experimented and analysed
using concatenation of feature vectors and weighted averaging of probabilities
named as Early Fusion and Late Fusion respectively. Three publicly available
broadly accepted datasets All Data, Weibo and MediaEval 2016 that incorporates
English news articles, Chinese news articles, and Tweets correspondingly are
used so that our designed framework's outcomes can be properly tested and
compared with previous notable work in the domain.","['Priyanka Meel', 'Dinesh Kumar Vishwakarma']",8,0.7125083
"The online spreading of fake news is a major issue threatening entire
societies. Much of this spreading is enabled by new media formats, namely
social networks and online media sites. Researchers and practitioners have been
trying to answer this by characterizing the fake news and devising automated
methods for detecting them. The detection methods had so far only limited
success, mostly due to the complexity of the news content and context and lack
of properly annotated datasets. One possible way to boost the efficiency of
automated misinformation detection methods, is to imitate the detection work of
humans. It is also important to understand the news consumption behavior of
online users. In this paper, we present an eye-tracking study, in which we let
44 lay participants to casually read through a social media feed containing
posts with news articles, some of which were fake. In a second run, we asked
the participants to decide on the truthfulness of these articles. We also
describe a follow-up qualitative study with a similar scenario but this time
with 7 expert fake news annotators. We present the description of both studies,
characteristics of the resulting dataset (which we hereby publish) and several
findings.","['Jakub Simko', 'Patrik Racsko', 'Matus Tomlein', 'Martin Hanakova', 'Robert Moro', 'Maria Bielikova']",4,0.8831266
"We present an overview of the second edition of the CheckThat! Lab at CLEF
2019. The lab featured two tasks in two different languages: English and
Arabic. Task 1 (English) challenged the participating systems to predict which
claims in a political debate or speech should be prioritized for fact-checking.
Task 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a
check-worthy claim based on their usefulness for fact-checking that claim, (B)
classify these same Web pages according to their degree of usefulness for
fact-checking the target claim, (C) identify useful passages from these pages,
and (D) use the useful pages to predict the claim's factuality. CheckThat!
provided a full evaluation framework, consisting of data in English (derived
from fact-checking sources) and Arabic (gathered and annotated from scratch)
and evaluation based on mean average precision (MAP) and normalized discounted
cumulative gain (nDCG) for ranking, and F1 for classification. A total of 47
teams registered to participate in this lab, and fourteen of them actually
submitted runs (compared to nine last year). The evaluation results show that
the most successful approaches to Task 1 used various neural networks and
logistic regression. As for Task 2, learning-to-rank was used by the highest
scoring runs for subtask A, while different classifiers were used in the other
subtasks. We release to the research community all datasets from the lab as
well as the evaluation scripts, which should enable further research in the
important tasks of check-worthiness estimation and automatic claim
verification.","['Tamer Elsayed', 'Preslav Nakov', 'Alberto Barr√≥n-Cede√±o', 'Maram Hasanain', 'Reem Suwaileh', 'Giovanni Da San Martino', 'Pepa Atanasova']",1,0.6755244
"While COVID-19 vaccines are finally becoming widely available, a second
pandemic that revolves around the circulation of anti-vaxxer fake news may
hinder efforts to recover from the first one. With this in mind, we performed
an extensive analysis of Arabic and English tweets about COVID-19 vaccines,
with focus on messages originating from Qatar. We found that Arabic tweets
contain a lot of false information and rumors, while English tweets are mostly
factual. However, English tweets are much more propagandistic than Arabic ones.
In terms of propaganda techniques, about half of the Arabic tweets express
doubt, and 1/5 use loaded language, while English tweets are abundant in loaded
language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in
terms of framing, Arabic tweets adopt a health and safety perspective, while in
English economic concerns dominate.","['Preslav Nakov', 'Firoj Alam', 'Shaden Shaar', 'Giovanni Da San Martino', 'Yifan Zhang']",12,0.5619951
"WhatsApp emerged as a major communication platform in many countries in the
recent years. Despite offering only one-to-one and small group conversations,
WhatsApp has been shown to enable the formation of a rich underlying network,
crossing the boundaries of existing groups, and with structural properties that
favor information dissemination at large. Indeed, WhatsApp has reportedly been
used as a forum of misinformation campaigns with significant social, political
and economic consequences in several countries. In this article, we aim at
complementing recent studies on misinformation spread on WhatsApp, mostly
focused on content properties and propagation dynamics, by looking into the
network that connects users sharing the same piece of content. Specifically, we
present a hierarchical network-oriented characterization of the users engaged
in misinformation spread by focusing on three perspectives: individuals,
WhatsApp groups and user communities, i.e., groupings of users who,
intentionally or not, share the same content disproportionately often. By
analyzing sharing and network topological properties, our study offers valuable
insights into how WhatsApp users leverage the underlying network connecting
different groups to gain large reach in the spread of misinformation on the
platform.","['Gabriel Peres Nobre', 'Carlos H. G. Ferreira', 'Jussara M. Almeida']",3,0.6715896
"Though significant efforts such as removing false claims and promoting
reliable sources have been increased to combat COVID-19 ""misinfodemic"", it
remains an unsolved societal challenge if lacking a proper understanding of
susceptible online users, i.e., those who are likely to be attracted by,
believe and spread misinformation. This study attempts to answer {\it who}
constitutes the population vulnerable to the online misinformation in the
pandemic, and what are the robust features and short-term behavior signals that
distinguish susceptible users from others. Using a 6-month longitudinal user
panel on Twitter collected from a geopolitically diverse network-stratified
samples in the US, we distinguish different types of users, ranging from social
bots to humans with various level of engagement with COVID-related
misinformation. We then identify users' online features and situational
predictors that correlate with their susceptibility to COVID-19 misinformation.
This work brings unique contributions: First, contrary to the prior studies on
bot influence, our analysis shows that social bots' contribution to
misinformation sharing was surprisingly low, and human-like users'
misinformation behaviors exhibit heterogeneity and temporal variability. While
the sharing of misinformation was highly concentrated, the risk of occasionally
sharing misinformation for average users remained alarmingly high. Second, our
findings highlight the political sensitivity activeness and responsiveness to
emotionally-charged content among susceptible users. Third, we demonstrate a
feasible solution to efficiently predict users' transient susceptibility solely
based on their short-term news consumption and exposure from their networks.
Our work has an implication in designing effective intervention mechanism to
mitigate the misinformation dissipation.","['Xian Teng', 'Yu-Ru Lin', 'Wen-Ting Chung', 'Ang Li', 'Adriana Kovashka']",0,0.7651654
"Recent research suggests that not all fact-checking efforts are equal: when
and what is fact-checked plays a pivotal role in effectively correcting
misconceptions. In that context, signals capturing how much attention specific
topics receive on the Internet have the potential to study (and possibly
support) fact-checking efforts. This paper proposes a framework to study
fact-checking with online attention signals. The framework consists of: 1)
extracting claims from fact-checking efforts; 2) linking such claims with
knowledge graph entities; and 3) estimating the online attention these entities
receive. We use this framework to conduct a preliminary study of a dataset of
879 COVID-19-related fact-checks done in 2020 by 81 international
organizations. Our findings suggest that there is often a disconnect between
online attention and fact-checking efforts. For example, in around 40% of
countries that fact-checked ten or more claims, half or more than half of the
ten most popular claims were not fact-checked. Our analysis also shows that
claims are first fact-checked after receiving, on average, 35% of the total
online attention they would eventually receive in 2020. Yet, there is a
considerable variation among claims: some were fact-checked before receiving a
surge of misinformation-induced online attention; others are fact-checked much
later. Overall, our work suggests that the incorporation of online attention
signals may help organizations assess their fact-checking efforts and choose
what and when to fact-check claims or stories. Also, in the context of
international collaboration, where claims are fact-checked multiple times
across different countries, online attention could help organizations keep
track of which claims are ""migrating"" between countries.","['Manoel Horta Ribeiro', 'Savvas Zannettou', 'Oana Goga', 'Fabr√≠cio Benevenuto', 'Robert West']",1,0.5880665
"Irrespective of the success of the deep learning-based mixed-domain transfer
learning approach for solving various Natural Language Processing tasks, it
does not lend a generalizable solution for detecting misinformation from
COVID-19 social media data. Due to the inherent complexity of this type of
data, caused by its dynamic (context evolves rapidly), nuanced (misinformation
types are often ambiguous), and diverse (skewed, fine-grained, and overlapping
categories) nature, it is imperative for an effective model to capture both the
local and global context of the target domain. By conducting a systematic
investigation, we show that: (i) the deep Transformer-based pre-trained models,
utilized via the mixed-domain transfer learning, are only good at capturing the
local context, thus exhibits poor generalization, and (ii) a combination of
shallow network-based domain-specific models and convolutional neural networks
can efficiently extract local as well as global context directly from the
target data in a hierarchical fashion, enabling it to offer a more
generalizable solution.","['Yuanzhi Chen', 'Mohammad Rashedul Hasan']",2,0.73147315
"The outbreak of COVID-19 has resulted in an ""infodemic"" that has encouraged
the propagation of misinformation about COVID-19 and cure methods which, in
turn, could negatively affect the adoption of recommended public health
measures in the larger population. In this paper, we provide a new multimodal
(consisting of images, text and temporal information) labeled dataset
containing news articles and tweets on the COVID-19 vaccine. We collected 2,593
news articles from 80 publishers for one year between Feb 16th 2020 to May 8th
2021 and 24184 Twitter posts (collected between April 17th 2021 to May 8th
2021). We combine ratings from two news media ranking sites: Medias Bias Chart
and Media Bias/Fact Check (MBFC) to classify the news dataset into two levels
of credibility: reliable and unreliable. The combination of two filters allows
for higher precision of labeling. We also propose a stance detection mechanism
to annotate tweets into three levels of credibility: reliable, unreliable and
inconclusive. We provide several statistics as well as other analytics like,
publisher distribution, publication date distribution, topic analysis, etc. We
also provide a novel architecture that classifies the news data into
misinformation or truth to provide a baseline performance for this dataset. We
find that the proposed architecture has an F-Score of 0.919 and accuracy of
0.882 for fake news detection. Furthermore, we provide benchmark performance
for misinformation detection on tweet dataset. This new multimodal dataset can
be used in research on COVID-19 vaccine, including misinformation detection,
influence of fake COVID-19 vaccine information, etc.","['Mingxuan Chen', 'Xinqiao Chu', 'K. P. Subbalakshmi']",4,0.71385306
"Many researchers studying online communities seek to make them better.
However, beyond a small set of widely-held values, such as combating
misinformation and abuse, determining what 'better' means can be challenging,
as community members may disagree, values may be in conflict, and different
communities may have differing preferences as a whole. In this work, we present
the first study that elicits values directly from members across a diverse set
of communities. We survey 212 members of 627 unique subreddits and ask them to
describe their values for their communities in their own words. Through
iterative categorization of 1,481 responses, we develop and validate a
comprehensive taxonomy of community values, consisting of 29 subcategories
within nine top-level categories, enabling principled, quantitative study of
community values by researchers. Using our taxonomy, we reframe existing
research problems, such as managing influxes of new members, as tensions
between different values, and we identify understudied values, such as those
regarding content quality and community size. We call for greater attention to
vulnerable community members' values, and we make our codebook public for use
in future research.","['Galen Weld', 'Amy X. Zhang', 'Tim Althoff']",3,0.5382885
"Recent years have seen a strong uptick in both the prevalence and real-world
consequences of false information spread through online platforms. At the same
time, encrypted messaging systems such as WhatsApp, Signal, and Telegram, are
rapidly gaining popularity as users seek increased privacy in their digital
lives.
  The challenge we address is how to combat the viral spread of misinformation
without compromising privacy. Our FACTS system tracks user complaints on
messages obliviously, only revealing the message's contents and originator once
sufficiently many complaints have been lodged.
  Our system is private, meaning it does not reveal anything about the senders
or contents of messages which have received few or no complaints; secure,
meaning there is no way for a malicious user to evade the system or gain an
outsized impact over the complaint system; and scalable, as we demonstrate
excellent practical efficiency for up to millions of complaints per day.
  Our main technical contribution is a new collaborative counting Bloom filter,
a simple construction with difficult probabilistic analysis, which may have
independent interest as a privacy-preserving randomized count sketch data
structure.
  Compared to prior work on message flagging and tracing in end-to-end
encrypted messaging, our novel contribution is the addition of a high threshold
of multiple complaints that are needed before a message is audited or flagged.
  We present and carefully analyze the probabilistic performance of our data
structure, provide a precise security definition and proof, and then measure
the accuracy and scalability of our scheme via experimentation.","['Linsheng Liu', 'Daniel S. Roche', 'Austin Theriault', 'Arkady Yerukhimovich']",4,0.60472786
"The role of social media in opinion formation has far-reaching implications
in all spheres of society. Though social media provide platforms for expressing
news and views, it is hard to control the quality of posts due to the sheer
volumes of posts on platforms like Twitter and Facebook. Misinformation and
rumours have lasting effects on society, as they tend to influence people's
opinions and also may motivate people to act irrationally. It is therefore very
important to detect and remove rumours from these platforms. The only way to
prevent the spread of rumours is through automatic detection and classification
of social media posts. Our focus in this paper is the Twitter social medium, as
it is relatively easy to collect data from Twitter. The majority of previous
studies used supervised learning approaches to classify rumours on Twitter.
These approaches rely on feature extraction to obtain both content and context
features from the text of tweets to distinguish rumours and non-rumours.
Manually extracting features however is time-consuming considering the volume
of tweets. We propose a novel approach to deal with this problem by utilising
sentence embedding using BERT to identify rumours on Twitter, rather than the
usual feature extraction techniques. We use sentence embedding using BERT to
represent each tweet's sentences into a vector according to the contextual
meaning of the tweet. We classify those vectors into rumours or non-rumours by
using various supervised learning techniques. Our BERT based models improved
the accuracy by approximately 10% as compared to previous methods.","['Rini Anggrainingsih', 'Ghulam Mubashar Hassan', 'Amitava Datta']",10,0.68716586
"Twitter bot detection has become an important and challenging task to combat
misinformation and protect the integrity of the online discourse.
State-of-the-art approaches generally leverage the topological structure of the
Twittersphere, while they neglect the heterogeneity of relations and influence
among users. In this paper, we propose a novel bot detection framework to
alleviate this problem, which leverages the topological structure of
user-formed heterogeneous graphs and models varying influence intensity between
users. Specifically, we construct a heterogeneous information network with
users as nodes and diversified relations as edges. We then propose relational
graph transformers to model heterogeneous influence between users and learn
node representations. Finally, we use semantic attention networks to aggregate
messages across users and relations and conduct heterogeneity-aware Twitter bot
detection. Extensive experiments demonstrate that our proposal outperforms
state-of-the-art methods on a comprehensive Twitter bot detection benchmark.
Additional studies also bear out the effectiveness of our proposed relational
graph transformers, semantic attention networks and the graph-based approach in
general.","['Shangbin Feng', 'Zhaoxuan Tan', 'Rui Li', 'Minnan Luo']",2,0.7219615
"In this paper, we explore whether automatic face recognition can help in
verifying widespread misinformation on social media, particularly conspiracy
theories that are based on the existence of body doubles. The conspiracy theory
addressed in this paper is the case of the Melania Trump body double. We
employed four different state-of-the-art descriptors for face recognition to
verify the integrity of the claim of the studied conspiracy theory. In
addition, we assessed the impact of different image quality metrics on the
variation of face recognition results. Two sets of image quality metrics were
considered: acquisition-related metrics and subject-related metrics.","['Khawla Mallat', 'Fabiola Becerra-Riera', 'Annette Morales-Gonz√°lez', 'Heydi M√©ndez-V√°zquez', 'Jean-Luc Dugelay']",7,0.5345838
"Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.
  In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.","['Shiri Dori-Hacohen', 'Roberto Montenegro', 'Fabricio Murai', 'Scott A. Hale', 'Keen Sung', 'Michela Blain', 'Jennifer Edwards-Johnson']",9,0.7597822
"A great deal of empirical research has examined who falls for misinformation
and why. Here, we introduce a formal game-theoretic model of engagement with
news stories that captures the strategic interplay between (mis)information
consumers and producers. A key insight from the model is that observed patterns
of engagement do not necessarily reflect the preferences of consumers. This is
because producers seeking to promote misinformation can use strategies that
lead moderately inattentive readers to engage more with false stories than true
ones -- even when readers prefer more accurate over less accurate information.
We then empirically test people's preferences for accuracy in the news. In
three studies, we find that people strongly prefer to click and share news they
perceive as more accurate -- both in a general population sample, and in a
sample of users recruited through Twitter who had actually shared links to
misinformation sites online. Despite this preference for accurate news -- and
consistent with the predictions of our model -- we find markedly different
engagement patterns for articles from misinformation versus mainstream news
sites. Using 1,000 headlines from 20 misinformation and 20 mainstream news
sites, we compare Facebook engagement data with 20,000 accuracy ratings
collected in a survey experiment. Engagement with a headline is negatively
correlated with perceived accuracy for misinformation sites, but positively
correlated with perceived accuracy for mainstream sites. Taken together, these
theoretical and empirical results suggest that consumer preferences cannot be
straightforwardly inferred from empirical patterns of engagement.","['Alexander J. Stewart', 'Antonio A. Arechar', 'David G. Rand', 'Joshua B. Plotkin']",4,0.7350405
"Research in media forensics has gained traction to combat the spread of
misinformation. However, most of this research has been directed towards
content generated on social media. Biomedical image forensics is a related
problem, where manipulation or misuse of images reported in biomedical research
documents is of serious concern. The problem has failed to gain momentum beyond
an academic discussion due to an absence of benchmark datasets and standardized
tasks. In this paper we present BioFors -- the first dataset for benchmarking
common biomedical image manipulations. BioFors comprises 47,805 images
extracted from 1,031 open-source research papers. Images in BioFors are divided
into four categories -- Microscopy, Blot/Gel, FACS and Macroscopy. We also
propose three tasks for forensic analysis -- external duplication detection,
internal duplication detection and cut/sharp-transition detection. We benchmark
BioFors on all tasks with suitable state-of-the-art algorithms. Our results and
analysis show that existing algorithms developed on common computer vision
datasets are not robust when applied to biomedical images, validating that more
research is required to address the unique challenges of biomedical image
forensics.","['Ekraam Sabir', 'Soumyaroop Nandi', 'Wael AbdAlmageed', 'Prem Natarajan']",7,0.5666455
"Online users today are exposed to misleading and propagandistic news articles
and media posts on a daily basis. To counter thus, a number of approaches have
been designed aiming to achieve a healthier and safer online news and media
consumption. Automatic systems are able to support humans in detecting such
content; yet, a major impediment to their broad adoption is that besides being
accurate, the decisions of such systems need also to be interpretable in order
to be trusted and widely adopted by users. Since misleading and propagandistic
content influences readers through the use of a number of deception techniques,
we propose to detect and to show the use of such techniques as a way to offer
interpretability. In particular, we define qualitatively descriptive features
and we analyze their suitability for detecting deception techniques. We further
show that our interpretable features can be easily combined with pre-trained
language models, yielding state-of-the-art results.","['Seunghak Yu', 'Giovanni Da San Martino', 'Mitra Mohtarami', 'James Glass', 'Preslav Nakov']",1,0.6686511
"We propose a novel framework for predicting the factuality of reporting of
news media outlets by studying the user attention cycles in their YouTube
channels. In particular, we design a rich set of features derived from the
temporal evolution of the number of views, likes, dislikes, and comments for a
video, which we then aggregate to the channel level. We develop and release a
dataset for the task, containing observations of user attention on YouTube
channels for 489 news media. Our experiments demonstrate both complementarity
and sizable improvements over state-of-the-art textual representations.","['Krasimira Bozhanova', 'Yoan Dinkov', 'Ivan Koychev', 'Maria Castaldo', 'Tommaso Venturini', 'Preslav Nakov']",4,0.55693066
"Historians and researchers trust web archives to preserve social media
content that no longer exists on the live web. However, what we see on the live
web and how it is replayed in the archive are not always the same. In this
paper, we document and analyze the problems in archiving Twitter ever since
Twitter forced the use of its new UI in June 2020. Most web archives were
unable to archive the new UI, resulting in archived Twitter pages displaying
Twitter's ""Something went wrong"" error. The challenges in archiving the new UI
forced web archives to continue using the old UI. To analyze the potential loss
of information in web archival data due to this change, we used the personal
Twitter account of the 45th President of the United States, @realDonaldTrump,
which was suspended by Twitter on January 8, 2021. Trump's account was heavily
labeled by Twitter for spreading misinformation, however we discovered that
there is no evidence in web archives to prove that some of his tweets ever had
a label assigned to them. We also studied the possibility of temporal
violations in archived versions of the new UI, which may result in the replay
of pages that never existed on the live web. Our goal is to educate researchers
who may use web archives and caution them when drawing conclusions based on
archived Twitter pages.","['Kritika Garg', 'Himarsha R. Jayanetti', 'Sawood Alam', 'Michele C. Weigle', 'Michael L. Nelson']",10,0.5846907
"Fact-checking has become increasingly important due to the speed with which
both information and misinformation can spread in the modern media ecosystem.
Therefore, researchers have been exploring how fact-checking can be automated,
using techniques based on natural language processing, machine learning,
knowledge representation, and databases to automatically predict the veracity
of claims. In this paper, we survey automated fact-checking stemming from
natural language processing, and discuss its connections to related tasks and
disciplines. In this process, we present an overview of existing datasets and
models, aiming to unify the various definitions given and identify common
concepts. Finally, we highlight challenges for future research.","['Zhijiang Guo', 'Michael Schlichtkrull', 'Andreas Vlachos']",1,0.7867355
"The past decade has seen a substantial rise in the amount of mis- and
disinformation online, from targeted disinformation campaigns to influence
politics, to the unintentional spreading of misinformation about public health.
This development has spurred research in the area of automatic fact checking,
from approaches to detect check-worthy claims and determining the stance of
tweets towards claims, to methods to determine the veracity of claims given
evidence documents. These automatic methods are often content-based, using
natural language processing methods, which in turn utilise deep neural networks
to learn higher-order features from text in order to make predictions. As deep
neural networks are black-box models, their inner workings cannot be easily
explained. At the same time, it is desirable to explain how they arrive at
certain decisions, especially if they are to be used for decision making. While
this has been known for some time, the issues this raises have been exacerbated
by models increasing in size, and by EU legislation requiring models to be used
for decision making to provide explanations, and, very recently, by legislation
requiring online platforms operating in the EU to provide transparent reporting
on their services. Despite this, current solutions for explainability are still
lacking in the area of fact checking. This thesis presents my research on
automatic fact checking, including claim check-worthiness detection, stance
detection and veracity prediction. Its contributions go beyond fact checking,
with the thesis proposing more general machine learning solutions for natural
language processing in the area of learning with limited labelled data.
Finally, the thesis presents some first solutions for explainable fact
checking.",['Isabelle Augenstein'],1,0.79843974
"The COVID-19 pandemic fueled one of the most rapid vaccine developments in
history. However, misinformation spread through online social media often leads
to negative vaccine sentiment and hesitancy. To investigate COVID-19
vaccine-related discussion in social media, we conducted a sentiment analysis
and Latent Dirichlet Allocation topic modeling on textual data collected from
13 Reddit communities focusing on the COVID-19 vaccine from Dec 1, 2020, to May
15, 2021. Data were aggregated and analyzed by month to detect changes in any
sentiment and latent topics. ty analysis suggested these communities expressed
more positive sentiment than negative regarding the vaccine-related discussions
and has remained static over time. Topic modeling revealed community members
mainly focused on side effects rather than outlandish conspiracy theories.
Covid-19 vaccine-related content from 13 subreddits show that the sentiments
expressed in these communities are overall more positive than negative and have
not meaningfully changed since December 2020. Keywords indicating vaccine
hesitancy were detected throughout the LDA topic modeling. Public sentiment and
topic modeling analysis regarding vaccines could facilitate the implementation
of appropriate messaging, digital interventions, and new policies to promote
vaccine confidence.","['Chad A Melton', 'Olufunto A Olusanya', 'Nariman Ammar', 'Arash Shaban-Nejad']",12,0.870205
"Can crowd workers be trusted to judge whether news-like articles circulating
on the Internet are misleading, or does partisanship and inexperience get in
the way? And can the task be structured in a way that reduces partisanship? We
assembled pools of both liberal and conservative crowd raters and tested three
ways of asking them to make judgments about 374 articles. In a no research
condition, they were just asked to view the article and then render a judgment.
In an individual research condition, they were also asked to search for
corroborating evidence and provide a link to the best evidence they found. In a
collective research condition, they were not asked to search, but instead to
review links collected from workers in the individual research condition. Both
research conditions reduced partisan disagreement in judgments. The individual
research condition was most effective at producing alignment with journalists'
assessments. In this condition, the judgments of a panel of sixteen or more
crowd workers were better than that of a panel of three expert journalists, as
measured by alignment with a held out journalist's ratings.","['Paul Resnick', 'Aljohara Alfayez', 'Jane Im', 'Eric Gilbert']",3,0.51627445
"In this work, we collect a moderate-sized representative corpus of tweets
(200,000 approx.) pertaining Covid-19 vaccination spanning over a period of
seven months (September 2020 - March 2021). Following a Transfer Learning
approach, we utilize the pre-trained Transformer-based XLNet model to classify
tweets as Misleading or Non-Misleading and validate against a random subset of
results manually. We build on this to study and contrast the characteristics of
tweets in the corpus that are misleading in nature against non-misleading ones.
This exploratory analysis enables us to design features (such as sentiments,
hashtags, nouns, pronouns, etc) that can, in turn, be exploited for classifying
tweets as (Non-)Misleading using various ML models in an explainable manner.
Specifically, several ML models are employed for prediction, with up to 90%
accuracy, and the importance of each feature is explained using SHAP
Explainable AI (XAI) tool. While the thrust of this work is principally
exploratory analysis in order to obtain insights on the online discourse on
Covid-19 vaccination, we conclude the paper by outlining how these insights
provide the foundations for a more actionable approach to mitigate
misinformation. The curated dataset and code is made available (Github
repository) so that the research community at large can reproduce, compare
against, or build upon this work.","['Shakshi Sharma', 'Rajesh Sharma', 'Anwitaman Datta']",1,0.7251376
"Most corpora approach misinformation as a binary problem, classifying texts
as real or fake. However, they fail to consider the diversity of existing
textual genres and types, which present different properties usually associated
with credibility. To address this problem, we created MINT, a comprehensive
corpus of news articles collected from mainstream and independent Portuguese
media sources, over a full year period. MINT includes five categories of
content: hard news, opinion articles, soft news, satirical news, and conspiracy
theories. This paper presents a set of linguistic metrics for characterization
of the articles in each category, based on the analysis of an annotation
initiative performed by online readers. The results show that (i) conspiracy
theories and opinion articles present similar levels of subjectivity, and make
use of fallacious arguments; (ii) irony and sarcasm are not only prevalent in
satirical news, but also in conspiracy and opinion news articles; and (iii)
hard news differ from soft news by resorting to more sources of information,
and presenting a higher degree of objectivity.","['Danielle Caled', 'Paula Carvalho', 'M√°rio J. Silva']",4,0.74710834
"Narrative sensemaking is a fundamental process to understand sequential
information. Narrative maps are a visual representation framework that can aid
analysts in this process. They allow analysts to understand the big picture of
a narrative, uncover new relationships between events, and model connections
between storylines. As a sensemaking tool, narrative maps have applications in
intelligence analysis, misinformation modeling, and computational journalism.
In this work, we seek to understand how analysts construct narrative maps in
order to improve narrative map representation and extraction methods. We
perform an experiment with a data set of news articles. Our main contribution
is an analysis of how analysts construct narrative maps. The insights extracted
from our study can be used to design narrative map visualizations, extraction
algorithms, and visual analytics tools to support the sensemaking process.","['Brian Felipe Keith Norambuena', 'Tanushree Mitra', 'Chris North']",0,0.53337836
"Natural language processing (NLP) plays a significant role in tools for the
COVID-19 pandemic response, from detecting misinformation on social media to
helping to provide accurate clinical information or summarizing scientific
research. However, the approaches developed thus far have not benefited all
populations, regions or languages equally. We discuss ways in which current and
future NLP approaches can be made more inclusive by covering low-resource
languages, including alternative modalities, leveraging out-of-the-box tools
and forming meaningful partnerships. We suggest several future directions for
researchers interested in maximizing the positive societal impacts of NLP.","['Alexandra Sasha Luccioni', 'Katherine Hoffmann Pham', 'Cynthia Sin Nga Lam', 'Joseph Aylett-Bullock', 'Miguel Luengo-Oroz']",8,0.6119487
"The growth of online Digital/social media has allowed a variety of ideas and
opinions to coexist. Social Media has appealed users due to the ease of fast
dissemination of information at low cost and easy access. However, due to the
growth in affordance of Digital platforms, users have become prone to consume
disinformation, misinformation, propaganda, and conspiracy theories. In this
paper, we wish to explore the links between the personality traits given by the
Big Five Inventory and their susceptibility to disinformation. More
speciDically, this study is attributed to capture the short- term as well as
the long-term effects of disinformation and its effects on the Dive personality
traits. Further, we expect to observe that different personalities traits have
different shifts in opinion and different increase or decrease of uncertainty
on an issue after consuming the disinformation. Based on the Dindings of this
study, we would like to propose a personalized narrative-based change in
behavior for different personality traits.","['Dipto Barman', 'Owen Conlan']",3,0.7039565
"The quality of digital information on the web has been disquieting due to the
lack of careful manual review. Consequently, a large volume of false textual
information has been disseminating for a long time since the prevalence of
social media. The potential negative influence of misinformation on the public
is a growing concern. Therefore, it is strongly motivated to detect online
misinformation as early as possible. Few-shot-few-clue learning applies in this
misinformation detection task when the number of annotated statements is quite
few (called few shots) and the corresponding evidence is also quite limited in
each shot (called few clues). Within the few-shot-few-clue framework, we
propose a Bayesian meta-learning algorithm to extract the shared patterns among
different topics (i.e.different tasks) of misinformation. Moreover, we derive a
scalable method, i.e., amortized variational inference, to optimize the
Bayesian meta-learning algorithm. Empirical results on three benchmark datasets
demonstrate the superiority of our algorithm. This work focuses more on
optimizing parameters than designing detection models, and will generate fresh
insights into data-efficient detection of online misinformation at early
stages.","['Qiang Zhang', 'Hongbin Huang', 'Shangsong Liang', 'Zaiqiao Meng', 'Emine Yilmaz']",2,0.7607614
"As news organizations embrace transparency practices on their websites to
distinguish themselves from those spreading misinformation, HCI designers have
the opportunity to help them effectively utilize the ideals of transparency to
build trust. How can we utilize transparency to promote trust in news? We
examine this question through a qualitative lens by interviewing journalists
and news consumers -- the two stakeholders in a news system. We designed a
scenario to demonstrate transparency features using two fundamental news
attributes that convey the trustworthiness of a news article: source and
message. In the interviews, our news consumers expressed the idea that news
transparency could be best shown by providing indicators of objectivity in two
areas (news selection and framing) and by providing indicators of evidence in
four areas (presence of source materials, anonymous sourcing, verification, and
corrections upon erroneous reporting). While our journalists agreed with news
consumers' suggestions of using evidence indicators, they also suggested
additional transparency indicators in areas such as the news reporting process
and personal/organizational conflicts of interest. Prompted by our scenario,
participants offered new design considerations for building trustworthy news
platforms, such as designing for easy comprehension, presenting appropriate
details in news articles (e.g., showing the number and nature of corrections
made to an article), and comparing attributes across news organizations to
highlight diverging practices. Comparing the responses from our two stakeholder
groups reveals conflicting suggestions with trade-offs between them. Our study
has implications for HCI designers in building trustworthy news systems.","['Md Momen Bhuiyan', 'Hayden Whitley', 'Michael Horning', 'Sang Won Lee', 'Tanushree Mitra']",4,0.70574975
"Enormous hope in the efficacy of vaccines became recently a successful
reality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,
fueled by exposure to social media misinformation about COVID-19 vaccines
became a major hurdle. Therefore, it is essential to automatically detect where
misinformation about COVID-19 vaccines on social media is spread and what kind
of misinformation is discussed, such that inoculation interventions can be
delivered at the right time and in the right place, in addition to
interventions designed to address vaccine hesitancy. This paper is addressing
the first step in tackling hesitancy against COVID-19 vaccines, namely the
automatic detection of known misinformation about the vaccines on Twitter, the
social media platform that has the highest volume of conversations about
COVID-19 and its vaccines. We present CoVaxLies, a new dataset of tweets judged
relevant to several misinformation targets about COVID-19 vaccines on which a
novel method of detecting misinformation was developed. Our method organizes
CoVaxLies in a Misinformation Knowledge Graph as it casts misinformation
detection as a graph link prediction problem. The misinformation detection
method detailed in this paper takes advantage of the link scoring functions
provided by several knowledge embedding methods. The experimental results
demonstrate the superiority of this method when compared with
classification-based methods, widely used currently.","['Maxwell A. Weinzierl', 'Sanda M. Harabagiu']",12,0.8653408
"Struggling to curb misinformation, social media platforms are experimenting
with design interventions to enhance consumption of credible news on their
platforms. Some of these interventions, such as the use of warning messages,
are examples of nudges -- a choice-preserving technique to steer behavior.
Despite their application, we do not know whether nudges could steer people
into making conscious news credibility judgments online and if they do, under
what constraints. To answer, we combine nudge techniques with heuristic based
information processing to design NudgeCred -- a browser extension for Twitter.
NudgeCred directs users' attention to two design cues: authority of a source
and other users' collective opinion on a report by activating three design
nudges -- Reliable, Questionable, and Unreliable, each denoting particular
levels of credibility for news tweets. In a controlled experiment, we found
that NudgeCred significantly helped users (n=430) distinguish news tweets'
credibility, unrestricted by three behavioral confounds -- political ideology,
political cynicism, and media skepticism. A five-day field deployment with
twelve participants revealed that NudgeCred improved their recognition of news
items and attention towards all of our nudges, particularly towards
Questionable. Among other considerations, participants proposed that designers
should incorporate heuristics that users' would trust. Our work informs
nudge-based system design approaches for online media.","['Md Momen Bhuiyan', 'Michael Horning', 'Sang Won Lee', 'Tanushree Mitra']",4,0.67175865
"Recent work has demonstrated the viability of using crowdsourcing as a tool
for evaluating the truthfulness of public statements. Under certain conditions
such as: (1) having a balanced set of workers with different backgrounds and
cognitive abilities; (2) using an adequate set of mechanisms to control the
quality of the collected data; and (3) using a coarse grained assessment scale,
the crowd can provide reliable identification of fake news. However, fake news
are a subtle matter: statements can be just biased (""cherrypicked""), imprecise,
wrong, etc. and the unidimensional truth scale used in existing work cannot
account for such differences. In this paper we propose a multidimensional
notion of truthfulness and we ask the crowd workers to assess seven different
dimensions of truthfulness selected based on existing literature: Correctness,
Neutrality, Comprehensibility, Precision, Completeness, Speaker's
Trustworthiness, and Informativeness. We deploy a set of quality control
mechanisms to ensure that the thousands of assessments collected on 180
publicly available fact-checked statements distributed over two datasets are of
adequate quality, including a custom search engine used by the crowd workers to
find web pages supporting their truthfulness assessments. A comprehensive
analysis of crowdsourced judgments shows that: (1) the crowdsourced assessments
are reliable when compared to an expert-provided gold standard; (2) the
proposed dimensions of truthfulness capture independent pieces of information;
(3) the crowdsourcing task can be easily learned by the workers; and (4) the
resulting assessments provide a useful basis for a more complete estimation of
statement truthfulness.","['Michael Soprano', 'Kevin Roitero', 'David La Barbera', 'Davide Ceolin', 'Damiano Spina', 'Stefano Mizzaro', 'Gianluca Demartini']",1,0.6478853
"The spread of misinformation, conspiracy, and questionable content and
information manipulation by foreign adversaries on social media has surged
along with the COVID-19 pandemic. Such malicious cyber-enabled actions may
cause increasing social polarization, health crises, and property loss. In this
paper, using fine-tuned contextualized embedding trained on Reddit, we tackle
the detection of the propaganda of such user accounts and their targeted issues
on Twitter during March 2020 when the COVID-19 epidemic became recognized as a
pandemic. Our result shows that the pro-China group appeared to be tweeting 35
to 115 times more than the neutral group. At the same time, neutral groups were
tweeting more positive-attitude content and voicing alarm for the COVID-19
situation. The pro-China group was also using more call-for-action words on
political issues not necessarily China-related.","['Rong-Ching Chang', 'Chu-Hsing Lin']",5,0.72254217
"In this paper, we employ a hypergame framework to analyze the
single-leader-multiple-followers (SLMF) Stackelberg security game with two
typical misinformed situations: misperception and deception. We provide a
stability criterion with the help of hyper Nash equilibrium (HNE) to
investigate both strategic stability and cognitive stability of equilibria in
SLMF games with misinformation. In fact, we find mild stable conditions such
that the equilibria with misperception and deception can become HNE. Moreover,
we discuss the robustness of the equilibria to reveal whether players have the
ability to keep their profits under the influence of some misinformation.","['Zhaoyang Cheng', 'Guanpu Chen', 'Yiguang Hong']",2,0.46024016
"Online Social Networks represent a novel opportunity for political campaigns,
revolutionising the paradigm of political communication. Nevertheless, many
studies uncovered the presence of d/misinformation campaigns or of malicious
activities by genuine or automated users, putting at severe risk the
credibility of online platforms. This phenomenon is particularly evident during
crucial political events, as political elections. In the present paper, we
provide a comprehensive description of the structure of the networks of
interactions among users and bots during the UK elections of 2019. In
particular, we focus on the polarised discussion about Brexit on Twitter
analysing a data set made of more than 10 million tweets posted for over a
month. We found that the presence of automated accounts fostered the debate
particularly in the days before the UK national elections, in which we find a
steep increase of bots in the discussion; in the days after the election day,
their incidence returned to values similar to the ones observed few weeks
before the elections. On the other hand, we found that the number of suspended
users (i.e. accounts that were removed by the platform for some violation of
the Twitter policy) remained constant until the election day, after which it
reached significantly higher values. Remarkably, after the TV debate between
Boris Johnson and Jeremy Corbyn, we observed the injection of a large number of
novel bots whose behaviour is markedly different from that of pre-existing
ones. Finally, we explored the bots' stance, finding that their activity is
spread across the whole political spectrum, although in different proportions,
and we studied the different usage of hashtags by automated accounts and
suspended users, thus targeting the formation of common narratives in different
sides of the debate.","['Matteo Bruno', 'Renaud Lambiotte', 'Fabio Saracco']",10,0.7953943
"The onset of the COVID-19 pandemic led to a global infodemic that has brought
unprecedented challenges for citizens, media, and fact-checkers worldwide. To
address this challenge, over a hundred fact-checking initiatives worldwide have
been monitoring the information space in their countries and publishing regular
debunks of viral false COVID-19 narratives. This study examines the database of
the CoronaVirusFacts Alliance, which contains 10,381 debunks related to
COVID-19 published in multiple languages by different fact-checking
organisations. Our spatiotemporal analysis reveals that similar or nearly
duplicate false COVID-19 narratives have been spreading in multiple modalities
and on various social media platforms in different countries, sometimes as much
as several months after the first debunk of that narrative has been published
by an International Fact-checking Network (IFCN) fact-checker. We also find
that misinformation involving general medical advice has spread across multiple
countries and hence has the highest proportion of false COVID-19 narratives
that keep being debunked. Furthermore, as manual fact-checking is an onerous
task in itself, therefore the need to repeatedly debunk the same narrative in
different countries is leading, over time, to a significant waste of
fact-checker resources. To this end, we propose the idea of including a
multilingual debunk search tool in the fact-checking pipeline, in addition to
recommending strongly that social media platforms need to adopt the same
technology at scale, so as to make the best use of scarce fact-checker
resources.","['Iknoor Singh', 'Kalina Bontcheva', 'Carolina Scarton']",5,0.7056223
"Recently, the misinformation problem has been addressed with a
crowdsourcing-based approach: to assess the truthfulness of a statement,
instead of relying on a few experts, a crowd of non-expert is exploited. We
study whether crowdsourcing is an effective and reliable method to assess
truthfulness during a pandemic, targeting statements related to COVID-19, thus
addressing (mis)information that is both related to a sensitive and personal
issue and very recent as compared to when the judgment is done. In our
experiments, crowd workers are asked to assess the truthfulness of statements,
and to provide evidence for the assessments. Besides showing that the crowd is
able to accurately judge the truthfulness of the statements, we report results
on workers behavior, agreement among workers, effect of aggregation functions,
of scales transformations, and of workers background and bias. We perform a
longitudinal study by re-launching the task multiple times with both novice and
experienced workers, deriving important insights on how the behavior and
quality change over time. Our results show that: workers are able to detect and
objectively categorize online (mis)information related to COVID-19; both
crowdsourced and expert judgments can be transformed and aggregated to improve
quality; worker background and other signals (e.g., source of information,
behavior) impact the quality of the data. The longitudinal study demonstrates
that the time-span has a major effect on the quality of the judgments, for both
novice and experienced workers. Finally, we provide an extensive failure
analysis of the statements misjudged by the crowd-workers.","['Kevin Roitero', 'Michael Soprano', 'Beatrice Portelli', 'Massimiliano De Luise', 'Damiano Spina', 'Vincenzo Della Mea', 'Giuseppe Serra', 'Stefano Mizzaro', 'Gianluca Demartini']",0,0.67073464
"Online discussion platforms offer a forum to strengthen and propagate belief
in misinformed conspiracy theories. Yet, they also offer avenues for conspiracy
theorists to express their doubts and experiences of cognitive dissonance. Such
expressions of dissonance may shed light on who abandons misguided beliefs and
under which circumstances. This paper characterizes self-disclosures of
dissonance about QAnon, a conspiracy theory initiated by a mysterious leader Q
and popularized by their followers, anons in conspiracy theory subreddits. To
understand what dissonance and disbelief mean within conspiracy communities, we
first characterize their social imaginaries, a broad understanding of how
people collectively imagine their social existence. Focusing on 2K posts from
two image boards, 4chan and 8chan, and 1.2 M comments and posts from 12
subreddits dedicated to QAnon, we adopt a mixed methods approach to uncover the
symbolic language representing the movement, expectations, practices, heroes
and foes of the QAnon community. We use these social imaginaries to create a
computational framework for distinguishing belief and dissonance from general
discussion about QAnon. Further, analyzing user engagement with QAnon
conspiracy subreddits, we find that self-disclosures of dissonance correlate
with a significant decrease in user contributions and ultimately with their
departure from the community. We contribute a computational framework for
identifying dissonance self-disclosures and measuring the changes in user
engagement surrounding dissonance. Our work can provide insights into designing
dissonance-based interventions that can potentially dissuade conspiracists from
online conspiracy discussion communities.","['Shruti Phadke', 'Mattia Samory', 'Tanushree Mitra']",3,0.6173854
"During the COVID-19 pandemic, social media platforms were ideal for
communicating due to social isolation and quarantine. Also, it was the primary
source of misinformation dissemination on a large scale, referred to as the
infodemic. Therefore, automatic debunking misinformation is a crucial problem.
To tackle this problem, we present two COVID-19 related misinformation datasets
on Twitter and propose a misinformation detection system comprising
network-based and content-based processes based on machine learning algorithms
and NLP techniques. In the network-based process, we focus on social
properties, network characteristics, and users. On the other hand, we classify
misinformation using the content of the tweets directly in the content-based
process, which contains text classification models (paragraph-level and
sentence-level) and similarity models. The evaluation results on the
network-based process show the best results for the artificial neural network
model with an F1 score of 88.68%. In the content-based process, our novel
similarity models, which obtained an F1 score of 90.26%, show an improvement in
the misinformation classification results compared to the network-based models.
In addition, in the text classification models, the best result was achieved
using the stacking ensemble-learning model by obtaining an F1 score of 95.18%.
Furthermore, we test our content-based models on the Constraint@AAAI2021
dataset, and by getting an F1 score of 94.38%, we improve the baseline results.
Finally, we develop a fact-checking website called Checkovid that uses each
process to detect misinformative and informative claims in the domain of
COVID-19 from different perspectives.","['Sajad Dadgar', 'Mehdi Ghatee']",1,0.6613366
"The spread of coronavirus and anti-vaccine conspiracies online hindered
public health responses to the pandemic. We examined the content of external
articles shared on Twitter from February to June 2020 to understand how
conspiracy theories and fake news competed with legitimate sources of
information. Examining external content--articles, rather than social media
posts--is a novel methodology that allows for non-social media specific
analysis of misinformation, tracking of changing narratives over time, and
determining which types of resources (government, news, scientific, or dubious)
dominate the pandemic vaccine conversation. We find that distinct narratives
emerge, those narratives change over time, and lack of government and
scientific messaging on coronavirus created an information vacuum filled by
both traditional news and conspiracy theories.","['Richard Kuzma', 'Iain J. Cruickshank', 'Kathleen M. Carley']",12,0.7083447
"Mistranslated numbers have the potential to cause serious effects, such as
financial loss or medical misinformation. In this work we develop comprehensive
assessments of the robustness of neural machine translation systems to
numerical text via behavioural testing. We explore a variety of numerical
translation capabilities a system is expected to exhibit and design effective
test examples to expose system underperformance. We find that numerical
mistranslation is a general issue: major commercial systems and
state-of-the-art research models fail on many of our test examples, for high-
and low-resource languages. Our tests reveal novel errors that have not
previously been reported in NMT systems, to the best of our knowledge. Lastly,
we discuss strategies to mitigate numerical mistranslation.","['Jun Wang', 'Chang Xu', 'Francisco Guzman', 'Ahmed El-Kishky', 'Benjamin I. P. Rubinstein', 'Trevor Cohn']",8,0.6690475
"Amidst the threat of digital misinformation, we offer a pilot study regarding
the efficacy of an online social media literacy campaign aimed at empowering
individuals in Indonesia with skills to help them identify misinformation. We
found that users who engaged with our online training materials and educational
videos were more likely to identify misinformation than those in our control
group (total $N$=1000). Given the promising results of our preliminary study,
we plan to expand efforts in this area, and build upon lessons learned from
this pilot study.","['Pamela Bilo Thomas', 'Clark Hogan-Taylor', 'Michael Yankoski', 'Tim Weninger']",0,0.63030076
"Neural machine translation systems are known to be vulnerable to adversarial
test inputs, however, as we show in this paper, these systems are also
vulnerable to training attacks. Specifically, we propose a poisoning attack in
which a malicious adversary inserts a small poisoned sample of monolingual text
into the training set of a system trained using back-translation. This sample
is designed to induce a specific, targeted translation behaviour, such as
peddling misinformation. We present two methods for crafting poisoned examples,
and show that only a tiny handful of instances, amounting to only 0.02% of the
training set, is sufficient to enact a successful attack. We outline a defence
method against said attacks, which partly ameliorates the problem. However, we
stress that this is a blind-spot in modern NMT, demanding immediate attention.","['Jun Wang', 'Chang Xu', 'Francisco Guzman', 'Ahmed El-Kishky', 'Yuqing Tang', 'Benjamin I. P. Rubinstein', 'Trevor Cohn']",8,0.55573356
"The declaration of COVID-19 as a pandemic has largely amplified the spread of
related information on social media, such as Twitter, Facebook, and
WeChat.Unlike the previous studies which focused on how to detect the
misinformation or fake news related toCOVID-19, we investigate how the disease
and information co-evolve in the population. We focus onCOVID-19and its
information during the period when the disease was widely spread in China,
i.e., from January 25th to March 24th, 2020. We first explore how the disease
and information co-evolve via the spatial analysis of the two spreading
processes. We visualize the geo-location of both disease and information at the
province level and find that disease is more geo-localized compared to
information. We find a high correlation between the disease and information
data, and also people care about the spread only when it comes to their
neighborhood. Regard to the content of the information, we find that positive
messages are more negatively correlated with the disease compared to negative
and neutral messages. Additionally, we introduce machine learning algorithms,
i.e., linear regression and random forest, to further predict the number of
infected using different disease spatial related and information-related
characteristics. We obtain that the disease spatial related characteristics of
nearby cities can help to improve the prediction accuracy. Meanwhile,
information-related characteristics can also help to improve the prediction
performance, but with a delay, i.e., the improvement comes from using, for
instance, the number of messages 10 days ago, for disease prediction. The
methodology proposed in this paper may shed light on new clues of emerging
infections","['Xiu-Xiu Zhan', 'Kaiyue Zhang', 'Lun Ge', 'Junming Huang', 'Zinan Zhang', 'Lu Wei', 'Gui-Quan Sun', 'Chuang Liu', 'Zi-Ke Zhang']",5,0.7493899
"Understanding the spread of false or dangerous beliefs through a population
has never seemed so urgent. Network science researchers have often taken a page
from epidemiologists, and modeled the spread of false beliefs as similar to how
a disease spreads through a social network. However, absent from those
disease-inspired models is an internal model of an individual's set of current
beliefs, where cognitive science has increasingly documented how the
interaction between mental models and incoming messages seems to be crucially
important for their adoption or rejection. Some computational social science
modelers analyze agent-based models where individuals do have simulated
cognition, but they often lack the strengths of network science, namely in
empirically-driven network structures. We introduce a cognitive cascade model
that combines a network science belief cascade approach with an internal
cognitive model of the individual agents as in opinion diffusion models as a
public opinion diffusion (POD) model, adding media institutions as agents which
begin opinion cascades. We conduct an analysis of the cognitive cascade model
with our simple cognitive function across various graph topologies and
institutional messaging patterns. We argue from our results that
population-level aggregate outcomes of the model qualitatively match what has
been reported in COVID-related public opinion polls, and that the model
dynamics lend insights as to how to address the spread of problematic beliefs.
The overall model sets up a framework with which social science misinformation
researchers and computational opinion diffusion modelers can join forces to
understand, and hopefully learn how to best counter, the spread of
disinformation and ""alternative facts.""","['Nicholas Rabb', 'Lenore Cowen', 'Jan P. de Ruiter', 'Matthias Scheutz']",2,0.6779691
"Fake news is a growing problem in developing countries with potentially
far-reaching consequences. We conduct a randomized experiment in urban Pakistan
to evaluate the effectiveness of two educational interventions to counter
misinformation among low-digital literacy populations. We do not find a
significant effect of video-based general educational messages about
misinformation. However, when such messages are augmented with personalized
feedback based on individuals' past engagement with fake news, we find an
improvement of 0.14 standard deviations in identifying fake news. We also find
negative but insignificant effects on identifying true news, driven by female
respondents. Our results suggest that educational interventions can enable
information discernment but their effectiveness critically depends on how well
their features and delivery are customized for the population of interest.","['Ayesha Ali', 'Ihsan Ayyub Qazi']",4,0.75618994
"Millions of people use platforms such as YouTube, Facebook, Twitter, and
other mass media. Due to the accessibility of these platforms, they are often
used to establish a narrative, conduct propaganda, and disseminate
misinformation. This work proposes an approach that uses state-of-the-art NLP
techniques to extract features from video captions (subtitles). To evaluate our
approach, we utilize a publicly accessible and labeled dataset for classifying
videos as misinformation or not. The motivation behind exploring video captions
stems from our analysis of videos metadata. Attributes such as the number of
views, likes, dislikes, and comments are ineffective as videos are hard to
differentiate using this information. Using caption dataset, the proposed
models can classify videos among three classes (Misinformation, Debunking
Misinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the
relevance of the misinformation class, we re-formulate our classification
problem as a two-class classification - Misinformation vs. others (Debunking
Misinformation and Neutral). In our experiments, the proposed models can
classify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.","['Raj Jagtap', 'Abhinav Kumar', 'Rahul Goel', 'Shakshi Sharma', 'Rajesh Sharma', 'Clint P. George']",4,0.58812314
"Humanity is battling one of the most deleterious virus in modern history, the
COVID-19 pandemic, but along with the pandemic there's an infodemic permeating
the pupil and society with misinformation which exacerbates the current malady.
We try to detect and classify fake news on online media to detect fake
information relating to COVID-19 and coronavirus. The dataset contained fake
posts, articles and news gathered from fact checking websites like politifact
whereas real tweets were taken from verified twitter handles. We incorporated
multiple conventional classification techniques like Naive Bayes, KNN, Gradient
Boost and Random Forest along with Deep learning approaches, specifically CNN,
RNN, DNN and the ensemble model RMDL. We analyzed these approaches with two
feature extraction techniques, TF-IDF and GloVe Word Embeddings which would
provide deeper insights into the dataset containing COVID-19 info on online
media.","['Prathmesh Pathwar', 'Simran Gill']",5,0.7204073
"Wikipedia is one of the main repositories of free knowledge available today,
with a central role in the Web ecosystem. For this reason, it can also be a
battleground for actors trying to impose specific points of view or even
spreading disinformation online. There is a growing need to monitor its
""health"" but this is not an easy task. Wikipedia exists in over 300 language
editions and each project is maintained by a different community, with their
own strengths, weaknesses and limitations. In this paper, we introduce a
taxonomy of knowledge integrity risks across Wikipedia projects and a first set
of indicators to assess internal risks related to community and content issues,
as well as external threats such as the geopolitical and media landscape. On
top of this taxonomy, we offer a preliminary analysis illustrating how the lack
of editors' geographical diversity might represent a knowledge integrity risk.
These are the first steps of a research project to build a Wikipedia Knowledge
Integrity Risk Observatory.","['Pablo Arag√≥n', 'Diego S√°ez-Trumper']",0,0.5724912
"QAnon is a far-right conspiracy theory whose followers largely organize
online. In this work, we use web crawls seeded from two of the largest QAnon
hotbeds on the Internet, Voat and 8kun, to build a QAnon-centered domain-based
hyperlink graph. We use this graph to identify, understand, and learn about the
set of websites that spread QAnon content online. Specifically, we curate the
largest list of QAnon centered websites to date, from which we document the
types of QAnon sites, their hosting providers, as well as their popularity. We
further analyze QAnon websites' connection to mainstream news and
misinformation online, highlighting the outsized role misinformation websites
play in spreading the conspiracy. Finally, we leverage the observed
relationship between QAnon and misinformation sites to build a highly accurate
random forest classifier that distinguishes between misinformation and
authentic news sites. Our results demonstrate new and effective ways to study
the growing presence of conspiracy theories and misinformation on the Internet.","['Hans W. A. Hanley', 'Deepak Kumar', 'Zakir Durumeric']",4,0.55046713
"The sudden outbreak of COVID-19 resulted in large volumes of data shared on
different social media platforms. Analyzing and visualizing these data is
doubtlessly essential to having a deep understanding of the pandemic's impacts
on people's lives and their reactions to them. In this work, we conduct a
large-scale spatiotemporal data analytic study to understand peoples' reactions
to the COVID-19 pandemic during its early stages. In particular, we analyze a
JSON-based dataset that is collected from news/messages/boards/blogs in English
about COVID-19 over a period of 4 months, for a total of 5.2M posts. The data
are collected from December 2019 to March 2020 from several social media
platforms such as Facebook, LinkedIn, Pinterest, StumbleUpon and VK. Our study
aims mainly to understand which implications of COVID-19 have interested social
media users the most and how did they vary over time, the spatiotemporal
distribution of misinformation, and the public opinion toward public figures
during the pandemic. Our results can be used by many parties (e.g.,
governments, psychologists, etc.) to make more informative decisions, taking
into account the actual interests and opinions of the people.","['Omar Abdel Wahab', 'Ali Mustafa', 'Andr√© Bertrand Abisseck Bamatakina']",5,0.82788724
"The spreading COVID-19 misinformation over social media already draws the
attention of many researchers. According to Google Scholar, about 26000
COVID-19 related misinformation studies have been published to date. Most of
these studies focusing on 1) detect and/or 2) analysing the characteristics of
COVID-19 related misinformation. However, the study of the social behaviours
related to misinformation is often neglected. In this paper, we introduce a
fine-grained annotated misinformation tweets dataset including social
behaviours annotation (e.g. comment or question to the misinformation). The
dataset not only allows social behaviours analysis but also suitable for both
evidence-based or non-evidence-based misinformation classification task. In
addition, we introduce leave claim out validation in our experiments and
demonstrate the misinformation classification performance could be
significantly different when applying to real-world unseen misinformation.","['Ye Jiang', 'Xingyi Song', 'Carolina Scarton', 'Ahmet Aker', 'Kalina Bontcheva']",5,0.7300025
"We modify a canonical experimental design to identify the effectiveness of
retractions. Comparing beliefs after retractions to beliefs (a) without the
retracted information and (b) after equivalent new information, we find that
retractions result in diminished belief updating in both cases. We propose this
reflects updating from retractions being more complex, and our analysis
supports this: we find longer response times, lower accuracy, and higher
variability. The results -- robust across diverse subject groups and design
variations -- enhance our understanding of belief updating and offer insights
into addressing misinformation.","['Duarte Gon√ßalves', 'Jonathan Libgober', 'Jack Willis']",1,0.57225144
"Online social media has been a popular source for people to consume and share
news content. More recently, the spread of misinformation online has caused
widespread concerns. In this work, we focus on a critical task of detecting
fauxtography on social media where the image and associated text together
convey misleading information. Many efforts have been made to mitigate
misinformation online, but we found that the fauxtography problem has not been
fully addressed by existing work. Solutions focusing on detecting fake images
or misinformed texts alone on social media often fail to identify the
misinformation delivered together by the image and the associated text of a
fauxtography post. In this paper, we develop FauxWard, a novel graph
convolutional neural network framework that explicitly explores the complex
information extracted from a user comment network of a social media post to
effectively identify fauxtography. FauxWard is content-free in the sense that
it does not analyze the visual or textual contents of the post itself, which
makes it robust against sophisticated fauxtography uploaders who intentionally
craft image-centric posts by editing either the text or image content. We
evaluate FauxWard on two real-world datasets collected from mainstream social
media platforms (i.e., Reddit and Twitter). The results show that FauxWard is
both effective and efficient in identifying fauxtography posts on social media.","['Lanyu Shang', 'Yang Zhang', 'Daniel Zhang', 'Dong Wang']",4,0.6719045
"This paper introduces a new benchmark for large-scale image similarity
detection. This benchmark is used for the Image Similarity Challenge at
NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a
modified copy of any image in a reference corpus of size 1~million. The
benchmark features a variety of image transformations such as automated
transformations, hand-crafted image edits and machine-learning based
manipulations. This mimics real-life cases appearing in social media, for
example for integrity-related problems dealing with misinformation and
objectionable content. The strength of the image manipulations, and therefore
the difficulty of the benchmark, is calibrated according to the performance of
a set of baseline approaches. Both the query and reference set contain a
majority of ""distractor"" images that do not match, which corresponds to a
real-life needle-in-haystack setting, and the evaluation metric reflects that.
We expect the DISC21 benchmark to promote image copy detection as an important
and challenging computer vision task and refresh the state of the art. Code and
data are available at https://github.com/facebookresearch/isc2021","['Matthijs Douze', 'Giorgos Tolias', 'Ed Pizzi', 'Zo√´ Papakipos', 'Lowik Chanussot', 'Filip Radenovic', 'Tomas Jenicek', 'Maxim Maximov', 'Laura Leal-Taix√©', 'Ismail Elezi', 'Ond≈ôej Chum', 'Cristian Canton Ferrer']",7,0.67864937
"Fake news and misinformation are one of the most significant challenges
brought about by advances in communication technologies. We chose to research
the spread of fake news in Pakistan because of some unfortunate incidents that
took place during 2020. These included the downplaying of the severity of the
COVID-19 pandemic, and protests by right-wing political movements. We observed
that fake news and misinformation contributed significantly to these events and
especially affected low-literate and low-income populations. We conducted a
cross-platform comparison of misinformation on WhatsApp, Twitter and YouTube
with a primary focus on messages shared in public WhatsApp groups, and analysed
the characteristics of misinformation, techniques used to make is believable,
and how users respond to it. To the best of our knowledge, this is the first
attempt to compare misinformation on all three platforms in Pakistan. Data
collected over a span of eight months helped us identify fake news and
misinformation related to politics, religion and health, among other
categories. Common elements which were used by fake news creators in Pakistan
to make false content seem believable included: appeals to emotion, conspiracy
theories, political and religious polarization, incorrect facts and
impersonation of credible sources.","['Danyal Haroon', 'Hammad Arif', 'Ahmed Abdullah Tariq', 'fareeda nawaz', 'Ihsan Ayyub Qazi', 'Maryam mustafa']",4,0.74000657
"COVID-19 vaccine hesitancy has increased concerns about vaccine uptake
required to overcome the pandemic and protect public health. A critical factor
associated with anti-vaccine attitudes is the information shared on social
media. In this work, we investigate misinformation communities and narratives
that can contribute to COVID-19 vaccine hesitancy. During the pandemic,
anti-science and political misinformation/conspiracies have been rampant on
social media. Therefore, we investigate misinformation and conspiracy groups
and their characteristic behaviours in Twitter data collected on COVID-19
vaccines. We identify if any suspicious coordinated efforts are present in
promoting vaccine misinformation, and find two suspicious groups - one
promoting a 'Great Reset' conspiracy which suggests that the pandemic is
orchestrated by world leaders to take control of the economy, with vaccine
related misinformation and strong anti-vaccine and anti-social messages such as
no lock-downs; and another promoting the Bioweapon theory. Misinformation
promoted is largely from the anti-vaccine and far-right communities in the
3-core of the retweet graph, with its tweets proportion of conspiracy and
questionable sources to reliable sources being much higher. In comparison with
the mainstream and health news, the right-leaning community is more influenced
by the anti-vaccine and far-right communities, which is also reflected in the
disparate vaccination rates in left and right U.S. states. The misinformation
communities are also more vocal, either in vaccine or other discussions,
relative to remaining communities, besides other behavioral differences.","['Karishma Sharma', 'Yizhou Zhang', 'Yan Liu']",12,0.8628259
"There is a lot of fact-based information and misinformation in the online
discourses and discussions about the COVID-19 vaccines. Using a sample of
nearly four million geotagged English tweets and the data from the CDC COVID
Data Tracker, we conducted the Fama-MacBeth regression with the Newey-West
adjustment to understand the influence of both misinformation and fact-based
news on Twitter on the COVID-19 vaccine uptake in the U.S. from April 19 when
U.S. adults were vaccine eligible to June 30, 2021, after controlling
state-level factors such as demographics, education, and the pandemic severity.
We identified the tweets related to either misinformation or fact-based news by
analyzing the URLs. One percent increase in fact-related Twitter users is
associated with an approximately 0.87 decrease (B = -0.87, SE = 0.25, p<.001)
in the number of daily new vaccinated people per hundred. No significant
relationship was found between the percentage of fake-news-related users and
the vaccination rate. The negative association between the percentage of
fact-related users and the vaccination rate might be due to a combination of a
larger user-level influence and the negative impact of online social
endorsement on vaccination intent.","['Hanjia Lyu', 'Zihe Zheng', 'Jiebo Luo']",12,0.85195065
"COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.","['Mir Mehedi A. Pritom', 'Rosana Montanez Rodriguez', 'Asad Ali Khan', 'Sebastian A. Nugroho', ""Esra'a Alrashydah"", 'Beatrice N. Ruiz', 'Anthony Rios']",0,0.75329345
"The Covid-19 pandemic has had a deep impact on the lives of the entire world
population, inducing a participated societal debate. As in other contexts, the
debate has been the subject of several d/misinformation campaigns; in a quite
unprecedented fashion, however, the presence of false information has seriously
put at risk the public health. In this sense, detecting the presence of
malicious narratives and identifying the kinds of users that are more prone to
spread them represent the first step to limit the persistence of the former
ones. In the present paper we analyse the semantic network observed on Twitter
during the first Italian lockdown (induced by the hashtags contained in
approximately 1.5 millions tweets published between the 23rd of March 2020 and
the 23rd of April 2020) and study the extent to which various discursive
communities are exposed to d/misinformation arguments. As observed in other
studies, the recovered discursive communities largely overlap with traditional
political parties, even if the debated topics concern different facets of the
management of the pandemic. Although the themes directly related to
d/misinformation are a minority of those discussed within our semantic
networks, their popularity is unevenly distributed among the various discursive
communities.","['Mattia Mattei', 'Guido Caldarelli', 'Tiziano Squartini', 'Fabio Saracco']",5,0.742062
"Fact verification has attracted a lot of attention in the machine learning
and natural language processing communities, as it is one of the key methods
for detecting misinformation. Existing large-scale benchmarks for this task
have focused mostly on textual sources, i.e. unstructured information, and thus
ignored the wealth of information available in structured formats, such as
tables. In this paper we introduce a novel dataset and benchmark, Fact
Extraction and VERification Over Unstructured and Structured information
(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated
with evidence in the form of sentences and/or cells from tables in Wikipedia,
as well as a label indicating whether this evidence supports, refutes, or does
not provide enough information to reach a verdict. Furthermore, we detail our
efforts to track and minimize the biases present in the dataset and could be
exploited by models, e.g. being able to predict the label without using
evidence. Finally, we develop a baseline for verifying claims against text and
tables which predicts both the correct evidence and verdict for 18% of the
claims.","['Rami Aly', 'Zhijiang Guo', 'Michael Schlichtkrull', 'James Thorne', 'Andreas Vlachos', 'Christos Christodoulopoulos', 'Oana Cocarascu', 'Arpit Mittal']",1,0.7902339
"Echo chambers may exclude social media users from being exposed to other
opinions, therefore, can cause rampant negative effects. Among abundant
evidence are the 2016 and 2020 US presidential elections conspiracy theories
and polarization, as well as the COVID-19 disinfodemic. To help better detect
echo chambers and mitigate its negative effects, this paper explores the
mechanisms and attributes of echo chambers in social media. In particular, we
first illustrate four primary mechanisms related to three main factors: human
psychology, social networks, and automatic systems. We then depict common
attributes of echo chambers with a focus on the diffusion of misinformation,
spreading of conspiracy theory, creation of social trends, political
polarization, and emotional contagion of users. We illustrate each mechanism
and attribute in a multi-perspective of sociology, psychology, and social
computing with recent case studies. Our analysis suggest an emerging need to
detect echo chambers and mitigate their negative effects.","['Bohan Jiang', 'Mansooreh Karami', 'Lu Cheng', 'Tyler Black', 'Huan Liu']",3,0.6807766
"There is currently no easy way to fact-check content on WhatsApp and other
end-to-end encrypted platforms at scale. In this paper, we analyze the
usefulness of a crowd-sourced ""tipline"" through which users can submit content
(""tips"") that they want fact-checked. We compare the tips sent to a WhatsApp
tipline run during the 2019 Indian national elections with the messages
circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, our
findings suggest tiplines can be an effective source for discovering content to
fact-check.","['Ashkan Kazemi', 'Kiran Garimella', 'Gautam Kishore Shahi', 'Devin Gaffney', 'Scott A. Hale']",10,0.5265955
"We introduce a FEVER-like dataset COVID-Fact of $4,086$ claims concerning the
COVID-19 pandemic. The dataset contains claims, evidence for the claims, and
contradictory claims refuted by the evidence. Unlike previous approaches, we
automatically detect true claims and their source articles and then generate
counter-claims using automatic methods rather than employing human annotators.
Along with our constructed resource, we formally present the task of
identifying relevant evidence for the claims and verifying whether the evidence
refutes or supports a given claim. In addition to scientific claims, our data
contains simplified general claims from media sources, making it better suited
for detecting general misinformation regarding COVID-19. Our experiments
indicate that COVID-Fact will provide a challenging testbed for the development
of new systems and our approach will reduce the costs of building
domain-specific datasets for detecting misinformation.","['Arkadiy Saakyan', 'Tuhin Chakrabarty', 'Smaranda Muresan']",1,0.757712
"The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.","['Anusua Trivedi', 'Alyssa Suhm', 'Prathamesh Mahankal', 'Subhiksha Mukuntharaj', 'Meghana D. Parab', 'Malvika Mohan', 'Meredith Berger', 'Arathi Sethumadhavan', 'Ashish Jaiman', 'Rahul Dodhia']",0,0.7526229
"In this position paper we approach problems concerning critical digital and
information literacy with ideas to provide more digestible explanations of
abstract concepts through interface design. In particular, we focus on social
media platforms where we see the possibility of counteracting the spread of
misinformation by providing users with more proficiency through our approaches.
We argue that the omnipresent trend to abstract away and hide information from
users via UI/UX design opposes their ability to self-learn. This leads us to
propose a different framework in which we unify elegant and simple interfaces
with nudges that promote a look behind the curtain. Such designs serve to
foster a deeper understanding of employed technologies and aim to increase the
critical assessment of content encountered on social platforms. Furthermore, we
consider users with an intermediary skill level to be largely ignored in
current approaches, as they are given no tools to broaden their knowledge
without consultation of expert material. The resulting stagnation is
exemplified by the tactics of misinformation campaigns, which exploit the
ensuing lack of information literacy and critical thinking. We propose an
approach to design that sufficiently emancipates users in both aspects by
promoting a look behind the abstraction of UI/UX so that an autonomous learning
process is given the chance to occur. Furthermore, we name ideas for future
research within this area that take our considerations into account.",['Jan Wolff'],0,0.7135243
"Language generation models' democratization benefits many domains, from
answering health-related questions to enhancing education by providing
AI-driven tutoring services. However, language generation models'
democratization also makes it easier to generate human-like text at-scale for
nefarious activities, from spreading misinformation to targeting specific
groups with hate speech. Thus, it is essential to understand how people
interact with bots and develop methods to detect bot-generated text. This paper
shows that bot-generated text detection methods are more robust across datasets
and models if we use information about how people respond to it rather than
using the bot's text directly. We also analyze linguistic alignment, providing
insight into differences between human-human and human-bot conversations.","['Paras Bhatt', 'Anthony Rios']",13,0.7552792
"This paper evaluates Parler, the controversial social media platform, from
two seemingly orthogonal perspectives: UX design perspective and data science.
UX design researchers explore how users react to the interface/content of their
social media feeds; Data science researchers analyze the misinformation flow in
these feeds to detect alternative narratives and state-sponsored disinformation
campaigns. We took a critical look into the intersection of these approaches to
understand how Parler's interface itself is conductive to the flow of
misinformation and the perception of ""free speech"" among its audience. Parler
drew widespread attention leading up to and after the 2020 U.S. elections as
the ""alternative"" place for free speech, as a reaction to other mainstream
social media platform which actively engaged in labeling misinformation with
content warnings. Because platforms like Parler are disruptive to the social
media landscape, we believe the evaluation uniquely uncovers the platform's
conductivity to the spread of misinformation.","['Emma Pieroni', 'Peter Jachim', 'Nathaniel Jachim', 'Filipo Sharevski']",3,0.8106909
"The rapid advances in deep generative models over the past years have led to
highly {realistic media, known as deepfakes,} that are commonly
indistinguishable from real to human eyes. These advances make assessing the
authenticity of visual data increasingly difficult and pose a misinformation
threat to the trustworthiness of visual content in general. Although recent
work has shown strong detection accuracy of such deepfakes, the success largely
relies on identifying frequency artifacts in the generated images, which will
not yield a sustainable detection approach as generative models continue
evolving and closing the gap to real images. In order to overcome this issue,
we propose a novel fake detection that is designed to re-synthesize testing
images and extract visual cues for detection. The re-synthesis procedure is
flexible, allowing us to incorporate a series of visual tasks - we adopt
super-resolution, denoising and colorization as the re-synthesis. We
demonstrate the improved effectiveness, cross-GAN generalization, and
robustness against perturbations of our approach in a variety of detection
scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN
datasets. Source code is available at
https://github.com/SSAW14/BeyondtheSpectrum.","['Yang He', 'Ning Yu', 'Margret Keuper', 'Mario Fritz']",7,0.86133814
"Online debates are often characterised by extreme polarisation and heated
discussions among users. The presence of hate speech online is becoming
increasingly problematic, making necessary the development of appropriate
countermeasures. In this work, we perform hate speech detection on a corpus of
more than one million comments on YouTube videos through a machine learning
model fine-tuned on a large set of hand-annotated data. Our analysis shows that
there is no evidence of the presence of ""serial haters"", intended as active
users posting exclusively hateful comments. Moreover, coherently with the echo
chamber hypothesis, we find that users skewed towards one of the two categories
of video channels (questionable, reliable) are more prone to use inappropriate,
violent, or hateful language within their opponents community. Interestingly,
users loyal to reliable sources use on average a more toxic language than their
counterpart. Finally, we find that the overall toxicity of the discussion
increases with its length, measured both in terms of number of comments and
time. Our results show that, coherently with Godwin's law, online debates tend
to degenerate towards increasingly toxic exchanges of views.","['Matteo Cinelli', 'Andra≈æ Pelicon', 'Igor Mozetiƒç', 'Walter Quattrociocchi', 'Petra Kralj Novak', 'Fabiana Zollo']",3,0.6879403
"Generative Adversarial Networks (GANs) have recently achieved unprecedented
success in photo-realistic image synthesis from low-dimensional random noise.
The ability to synthesize high-quality content at a large scale brings
potential risks as the generated samples may lead to misinformation that can
create severe social, political, health, and business hazards. We propose
SubsetGAN to identify generated content by detecting a subset of anomalous
node-activations in the inner layers of pre-trained neural networks. These
nodes, as a group, maximize a non-parametric measure of divergence away from
the expected distribution of activations created from real data. This enable us
to identify synthesised images without prior knowledge of their distribution.
SubsetGAN efficiently scores subsets of nodes and returns the group of nodes
within the pre-trained classifier that contributed to the maximum score. The
classifier can be a general fake classifier trained over samples from multiple
sources or the discriminator network from different GANs. Our approach shows
consistently higher detection power than existing detection methods across
several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different
proportions of generated content.","['Celia Cintas', 'Skyler Speakman', 'Girmaw Abebe Tadesse', 'Victor Akinwande', 'Edward McFowland III', 'Komminist Weldemariam']",2,0.7397299
"Social Networks' omnipresence and ease of use has revolutionized the
generation and distribution of information in today's world. However, easy
access to information does not equal an increased level of public knowledge.
Unlike traditional media channels, social networks also facilitate faster and
wider spread of disinformation and misinformation. Viral spread of false
information has serious implications on the behaviors, attitudes and beliefs of
the public, and ultimately can seriously endanger the democratic processes.
Limiting false information's negative impact through early detection and
control of extensive spread presents the main challenge facing researchers
today. In this survey paper, we extensively analyze a wide range of different
solutions for the early detection of fake news in the existing literature. More
precisely, we examine Machine Learning (ML) models for the identification and
classification of fake news, online fake news detection competitions,
statistical outputs as well as the advantages and disadvantages of some of the
available data sets. Finally, we evaluate the online web browsing tools
available for detecting and mitigating fake news and present some open research
challenges.","['Tanveer Khan', 'Antonis Michalas', 'Adnan Akhunzada']",0,0.82430875
"The evolution of electronic media is a mixed blessing. Due to the easy
access, low cost, and faster reach of the information, people search out and
devour news from online social networks. In contrast, the increasing acceptance
of social media reporting leads to the spread of fake news. This is a minacious
problem that causes disputes and endangers societal stability and harmony. Fake
news spread has gained attention from researchers due to its vicious nature.
proliferation of misinformation in all media, from the internet to cable news,
paid advertising and local news outlets, has made it essential for people to
identify the misinformation and sort through the facts. Researchers are trying
to analyze the credibility of information and curtail false information on such
platforms. Credibility is the believability of the piece of information at
hand. Analyzing the credibility of fake news is challenging due to the intent
of its creation and the polychromatic nature of the news. In this work, we
propose a model for detecting fake news. Our method investigates the content of
the news at the early stage i.e. when the news is published but is yet to be
disseminated through social media. Our work interprets the content with
automatic feature extraction and the relevance of the text pieces. In summary,
we introduce stance as one of the features along with the content of the
article and employ the pre-trained contextualized word embeddings BERT to
obtain the state-of-art results for fake news detection. The experiment
conducted on the real-world dataset indicates that our model outperforms the
previous work and enables fake news detection with an accuracy of 95.32%.","['Hema Karande', 'Rahee Walambe', 'Victor Benjamin', 'Ketan Kotecha', 'T. S. Raghu']",4,0.8903798
"YouTube has revolutionized the way people discover and consume video.
Although YouTube facilitates easy access to hundreds of well-produced and
trustworthy videos, abhorrent, misinformative, and mistargeted content is also
common. The platform is plagued by various types of problematic content: 1)
disturbing videos targeting young children; 2) hateful and misogynistic
content; and 3) pseudoscientific misinformation. While YouTube's recommendation
algorithm plays a vital role in increasing user engagement and YouTube's
monetization, its role in unwittingly promoting problematic content is not
entirely understood. In this thesis, we shed some light on the degree of
problematic content on YouTube and the role of the recommendation algorithm in
the dissemination of such content. Following a data-driven quantitative
approach, we analyze thousands of videos on YouTube, to shed light on: 1) the
risks of YouTube media consumption by young children; 2) the role of the
recommendation algorithm in the dissemination of misogynistic content, by
focusing on the Involuntary Celibates (Incels) community; and 3) user exposure
to pseudoscientific content on various parts of the platform and how this
exposure changes based on the user's watch history. Our analysis reveals that
young children are likely to encounter disturbing content when they randomly
browse the platform. By analyzing the Incel community on YouTube, we find that
Incel activity is increasing over time and that platforms may play an active
role in steering users towards extreme content. Finally, when studying
pseudoscientific misinformation, we find that YouTube suggests more
pseudoscientific content regarding traditional pseudoscientific topics (e.g.,
flat earth) than for emerging ones (like COVID-19) and that these
recommendations are more common on the search results page than on a user's
homepage or the video recommendations section.",['Kostantinos Papadamou'],3,0.54298484
"The proliferation of fake news, i.e., news intentionally spread for
misinformation, poses a threat to individuals and society. Despite various
fact-checking websites such as PolitiFact, robust detection techniques are
required to deal with the increase in fake news. Several deep learning models
show promising results for fake news classification, however, their black-box
nature makes it difficult to explain their classification decisions and
quality-assure the models. We here address this problem by proposing a novel
interpretable fake news detection framework based on the recently introduced
Tsetlin Machine (TM). In brief, we utilize the conjunctive clauses of the TM to
capture lexical and semantic properties of both true and fake news text.
Further, we use the clause ensembles to calculate the credibility of fake news.
For evaluation, we conduct experiments on two publicly available datasets,
PolitiFact and GossipCop, and demonstrate that the TM framework significantly
outperforms previously published baselines by at least $5\%$ in terms of
accuracy, with the added benefit of an interpretable logic-based
representation. Further, our approach provides higher F1-score than BERT and
XLNet, however, we obtain slightly lower accuracy. We finally present a case
study on our model's explainability, demonstrating how it decomposes into
meaningful words and their negations.","['Bimal Bhattarai', 'Ole-Christoffer Granmo', 'Lei Jiao']",4,0.8462762
"The 3rd edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in AI Ethics since October 2020. It
aims to help anyone, from machine learning experts to human rights activists
and policymakers, quickly digest and understand the field's ever-changing
developments. Through research and article summaries, as well as expert
commentary, this report distills the research and reporting surrounding various
domains related to the ethics of AI, including: algorithmic injustice,
discrimination, ethical AI, labor impacts, misinformation, privacy, risk and
security, social media, and more.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. Unique to this report is ""The Abuse and
Misogynoir Playbook,"" written by Dr. Katlyn Tuner (Research Scientist, Space
Enabled Research Group, MIT), Dr. Danielle Wood (Assistant Professor, Program
in Media Arts and Sciences; Assistant Professor, Aeronautics and Astronautics;
Lead, Space Enabled Research Group, MIT) and Dr. Catherine D'Ignazio (Assistant
Professor, Urban Science and Planning; Director, Data + Feminism Lab, MIT). The
piece (and accompanying infographic), is a deep-dive into the historical and
systematic silencing, erasure, and revision of Black women's contributions to
knowledge and scholarship in the United Stations, and globally. Exposing and
countering this Playbook has become increasingly important following the firing
of AI Ethics expert Dr. Timnit Gebru (and several of her supporters) at Google.
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.","['Abhishek Gupta', 'Alexandrine Royer', 'Connor Wright', 'Falaah Arif Khan', 'Victoria Heath', 'Erick Galinkin', 'Ryan Khurana', 'Marianna Bergamaschi Ganapini', 'Muriam Fancy', 'Masa Sweidan', 'Mo Akif', 'Renjie Butalid']",9,0.6973446
"Following the wave of misinterpreted, manipulated and malicious information
growing on the Internet, the misinformation surrounding COVID-19 has become a
paramount issue. In the context of the current COVID-19 pandemic, social media
posts and platforms are at risk of rumors and misinformation in the face of the
serious uncertainty surrounding the virus itself. At the same time, the
uncertainty and new nature of COVID-19 means that other unconfirmed information
that may appear ""rumored"" may be an important indicator of the behavior and
impact of this new virus. Twitter, in particular, has taken a center stage in
this storm where Covid-19 has been a much talked about subject. We have
presented an exploratory analysis of the tweets and the users who are involved
in spreading misinformation and then delved into machine learning models and
natural language processing techniques to identify if a tweet contains
misinformation.","['Drishti Jain', 'Tavpritesh Sethi']",5,0.78639686
"In 2020, amidst the COVID pandemic and a polarized political climate, the
Sleeping Giants online activist movement gained traction in Brazil. Its
rationale was simple: to curb the spread of misinformation by harming the
advertising revenue of sources that produce this type of content. Like its
international counterparts, Sleeping Giants Brasil (SGB) campaigned against
media outlets using Twitter to ask companies to remove ads from the targeted
outlets. This work presents a thorough quantitative characterization of this
activism model, analyzing the three campaigns carried out by SGB between May
and September 2020. To do so, we use digital traces from both Twitter and
Google Trends, toxicity and sentiment classifiers trained for the Portuguese
language, and an annotated corpus of SGB's tweets. Our key findings were
threefold. First, we found that SGB's requests to companies were largely
successful (with 83.85\% of all 192 targeted companies responding positively)
and that user pressure was correlated to the speed of companies' responses.
Second, there were no significant changes in the online attention and the user
engagement going towards the targeted media outlets in the six months that
followed SGB's campaign (as measured by Google Trends and Twitter engagement).
Third, we observed that user interactions with companies changed only
transiently, even if the companies did not respond to SGB's request. Overall,
our results paint a nuanced portrait of internet activism. On the one hand,
they suggest that SGB was successful in getting companies to boycott specific
media outlets, which may have harmed their advertisement revenue stream. On the
other hand, they also suggest that the activist movement did not impact the
online attention these media outlets received nor the online image of companies
that did not respond positively to their requests.","['B√°rbara Gomes Ribeiro', 'Manoel Horta Ribeiro', 'Virg√≠lio Almeida', 'Wagner Meira Jr']",10,0.72190803
"The study of coordinated manipulation of conversations on social media has
become more prevalent as social media's role in amplifying misinformation,
hate, and polarization has come under scrutiny. We discuss the implications of
successful coordination detection algorithms based on shifts of power, and
consider how responsible coordination detection may be carried out through
synchronized action. We then propose a Synchronized Action Framework for
detection of automated coordination through construction and analysis of
multi-view networks. We validate our framework by examining the Reopen America
conversation on Twitter, discovering three coordinated campaigns. We further
investigate covert coordination surrounding the protests and find the task to
be far more complex than examples seen in prior work, demonstrating the need
for our multi-view approach. A cluster of suspicious users is identified and
the activity of three members is detailed. These users amplify protest messages
using the same hashtags at very similar times, though they all focus on
different states. Through this analysis, we emphasize both the potential
usefulness of coordination detection algorithms in investigating amplification,
and the need for careful and responsible deployment of such tools.","['Thomas Magelinski', 'Lynnette Hui Xian Ng', 'Kathleen M. Carley']",2,0.57422173
"False claims about COVID-19 vaccines can undermine public trust in ongoing
vaccination campaigns, thus posing a threat to global public health.
Misinformation originating from various sources has been spreading online since
the beginning of the COVID-19 pandemic. In this paper, we present a dataset of
Twitter posts that exhibit a strong anti-vaccine stance. The dataset consists
of two parts: a) a streaming keyword-centered data collection with more than
1.8 million tweets, and b) a historical account-level collection with more than
135 million tweets. The former leverages the Twitter streaming API to follow a
set of specific vaccine-related keywords starting from mid-October 2020. The
latter consists of all historical tweets of 70K accounts that were engaged in
the active spreading of anti-vaccine narratives. We present descriptive
analyses showing the volume of activity over time, geographical distributions,
topics, news sources, and inferred account political leaning. This dataset can
be used in studying anti-vaccine misinformation on social media and enable a
better understanding of vaccine hesitancy. In compliance with Twitter's Terms
of Service, our anonymized dataset is publicly available at:
https://github.com/gmuric/avax-tweets-dataset","['Goran Muric', 'Yusong Wu', 'Emilio Ferrara']",12,0.8157715
"In this paper, we analyzed the perceived accuracy of COVID-19 vaccine
information spoken back by Amazon Alexa. Unlike social media, Amazon Alexa
doesn't apply soft moderation to unverified content, allowing for use of
third-party malicious skills to arbitrarily phrase COVID-19 vaccine
information. The results from a 210-participant study suggest that a
third-party malicious skill could successful reduce the perceived accuracy
among the users of information as to who gets the vaccine first, vaccine
testing, and the side effects of the vaccine. We also found that the
vaccine-hesitant participants are drawn to pessimistically rephrased Alexa
responses focused on the downsides of the mass immunization. We discuss
solutions for soft moderation against misperception-inducing or altogether
COVID-19 misinformation malicious third-party skills.","['Filipo Sharevski', 'Anna Slowinski', 'Peter Jachim', 'Emma Pieroni']",12,0.7872748
"Social networks like Facebook and WhatsApp have enabled users to share images
with other users around the world. Along with this has come the rapid spread of
misinformation. One step towards verifying the authenticity of an image is
understanding its origin, including it distribution history through social
media. In this paper, we present a method for tracing the posting history of an
image across different social networks. To do this, we propose a two-stage
deep-learning-based approach, which takes advantage of cascaded fingerprints in
images left by social networks during uploading. Our proposed system is not
reliant upon metadata or similar easily falsifiable information. Through a
series of experiments, we show that we are able to outperform existing social
media source identification algorithms. and identify chains of social networks
up to length two with over over 84% accuracy.","['Brian C Hosler', 'Matthew C Stamm']",4,0.61914194
"This paper presents the Multilingual COVID-19 Analysis Method (CMTA) for
detecting and observing the spread of misinformation about this disease within
texts. CMTA proposes a data science (DS) pipeline that applies machine learning
models for processing, classifying (Dense-CNN) and analyzing (MBERT)
multilingual (micro)-texts. DS pipeline data preparation tasks extract features
from multilingual textual data and categorize it into specific information
classes (i.e., 'false', 'partly false', 'misleading'). The CMTA pipeline has
been experimented with multilingual micro-texts (tweets), showing
misinformation spread across different languages. To assess the performance of
CMTA and put it in perspective, we performed a comparative analysis of CMTA
with eight monolingual models used for detecting misinformation. The comparison
shows that CMTA has surpassed various monolingual models and suggests that it
can be used as a general method for detecting misinformation in multilingual
micro-texts. CMTA experimental results show misinformation trends about
COVID-19 in different languages during the first pandemic months.","['Raj Ratn Pranesh', 'Mehrdad Farokhnejad', 'Ambesh Shekhar', 'Genoveva Vargas-Solar']",8,0.75133866
"Under the aegis of computer vision and deep learning technology, a new
emerging techniques has introduced that anyone can make highly realistic but
fake videos, images even can manipulates the voices. This technology is widely
known as Deepfake Technology. Although it seems interesting techniques to make
fake videos or image of something or some individuals but it could spread as
misinformation via internet. Deepfake contents could be dangerous for
individuals as well as for our communities, organizations, countries religions
etc. As Deepfake content creation involve a high level expertise with
combination of several algorithms of deep learning, it seems almost real and
genuine and difficult to differentiate. In this paper, a wide range of articles
have been examined to understand Deepfake technology more extensively. We have
examined several articles to find some insights such as what is Deepfake, who
are responsible for this, is there any benefits of Deepfake and what are the
challenges of this technology. We have also examined several creation and
detection techniques. Our study revealed that although Deepfake is a threat to
our societies, proper measures and strict regulations could prevent this.","['Bahar Uddin Mahmud', 'Afsana Sharmin']",11,0.7537377
"In this work we looked into a dataset of 114 thousands of suspicious messages
collected from the most popular closed messaging platform in Taiwan between
January and July, 2020. We proposed an hybrid algorithm that could efficiently
cluster a large number of text messages according their topics and narratives.
That is, we obtained groups of messages that are within a limited content
alterations within each other. By employing the algorithm to the dataset, we
were able to look at the content alterations and the temporal dynamics of each
particular rumor over time. With qualitative case studies of three COVID-19
related rumors, we have found that key authoritative figures were often
misquoted in false information. It was an effective measure to increase the
popularity of one false information. In addition, fact-check was not effective
in stopping misinformation from getting attention. In fact, the popularity of
one false information was often more influenced by major societal events and
effective content alterations.","['Andrea W Wang', 'Jo-Yu Lan', 'Chihhao Yu', 'Ming-Hung Wang']",3,0.6702111
"Facebook and Twitter recently announced community-based review platforms to
address misinformation. We provide an overview of the potential affordances of
such community-based approaches to content moderation based on past research
and preliminary analysis of Twitter's Birdwatch data. While our analysis
generally supports a community-based approach to content moderation, it also
warns against potential pitfalls, particularly when the implementation of the
new infrastructure focuses on crowd-based ""validation"" rather than
""collaboration."" We call for multidisciplinary research utilizing methods from
complex systems studies, behavioural sociology, and computational social
science to advance the research on crowd-based content moderation.","['Taha Yasseri', 'Filippo Menczer']",0,0.6670915
"The World Wide Web and social media platforms have become popular sources for
news and information. Typically, multimodal information, e.g., image and text
is used to convey information more effectively and to attract attention. While
in most cases image content is decorative or depicts additional information, it
has also been leveraged to spread misinformation and rumors in recent years. In
this paper, we present a Web-based demo application that automatically
quantifies the cross-modal relations of entities (persons, locations, and
events) in image and text. The applications are manifold. For example, the
system can help users to explore multimodal articles more efficiently, or can
assist human assessors and fact-checking efforts in the verification of the
credibility of news stories, tweets, or other multimodal documents.","['Matthias Springstein', 'Eric M√ºller-Budack', 'Ralph Ewerth']",4,0.6727698
"With the continuing spread of misinformation and disinformation online, it is
of increasing importance to develop combating mechanisms at scale in the form
of automated systems that support multiple languages. One task of interest is
claim veracity prediction, which can be addressed using stance detection with
respect to relevant documents retrieved online. To this end, we present our new
Arabic Stance Detection dataset (AraStance) of 4,063 claim--article pairs from
a diverse set of sources comprising three fact-checking websites and one news
website. AraStance covers false and true claims from multiple domains (e.g.,
politics, sports, health) and several Arab countries, and it is well-balanced
between related and unrelated documents with respect to the claims. We
benchmark AraStance, along with two other stance detection datasets, using a
number of BERT-based models. Our best model achieves an accuracy of 85\% and a
macro F1 score of 78\%, which leaves room for improvement and reflects the
challenging nature of AraStance and the task of stance detection in general.","['Tariq Alhindi', 'Amal Alabdulkarim', 'Ali Alshehri', 'Muhammad Abdul-Mageed', 'Preslav Nakov']",8,0.74293816
"In this paper, we explore the construction of natural language explanations
for news claims, with the goal of assisting fact-checking and news evaluation
applications. We experiment with two methods: (1) an extractive method based on
Biased TextRank -- a resource-effective unsupervised graph-based algorithm for
content extraction; and (2) an abstractive method based on the GPT-2 language
model. We perform comparative evaluations on two misinformation datasets in the
political and health news domains, and find that the extractive method shows
the most promise.","['Ashkan Kazemi', 'Zehua Li', 'Ver√≥nica P√©rez-Rosas', 'Rada Mihalcea']",1,0.618925
"Visually realistic GAN-generated images have recently emerged as an important
misinformation threat. Research has shown that these synthetic images contain
forensic traces that are readily identifiable by forensic detectors.
Unfortunately, these detectors are built upon neural networks, which are
vulnerable to recently developed adversarial attacks. In this paper, we propose
a new anti-forensic attack capable of fooling GAN-generated image detectors.
Our attack uses an adversarially trained generator to synthesize traces that
these detectors associate with real images. Furthermore, we propose a technique
to train our attack so that it can achieve transferability, i.e. it can fool
unknown CNNs that it was not explicitly trained against. We evaluate our attack
through an extensive set of experiments, where we show that our attack can fool
eight state-of-the-art detection CNNs with synthetic images created using seven
different GANs, and outperform other alternative attacks.","['Xinwei Zhao', 'Matthew C. Stamm']",7,0.72712946
"We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in
Texts and Images: the data, the annotation guidelines, the evaluation setup,
the results, and the participating systems. The task focused on memes and had
three subtasks: (i) detecting the techniques in the text, (ii) detecting the
text spans where the techniques are used, and (iii) detecting techniques in the
entire meme, i.e., both in the text and in the image. It was a popular task,
attracting 71 registrations, and 22 teams that eventually made an official
submission on the test set. The evaluation results for the third subtask
confirmed the importance of both modalities, the text and the image. Moreover,
some teams reported benefits when not just combining the two modalities, e.g.,
by using early or late fusion, but rather modeling the interaction between them
in a joint model.","['Dimitar Dimitrov', 'Bishr Bin Ali', 'Shaden Shaar', 'Firoj Alam', 'Fabrizio Silvestri', 'Hamed Firooz', 'Preslav Nakov', 'Giovanni Da San Martino']",1,0.5654868
"On 26 January 2021, India witnessed a national embarrassment from the
demographic least expected from - farmers. People across the nation watched in
horror as a pseudo-patriotic mob of farmers stormed capital Delhi and
vandalized the national pride- Red Fort. Investigations that followed the event
revealed the existence of a social media trail that led to the likes of such an
event. Consequently, it became essential and necessary to archive this trail
for social media analysis - not only to understand the bread-crumbs that are
dispersed across the trail but also to visualize the role played by
misinformation and fake news in this event. In this paper, we propose the
tractor2twitter dataset which contains around 0.05 million tweets that were
posted before, during, and after this event. Also, we benchmark our dataset
with an Explainable AI ML model for classification of each tweet into either of
the three categories - disinformation, misinformation, and opinion.","['Ajay Agarwal', 'Basant Agarwal']",10,0.61676675
"With the increasing use of machine-learning driven algorithmic judgements, it
is critical to develop models that are robust to evolving or manipulated
inputs. We propose an extensive analysis of model robustness against linguistic
variation in the setting of deceptive news detection, an important task in the
context of misinformation spread online. We consider two prediction tasks and
compare three state-of-the-art embeddings to highlight consistent trends in
model performance, high confidence misclassifications, and high impact
failures. By measuring the effectiveness of adversarial defense strategies and
evaluating model susceptibility to adversarial attacks using character- and
word-perturbed text, we find that character or mixed ensemble models are the
most effective defenses and that character perturbation-based attack tactics
are more successful.","['Maria Glenski', 'Ellyn Ayton', 'Robin Cosbey', 'Dustin Arendt', 'Svitlana Volkova']",7,0.63711
"Social media contains unfiltered and unique information, which is potentially
of great value, but, in the case of misinformation, can also do great harm.
With regards to biomedical topics, false information can be particularly
dangerous. Methods of automatic fact-checking and fake news detection address
this problem, but have not been applied to the biomedical domain in social
media yet. We aim to fill this research gap and annotate a corpus of 1200
tweets for implicit and explicit biomedical claims (the latter also with span
annotations for the claim phrase). With this corpus, which we sample to be
related to COVID-19, measles, cystic fibrosis, and depression, we develop
baseline models which detect tweets that contain a claim automatically. Our
analyses reveal that biomedical tweets are densely populated with claims (45 %
in a corpus sampled to contain 1200 tweets focused on the domains mentioned
above). Baseline classification experiments with embedding-based classifiers
and BERT-based transfer learning demonstrate that the detection is challenging,
however, shows acceptable performance for the identification of explicit
expressions of claims. Implicit claim tweets are more challenging to detect.","['Amelie W√ºhrl', 'Roman Klinger']",1,0.63231385
"The COVID-19 pandemic has been damaging to the lives of people all around the
world. Accompanied by the pandemic is an infodemic, an abundant and
uncontrolled spreading of potentially harmful misinformation. The infodemic may
severely change the pandemic's course by interfering with public health
interventions such as wearing masks, social distancing, and vaccination. In
particular, the impact of the infodemic on vaccination is critical because it
holds the key to reverting to pre-pandemic normalcy. This paper presents
findings from a global survey on the extent of worldwide exposure to the
COVID-19 infodemic, assesses different populations' susceptibility to false
claims, and analyzes its association with vaccine acceptance. Based on
responses gathered from over 18,400 individuals from 40 countries, we find a
strong association between perceived believability of misinformation and
vaccination hesitancy. Additionally, our study shows that only half of the
online users exposed to rumors might have seen the fact-checked information.
Moreover, depending on the country, between 6% and 37% of individuals
considered these rumors believable. Our survey also shows that poorer regions
are more susceptible to encountering and believing COVID-19 misinformation. We
discuss implications of our findings on public campaigns that proactively
spread accurate information to countries that are more susceptible to the
infodemic. We also highlight fact-checking platforms' role in better
identifying and prioritizing claims that are perceived to be believable and
have wide exposure. Our findings give insights into better handling of risk
communication during the initial phase of a future pandemic.","['Karandeep Singh', 'Gabriel Lima', 'Meeyoung Cha', 'Chiyoung Cha', 'Juhi Kulshrestha', 'Yong-Yeol Ahn', 'Onur Varol']",5,0.8121139
"Widespread uptake of vaccines is necessary to achieve herd immunity. However,
uptake rates have varied across U.S. states during the first six months of the
COVID-19 vaccination program. Misbeliefs may play an important role in vaccine
hesitancy, and there is a need to understand relationships between
misinformation, beliefs, behaviors, and health outcomes. Here we investigate
the extent to which COVID-19 vaccination rates and vaccine hesitancy are
associated with levels of online misinformation about vaccines. We also look
for evidence of directionality from online misinformation to vaccine hesitancy.
We find a negative relationship between misinformation and vaccination uptake
rates. Online misinformation is also correlated with vaccine hesitancy rates
taken from survey data. Associations between vaccine outcomes and
misinformation remain significant when accounting for political as well as
demographic and socioeconomic factors. While vaccine hesitancy is strongly
associated with Republican vote share, we observe that the effect of online
misinformation on hesitancy is strongest across Democratic rather than
Republican counties. Granger causality analysis shows evidence for a
directional relationship from online misinformation to vaccine hesitancy. Our
results support a need for interventions that address misbeliefs, allowing
individuals to make better-informed health decisions.","['Francesco Pierri', 'Brea Perry', 'Matthew R. DeVerna', 'Kai-Cheng Yang', 'Alessandro Flammini', 'Filippo Menczer', 'John Bryden']",12,0.83176523
"The internet promised to democratize access to knowledge and make the world
more open and understanding. The reality of today's internet, however, is far
from this ideal. Misinformation, lies, and conspiracies dominate many social
media platforms. This toxic online world has had real-world implications
ranging from genocide to, election interference, and threats to global public
health. A frustrated public and impatient government regulators are calling for
a more vigorous response to mis- and disinformation campaigns designed to sow
civil unrest and inspire violence against individuals, societies, and
democracies. We describe a large-scale, domain-level analysis that reveals
seemingly coordinated efforts between multiple domains to spread and amplify
misinformation. We also describe how the hyperlinks shared by certain Twitter
users can be used to surface problematic domains. These analyses can be used by
search engines and social media recommendation algorithms to systematically
discover and demote misinformation peddlers.","['Vibhor Sehgal', 'Ankit Peshin', 'Sadia Afroz', 'Hany Farid']",10,0.6826741
"Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.
feeling distrust), and behaviorally (e.g. sharing the news with their friends).
Such reactions are instantaneous and yet complex, as they rely on factors that
go beyond interpreting factual content of news. We propose Misinfo Reaction
Frames (MRF), a pragmatic formalism for modeling how readers might react to a
news headline. In contrast to categorical schema, our free-text dimensions
provide a more nuanced way of understanding intent beyond being benign or
malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced
dataset of reactions to over 25k news headlines focusing on global crises: the
Covid-19 pandemic, climate change, and cancer. Empirical results confirm that
it is indeed possible for neural models to predict the prominent patterns of
readers' reactions to previously unseen news headlines. Additionally, our user
study shows that displaying machine-generated MRF implications alongside news
headlines to readers can increase their trust in real news while decreasing
their trust in misinformation. Our work demonstrates the feasibility and
importance of pragmatic inferences on news headlines to help enhance AI-guided
misinformation detection and mitigation.","['Saadia Gabriel', 'Skyler Hallinan', 'Maarten Sap', 'Pemi Nguyen', 'Franziska Roesner', 'Eunsol Choi', 'Yejin Choi']",4,0.75756186
"The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and
overwhelming demand, challenges and opportunities to domain, model and data
driven modeling. This paper provides a comprehensive review of the challenges,
tasks, methods, progress, gaps and opportunities in relation to modeling
COVID-19 problems, data and objectives. It constructs a research landscape of
COVID-19 modeling tasks and methods, and further categorizes, summarizes,
compares and discusses the related methods and progress of modeling COVID-19
epidemic transmission processes and dynamics, case identification and tracing,
infection diagnosis and medical treatments, non-pharmaceutical interventions
and their effects, drug and vaccine development, psychological, economic and
social influence and impact, and misinformation, etc. The modeling methods
involve mathematical and statistical models, domain-driven modeling by
epidemiological compartmental models, medical and biomedical analysis, AI and
data science in particular shallow and deep machine learning, simulation
modeling, social science methods, and hybrid modeling.","['Longbing Cao', 'Qing Liu']",2,0.45764107
"Misinformation undermines the credibility of social media and poses
significant threats to modern societies. As a countermeasure, Twitter has
recently introduced ""Birdwatch,"" a community-driven approach to address
misinformation on Twitter. On Birdwatch, users can identify tweets they believe
are misleading, write notes that provide context to the tweet and rate the
quality of other users' notes. In this work, we empirically analyze how users
interact with this new feature. For this purpose, we collect {all} Birdwatch
notes and ratings between the introduction of the feature in early 2021 and end
of July 2021. We then map each Birdwatch note to the fact-checked tweet using
Twitter's historical API. In addition, we use text mining methods to extract
content characteristics from the text explanations in the Birdwatch notes
(e.g., sentiment). Our empirical analysis yields the following main findings:
(i) users more frequently file Birdwatch notes for misleading than not
misleading tweets. These misleading tweets are primarily reported because of
factual errors, lack of important context, or because they treat unverified
claims as facts. (ii) Birdwatch notes are more helpful to other users if they
link to trustworthy sources and if they embed a more positive sentiment. (iii)
The social influence of the author of the source tweet is associated with
differences in the level of user consensus. For influential users with many
followers, Birdwatch notes yield a lower level of consensus among users and
community-created fact checks are more likely to be seen as being incorrect and
argumentative. Altogether, our findings can help social media platforms to
formulate guidelines for users on how to write more helpful fact checks. At the
same time, our analysis suggests that community-based fact-checking faces
challenges regarding opinion speculation and polarization among the user base.",['Nicolas Pr√∂llochs'],3,0.66719806
"As social media becomes increasingly prominent in our day to day lives, it is
increasingly important to detect informative content and prevent the spread of
disinformation and unverified rumours. While many sophisticated and successful
models have been proposed in the literature, they are often compared with older
NLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the
performance of a broad set of modern transformer-based language models and show
that with basic fine-tuning, these models are competitive with and can even
significantly outperform recently proposed state-of-the-art methods. We present
our framework as a baseline for creating and evaluating new methods for
misinformation detection. We further study a comprehensive set of benchmark
datasets, and discuss potential data leakage and the need for careful design of
the experiments and understanding of datasets to account for confounding
variables. As an extreme case example, we show that classifying only based on
the first three digits of tweet ids, which contain information on the date,
gives state-of-the-art performance on a commonly used benchmark dataset for
fake news detection --Twitter16. We provide a simple tool to detect this
problem and suggest steps to mitigate it in future datasets.","['Kellin Pelrine', 'Jacob Danovitch', 'Reihaneh Rabbany']",1,0.75205326
"Online misinformation is a prevalent societal issue, with adversaries relying
on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated
by the threat scenario where an image is used out of context to support a
certain narrative. While some prior datasets for detecting image-text
inconsistency generate samples via text manipulation, we propose a dataset
where both image and text are unmanipulated but mismatched. We introduce
several strategies for automatically retrieving convincing images for a given
caption, capturing cases with inconsistent entities or semantic context. Our
large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates
that machine-driven image repurposing is now a realistic threat, and (2)
provides samples that represent challenging instances of mismatch between text
and image in news that are able to mislead humans. We benchmark several
state-of-the-art multimodal models on our dataset and analyze their performance
across different pretraining domains and visual backbones.","['Grace Luo', 'Trevor Darrell', 'Anna Rohrbach']",7,0.7580941
"In the era of misinformation and information inflation, the credibility
assessment of the produced news is of the essence. However, fact-checking can
be challenging considering the limited references presented in the news. This
challenge can be transcended by utilizing the knowledge graph that is related
to the news articles. In this work, we present a methodology for creating
scientific news article representations by modeling the directed graph between
the scientific news articles and the cited scientific publications. The network
used for the experiments is comprised of the scientific news articles, their
topic, the cited research literature, and their corresponding authors. We
implement and present three different approaches: 1) a baseline Relational
Graph Convolutional Network (R-GCN), 2) a Heterogeneous Graph Neural Network
(HetGNN) and 3) a Heterogeneous Graph Transformer (HGT). We test these models
in the downstream task of link prediction on the: a) news article - paper links
and b) news article - article topic links. The results show promising
applications of graph neural network approaches in the domains of knowledge
tracing and scientific news credibility assessment.","['Angelika Romanou', 'Panayiotis Smeros', 'Karl Aberer']",4,0.694466
"This paper describes the TOKOFOU system, an ensemble model for misinformation
detection tasks based on six different transformer-based pre-trained encoders,
implemented in the context of the COVID-19 Infodemic Shared Task for English.
We fine tune each model on each of the task's questions and aggregate their
prediction scores using a majority voting approach. TOKOFOU obtains an overall
F1 score of 89.7%, ranking first.","['Giorgos Tziafas', 'Konstantinos Kogkalidis', 'Tommaso Caselli']",2,0.44177812
"Fake tweets are observed to be ever-increasing, demanding immediate
countermeasures to combat their spread. During COVID-19, tweets with
misinformation should be flagged and neutralized in their early stages to
mitigate the damages. Most of the existing methods for early detection of fake
news assume to have enough propagation information for large labeled tweets --
which may not be an ideal setting for cases like COVID-19 where both aspects
are largely absent. In this work, we present ENDEMIC, a novel early detection
model which leverages exogenous and endogenous signals related to tweets, while
learning on limited labeled data. We first develop a novel dataset, called CTF
for early COVID-19 Twitter fake news, with additional behavioral test sets to
validate early detection. We build a heterogeneous graph with
follower-followee, user-tweet, and tweet-retweet connections and train a graph
embedding model to aggregate propagation information. Graph embeddings and
contextual features constitute endogenous, while time-relative web-scraped
information constitutes exogenous signals. ENDEMIC is trained in a
semi-supervised fashion, overcoming the challenge of limited labeled data. We
propose a co-attention mechanism to fuse signal representations optimally.
Experimental results on ECTF, PolitiFact, and GossipCop show that ENDEMIC is
highly reliable in detecting early fake tweets, outperforming nine
state-of-the-art methods significantly.","['Rachit Bansal', 'William Scott Paka', 'Nidhi', 'Shubhashis Sengupta', 'Tanmoy Chakraborty']",4,0.70167744
"In this paper, we introduce UnifiedM2, a general-purpose misinformation model
that jointly models multiple domains of misinformation with a single, unified
setup. The model is trained to handle four tasks: detecting news bias,
clickbait, fake news, and verifying rumors. By grouping these tasks together,
UnifiedM2learns a richer representation of misinformation, which leads to
state-of-the-art or comparable performance across all tasks. Furthermore, we
demonstrate that UnifiedM2's learned representation is helpful for few-shot
learning of unseen misinformation tasks/datasets and model's generalizability
to unseen events.","['Nayeon Lee', 'Belinda Z. Li', 'Sinong Wang', 'Pascale Fung', 'Hao Ma', 'Wen-tau Yih', 'Madian Khabsa']",1,0.597929
"This paper presents a few observations about pro-Kremlin propaganda between
2015 and early 2021 with a dataset from the East Stratcom Task Force (ESTF),
which is affiliated with the European Union (EU) but working independently from
it. Instead of focusing on misinformation and disinformation, the observations
are motivated by classical propaganda research and the ongoing transformation
of media systems. According to the tentative results, (i) the propaganda can be
assumed to target both domestic and foreign audiences. Of the countries and
regions discussed, (ii) Russia, Ukraine, the United States, and within Europe,
Germany, Poland, and the EU have been the most frequently discussed. Also other
conflict regions such as Syria have often appeared in the propaganda. In terms
of longitudinal trends, however, (iii) most of these discussions have decreased
in volume after the digital tsunami in 2016, although the conflict in Ukraine
seems to have again increased the intensity of pro-Kremlin propaganda. Finally,
(iv) the themes discussed align with state-centric war propaganda and conflict
zones, although also post-truth themes frequently appear; from conspiracy
theories via COVID-19 to fascism -- anything goes, as is typical to propaganda.",['Jukka Ruohonen'],10,0.54466885
"At the latest since the advent of the Internet, disinformation and conspiracy
theories have become ubiquitous. Recent examples like QAnon and Pizzagate prove
that false information can lead to real violence. In this motivation statement
for the Workshop on Human Aspects of Misinformation at CHI 2021, I explain my
research agenda focused on 1. why people believe in disinformation, 2. how
people can be best supported in recognizing disinformation, and 3. what the
potentials and risks of different tools designed to fight disinformation are.",['Hendrik Heuer'],0,0.576874
"Future societal systems will be characterized by heterogeneous human
behaviors and also collective action. The interaction between local systems and
global systems will be complex. Humandemics will propagate because of the
pathways that connect the different systems and several invariant behaviors and
patterns that have emerged globally. On the contrary, infodemics of
misinformation can be a risk as it has occurred in the COVID-19 pandemic. The
emerging fragility or robustness of the system will depend on how this complex
network of systems is governed. Future societal systems will not be only
multiscale in terms of the social dimension, but also in the temporality.
Necessary and proper prevention and response systems based on complexity, ethic
and multi-scale governance will be required. Real-time response systems are the
basis for resilience to be the foundation of robust societies. A top-down
approach led by Governmental organs for managing humandemics is not sufficient
and may be only effective if policies are very restrictive and their efficacy
depends not only in the measures implemented but also on the dynamics of the
policies and the population perception and compliance. This top-down approach
is even weaker if there is not national and international coordination.
Coordinating top-down agencies with bottom-up constructs will be the design
principle. Multi-scale governance integrates decision-making processes with
signaling, sensing and leadership mechanisms to drive thriving societal systems
with real-time sensitivity.","['David Pastor-Escuredo', 'Philip Treleaven']",9,0.579831
"In this paper, we investigate the so-called ODP-problem that has been
formulated by Caragiannis and Micha [10]. Here, we are in a setting with two
election alternatives out of which one is assumed to be correct. In ODP, the
goal is to organise the delegations in the social network in order to maximize
the probability that the correct alternative, referred to as ground truth, is
elected. While the problem is known to be computationally hard, we strengthen
existing hardness results by providing a novel strong approximation hardness
result: For any positive constant $C$, we prove that, unless $P=NP$, there is
no polynomial-time algorithm for ODP that achieves an approximation guarantee
of $\alpha \ge (\ln n)^{-C}$, where $n$ is the number of voters. The reduction
designed for this result uses poorly connected social networks in which some
voters suffer from misinformation. Interestingly, under some hypothesis on
either the accuracies of voters or the connectivity of the network, we obtain a
polynomial-time $1/2$-approximation algorithm. This observation proves formally
that the connectivity of the social network is a key feature for the efficiency
of the liquid democracy paradigm. Lastly, we run extensive simulations and
observe that simple algorithms (working either in a centralized or
decentralized way) outperform direct democracy on a large class of instances.
Overall, our contributions yield new insights on the question in which
situations liquid democracy can be beneficial.","['Ruben Becker', ""Gianlorenzo D'Angelo"", 'Esmaeil Delfaraz', 'Hugo Gilbert']",2,0.7138003
"The 2020 coronavirus pandemic has heightened the need to flag
coronavirus-related misinformation, and fact-checking groups have taken to
verifying misinformation on the Internet. We explore stories reported by
fact-checking groups PolitiFact, Poynter and Snopes from January to June 2020,
characterising them into six story clusters before then analyse time-series and
story validity trends and the level of agreement across sites. We further break
down the story clusters into more granular story types by proposing a unique
automated method with a BERT classifier, which can be used to classify diverse
story sources, in both fact-checked stories and tweets.","['Lynnette Hui Xian Ng', 'Kathleen M. Carley']",4,0.6107561
"A review bomb is a large and quick surge in online reviews about a product,
service, or business, coordinated by a group of people willing to manipulate
public opinion about that entity. This study challenges the assumption that
review bombing is solely a phenomenon of misinformation and connects
motivations and substantial content of online reviews with the broader theory
of judgement of facts and of value. These theories are verified in a
quantitative analysis of the most prominent case of review bombing, which
involves the video game The Last of Us Part II. It is discovered that
ideology-driven ratings are followed by a grassroots counter-bombing, aimed at
mitigating the effects of the negative ratings. The two factions of bombers,
despite being politically polar opposites, are very similar in terms of other
metrics. Evidence suggests the theoretical framework of political
disinformation is insufficient to explain this case of review bombing. In light
of the need to re-frame review bombing, recommendations are proposed for the
preventive management of future cases.","['Giulio Giacomo Cantone', 'Venera Tomaselli', 'Valeria Mazzeo']",3,0.59821284
"Social media plays a pivotal role in disseminating news globally and acts as
a platform for people to express their opinions on various topics. A wide
variety of views accompanies COVID-19 vaccination drives across the globe,
often colored by emotions, which change along with rising cases, approval of
vaccines, and multiple factors discussed online. This study aims at analyzing
the temporal evolution of different Emotion categories: Hesitation, Rage,
Sorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine
Rollout, Misinformation, Health Effects, and Inequities as lexical categories
created from Tweets belonging to five countries with vital vaccine roll-out
programs, namely, India, United States of America, Brazil, United Kingdom, and
Australia. We extracted a corpus of nearly 1.8 million Twitter posts related to
COVID-19 vaccination. Using cosine distance from selected seed words, we
expanded the vocabulary of each category and tracked the longitudinal change in
their strength from June 2020 to April 2021. We used community detection
algorithms to find modules in positive correlation networks. Our findings
suggest that tweets expressing hesitancy towards vaccines contain the highest
mentions of health-related effects in all countries. Our results indicated that
the patterns of hesitancy were variable across geographies and can help us
learn targeted interventions. We also observed a significant change in the
linear trends of categories like hesitation and contentment before and after
approval of vaccines. Negative emotions like rage and sorrow gained the highest
importance in the alluvial diagram. They formed a significant module with all
the influencing factors in April 2021, when India observed the second wave of
COVID-19 cases. The relationship between Emotions and Influencing Factors was
found to be variable across the countries.","['Harshita Chopra', 'Aniket Vashishtha', 'Ridam Pal', 'Ashima', 'Ananya Tyagi', 'Tavpritesh Sethi']",12,0.7597288
"In this paper, we analyzed the perceived accuracy of COVID-19 vaccine Tweets
when they were spoken back by a third-party Amazon Alexa skill. We mimicked the
soft moderation that Twitter applies to COVID-19 misinformation content in both
forms of warning covers and warning tags to investigate whether the third-party
skill could affect how and when users heed these warnings. The results from a
304-participant study suggest that the spoken back warning covers may not work
as intended, even when converted from text to speech. We controlled for
COVID-19 vaccination hesitancy and political leanings and found that the
vaccination hesitant Alexa users ignored any type of warning as long as the
Tweets align with their personal beliefs. The politically independent users
trusted Alexa less then their politically-laden counterparts and that helped
them accurately perceiving truthful COVID-19 information. We discuss soft
moderation adaptations for voice assistants to achieve the intended effect of
curbing COVID-19 misinformation.","['Donald Gover', 'Filipo Sharevski']",12,0.6428787
"Twitter, prompted by the rapid spread of alternative narratives, started
actively warning users about the spread of COVID-19 misinformation. This form
of soft moderation comes in two forms: as a warning cover before the Tweet is
displayed to the user and as a warning tag below the Tweet. This study
investigates how each of the soft moderation forms affects the perceived
accuracy of COVID-19 vaccine misinformation on Twitter. The results suggest
that the warning covers work, but not the tags, in reducing the perception of
accuracy of COVID-19 vaccine misinformation on Twitter. ""Belief echoes"" do
exist among Twitter users, unfettered by any warning labels, in relationship to
the perceived safety and efficacy of the COVID-19 vaccine as well as the
vaccination hesitancy for themselves and their children. The implications of
these results are discussed in the context of usable security affordances for
combating misinformation on social media.","['Filipo Sharevski', 'Raniem Alsaadi', 'Peter Jachim', 'Emma Pieroni']",12,0.765106
"The increasing occurrence, forms, and negative effects of misinformation on
social media platforms has necessitated more misinformation detection tools.
Currently, work is being done addressing COVID-19 misinformation however, there
are no misinformation detection tools for any of the 40 distinct indigenous
Ugandan languages. This paper addresses this gap by presenting basic language
resources and a misinformation detection data set based on code-mixed
Luganda-English messages sourced from the Facebook and Twitter social media
platforms. Several machine learning methods are applied on the misinformation
detection data set to develop classification models for detecting whether a
code-mixed Luganda-English message contains misinformation or not. A 10-fold
cross validation evaluation of the classification methods in an experimental
misinformation detection task shows that a Discriminative Multinomial Naive
Bayes (DMNB) method achieves the highest accuracy and F-measure of 78.19% and
77.90% respectively. Also, Support Vector Machine and Bagging ensemble
classification models achieve comparable results. These results are promising
since the machine learning models are based on n-gram features from only the
misinformation detection dataset.","['Peter Nabende', 'David Kabiito', 'Claire Babirye', 'Hewitt Tusiime', 'Joyce Nakatumba-Nabende']",8,0.75051165
"Understanding how information propagates in real-life complex networks yields
a better understanding of dynamic processes such as misinformation or epidemic
spreading. The recently introduced branch of machine learning methods for
learning node representations offers many novel applications, one of them being
the task of spreading prediction addressed in this paper. We explore the
utility of the state-of-the-art node representation learners when used to
assess the effects of spreading from a given node, estimated via extensive
simulations. Further, as many real-life networks are topologically similar, we
systematically investigate whether the learned models generalize to previously
unseen networks, showing that in some cases very good model transfer can be
obtained. This work is one of the first to explore transferability of the
learned representations for the task of node regression; we show there exist
pairs of networks with similar structure between which the trained models can
be transferred (zero-shot), and demonstrate their competitive performance. To
our knowledge, this is one of the first attempts to evaluate the utility of
zero-shot transfer for the task of node regression.","['Sebastian Me≈ænar', 'Nada Lavraƒç', 'Bla≈æ ≈†krlj']",2,0.74648714
"The Covid-19 pandemic presented an unprecedented global public health
emergency, and concomitantly an unparalleled opportunity to investigate public
responses to adverse social conditions. The widespread ability to post messages
to social media platforms provided an invaluable outlet for such an outpouring
of public sentiment, including not only expressions of social solidarity, but
also the spread of misinformation and misconceptions around the effect and
potential risks of the pandemic. This archive of message content therefore
represents a key resource in understanding public responses to health crises,
analysis of which could help to inform public policy interventions to better
respond to similar events in future. We present a benchmark database of public
social media postings from the United Kingdom related to the Covid-19 pandemic
for academic research purposes, along with some initial analysis, including a
taxonomy of key themes organised by keyword. This release supports the findings
of a research study funded by the Scottish Government Chief Scientists' Office
that aims to investigate social sentiment in order to understand the response
to public health measures implemented during the pandemic.","['Richard Plant', 'Amir Hussain']",5,0.6931963
"Fake information poses one of the major threats for society in the 21st
century. Identifying misinformation has become a key challenge due to the
amount of fake news that is published daily. Yet, no approach is established
that addresses the dynamics and versatility of fake news editorials. Instead of
classifying content, we propose an evidence retrieval approach to handle fake
news. The learning task is formulated as an unsupervised machine learning
problem. For validation purpose, we provide the user with a set of news
articles from reliable news sources supporting the hypothesis of the news
article in query and the final decision is left to the user. Technically we
propose a two-step process: (i) Aggregation-step: With information extracted
from the given text we query for similar content from reliable news sources.
(ii) Refining-step: We narrow the supporting evidence down by measuring the
semantic distance of the text with the collection from step (i). The distance
is calculated based on Word2Vec and the Word Mover's Distance. In our
experiments, only content that is below a certain distance threshold is
considered as supporting evidence. We find that our approach is agnostic to
concept drifts, i.e. the machine learning task is independent of the hypotheses
in a text. This makes it highly adaptable in times where fake news is as
diverse as classical news is. Our pipeline offers the possibility for further
analysis in the future, such as investigating bias and differences in news
reporting.","['Vishwani Gupta', 'Katharina Beckh', 'Sven Giesselbach', 'Dennis Wegener', 'Tim Wirtz']",4,0.85539126
"Recommendation algorithms have been pointed out as one of the major culprits
of misinformation spreading in the digital sphere. However, it is still unclear
how these algorithms really propagate misinformation, e.g., it has not been
shown which particular recommendation approaches are more prone to suggest
misinforming items, or which internal parameters of the algorithms could be
influencing more on their misinformation propagation capacity.
  Motivated by this fact, in this paper we present an analysis of the effect of
some of the most popular recommendation algorithms on the spread of
misinformation in Twitter. A set of guidelines on how to adapt these algorithms
is provided based on such analysis and a comprehensive review of the research
literature. A dataset is also generated and released to the scientific
community to stimulate discussions on the future design and development of
recommendation algorithms to counter misinformation. The dataset includes
editorially labelled news items and claims regarding their misinformation
nature.","['Miriam Fern√°ndez', 'Alejandro Bellog√≠n', 'Iv√°n Cantador']",0,0.63993627
"The proliferation of social media platforms like Twitter has heightened the
consequences of the spread of misinformation. To understand and model the
spread of misinformation, in this paper, we leveraged the SEIZ (Susceptible,
Exposed, Infected, Skeptics) epidemiological model to describe the underlying
process that delineates the spread of misinformation on Twitter. Compared to
the other epidemiological models, this model produces broader results because
it includes the additional Skeptics (Z) compartment, wherein a user may be
exposed to an item of misinformation but not engage in any reaction to it, and
the additional Exposed (E) compartment, wherein the user may need some time
before deciding to spread a misinformation item. We analyzed misinformation
regarding the unrest in Washington, D.C. in the month of March 2020 which was
propagated by the use of the #DCblackout hashtag by different users across the
U.S. on Twitter. Our analysis shows that misinformation can be modeled using
the concept of epidemiology. To the best of our knowledge, this research is the
first to attempt to apply the SEIZ epidemiological model to the spread of a
specific item of misinformation, which is a category distinct from that of
rumor, and a hoax on online social media platforms. Applying a mathematical
model can help to understand the trends and dynamics of the spread of
misinformation on Twitter and ultimately help to develop techniques to quickly
identify and control it.","['Maryam Maleki', 'Esther Mead', 'Mohammad Arani', 'Nitin Agarwal']",5,0.69913936
"Over the past decade, we have witnessed the rise of misinformation on the
Internet, with online users constantly falling victims of fake news. A
multitude of past studies have analyzed fake news diffusion mechanics and
detection and mitigation techniques. However, there are still open questions
about their operational behavior such as: How old are fake news websites? Do
they typically stay online for long periods of time? Do such websites
synchronize with each other their up and down time? Do they share similar
content through time? Which third-parties support their operations? How much
user traffic do they attract, in comparison to mainstream or real news
websites? In this paper, we perform a first of its kind investigation to answer
such questions regarding the online presence of fake news websites and
characterize their behavior in comparison to real news websites. Based on our
findings, we build a content-agnostic ML classifier for automatic detection of
fake news websites (i.e. accuracy) that are not yet included in manually
curated blacklists.","['Manolis Chalkiadakis', 'Alexandros Kornilakis', 'Panagiotis Papadopoulos', 'Evangelos P. Markatos', 'Nicolas Kourtellis']",4,0.6989892
"Recent years have witnessed the proliferation of offensive content online
such as fake news, propaganda, misinformation, and disinformation. While
initially this was mostly about textual content, over time images and videos
gained popularity, as they are much easier to consume, attract more attention,
and spread further than text. As a result, researchers started leveraging
different modalities and combinations thereof to tackle online multimodal
offensive content. In this study, we offer a survey on the state-of-the-art on
multimodal disinformation detection covering various combinations of
modalities: text, images, speech, video, social media network structure, and
temporal information. Moreover, while some studies focused on factuality,
others investigated how harmful the content is. While these two components in
the definition of disinformation (i) factuality, and (ii) harmfulness, are
equally important, they are typically studied in isolation. Thus, we argue for
the need to tackle disinformation detection by taking into account multiple
modalities as well as both factuality and harmfulness, in the same framework.
Finally, we discuss current challenges and future research directions","['Firoj Alam', 'Stefano Cresci', 'Tanmoy Chakraborty', 'Fabrizio Silvestri', 'Dimiter Dimitrov', 'Giovanni Da San Martino', 'Shaden Shaar', 'Hamed Firooz', 'Preslav Nakov']",0,0.72984076
"Political misinformation, astroturfing and organised trolling are online
malicious behaviours with significant real-world effects. Many previous
approaches examining these phenomena have focused on broad campaigns rather
than the small groups responsible for instigating or sustaining them. To reveal
latent (i.e., hidden) networks of cooperating accounts, we propose a novel
temporal window approach that relies on account interactions and metadata
alone. It detects groups of accounts engaging in various behaviours that, in
concert, come to execute different goal-based strategies, a number of which we
describe. The approach relies upon a pipeline that extracts relevant elements
from social media posts, infers connections between accounts based on criteria
matching the coordination strategies to build an undirected weighted network of
accounts, which is then mined for communities exhibiting high levels of
evidence of coordination using a novel community extraction method. We address
the temporal aspect of the data by using a windowing mechanism, which may be
suitable for near real-time application. We further highlight consistent
coordination with a sliding frame across multiple windows and application of a
decay factor. Our approach is compared with other recent similar processing
approaches and community detection methods and is validated against two
relevant datasets with ground truth data, using content, temporal, and network
analyses, as well as with the design, training and application of three
one-class classifiers built using the ground truth; its utility is furthermore
demonstrated in two case studies of contentious online discussions.","['Derek Weber', 'Frank Neumann']",2,0.7348362
"In this paper, we describe our participation in the TREC Health
Misinformation Track 2020. We submitted $11$ runs to the Total Recall Task and
13 runs to the Ad Hoc task. Our approach consists of 3 steps: (1) we create an
initial run with BM25 and RM3; (2) we estimate credibility and misinformation
scores for the documents in the initial run; (3) we merge the relevance,
credibility and misinformation scores to re-rank documents in the initial run.
To estimate credibility scores, we implement a classifier which exploits
features based on the content and the popularity of a document. To compute the
misinformation score, we apply a stance detection approach with a pretrained
Transformer language model. Finally, we use different approaches to merge
scores: weighted average, the distance among score vectors and rank fusion.","['Lucas Chaves Lima', 'Dustin Brandon Wright', 'Isabelle Augenstein', 'Maria Maistro']",1,0.5916543
"Differential privacy (DP) is getting attention as a privacy definition when
publishing statistics of a dataset. This paper focuses on the limitation that
DP inevitably causes two-sided error, which is not desirable for epidemic
analysis such as how many COVID-19 infected individuals visited location A. For
example, consider publishing misinformation that many infected people did not
visit location A, which may lead to miss decision-making that expands the
epidemic. To fix this issue, we propose a relaxation of DP, called asymmetric
differential privacy (ADP). We show that ADP can provide reasonable privacy
protection while achieving one-sided error. Finally, we conduct experiments to
evaluate the utility of proposed mechanisms for epidemic analysis using a
real-world dataset, which shows the practicality of our mechanisms.","['Shun Takagi', 'Yang Cao', 'Masatoshi Yoshikawa']",2,0.60471386
"Misinformation of COVID-19 is prevalent on social media as the pandemic
unfolds, and the associated risks are extremely high. Thus, it is critical to
detect and combat such misinformation. Recently, deep learning models using
natural language processing techniques, such as BERT (Bidirectional Encoder
Representations from Transformers), have achieved great successes in detecting
misinformation. In this paper, we proposed an explainable natural language
processing model based on DistilBERT and SHAP (Shapley Additive exPlanations)
to combat misinformation about COVID-19 due to their efficiency and
effectiveness. First, we collected a dataset of 984 claims about COVID-19 with
fact checking. By augmenting the data using back-translation, we doubled the
sample size of the dataset and the DistilBERT model was able to obtain good
performance (accuracy: 0.972; areas under the curve: 0.993) in detecting
misinformation about COVID-19. Our model was also tested on a larger dataset
for AAAI2021 - COVID-19 Fake News Detection Shared Task and obtained good
performance (accuracy: 0.938; areas under the curve: 0.985). The performance on
both datasets was better than traditional machine learning models. Second, in
order to boost public trust in model prediction, we employed SHAP to improve
model explainability, which was further evaluated using a between-subjects
experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE),
and text+SHAP explanation+source and evidence (TSESE). The participants were
significantly more likely to trust and share information related to COVID-19 in
the TSE and TSESE conditions than in the T condition. Our results provided good
implications in detecting misinformation about COVID-19 and improving public
trust.","['Jackie Ayoub', 'X. Jessie Yang', 'Feng Zhou']",1,0.65503526
"Understanding attitudes expressed in texts, also known as stance detection,
plays an important role in systems for detecting false information online, be
it misinformation (unintentionally false) or disinformation (intentionally
false information). Stance detection has been framed in different ways,
including (a) as a component of fact-checking, rumour detection, and detecting
previously fact-checked claims, or (b) as a task in its own right. While there
have been prior efforts to contrast stance detection with other related tasks
such as argumentation mining and sentiment analysis, there is no existing
survey on examining the relationship between stance detection and mis- and
disinformation detection. Here, we aim to bridge this gap by reviewing and
analysing existing work in this area, with mis- and disinformation in focus,
and discussing lessons learnt and future challenges.","['Momchil Hardalov', 'Arnav Arora', 'Preslav Nakov', 'Isabelle Augenstein']",0,0.60518694
"The creation or manipulation of facial appearance through deep generative
approaches, known as DeepFake, have achieved significant progress and promoted
a wide range of benign and malicious applications, e.g., visual effect
assistance in movie and misinformation generation by faking famous persons. The
evil side of this new technique poses another popular study, i.e., DeepFake
detection aiming to identify the fake faces from the real ones. With the rapid
development of the DeepFake-related studies in the community, both sides have
formed the relationship of battleground, pushing the improvements of each other
and inspiring new directions, e.g., the evasion of DeepFake detection.
Nevertheless, the overview of such battleground and the new direction is
unclear and neglected by recent surveys due to the rapid increase of related
publications, limiting the in-depth understanding of the tendency and future
works. To fill this gap, in this paper, we provide a comprehensive overview and
detailed analysis of the research work on the topic of DeepFake generation,
DeepFake detection as well as evasion of DeepFake detection, with more than 318
research papers carefully surveyed. We present the taxonomy of various DeepFake
generation methods and the categorization of various DeepFake detection
methods, and more importantly, we showcase the battleground between the two
parties with detailed interactions between the adversaries (DeepFake
generation) and the defenders (DeepFake detection). The battleground allows
fresh perspective into the latest landscape of the DeepFake research and can
provide valuable analysis towards the research challenges and opportunities as
well as research trends and future directions. We also elaborately design
interactive diagrams (http://www.xujuefei.com/dfsurvey) to allow researchers to
explore their own interests on popular DeepFake generators or detectors.","['Felix Juefei-Xu', 'Run Wang', 'Yihao Huang', 'Qing Guo', 'Lei Ma', 'Yang Liu']",11,0.7557409
"The proliferation of harmful content on online platforms is a major societal
problem, which comes in many different forms including hate speech, offensive
language, bullying and harassment, misinformation, spam, violence, graphic
content, sexual abuse, self harm, and many other. Online platforms seek to
moderate such content to limit societal harm, to comply with legislation, and
to create a more inclusive environment for their users. Researchers have
developed different methods for automatically detecting harmful content, often
focusing on specific sub-problems or on narrow communities, as what is
considered harmful often depends on the platform and on the context. We argue
that there is currently a dichotomy between what types of harmful content
online platforms seek to curb, and what research efforts there are to
automatically detect such content. We thus survey existing methods as well as
content moderation policies by online platforms in this light and we suggest
directions for future work.","['Arnav Arora', 'Preslav Nakov', 'Momchil Hardalov', 'Sheikh Muhammad Sarwar', 'Vibha Nayak', 'Yoan Dinkov', 'Dimitrina Zlatkova', 'Kyle Dent', 'Ameya Bhatawdekar', 'Guillaume Bouchard', 'Isabelle Augenstein']",3,0.58474827
"Images are an indispensable part of the news content we consume. Highly
emotional images from sources of misinformation can greatly influence our
judgements. We present two studies on the effects of emotional facial images on
users' perception of bias in news content and the credibility of sources. In
study 1, we investigate the impact of happy and angry facial images on users'
decisions. In study 2, we focus on sources' systematic emotional treatment of
specific politicians. Our results show that depending on the political
orientation of the source, the cumulative effect of angry facial emotions
impacts users' perceived content bias and source credibility. When sources
systematically portray specific politicians as angry, users are more likely to
find those sources as less credible and their content as more biased. These
results highlight how implicit visual propositions manifested by emotions in
facial expressions might have a substantial effect on our trust of news content
and sources.","['Alireza Karduni', 'Ryan Wesslen', 'Douglas Markant', 'Wenwen Dou']",4,0.62398016
"Authorship analysis is an important subject in the field of natural language
processing. It allows the detection of the most likely writer of articles,
news, books, or messages. This technique has multiple uses in tasks related to
authorship attribution, detection of plagiarism, style analysis, sources of
misinformation, etc. The focus of this paper is to explore the limitations and
sensitiveness of established approaches to adversarial manipulations of inputs.
To this end, and using those established techniques, we first developed an
experimental frame-work for author detection and input perturbations. Next, we
experimentally evaluated the performance of the authorship detection model to a
collection of semantic-preserving adversarial perturbations of input
narratives. Finally, we compare and analyze the effects of different
perturbation strategies, input and model configurations, and the effects of
these on the author detection model.","['Jeremiah Duncan', 'Fabian Fallas', 'Chris Gropp', 'Emily Herron', 'Maria Mahbub', 'Paula Olaya', 'Eduardo Ponce', 'Tabitha K. Samuel', 'Daniel Schultz', 'Sudarshan Srinivasan', 'Maofeng Tang', 'Viktor Zenkov', 'Quan Zhou', 'Edmon Begoli']",7,0.63265085
"The internet, Online Social Networks (OSNs) and smart phones enable users to
create tremendous amount of information. Users who search for general or
specific knowledge may not have these days problems of information scarce but
misinformation. Misinformation nowadays can refer to a continuous spectrum
between what can be seen as ""facts"" or ""truth"", if humans agree on the
existence of such, to false information that everyone agree that it is false.
In this paper, we will look at this spectrum of information/misinformation and
compare between some of the major relevant concepts. While few fact-checking
websites exist to evaluate news articles or some of the popular claims people
exchange, nonetheless this can be seen as a little effort in the mission to tag
online information with their ""proper"" category or label.","['Izzat Alsmadi', 'Iyad Alazzam', 'Mohammad A. AlRamahi']",4,0.65418124
"The enormous amount of discourse taking place online poses challenges to the
functioning of a civil and informed public sphere. Efforts to standardize
online discourse data, such as ClaimReview, are making available a wealth of
new data about potentially inaccurate claims, reviewed by third-party
fact-checkers. These data could help shed light on the nature of online
discourse, the role of political elites in amplifying it, and its implications
for the integrity of the online information ecosystem. Unfortunately, the
semi-structured nature of much of this data presents significant challenges
when it comes to modeling and reasoning about online discourse. A key challenge
is relation extraction, which is the task of determining the semantic
relationships between named entities in a claim. Here we develop a novel
supervised learning method for relation extraction that combines graph
embedding techniques with path traversal on semantic dependency graphs. Our
approach is based on the intuitive observation that knowledge of the entities
along the path between the subject and object of a triple (e.g.
Washington,_D.C.}, and United_States_of_America) provides useful information
that can be leveraged for extracting its semantic relation (i.e. capitalOf). As
an example of a potential application of this technique for modeling online
discourse, we show that our method can be integrated into a pipeline to reason
about potential misinformation claims.","['Matthew Sumpter', 'Giovanni Luca Ciampaglia']",1,0.7151553
"As the COVID-19 pandemic sweeps across the world, it has been accompanied by
a tsunami of fake news and misinformation on social media. At the time when
reliable information is vital for public health and safety, COVID-19 related
fake news has been spreading even faster than the facts. During times such as
the COVID-19 pandemic, fake news can not only cause intellectual confusion but
can also place lives of people at risk. This calls for an immediate need to
contain the spread of such misinformation on social media. We introduce CTF,
the first COVID-19 Twitter fake news dataset with labeled genuine and fake
tweets. Additionally, we propose Cross-SEAN, a cross-stitch based
semi-supervised end-to-end neural attention model, which leverages the large
amount of unlabelled data. Cross-SEAN partially generalises to emerging fake
news as it learns from relevant external knowledge. We compare Cross-SEAN with
seven state-of-the-art fake news detection methods. We observe that it achieves
$0.95$ F1 Score on CTF, outperforming the best baseline by $9\%$. We also
develop Chrome-SEAN, a Cross-SEAN based chrome extension for real-time
detection of fake tweets.","['William Scott Paka', 'Rachit Bansal', 'Abhay Kaushik', 'Shubhashis Sengupta', 'Tanmoy Chakraborty']",4,0.73264945
"As civil discourse increasingly takes place online, misinformation and the
polarization of news shared in online communities have become ever more
relevant concerns with real world harms across our society. Studying online
news sharing at scale is challenging due to the massive volume of content which
is shared by millions of users across thousands of communities. Therefore,
existing research has largely focused on specific communities or specific
interventions, such as bans. However, understanding the prevalence and spread
of misinformation and polarization more broadly, across thousands of online
communities, is critical for the development of governance strategies,
interventions, and community design. Here, we conduct the largest study of news
sharing on reddit to date, analyzing more than 550 million links spanning 4
years. We use non-partisan news source ratings from Media Bias/Fact Check to
annotate links to news sources with their political bias and factualness. We
find that, compared to left-leaning communities, right-leaning communities have
105% more variance in the political bias of their news sources, and more links
to relatively-more biased sources, on average. We observe that reddit users'
voting and re-sharing behaviors generally decrease the visibility of extremely
biased and low factual content, which receives 20% fewer upvotes and 30% fewer
exposures from crossposts than more neutral or more factual content. This
suggests that reddit is more resilient to low factual content than Twitter. We
show that extremely biased and low factual content is very concentrated, with
99% of such content being shared in only 0.5% of communities, giving credence
to the recent strategy of community-wide bans and quarantines.","['Galen Weld', 'Maria Glenski', 'Tim Althoff']",10,0.7085639
"The year 2020 will be remembered for two events of global significance: the
COVID-19 pandemic and 2020 U.S. Presidential Election. In this chapter, we
summarize recent studies using large public Twitter data sets on these issues.
We have three primary objectives. First, we delineate epistemological and
practical considerations when combining the traditions of computational
research and social science research. A sensible balance should be struck when
the stakes are high between advancing social theory and concrete, timely
reporting of ongoing events. We additionally comment on the computational
challenges of gleaning insight from large amounts of social media data. Second,
we characterize the role of social bots in social media manipulation around the
discourse on the COVID-19 pandemic and 2020 U.S. Presidential Election. Third,
we compare results from 2020 to prior years to note that, although bot accounts
still contribute to the emergence of echo-chambers, there is a transition from
state-sponsored campaigns to domestically emergent sources of distortion.
Furthermore, issues of public health can be confounded by political
orientation, especially from localized communities of actors who spread
misinformation. We conclude that automation and social media manipulation pose
issues to a healthy and democratic discourse, precisely because they distort
representation of pluralism within the public sphere.","['Ho-Chun Herbert Chang', 'Emily Chen', 'Meiqing Zhang', 'Goran Muric', 'Emilio Ferrara']",10,0.6579147
"Continual (or ""incremental"") learning approaches are employed when additional
knowledge or tasks need to be learned from subsequent batches or from streaming
data. However these approaches are typically adversary agnostic, i.e., they do
not consider the possibility of a malicious attack. In our prior work, we
explored the vulnerabilities of Elastic Weight Consolidation (EWC) to the
perceptible misinformation. We now explore the vulnerabilities of other
regularization-based as well as generative replay-based continual learning
algorithms, and also extend the attack to imperceptible misinformation. We show
that an intelligent adversary can take advantage of a continual learning
algorithm's capabilities of retaining existing knowledge over time, and force
it to learn and retain deliberately introduced misinformation. To demonstrate
this vulnerability, we inject backdoor attack samples into the training data.
These attack samples constitute the misinformation, allowing the attacker to
capture control of the model at test time. We evaluate the extent of this
vulnerability on both rotated and split benchmark variants of the MNIST dataset
under two important domain and class incremental learning scenarios. We show
that the adversary can create a ""false memory"" about any task by inserting
carefully-designed backdoor samples to the test instances of that task thereby
controlling the amount of forgetting of any task of its choosing. Perhaps most
importantly, we show this vulnerability to be very acute and damaging: the
model memory can be easily compromised with the addition of backdoor samples
into as little as 1\% of the training data, even when the misinformation is
imperceptible to human eye.","['Muhammad Umer', 'Robi Polikar']",1,0.6651825
"Graphs are one of the most efficacious structures for representing datapoints
and their relations, and they have been largely exploited for different
applications. Previously, the higher-order relations between the nodes have
been modeled by a generalization of graphs known as hypergraphs. In
hypergraphs, the edges are defined by a set of nodes i.e., hyperedges to
demonstrate the higher order relationships between the data. However, there is
no explicit higher-order generalization for nodes themselves. In this work, we
introduce a novel generalization of graphs i.e., K-Nearest Hyperplanes graph
(KNH) where the nodes are defined by higher order Euclidean subspaces for
multi-view modeling of the nodes. In fact, in KNH, nodes are hyperplanes or
more precisely m-flats instead of datapoints. We experimentally evaluate the
KNH graph on two multi-aspect datasets for misinformation detection. The
experimental results suggest that multi-view modeling of articles using KNH
graph outperforms the classic KNN graph in terms of classification performance.","['Sara Abdali', 'Neil Shah', 'Evangelos E. Papalexakis']",2,0.5558888
"Can the look and the feel of a website give information about the
trustworthiness of an article? In this paper, we propose to use a promising,
yet neglected aspect in detecting the misinformativeness: the overall look of
the domain webpage. To capture this overall look, we take screenshots of news
articles served by either misinformative or trustworthy web domains and
leverage a tensor decomposition based semi-supervised classification technique.
The proposed approach i.e., VizFake is insensitive to a number of image
transformations such as converting the image to grayscale, vectorizing the
image and losing some parts of the screenshots. VizFake leverages a very small
amount of known labels, mirroring realistic and practical scenarios, where
labels (especially for known misinformative articles), are scarce and quickly
become dated. The F1 score of VizFake on a dataset of 50k screenshots of news
articles spanning more than 500 domains is roughly 85% using only 5% of ground
truth labels. Furthermore, tensor representations of VizFake, obtained in an
unsupervised manner, allow for exploratory analysis of the data that provides
valuable insights into the problem. Finally, we compare VizFake with deep
transfer learning, since it is a very popular black-box approach for image
classification and also well-known text text-based methods. VizFake achieves
competitive accuracy with deep transfer learning models while being two orders
of magnitude faster and not requiring laborious hyper-parameter tuning.","['Sara Abdali', 'Rutuja Gurav', 'Siddharth Menon', 'Daniel Fonseca', 'Negin Entezari', 'Neil Shah', 'Evangelos E. Papalexakis']",7,0.64993536
"This paper investigates various ways in which a pandemic such as the novel
coronavirus, could be predicted using different mathematical models. It also
studies the various ways in which these models could be depicted using various
visualization techniques. This paper aims to present various statistical
techniques suggested by the Centres for Disease Control and Prevention in order
to represent the epidemiological data. The main focus of this paper is to
analyse how epidemiological data or contagious diseases are theorized using any
available information and later may be presented wrongly by not following the
guidelines, leading to inaccurate representation and interpretations of the
current scenario of the pandemic; with a special reference to the Indian
Subcontinent.","['Shailesh Bharati', 'Rahul Batra']",2,0.5239497
"Synthetic media detection technologies label media as either synthetic or
non-synthetic and are increasingly used by journalists, web platforms, and the
general public to identify misinformation and other forms of problematic
content. As both well-resourced organizations and the non-technical general
public generate more sophisticated synthetic media, the capacity for purveyors
of problematic content to adapt induces a \newterm{detection dilemma}: as
detection practices become more accessible, they become more easily
circumvented. This paper describes how a multistakeholder cohort from academia,
technology platforms, media entities, and civil society organizations active in
synthetic media detection and its socio-technical implications evaluates the
detection dilemma. Specifically, we offer an assessment of detection contexts
and adversary capacities sourced from the broader, global AI and media
integrity community concerned with mitigating the spread of harmful synthetic
media. A collection of personas illustrates the intersection between
unsophisticated and highly-resourced sponsors of misinformation in the context
of their technical capacities. This work concludes that there is no ""best""
approach to navigating the detector dilemma, but derives a set of implications
from multistakeholder input to better inform detection process decisions and
policies, in practice.","['Claire Leibowicz', 'Sean McGregor', 'Aviv Ovadya']",0,0.70912015
"In this paper, we present an updated version of the NELA-GT-2019 dataset,
entitled NELA-GT-2020. NELA-GT-2020 contains nearly 1.8M news articles from 519
sources collected between January 1st, 2020 and December 31st, 2020. Just as
with NELA-GT-2018 and NELA-GT-2019, these sources come from a wide range of
mainstream news sources and alternative news sources. Included in the dataset
are source-level ground truth labels from Media Bias/Fact Check (MBFC) covering
multiple dimensions of veracity. Additionally, new in the 2020 dataset are the
Tweets embedded in the collected news articles, adding an extra layer of
information to the data. The NELA-GT-2020 dataset can be found at
https://doi.org/10.7910/DVN/CHMUYZ.","['Maur√≠cio Gruppi', 'Benjamin D. Horne', 'Sibel Adalƒ±']",4,0.46649867
"A tweet from popular entertainer and businesswoman, Rihanna, bringing
attention to farmers' protests around Delhi set off heightened activity on
Indian social media. An immediate consequence was the weighing in by Indian
politicians, entertainers, media and other influencers on the issue. In this
paper, we use data from Twitter and an archive of debunked misinformation
stories to understand some of the patterns around influencer engagement with a
political issue. We found that more followed influencers were less likely to
come out in support of the tweet. We also find that the later engagement of
major influencers on the side of the government's position shows suggestion's
of collusion. Irrespective of their position on the issue, influencers who
engaged saw a significant rise in their following after their tweets. While a
number of tweets thanked Rihanna for raising awareness on the issue, she was
systematically trolled on the grounds of her gender, race, nationality and
religion. Finally, we observed how misinformation existing prior to the tweet
set up the grounds for alternative narratives that emerged.","['Dibyendu Mishra', 'Syeda Zainab Akbar', 'Arshia Arya', 'Saloni Dash', 'Rynaa Grover', 'Joyojeet Pal']",10,0.76489294
"The widespread of fake news and misinformation in various domains ranging
from politics, economics to public health has posed an urgent need to
automatically fact-check information. A recent trend in fake news detection is
to utilize evidence from external sources. However, existing evidence-aware
fake news detection methods focused on either only word-level attention or
evidence-level attention, which may result in suboptimal performance. In this
paper, we propose a Hierarchical Multi-head Attentive Network to fact-check
textual claims. Our model jointly combines multi-head word-level attention and
multi-head document-level attention, which aid explanation in both word-level
and evidence-level. Experiments on two real-word datasets show that our model
outperforms seven state-of-the-art baselines. Improvements over baselines are
from 6\% to 18\%. Our source code and datasets are released at
\texttt{\url{https://github.com/nguyenvo09/EACL2021}}.","['Nguyen Vo', 'Kyumin Lee']",4,0.79648334
"Parents - particularly moms - increasingly consult social media for support
when taking decisions about their young children, and likely also when advising
other family members such as elderly relatives. Minimizing malignant online
influences is therefore crucial to securing their assent for policies ranging
from vaccinations, masks and social distancing against the pandemic, to
household best practices against climate change, to acceptance of future 5G
towers nearby. Here we show how a strengthening of bonds across online
communities during the pandemic, has led to non-Covid-19 conspiracy theories
(e.g. fluoride, chemtrails, 5G) attaining heightened access to mainstream
parent communities. Alternative health communities act as the critical conduits
between conspiracy theorists and parents, and make the narratives more
palatable to the latter. We demonstrate experimentally that these
inter-community bonds can perpetually generate new misinformation, irrespective
of any changes in factual information. Our findings show explicitly why
Facebook's current policies have failed to stop the mainstreaming of
non-Covid-19 and Covid-19 conspiracy theories and misinformation, and why
targeting the largest communities will not work. A simple yet exactly solvable
and empirically grounded mathematical model, shows how modest tailoring of
mainstream communities' couplings could prevent them from tipping against
establishment guidance. Our conclusions should also apply to other social media
platforms and topics.","['N. F. Johnson', 'N. Velasquez', 'N. Johnson Restrepo', 'R. Leahy', 'R. Sear', 'N. Gabriel', 'H. Larson', 'Y. Lupu']",3,0.66948366
"Bot Detection is an essential asset in a period where Online Social
Networks(OSN) is a part of our lives. This task becomes more relevant in
crises, as the Covid-19 pandemic, where there is an incipient risk of
proliferation of social bots, producing a possible source of misinformation. In
order to address this issue, it has been compared different methods to detect
automatically social bots on Twitter using Data Selection. The techniques
utilized to elaborate the bot detection models include the utilization of
features as the tweets metadata or the Digital Fingerprint of the Twitter
accounts. In addition, it was analyzed the presence of bots in tweets from
different periods of the first months of the Covid-19 pandemic, using the bot
detection technique which best fits the scope of the task. Moreover, this work
includes also analysis over aspects regarding the discourse of bots and humans,
such as sentiment or hashtag utilization.","['Marzia Antenore', 'Jose M. Camacho-Rodriguez', 'Emanuele Panizzi']",13,0.8605249
"During COVID-19, misinformation on social media affects the adoption of
appropriate prevention behaviors. It is urgent to suppress the misinformation
to prevent negative public health consequences. Although an array of studies
has proposed misinformation suppression strategies, few have investigated the
role of predominant credible information during crises. None has examined its
effect quantitatively using longitudinal social media data. Therefore, this
research investigates the temporal correlations between credible information
and misinformation, and whether predominant credible information can suppress
misinformation for two prevention measures (i.e. topics), i.e. wearing masks
and social distancing using tweets collected from February 15 to June 30, 2020.
We trained Support Vector Machine classifiers to retrieve relevant tweets and
classify tweets containing credible information and misinformation for each
topic. Based on cross-correlation analyses of credible and misinformation time
series for both topics, we find that the previously predominant credible
information can lead to the decrease of misinformation (i.e. suppression) with
a time lag. The research findings provide empirical evidence for suppressing
misinformation with credible information in complex online environments and
suggest practical strategies for future information management during crises
and emergencies.","['Yan Wang', 'Shangde Gao', 'Wenyu Gao']",5,0.7454927
"This paper presents a secure reinforcement learning (RL) based control method
for unknown linear time-invariant cyber-physical systems (CPSs) that are
subjected to compositional attacks such as eavesdropping and covert attack. We
consider the attack scenario where the attacker learns about the dynamic model
during the exploration phase of the learning conducted by the designer to learn
a linear quadratic regulator (LQR), and thereafter, use such information to
conduct a covert attack on the dynamic system, which we refer to as doubly
learning-based control and attack (DLCA) framework. We propose a dynamic
camouflaging based attack-resilient reinforcement learning (ARRL) algorithm
which can learn the desired optimal controller for the dynamic system, and at
the same time, can inject sufficient misinformation in the estimation of system
dynamics by the attacker. The algorithm is accompanied by theoretical
guarantees and extensive numerical experiments on a consensus multi-agent
system and on a benchmark power grid model.","['Sayak Mukherjee', 'Veronica Adetola']",2,0.6277865
"The rapid outbreak of COVID-19 has caused humanity to come to a stand-still
and brought with it a plethora of other problems. COVID-19 is the first
pandemic in history when humanity is the most technologically advanced and
relies heavily on social media platforms for connectivity and other benefits.
Unfortunately, fake news and misinformation regarding this virus is also
available to people and causing some massive problems. So, fighting this
infodemic has become a significant challenge. We present our solution for the
""Constraint@AAAI2021 - COVID19 Fake News Detection in English"" challenge in
this work. After extensive experimentation with numerous architectures and
techniques, we use eight different transformer-based pre-trained models with
additional layers to construct a stacking ensemble classifier and fine-tuned
them for our purpose. We achieved 0.979906542 accuracy, 0.979913119 precision,
0.979906542 recall, and 0.979907901 f1-score on the test dataset of the
competition.","['S. M. Sadiq-Ur-Rahman Shifath', 'Mohammad Faiyaz Khan', 'Md. Saiful Islam']",5,0.6230215
"When users on social media share content without considering its veracity,
they may unwittingly be spreading misinformation. In this work, we investigate
the design of lightweight interventions that nudge users to assess the accuracy
of information as they share it. Such assessment may deter users from posting
misinformation in the first place, and their assessments may also provide
useful guidance to friends aiming to assess those posts themselves. In support
of lightweight assessment, we first develop a taxonomy of the reasons why
people believe a news claim is or is not true; this taxonomy yields a checklist
that can be used at posting time. We conduct evaluations to demonstrate that
the checklist is an accurate and comprehensive encapsulation of people's
free-response rationales. In a second experiment, we study the effects of three
behavioral nudges -- 1) checkboxes indicating whether headings are accurate, 2)
tagging reasons (from our taxonomy) that a post is accurate via a checklist and
3) providing free-text rationales for why a headline is or is not accurate --
on people's intention of sharing the headline on social media. From an
experiment with 1668 participants, we find that both providing accuracy
assessment and rationale reduce the sharing of false content. They also reduce
the sharing of true content, but to a lesser degree that yields an overall
decrease in the fraction of shared content that is false. Our findings have
implications for designing social media and news sharing platforms that draw
from richer signals of content credibility contributed by users. In addition,
our validated taxonomy can be used by platforms and researchers as a way to
gather rationales in an easier fashion than free-response.","['Farnaz Jahanbakhsh', 'Amy X. Zhang', 'Adam J. Berinsky', 'Gordon Pennycook', 'David G. Rand', 'David R. Karger']",3,0.72365105
"The spread of misinformation can cause social confusion. The authenticity of
information on a social networking service (SNS) is unknown, and false
information can be easily spread. Consequently, many studies have been
conducted on methods to control the spread of misinformation on social
networking sites. However, few studies have examined the impact of the spread
of misinformation and its corrections on society. This study models the impact
of the reduction of misinformation and the diffusion of corrective information
on social disruption, and it identifies the features of this impact. In this
study, we analyzed misinformation regarding the shortage of toilet paper during
the 2020 COVID-19 epidemic, its corrections, and the excessive purchasing
caused by this information. First, we analyze the amount of misinformation and
corrective information spread on SNS, and we create a regression model to
estimate the real-world impact of misinformation and its correction. This model
is used to analyze the change in real-world impact corresponding to the change
in the diffusion of misinformation and corrective information. Our analysis
shows that the corrective information was spread to a much greater extent than
the misinformation. In addition, our model reveals that the corrective
information was what caused the excessive purchasing behavior. As a result of
our further analysis, we found that the amount of diffusion of corrective
information required to minimize the impact on the real world depends on the
amount of the diffusion of misinformation.","['Ryusuke Iizuka', 'Fujio Toriumi', 'Mao Nishiguchi', 'Masanori Takano', 'Mitsuo Yoshida']",0,0.69448864
"Online social media enables mass-level, transparent, and democratized
discussion on numerous socio-political issues. Due to such openness, these
platforms often endure manipulation and misinformation - leading to negative
impacts. To prevent such harmful activities, platform moderators employ
countermeasures to safeguard against actors violating their rules. However, the
correlation between publicly outlined policies and employed action is less
clear to general people. In this work, we examine violations and subsequent
moderation related to the 2020 U.S. President Election discussion on Twitter, a
popular micro-blogging site. We focus on quantifying plausible reasons for the
suspension, drawing on Twitter's rules and policies by identifying suspended
users (Case) and comparing their activities and properties with (yet)
non-suspended (Control) users. Using a dataset of 240M election-related tweets
made by 21M unique users, we observe that Suspended users violate Twitter's
rules at a higher rate (statistically significant) than Control users across
all the considered aspects - hate speech, offensiveness, spamming, and civic
integrity. Moreover, through the lens of Twitter's suspension mechanism, we
qualitatively examine the targeted topics for manipulation.","['Farhan Asif Chowdhury', 'Dheeman Saha', 'Md Rashidul Hasan', 'Koustuv Saha', 'Abdullah Mueen']",10,0.777251
"There is a growing concern that e-commerce platforms are amplifying
vaccine-misinformation. To investigate, we conduct two-sets of algorithmic
audits for vaccine misinformation on the search and recommendation algorithms
of Amazon -- world's leading e-retailer. First, we systematically audit
search-results belonging to vaccine-related search-queries without logging into
the platform -- unpersonalized audits. We find 10.47% of search-results promote
misinformative health products. We also observe ranking-bias, with Amazon
ranking misinformative search-results higher than debunking search-results.
Next, we analyze the effects of personalization due to account-history, where
history is built progressively by performing various real-world user-actions,
such as clicking a product. We find evidence of filter-bubble effect in
Amazon's recommendations; accounts performing actions on misinformative
products are presented with more misinformation compared to accounts performing
actions on neutral and debunking products. Interestingly, once user clicks on a
misinformative product, homepage recommendations become more contaminated
compared to when user shows an intention to buy that product.","['Prerna Juneja', 'Tanushree Mitra']",3,0.5562568
"Despite the ubiquity and proliferation of images and videos in online news
environments, much of the existing research on misinformation and its
correction is solely focused on textual misinformation, and little is known
about how ordinary users evaluate fake or manipulated images and the most
effective ways to label and correct such falsities. We designed a visual
forensic label of image authenticity, Picture-O-Meter, and tested the label's
efficacy in relation to its source and placement in an experiment with 2440
participants. Our findings demonstrate that, despite human beings' general
inability to detect manipulated images on their own, image forensic labels are
an effective tool for counteracting visual misinformation.","['Cuihua Shen', 'Mona Kasra', ""James O'Brien""]",7,0.6266433
"Moral outrage has become synonymous with social media in recent years.
However, the preponderance of academic analysis on social media websites has
focused on hate speech and misinformation. This paper focuses on analyzing
moral judgements rendered on social media by capturing the moral judgements
that are passed in the subreddit /r/AmITheAsshole on Reddit. Using the labels
associated with each judgement we train a classifier that can take a comment
and determine whether it judges the user who made the original post to have
positive or negative moral valence. Then, we use this classifier to investigate
an assortment of website traits surrounding moral judgements in ten other
subreddits, including where negative moral users like to post and their posting
patterns. Our findings also indicate that posts that are judged in a positive
manner will score higher.","['Nicholas Botzer', 'Shawn Gu', 'Tim Weninger']",3,0.5341677
"Despite the recent attention to DeepFakes, one of the most prevalent ways to
mislead audiences on social media is the use of unaltered images in a new but
false context. To address these challenges and support fact-checkers, we
propose a new method that automatically detects out-of-context image and text
pairs. Our key insight is to leverage the grounding of image with text to
distinguish out-of-context scenarios that cannot be disambiguated with language
alone. We propose a self-supervised training strategy where we only need a set
of captioned images. At train time, our method learns to selectively align
individual objects in an image with textual claims, without explicit
supervision. At test time, we check if both captions correspond to the same
object(s) in the image but are semantically different, which allows us to make
fairly accurate out-of-context predictions. Our method achieves 85%
out-of-context detection accuracy. To facilitate benchmarking of this task, we
create a large-scale dataset of 200K images with 450K textual captions from a
variety of news websites, blogs, and social media posts. The dataset and source
code is publicly available at
https://shivangi-aneja.github.io/projects/cosmos/.","['Shivangi Aneja', 'Chris Bregler', 'Matthias Nie√üner']",8,0.62318426
"Amid the pandemic COVID-19, the world is facing unprecedented infodemic with
the proliferation of both fake and real information. Considering the
problematic consequences that the COVID-19 fake-news have brought, the
scientific community has put effort to tackle it. To contribute to this fight
against the infodemic, we aim to achieve a robust model for the COVID-19
fake-news detection task proposed at CONSTRAINT 2021 (FakeNews-19) by taking
two separate approaches: 1) fine-tuning transformers based language models with
robust loss functions and 2) removing harmful training instances through
influence calculation. We further evaluate the robustness of our models by
evaluating on different COVID-19 misinformation test set (Tweets-19) to
understand model generalization ability. With the first approach, we achieve
98.13% for weighted F1 score (W-F1) for the shared task, whereas 38.18% W-F1 on
the Tweets-19 highest. On the contrary, by performing influence data cleansing,
our model with 99% cleansing percentage can achieve 54.33% W-F1 score on
Tweets-19 with a trade-off. By evaluating our models on two COVID-19 fake-news
test sets, we suggest the importance of model generalization ability in this
task to step forward to tackle the COVID-19 fake-news problem in online social
media platforms.","['Yejin Bang', 'Etsuko Ishii', 'Samuel Cahyawijaya', 'Ziwei Ji', 'Pascale Fung']",5,0.56591785
"Fake news on social media has become a hot topic of research as it negatively
impacts the discourse of real news in the public. Specifically, the ongoing
COVID-19 pandemic has seen a rise of inaccurate and misleading information due
to the surrounding controversies and unknown details at the beginning of the
pandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating
a challenge to automatically detect tweets containing misinformation based on
text and structure from Twitter follower network. In this paper, we present a
simple approach that uses BERT embeddings and a shallow neural network for
classifying tweets using only text, and discuss our findings and limitations of
the approach in text-based misinformation detection.","['Gullal S. Cheema', 'Sherzod Hakimov', 'Ralph Ewerth']",4,0.7862567
"The rapid growth of social media content during the current pandemic provides
useful tools for disseminating information which has also become a root for
misinformation. Therefore, there is an urgent need for fact-checking and
effective techniques for detecting misinformation in social media. In this
work, we study the misinformation in the Arabic content of Twitter. We
construct a large Arabic dataset related to COVID-19 misinformation and
gold-annotate the tweets into two categories: misinformation or not. Then, we
apply eight different traditional and deep machine learning models, with
different features including word embeddings and word frequency. The word
embedding models (\textsc{FastText} and word2vec) exploit more than two million
Arabic tweets related to COVID-19. Experiments show that optimizing the area
under the curve (AUC) improves the models' performance and the Extreme Gradient
Boosting (XGBoost) presents the highest accuracy in detecting COVID-19
misinformation online.","['Sarah Alqurashi', 'Btool Hamoui', 'Abdulaziz Alashaikh', 'Ahmad Alhindi', 'Eisa Alanazi']",8,0.6951361
"In social networks, users often engage with like-minded peers. This selective
exposure to opinions might result in echo chambers, i.e., political
fragmentation and social polarization of user interactions. When echo chambers
form, opinions have a bimodal distribution with two peaks on opposite sides. In
certain issues, where either extreme positions contain a degree of
misinformation, neutral consensus is preferable for promoting discourse. In
this paper, we use an opinion dynamics model that naturally forms echo chambers
in order to find a feedback mechanism that bridges these communities and leads
to a neutral consensus. We introduce random dynamical nudge (RDN), which
presents each agent with input from a random selection of other agents'
opinions and does not require surveillance of every person's opinions. Our
computational results in two different models suggest that the RDN leads to a
unimodal distribution of opinions centered around the neutral consensus.
Furthermore, the RDN is effective both for preventing the formation of echo
chambers and also for depolarizing existing echo chambers. Due to the simple
and robust nature of the RDN, social media networks might be able to implement
a version of this self-feedback mechanism, when appropriate, to prevent the
segregation of online communities on complex social issues.","['Christopher Brian Currin', 'Sebasti√°n Vallejo Vera', 'Ali Khaledi-Nasab']",3,0.64935577
"With the onset of COVID-19 pandemic, social media has rapidly become a
crucial communication tool for information generation, dissemination, and
consumption. In this scoping review, we selected and examined peer-reviewed
empirical studies relating to COVID-19 and social media during the first
outbreak starting in November 2019 until May 2020. From an analysis of 81
studies, we identified five overarching public health themes concerning the
role of online social platforms and COVID-19. These themes focused on: (i)
surveying public attitudes, (ii) identifying infodemics, (iii) assessing mental
health, (iv) detecting or predicting COVID-19 cases, (v) analyzing government
responses to the pandemic, and (vi) evaluating quality of health information in
prevention education videos. Furthermore, our review highlights the paucity of
studies on the application of machine learning on social media data related to
COVID-19 and a lack of studies documenting real-time surveillance developed
with social media data on COVID-19. For COVID-19, social media can play a
crucial role in disseminating health information as well as tackling infodemics
and misinformation.","['Shu-Feng Tsao', 'Helen Chen', 'Therese Tisseverasinghe', 'Yang Yang', 'Lianghua Li', 'Zahid A. Butt']",5,0.7641939
"Recent rapid technological advancements in online social networks such as
Twitter have led to a great incline in spreading false information and fake
news. Misinformation is especially prevalent in the ongoing coronavirus disease
(COVID-19) pandemic, leading to individuals accepting bogus and potentially
deleterious claims and articles. Quick detection of fake news can reduce the
spread of panic and confusion among the public. For our analysis in this paper,
we report a methodology to analyze the reliability of information shared on
social media pertaining to the COVID-19 pandemic. Our best approach is based on
an ensemble of three transformer models (BERT, ALBERT, and XLNET) to detecting
fake news. This model was trained and evaluated in the context of the
ConstraintAI 2021 shared task COVID19 Fake News Detection in English. Our
system obtained 0.9855 f1-score on testset and ranked 5th among 160 teams.","['Sunil Gundapu', 'Radhika Mamidi']",4,0.76609457
"Even for domain experts, it is a non-trivial task to verify a scientific
claim by providing supporting or refuting evidence rationales. The situation
worsens as misinformation is proliferated on social media or news websites,
manually or programmatically, at every moment. As a result, an automatic
fact-verification tool becomes crucial for combating the spread of
misinformation. In this work, we propose a novel, paragraph-level, multi-task
learning model for the SciFact task by directly computing a sequence of
contextualized sentence embeddings from a BERT model and jointly training the
model on rationale selection and stance prediction.","['Xiangci Li', 'Gully Burns', 'Nanyun Peng']",1,0.7567524
"Fake news has now grown into a big problem for societies and also a major
challenge for people fighting disinformation. This phenomenon plagues
democratic elections, reputations of individual persons or organizations, and
has negatively impacted citizens, (e.g., during the COVID-19 pandemic in the US
or Brazil). Hence, developing effective tools to fight this phenomenon by
employing advanced Machine Learning (ML) methods poses a significant challenge.
The following paper displays the present body of knowledge on the application
of such intelligent tools in the fight against disinformation. It starts by
showing the historical perspective and the current role of fake news in the
information war. Proposed solutions based solely on the work of experts are
analysed and the most important directions of the application of intelligent
systems in the detection of misinformation sources are pointed out.
Additionally, the paper presents some useful resources (mainly datasets useful
when assessing ML solutions for fake news detection) and provides a short
overview of the most important R&D projects related to this subject. The main
purpose of this work is to analyse the current state of knowledge in detecting
fake news; on the one hand to show possible solutions, and on the other hand to
identify the main challenges and methodological gaps to motivate future
research.","['Michal Choras', 'Konstantinos Demestichas', 'Agata Gielczyk', 'Alvaro Herrero', 'Pawel Ksieniewicz', 'Konstantina Remoundou', 'Daniel Urda', 'Michal Wozniak']",0,0.79434717
"In 2019, outbreaks of vaccine-preventable diseases reached the highest number
in the US since 1992. Medical misinformation, such as antivaccine content
propagating through social media, is associated with increases in vaccine delay
and refusal. Our overall goal is to develop an automatic detector for
antivaccine messages to counteract the negative impact that antivaccine
messages have on the public health. Very few extant detection systems have
considered multimodality of social media posts (images, texts, and hashtags),
and instead focus on textual components, despite the rapid growth of
photo-sharing applications (e.g., Instagram). As a result, existing systems are
not sufficient for detecting antivaccine messages with heavy visual components
(e.g., images) posted on these newer platforms. To solve this problem, we
propose a deep learning network that leverages both visual and textual
information. A new semantic- and task-level attention mechanism was created to
help our model to focus on the essential contents of a post that signal
antivaccine messages. The proposed model, which consists of three branches, can
generate comprehensive fused features for predictions. Moreover, an ensemble
method is proposed to further improve the final prediction accuracy. To
evaluate the proposed model's performance, a real-world social media dataset
that consists of more than 30,000 samples was collected from Instagram between
January 2016 and October 2019. Our 30 experiment results demonstrate that the
final network achieves above 97% testing accuracy and outperforms other
relevant models, demonstrating that it can detect a large amount of antivaccine
messages posted daily. The implementation code is available at
https://github.com/wzhings/antivaccine_detection.","['Zuhui Wang', 'Zhaozheng Yin', 'Young Anna Argyris']",12,0.7297659
"The behaviour of sharing information on social media should be fulfilled only
when a user is exhibiting attentive behaviour. So that the useful information
can be consumed constructively, and misinformation can be identified and
ignored. Attentive behaviour is related to users' cognitive abilities in their
processing of set information. The work described in this paper examines the
issue of attentive factors that affect users' behaviour when they share
misinformation on social media. The research aims to identify the significance
of prevailing attention factors towards sharing misinformation on social media.
We used a closed-ended questionnaire which consisted of a psychometric scale to
measure attention behaviour with participants (n = 112). The regression
equation results are obtained as: y=(19,533-0,390+e) from a set of regression
analyses shows that attention factors have a significant negative correlation
effect for users to share misinformation on social media. Along with the
findings of the analysis results, we propose that attentive factors are
incorporated into a social media application's future design that could
intervene in user attention and avoid potential harm caused by the spread of
misinformation.","['Zaid Amin', 'Nazlena Mohamad Ali', 'Alan F. Smeaton']",0,0.70629174
"The COVID-19 pandemic has had a huge impact on various areas of human life.
Hence, the coronavirus pandemic and its consequences are being actively
discussed on social media. However, not all social media posts are truthful.
Many of them spread fake news that cause panic among readers, misinform people
and thus exacerbate the effect of the pandemic. In this paper, we present our
results at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in
English. In particular, we propose our approach using the transformer-based
ensemble of COVID-Twitter-BERT (CT-BERT) models. We describe the models used,
the ways of text preprocessing and adding extra data. As a result, our best
model achieved the weighted F1-score of 98.69 on the test set (the first place
in the leaderboard) of this shared task that attracted 166 submitted teams in
total.","['Anna Glazkova', 'Maksim Glazkov', 'Timofey Trifonov']",5,0.70497453
"Predicting video viewership is a top priority for content creators and
video-sharing sites. Content creators live on such predictions to maximize
influences and minimize budgets. Video-sharing sites rely on this prediction to
promote credible videos and curb violative videos. Although deep learning
champions viewership prediction, it lacks interpretability, which is
fundamental to increasing the adoption of predictive models and prescribing
measurements to improve viewership. Following the design-science paradigm, we
propose a novel interpretable IT system, Precise Wide and Deep Learning
(PrecWD), to precisely interpret viewership prediction. Improving upon
state-of-the-art frameworks, PrecWD offers precise feature effects and designs
an unstructured component. PrecWD outperforms benchmarks in two contexts:
health video viewership prediction and misinformation viewership prediction. A
user study confirms the superior interpretability of PrecWD. This study
contributes to IS design theory with generalizable design principles and an
interpretable predictive framework. Our findings provide implications to
improve video viewership and credibility.","['Jiaheng Xie', 'Xiao Liu']",0,0.4574494
"Amidst COVID-19 misinformation spreading, social media platforms like
Facebook and Twitter rolled out design interventions, including banners linking
to authoritative resources and more specific ""false information"" labels. In
late March 2020, shortly after these interventions began to appear, we
conducted an exploratory mixed-methods survey (N = 311) to learn: what are
social media users' attitudes towards these interventions, and to what extent
do they self-report effectiveness? We found that most participants indicated a
positive attitude towards interventions, particularly post-specific labels for
misinformation. Still, the majority of participants discovered or corrected
misinformation through other means, most commonly web searches, suggesting room
for platforms to do more to stem the spread of COVID-19 misinformation.","['Christine Geeng', 'Tiona Francisco', 'Jevin West', 'Franziska Roesner']",3,0.74277
"News recommenders help users to find relevant online content and have the
potential to fulfill a crucial role in a democratic society, directing the
scarce attention of citizens towards the information that is most important to
them. Simultaneously, recent concerns about so-called filter bubbles,
misinformation and selective exposure are symptomatic of the disruptive
potential of these digital news recommenders. Recommender systems can make or
break filter bubbles, and as such can be instrumental in creating either a more
closed or a more open internet. Current approaches to evaluating recommender
systems are often focused on measuring an increase in user clicks and
short-term engagement, rather than measuring the user's longer term interest in
diverse and important information.
  This paper aims to bridge the gap between normative notions of diversity,
rooted in democratic theory, and quantitative metrics necessary for evaluating
the recommender system. We propose a set of metrics grounded in social science
interpretations of diversity and suggest ways for practical implementations.","['Sanne Vrijenhoek', 'Mesut Kaya', 'Nadia Metoui', 'Judith M√∂ller', 'Daan Odijk', 'Natali Helberger']",0,0.6643789
"The omnipresent COVID-19 pandemic gave rise to a parallel spreading of
misinformation, also referred to as an Infodemic. Consequently, social media
have become targets for the application of social bots, that is, algorithms
that mimic human behaviour. Their ability to exert influence on social media
can be exploited by amplifying misinformation, rumours, or conspiracy theories
which might be harmful to society and the mastery of the pandemic. By applying
social bot detection and content analysis techniques, this study aims to
determine the extent to which social bots interfere with COVID- 19 discussions
on Twitter. A total of 78 presumptive bots were detected within a sample of
542,345 users. The analysis revealed that bot-like users who disseminate
misinformation, at the same time, intersperse news from renowned sources. The
findings of this research provide implications for improved bot detection and
managing potential threats through social bots during ongoing and future
crises.","['Julian Marx', 'Felix Br√ºnker', 'Milad Mirbabaie', 'Eric Hochstrate']",13,0.8095195
"The global spread of the novel coronavirus is affected by the spread of
related misinformation -- the so-called COVID-19 Infodemic -- that makes
populations more vulnerable to the disease through resistance to mitigation
efforts. Here we analyze the prevalence and diffusion of links to
low-credibility content about the pandemic across two major social media
platforms, Twitter and Facebook. We characterize cross-platform similarities
and differences in popular sources, diffusion patterns, influencers,
coordination, and automation. Comparing the two platforms, we find divergence
among the prevalence of popular low-credibility sources and suspicious videos.
A minority of accounts and pages exert a strong influence on each platform.
These misinformation ""superspreaders"" are often associated with the
low-credibility sources and tend to be verified by the platforms. On both
platforms, there is evidence of coordinated sharing of Infodemic content. The
overt nature of this manipulation points to the need for societal-level
solutions in addition to mitigation strategies within the platforms. However,
we highlight limits imposed by inconsistent data-access policies on our
capability to study harmful manipulations of information ecosystems.","['Kai-Cheng Yang', 'Francesco Pierri', 'Pik-Mai Hui', 'David Axelrod', 'Christopher Torres-Lugo', 'John Bryden', 'Filippo Menczer']",5,0.7076615
"Over the past years, deep generative models have achieved a new level of
performance. Generated data has become difficult, if not impossible, to be
distinguished from real data. While there are plenty of use cases that benefit
from this technology, there are also strong concerns on how this new technology
can be misused to generate deep fakes and enable misinformation at scale.
Unfortunately, current deep fake detection methods are not sustainable, as the
gap between real and fake continues to close. In contrast, our work enables a
responsible disclosure of such state-of-the-art generative models, that allows
model inventors to fingerprint their models, so that the generated samples
containing a fingerprint can be accurately detected and attributed to a source.
Our technique achieves this by an efficient and scalable ad-hoc generation of a
large population of models with distinct fingerprints. Our recommended
operation point uses a 128-bit fingerprint which in principle results in more
than $10^{38}$ identifiable models. Experiments show that our method fulfills
key properties of a fingerprinting mechanism and achieves effectiveness in deep
fake detection and attribution. Code and models are available at
https://github.com/ningyu1991/ScalableGANFingerprints .","['Ning Yu', 'Vladislav Skripniuk', 'Dingfan Chen', 'Larry Davis', 'Mario Fritz']",7,0.77675354
"Deepfake technology (DT) has taken a new level of sophistication.
Cybercriminals now can manipulate sounds, images, and videos to defraud and
misinform individuals and businesses. This represents a growing threat to
international institutions and individuals which needs to be addressed. This
paper provides an overview of deepfakes, their benefits to society, and how DT
works. Highlights the threats that are presented by deepfakes to businesses,
politics, and judicial systems worldwide. Additionally, the paper will explore
potential solutions to deepfakes and conclude with future research direction.",['Shadrack Awah Buo'],11,0.6265695
"As the globally increasing population drives rapid urbanisation in various
parts of the world, there is a great need to deliberate on the future of the
cities worth living. In particular, as modern smart cities embrace more and
more data-driven artificial intelligence services, it is worth remembering that
technology can facilitate prosperity, wellbeing, urban livability, or social
justice, but only when it has the right analog complements (such as
well-thought out policies, mature institutions, responsible governance); and
the ultimate objective of these smart cities is to facilitate and enhance human
welfare and social flourishing. Researchers have shown that various
technological business models and features can in fact contribute to social
problems such as extremism, polarization, misinformation, and Internet
addiction. In the light of these observations, addressing the philosophical and
ethical questions involved in ensuring the security, safety, and
interpretability of such AI algorithms that will form the technological bedrock
of future cities assumes paramount importance. Globally there are calls for
technology to be made more humane and human-centered. In this paper, we analyze
and explore key challenges including security, robustness, interpretability,
and ethical (data and algorithmic) challenges to a successful deployment of AI
in human-centric applications, with a particular emphasis on the convergence of
these concepts/challenges. We provide a detailed review of existing literature
on these key challenges and analyze how one of these challenges may lead to
others or help in solving other challenges. The paper also advises on the
current limitations, pitfalls, and future directions of research in these
domains, and how it can fill the current gaps and lead to better solutions. We
believe such rigorous analysis will provide a baseline for future research in
the domain.","['Kashif Ahmad', 'Majdi Maabreh', 'Mohamed Ghaly', 'Khalil Khan', 'Junaid Qadir', 'Ala Al-Fuqaha']",9,0.72674555
"Background: The COVID-19 outbreak has left many people isolated within their
homes; these people are turning to social media for news and social connection,
which leaves them vulnerable to believing and sharing misinformation.
Health-related misinformation threatens adherence to public health messaging,
and monitoring its spread on social media is critical to understanding the
evolution of ideas that have potentially negative public health impacts.
Results: Analysis using model-labeled data was beneficial for increasing the
proportion of data matching misinformation indicators. Random forest classifier
metrics varied across the four conspiracy theories considered (F1 scores
between 0.347 and 0.857); this performance increased as the given conspiracy
theory was more narrowly defined. We showed that misinformation tweets
demonstrate more negative sentiment when compared to nonmisinformation tweets
and that theories evolve over time, incorporating details from unrelated
conspiracy theories as well as real-world events. Conclusions: Although we
focus here on health-related misinformation, this combination of approaches is
not specific to public health and is valuable for characterizing misinformation
in general, which is an important first step in creating targeted messaging to
counteract its spread. Initial messaging should aim to preempt generalized
misinformation before it becomes widespread, while later messaging will need to
target evolving conspiracy theories and the new facets of each as they become
incorporated.","['Dax Gerts', 'Courtney D. Shelley', 'Nidhi Parikh', 'Travis Pitts', 'Chrysm Watson Ross', 'Geoffrey Fairchild', 'Nidia Yadria Vaquera Chavez', 'Ashlynn R. Daughton']",3,0.79103035
"This paper presents TrollHunter2020, a real-time detection mechanism we used
to hunt for trolling narratives on Twitter during the 2020 U.S. elections.
Trolling narratives form on Twitter as alternative explanations of polarizing
events like the 2020 U.S. elections with the goal to conduct information
operations or provoke emotional response. Detecting trolling narratives thus is
an imperative step to preserve constructive discourse on Twitter and remove an
influx of misinformation. Using existing techniques, this takes time and a
wealth of data, which, in a rapidly changing election cycle with high stakes,
might not be available. To overcome this limitation, we developed
TrollHunter2020 to hunt for trolls in real-time with several dozens of trending
Twitter topics and hashtags corresponding to the candidates' debates, the
election night, and the election aftermath. TrollHunter2020 collects trending
data and utilizes a correspondence analysis to detect meaningful relationships
between the top nouns and verbs used in constructing trolling narratives while
they emerge on Twitter. Our results suggest that the TrollHunter2020 indeed
captures the emerging trolling narratives in a very early stage of an unfolding
polarizing event. We discuss the utility of TrollHunter2020 for early detection
of information operations or trolling and the implications of its use in
supporting a constrictive discourse on the platform around polarizing topics.","['Peter Jachim', 'Filipo Sharevski', 'Emma Pieroni']",10,0.658736
"Misinformation entails the dissemination of falsehoods that leads to the slow
fracturing of society via decreased trust in democratic processes,
institutions, and science. The public has grown aware of the role of social
media as a superspreader of untrustworthy information, where even pandemics
have not been immune. In this paper, we focus on COVID-19 misinformation and
examine a subset of 2.1M tweets to understand misinformation as a function of
engagement, tweet content (COVID-19- vs. non-COVID-19-related), and veracity
(misleading or factual). Using correlation analysis, we show the most relevant
feature subsets among over 126 features that most heavily correlate with
misinformation or facts. We found that (i) factual tweets, regardless of
whether COVID-related, were more engaging than misinformation tweets; and (ii)
features that most heavily correlated with engagement varied depending on the
veracity and content of the tweet.","['Mirela Silva', 'Fabr√≠cio Ceschin', 'Prakash Shrestha', 'Christopher Brant', 'Shlok Gilda', 'Juliana Fernandes', 'Catia S. Silva', 'Andr√© Gr√©gio', 'Daniela Oliveira', 'Luiz Giovanini']",5,0.7669871
"As the world becomes more and more interconnected, our everyday objects
become part of the Internet of Things, and our lives get more and more mirrored
in virtual reality, where every piece of~information, including misinformation,
fake news and malware, can spread very fast practically anonymously. To
suppress such uncontrolled spread, efficient computer systems and algorithms
capable to~track down such malicious information spread have to be developed.
Currently, the most effective methods for source localization are based on
sensors which provide the times at which they detect the~spread. We investigate
the problem of the optimal placement of such sensors in complex networks and
propose a new graph measure, called Collective Betweenness, which we compare
against four other metrics. Extensive numerical tests are performed on
different types of complex networks over the wide ranges of densities of
sensors and stochasticities of signal. In these tests, we discovered clear
difference in comparative performance of the investigated optimal placement
methods between real or scale-free synthetic networks versus narrow degree
distribution networks. The former have a clear region for any given method's
dominance in contrast to the latter where the performance maps are less
homogeneous. We find that while choosing the best method is very network and
spread dependent, there are two methods that consistently stand out. High
Variance Observers seem to do very well for spread with low stochasticity
whereas Collective Betwenness, introduced in this paper, thrives when the
spread is highly unpredictable.","['Robert Paluch', '≈Åukasz G. Gajewski', 'Janusz A. Ho≈Çyst', 'Boleslaw K. Szymanski']",2,0.77583396
"Over the past few months, there were huge numbers of circulating tweets and
discussions about Coronavirus (COVID-19) in the Arab region. It is important
for policy makers and many people to identify types of shared tweets to better
understand public behavior, topics of interest, requests from governments,
sources of tweets, etc. It is also crucial to prevent spreading of rumors and
misinformation about the virus or bad cures. To this end, we present the
largest manually annotated dataset of Arabic tweets related to COVID-19. We
describe annotation guidelines, analyze our dataset and build effective machine
learning and transformer based models for classification.","['Hamdy Mubarak', 'Sabit Hassan']",5,0.7154944
"We introduce CLIMATE-FEVER, a new publicly available dataset for verification
of climate change-related claims. By providing a dataset for the research
community, we aim to facilitate and encourage work on improving algorithms for
retrieving evidential support for climate-specific claims, addressing the
underlying language understanding challenges, and ultimately help alleviate the
impact of misinformation on climate change. We adapt the methodology of FEVER
[1], the largest dataset of artificially designed claims, to real-life claims
collected from the Internet. While during this process, we could rely on the
expertise of renowned climate scientists, it turned out to be no easy task. We
discuss the surprising, subtle complexity of modeling real-world
climate-related claims within the \textsc{fever} framework, which we believe
provides a valuable challenge for general natural language understanding. We
hope that our work will mark the beginning of a new exciting long-term joint
effort by the climate science and AI community.","['Thomas Diggelmann', 'Jordan Boyd-Graber', 'Jannis Bulian', 'Massimiliano Ciaramita', 'Markus Leippold']",1,0.55390275
"The paper presents our solutions for the MediaEval 2020 task namely FakeNews:
Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis. The task
aims to analyze tweets related to COVID-19 and 5G conspiracy theories to detect
misinformation spreaders. The task is composed of two sub-tasks namely (i)
text-based, and (ii) structure-based fake news detection. For the first task,
we propose six different solutions relying on Bag of Words (BoW) and BERT
embedding. Three of the methods aim at binary classification task by
differentiating in 5G conspiracy and the rest of the COVID-19 related tweets
while the rest of them treat the task as ternary classification problem. In the
ternary classification task, our BoW and BERT based methods obtained an
F1-score of .606% and .566% on the development set, respectively. On the binary
classification, the BoW and BERT based solutions obtained an average F1-score
of .666% and .693%, respectively. On the other hand, for structure-based fake
news detection, we rely on Graph Neural Networks (GNNs) achieving an average
ROC of .95% on the development set.","['Abdullah Hamid', 'Nasrullah Shiekh', 'Naina Said', 'Kashif Ahmad', 'Asma Gul', 'Laiq Hassan', 'Ala Al-Fuqaha']",4,0.5966153
"Conventional preventive measures during pandemic include social distancing
and lockdown. Such measures in the time of social media brought about a new set
of challenges - vulnerability to the toxic impact of online misinformation is
high. A case in point is the prevailing COVID-19; as the virus propagates, so
does the associated misinformation and fake news about it leading to infodemic.
Since the outbreak, there has been a surge of studies investigating various
aspects of the pandemic. Of interest to this chapter include studies centring
on datasets from online social media platforms where the bulk of the public
discourse happen. Consequently, the main goal is to support the fight against
negative infodemic by (1) contributing a diverse set of curated relevant
datasets (2) recommending relevant areas to study using the datasets (3)
discussion on how relevant datasets, strategies and state-of-the-art IT tools
can be leveraged in managing the pandemic.",['Isa Inuwa-Dutse'],5,0.7687883
"The rapid advancement of technology in online communication via social media
platforms has led to a prolific rise in the spread of misinformation and fake
news. Fake news is especially rampant in the current COVID-19 pandemic, leading
to people believing in false and potentially harmful claims and stories.
Detecting fake news quickly can alleviate the spread of panic, chaos and
potential health hazards. We developed a two stage automated pipeline for
COVID-19 fake news detection using state of the art machine learning models for
natural language processing. The first model leverages a novel fact checking
algorithm that retrieves the most relevant facts concerning user claims about
particular COVID-19 claims. The second model verifies the level of truth in the
claim by computing the textual entailment between the claim and the true facts
retrieved from a manually curated COVID-19 dataset. The dataset is based on a
publicly available knowledge source consisting of more than 5000 COVID-19 false
claims and verified explanations, a subset of which was internally annotated
and cross-validated to train and evaluate our models. We evaluate a series of
models based on classical text-based features to more contextual Transformer
based models and observe that a model pipeline based on BERT and ALBERT for the
two stages respectively yields the best results.","['Rutvik Vijjali', 'Prathyush Potluri', 'Siddharth Kumar', 'Sundeep Teki']",4,0.7795784
"Since 2016, the amount of academic research with the keyword ""misinformation""
has more than doubled [2]. This research often focuses on article headlines
shown in artificial testing environments, yet misinformation largely spreads
through images and video posts shared in highly-personalized platform contexts.
A foundation of qualitative research is necessary to begin filling this gap to
ensure platforms' visual misinformation interventions are aligned with users'
needs and understanding of information in their personal contexts, across
platforms. In two studies, we combined in-depth interviews (n=15) with diary
and co-design methods (n=23) to investigate how a broad mix of Americans
exposed to misinformation during COVID-19 understand their visual information
environments, including encounters with interventions such as Facebook
fact-checking labels. Analysis reveals a deep division in user attitudes about
platform labeling interventions for visual information which are perceived by
many as overly paternalistic, biased, and punitive. Alongside these findings,
we discuss our methods as a model for continued independent qualitative
research on cross-platform user experiences of misinformation that inform
interventions.","['Emily Saltz', 'Claire Leibowicz', 'Claire Wardle']",0,0.82021344
"Facially manipulated images and videos or DeepFakes can be used maliciously
to fuel misinformation or defame individuals. Therefore, detecting DeepFakes is
crucial to increase the credibility of social media platforms and other media
sharing web sites. State-of-the art DeepFake detection techniques rely on
neural network based classification models which are known to be vulnerable to
adversarial examples. In this work, we study the vulnerabilities of
state-of-the-art DeepFake detection methods from a practical stand point. We
perform adversarial attacks on DeepFake detectors in a black box setting where
the adversary does not have complete knowledge of the classification models. We
study the extent to which adversarial perturbations transfer across different
models and propose techniques to improve the transferability of adversarial
examples. We also create more accessible attacks using Universal Adversarial
Perturbations which pose a very feasible attack scenario since they can be
easily shared amongst attackers. We perform our evaluations on the winning
entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they
can be easily bypassed in a practical attack scenario by designing transferable
and accessible adversarial attacks.","['Paarth Neekhara', 'Brian Dolhansky', 'Joanna Bitton', 'Cristian Canton Ferrer']",11,0.7545208
"The worldwide spread of COVID-19 has prompted extensive online discussions,
creating an `infodemic' on social media platforms such as WhatsApp and Twitter.
However, the information shared on these platforms is prone to be unreliable
and/or misleading. In this paper, we present the first analysis of COVID-19
discourse on public WhatsApp groups from Pakistan. Building on a large scale
annotation of thousands of messages containing text and images, we identify the
main categories of discussion. We focus on COVID-19 messages and understand the
different types of images/text messages being propagated. By exploring user
behavior related to COVID messages, we inspect how misinformation is spread.
Finally, by quantifying the flow of information across WhatsApp and Twitter, we
show how information spreads across platforms and how WhatsApp acts as a source
for much of the information shared on Twitter.","['R. Tallal Javed', 'Mirza Elaaf Shuja', 'Muhammad Usama', 'Junaid Qadir', 'Waleed Iqbal', 'Gareth Tyson', 'Ignacio Castro', 'Kiran Garimella']",5,0.65981567
"As the COVID19 pandemic has spread across the world, a concurrent pandemic of
information has spread with it. Deemed an infodemic by the World Health
Organization, and described as an overabundance of information, some accurate,
some not, that occurs during an epidemic, this proliferation of data, research
and opinions provides both opportunities and challenges for academics.
Academics and scientists have a key role to play in the solutions to the
infodemic challenge: as educators, influences and communicators, even where
their expertise and experience does not align precisely with the SARS-Cov2
virus and its impacts.
  Successful communication requires a better understanding of how the public
seeks, understands and processes scientific information, however, in order to
maximise the ways in which experts engage with traditional and social media and
to make sure that such engagement does not add to confusion and misinformation
alongside efforts to counter or challenge it. This paper will outline the key
advantages to be had from greater engagement with COVID19 discussions, the
popular channels through which such discussions take place and through which
information is disseminated. It also warns against the common pitfalls those
who choose to engage might encounter, whilst stressing that the disadvantages
of doing so are far outweighed by the advantages such engagement offers.",['Jennifer Cole'],0,0.71705484
"An infodemic is an emerging phenomenon caused by an overabundance of
information online. This proliferation of information makes it difficult for
the public to distinguish trustworthy news and credible information from
untrustworthy sites and non-credible sources. The perils of an infodemic
debuted with the outbreak of the COVID-19 pandemic and bots (i.e., automated
accounts controlled by a set of algorithms) that are suspected of spreading the
infodemic. Although previous research has revealed that bots played a central
role in spreading misinformation during major political events, how bots
behaved during the infodemic is unclear. In this paper, we examined the roles
of bots in the case of the COVID-19 infodemic and the diffusion of non-credible
information such as ""5G"" and ""Bill Gates"" conspiracy theories and content
related to ""Trump"" and ""WHO"" by analyzing retweet networks and retweeted items.
We show the segregated topology of their retweet networks, which indicates that
right-wing self-media accounts and conspiracy theorists may lead to this
opinion cleavage, while malicious bots might favor amplification of the
diffusion of non-credible information. Although the basic influence of
information diffusion could be larger in human users than bots, the effects of
bots are non-negligible under an infodemic situation.","['Wentao Xu', 'Kazutoshi Sasahara']",13,0.7284353
"Fact checking by professionals is viewed as a vital defense in the fight
against misinformation.While fact checking is important and its impact has been
significant, fact checks could have limited visibility and may not reach the
intended audience, such as those deeply embedded in polarized communities.
Concerned citizens (i.e., the crowd), who are users of the platforms where
misinformation appears, can play a crucial role in disseminating fact-checking
information and in countering the spread of misinformation. To explore if this
is the case, we conduct a data-driven study of misinformation on the Twitter
platform, focusing on tweets related to the COVID-19 pandemic, analyzing the
spread of misinformation, professional fact checks, and the crowd response to
popular misleading claims about COVID-19. In this work, we curate a dataset of
false claims and statements that seek to challenge or refute them. We train a
classifier to create a novel dataset of 155,468 COVID-19-related tweets,
containing 33,237 false claims and 33,413 refuting arguments.Our findings show
that professional fact-checking tweets have limited volume and reach. In
contrast, we observe that the surge in misinformation tweets results in a quick
response and a corresponding increase in tweets that refute such
misinformation. More importantly, we find contrasting differences in the way
the crowd refutes tweets, some tweets appear to be opinions, while others
contain concrete evidence, such as a link to a reputed source. Our work
provides insights into how misinformation is organically countered in social
platforms by some of their users and the role they play in amplifying
professional fact checks.These insights could lead to development of tools and
mechanisms that can empower concerned citizens in combating misinformation. The
code and data can be found in
http://claws.cc.gatech.edu/covid_counter_misinformation.html.","['Nicholas Micallef', 'Bing He', 'Srijan Kumar', 'Mustaque Ahamad', 'Nasir Memon']",3,0.7286775
"A rapidly evolving situation such as the COVID-19 pandemic is a significant
challenge for AI/ML models because of its unpredictability. %The most reliable
indicator of the pandemic spreading has been the number of test positive cases.
However, the tests are both incomplete (due to untested asymptomatic cases) and
late (due the lag from the initial contact event, worsening symptoms, and test
results). Social media can complement physical test data due to faster and
higher coverage, but they present a different challenge: significant amounts of
noise, misinformation and disinformation. We believe that social media can
become good indicators of pandemic, provided two conditions are met. The first
(True Novelty) is the capture of new, previously unknown, information from
unpredictably evolving situations. The second (Fact vs. Fiction) is the
distinction of verifiable facts from misinformation and disinformation. Social
media information that satisfy those two conditions are called live knowledge.
We apply evidence-based knowledge acquisition (EBKA) approach to collect,
filter, and update live knowledge through the integration of social media
sources with authoritative sources. Although limited in quantity, the reliable
training data from authoritative sources enable the filtering of misinformation
as well as capturing truly new information. We describe the EDNA/LITMUS tools
that implement EBKA, integrating social media such as Twitter and Facebook with
authoritative sources such as WHO and CDC, creating and updating live knowledge
on the COVID-19 pandemic.","['Calton Pu', 'Abhijit Suprem', 'Rodrigo Alves Lima']",5,0.7501106
"The COVID-19 pandemic has put immense pressure on health systems which are
further strained due to the misinformation surrounding it. Under such a
situation, providing the right information at the right time is crucial. There
is a growing demand for the management of information spread using Artificial
Intelligence. Hence, we have exploited the potential of Natural Language
Processing for identifying relevant information that needs to be disseminated
amongst the masses. In this work, we present a novel Cross-lingual Natural
Language Processing framework to provide relevant information by matching daily
news with trusted guidelines from the World Health Organization. The proposed
pipeline deploys various techniques of NLP such as summarizers, word
embeddings, and similarity metrics to provide users with news articles along
with a corresponding healthcare guideline. A total of 36 models were evaluated
and a combination of LexRank based summarizer on Word2Vec embedding with Word
Mover distance metric outperformed all other models. This novel open-source
approach can be used as a template for proactive dissemination of relevant
healthcare information in the midst of misinformation spread associated with
epidemics.","['Ridam Pal', 'Rohan Pandey', 'Vaibhav Gautam', 'Kanav Bhagat', 'Tavpritesh Sethi']",8,0.68082845
"We consider a population of mobile agents able to make noisy observation of
the environment and communicate their observation by production and
comprehension of signals. Individuals try to align their movement direction
with their neighbors. Besides, they try to collectively find and travel towards
an environmental direction. We show that, when the fraction of informed
individuals is small, by increasing the noise in communication, similarly to
the Viscek model, the model shows a discontinuous order-disorder transition
with strong finite size effects. In contrast, for large fraction of informed
individuals, it is possible to go from the ordered phase to the disordered
phase without passing any phase transition. The ordered phase is composed of
two phases separated by a discontinuous transition. Informed collective motion,
in which the population collectively infers the correct environmental
direction, occurs for high fraction of informed individuals. When the fraction
of informed individuals is low, misinformed collective motion, where the
population fails to find the environmental direction becomes stable as well.
Besides, we show that an amount of noise in the production of signals is more
detrimental for the inference capability of the population, and increases the
density fluctuations and the probability of group fragmentation, compared to
the same amount of noise in the comprehension.","['Mohammad Salahshour', 'Shahin Rouhani']",2,0.50714266
"A growing body of evidence points to critical vulnerabilities of social
media, such as the emergence of partisan echo chambers and the viral spread of
misinformation. We show that these vulnerabilities are amplified by abusive
behaviors associated with so-called ""follow trains"" on Twitter, in which long
lists of like-minded accounts are mentioned for others to follow. We present
the first systematic analysis of a large U.S. hyper-partisan train network. We
observe an artificial inflation of influence: accounts heavily promoted by
follow trains profit from a median six-fold increase in daily follower growth.
This catalyzes the formation of highly clustered echo chambers, hierarchically
organized around a dense core of active accounts. Train accounts also engage in
other behaviors that violate platform policies: we find evidence of activity by
inauthentic automated accounts and abnormal content deletion, as well as
amplification of toxic content from low-credibility and conspiratorial sources.
Some train accounts have been active for years, suggesting that platforms need
to pay greater attention to this kind of abuse.","['Christopher Torres-Lugo', 'Kai-Cheng Yang', 'Filippo Menczer']",3,0.67146873
"Volume of content and misinformation on social media is rapidly increasing.
There is a need for systems that can support fact checkers by prioritizing
content that needs to be fact checked. Prior research on prioritizing content
for fact-checking has focused on news media articles, predominantly in English
language. Increasingly, misinformation is found in user-generated content. In
this paper we present a novel dataset that can be used to prioritize
check-worthy posts from multi-media content in Hindi. It is unique in its 1)
focus on user generated content, 2) language and 3) accommodation of
multi-modality in social media posts. In addition, we also provide metadata for
each post such as number of shares and likes of the post on ShareChat, a
popular Indian social media platform, that allows for correlative analysis
around virality and misinformation. The data is accessible on Zenodo
(https://zenodo.org/record/4032629) under Creative Commons Attribution License
(CC BY 4.0).","['Tarunima Prabhakar', 'Anushree Gupta', 'Kruttika Nadig', 'Denny George']",4,0.6185713
"Older adults' need for quality health information has never been more
critical as during the COVID-19 pandemic. Yet, they are susceptible to the
wide-spread misinformation disseminated through search engines and social
media. To build a search-related behavioral profile of older adults, this
article surveys the empirical research on age-related differences in query
formulation, search strategies, information evaluation, and susceptibility to
misinformation effects. It also decomposes the mechanisms (i.e., cognitive
changes, development goal shift) and moderators (i.e., search task and
interface design) of such differences. To inform the design of information
systems to improve older adults' information search experience, we discuss
opportunities for future research.","['Zhaopeng Xing', 'Xiaojun Yuan', 'Lisa Vizer']",0,0.6578829
"The role played by YouTube's recommendation algorithm in unwittingly
promoting misinformation and conspiracy theories is not entirely understood.
Yet, this can have dire real-world consequences, especially when
pseudoscientific content is promoted to users at critical times, such as the
COVID-19 pandemic. In this paper, we set out to characterize and detect
pseudoscientific misinformation on YouTube. We collect 6.6K videos related to
COVID-19, the Flat Earth theory, as well as the anti-vaccination and anti-mask
movements. Using crowdsourcing, we annotate them as pseudoscience, legitimate
science, or irrelevant and train a deep learning classifier to detect
pseudoscientific videos with an accuracy of 0.79.
  We quantify user exposure to this content on various parts of the platform
and how this exposure changes based on the user's watch history. We find that
YouTube suggests more pseudoscientific content regarding traditional
pseudoscientific topics (e.g., flat earth, anti-vaccination) than for emerging
ones (like COVID-19). At the same time, these recommendations are more common
on the search results page than on a user's homepage or in the recommendation
section when actively watching videos. Finally, we shed light on how a user's
watch history substantially affects the type of recommended videos.","['Kostantinos Papadamou', 'Savvas Zannettou', 'Jeremy Blackburn', 'Emiliano De Cristofaro', 'Gianluca Stringhini', 'Michael Sirivianos']",3,0.57521266
"We analyse a Singapore-based COVID-19 Telegram group with more than 10,000
participants. First, we study the group's opinion over time, focusing on four
dimensions: participation, sentiment, topics, and psychological features. We
find that engagement peaked when the Ministry of Health raised the disease
alert level, but this engagement was not sustained. Second, we search for
government-identified misinformation in the group. We find that
government-identified misinformation is rare, and that messages discussing
these pieces of misinformation express skepticism.","['Lynnette Hui Xian Ng', 'Loke Jia Yuan']",5,0.7095494
"With the rapid increase in access to internet and the subsequent growth in
the population of online social media users, the quality of information posted,
disseminated and consumed via these platforms is an issue of growing concern. A
large fraction of the common public turn to social media platforms and in
general the internet for news and even information regarding highly concerning
issues such as COVID-19 symptoms. Given that the online information ecosystem
is extremely noisy, fraught with misinformation and disinformation, and often
contaminated by malicious agents spreading propaganda, identifying genuine and
good quality information from disinformation is a challenging task for humans.
In this regard, there is a significant amount of ongoing research in the
directions of disinformation detection and mitigation. In this survey, we
discuss the online disinformation problem, focusing on the recent 'infodemic'
in the wake of the coronavirus pandemic. We then proceed to discuss the
inherent challenges in disinformation research, and then elaborate on the
computational and interdisciplinary approaches towards mitigation of
disinformation, after a short overview of the various directions explored in
detection efforts.","['Amrita Bhattacharjee', 'Kai Shu', 'Min Gao', 'Huan Liu']",0,0.7838671
"The explosive growth and popularity of Social Media has revolutionised the
way we communicate and collaborate. Unfortunately, this same ease of accessing
and sharing information has led to an explosion of misinformation and
propaganda. Given that stance detection can significantly aid in veracity
prediction, this work focuses on boosting automated stance detection, a task on
which pre-trained models have been extremely successful on, as on several other
tasks. This work shows that the task of stance detection can benefit from
feature based information, especially on certain under performing classes,
however, integrating such features into pre-trained models using ensembling is
challenging. We propose a novel architecture for integrating features with
pre-trained models that address these challenges and test our method on the
RumourEval 2019 dataset. This method achieves state-of-the-art results with an
F1-score of 63.94 on the test set.","['Anushka Prakash', 'Harish Tayyar Madabushi']",7,0.6488967
"COVID-19 has impacted all lives. To maintain social distancing and avoiding
exposure, works and lives have gradually moved online. Under this trend, social
media usage to obtain COVID-19 news has increased. Also, misinformation on
COVID-19 is frequently spread on social media. In this work, we develop
CHECKED, the first Chinese dataset on COVID-19 misinformation. CHECKED provides
a total 2,104 verified microblogs related to COVID-19 from December 2019 to
August 2020, identified by using a specific list of keywords. Correspondingly,
CHECKED includes 1,868,175 reposts, 1,185,702 comments, and 56,852,736 likes
that reveal how these verified microblogs are spread and reacted on Weibo. The
dataset contains a rich set of multimedia information for each microblog
including ground-truth label, textual, visual, temporal, and network
information. Extensive experiments have been conducted to analyze CHECKED data
and to provide benchmark results for well-established methods when predicting
fake news using CHECKED. We hope that CHECKED can facilitate studies that
target misinformation on coronavirus. The dataset is available at
https://github.com/cyang03/CHECKED.","['Chen Yang', 'Xinyi Zhou', 'Reza Zafarani']",5,0.7081774
"In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset
for misinformation detection composed of tweets containing claims from 27th
January till the end of April 2020. We collected 138 verified claims, mostly
from popular fact-checking websites, and identified 9.4K relevant tweets to
those claims. Tweets were manually-annotated by veracity to support research on
misinformation detection, which is one of the major problems faced during a
pandemic. ArCOV19-Rumors supports two levels of misinformation detection over
Twitter: verifying free-text claims (called claim-level verification) and
verifying claims expressed in tweets (called tweet-level verification). Our
dataset covers, in addition to health, claims related to other topical
categories that were influenced by COVID-19, namely, social, politics, sports,
entertainment, and religious. Moreover, we present benchmarking results for
tweet-level verification on the dataset. We experimented with SOTA models of
versatile approaches that either exploit content, user profiles features,
temporal features and propagation structure of the conversational threads for
tweet verification.","['Fatima Haouari', 'Maram Hasanain', 'Reem Suwaileh', 'Tamer Elsayed']",5,0.67044914
"Given the widespread dissemination of inaccurate medical advice related to
the 2019 coronavirus pandemic (COVID-19), such as fake remedies, treatments and
prevention suggestions, misinformation detection has emerged as an open problem
of high importance and interest for the research community. Several works study
health misinformation detection, yet little attention has been given to the
perceived severity of misinformation posts. In this work, we frame health
misinformation as a risk assessment task. More specifically, we study the
severity of each misinformation story and how readers perceive this severity,
i.e., how harmful a message believed by the audience can be and what type of
signals can be used to recognize potentially malicious fake news and detect
refuted claims. To address our research questions, we introduce a new benchmark
dataset, accompanied by detailed data analysis. We evaluate several traditional
and state-of-the-art models and show there is a significant gap in performance
when applying traditional misinformation classification models to this task. We
conclude with open challenges and future directions.","['Arkin Dharawat', 'Ismini Lourentzou', 'Alex Morales', 'ChengXiang Zhai']",0,0.7336624
"Online Social Media (OSM) platforms such as Twitter, Facebook are extensively
exploited by the users of these platforms for spreading the (mis)information to
a large audience effortlessly at a rapid pace. It has been observed that the
misinformation can cause panic, fear, and financial loss to society. Thus, it
is important to detect and control the misinformation in such platforms before
it spreads to the masses. In this work, we focus on rumors, which is one type
of misinformation (other types are fake news, hoaxes, etc). One way to control
the spread of the rumors is by identifying users who are possibly the rumor
spreaders, that is, users who are often involved in spreading the rumors. Due
to the lack of availability of rumor spreaders labeled dataset (which is an
expensive task), we use publicly available PHEME dataset, which contains rumor
and non-rumor tweets information, and then apply a weak supervised learning
approach to transform the PHEME dataset into rumor spreaders dataset. We
utilize three types of features, that is, user, text, and ego-network features,
before applying various supervised learning approaches. In particular, to
exploit the inherent network property in this dataset (user-user reply graph),
we explore Graph Convolutional Network (GCN), a type of Graph Neural Network
(GNN) technique. We compare GCN results with the other approaches: SVM, RF, and
LSTM. Extensive experiments performed on the rumor spreaders dataset, where we
achieve up to 0.864 value for F1-Score and 0.720 value for AUC-ROC, shows the
effectiveness of our methodology for identifying possible rumor spreaders using
the GCN technique.","['Shakshi Sharma', 'Rajesh Sharma']",4,0.6522325
"The sudden widespread menace created by the present global pandemic COVID-19
has had an unprecedented effect on our lives. Man-kind is going through
humongous fear and dependence on social media like never before. Fear
inevitably leads to panic, speculations, and the spread of misinformation. Many
governments have taken measures to curb the spread of such misinformation for
public well being. Besides global measures, to have effective outreach, systems
for demographically local languages have an important role to play in this
effort. Towards this, we propose an approach to detect fake news about COVID-19
early on from social media, such as tweets, for multiple Indic-Languages
besides English. In addition, we also create an annotated dataset of Hindi and
Bengali tweet for fake news detection. We propose a BERT based model augmented
with additional relevant features extracted from Twitter to identify fake
tweets. To expand our approach to multiple Indic languages, we resort to mBERT
based model which is fine-tuned over created dataset in Hindi and Bengali. We
also propose a zero-shot learning approach to alleviate the data scarcity issue
for such low resource languages. Through rigorous experiments, we show that our
approach reaches around 89% F-Score in fake tweet detection which supercedes
the state-of-the-art (SOTA) results. Moreover, we establish the first benchmark
for two Indic-Languages, Hindi and Bengali. Using our annotated data, our model
achieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our
zero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali
Tweets without any annotated data, which clearly indicates the efficacy of our
approach.","['Debanjana Kar', 'Mohit Bhardwaj', 'Suranjana Samanta', 'Amar Prakash Azad']",8,0.70259035
"Misinformation/disinformation about COVID-19 has been rampant on social media
around the world. In this study, we investigate COVID-19 misinformation/
disinformation on social media in multiple languages - Farsi (Persian),
Chinese, and English, about multiple countries - Iran, China, and the United
States (US), and on multiple platforms such as Twitter, Facebook, Instagram,
Weibo, and WhatsApp. Misinformation, especially about a global pandemic, is a
global problem yet it is common for studies of COVID-19 misinformation on
social media to focus on a single language, like English, a single country,
like the US, or a single platform, like Twitter. We utilized opportunistic
sampling to compile 200 specific items of viral and yet debunked misinformation
across these languages, countries and platforms emerged between January 1 and
August 31. We then categorized this collection based both on the topics of the
misinformation and the underlying roots of that misinformation. Our
multi-cultural and multilingual team observed that the nature of COVID-19
misinformation on social media varied in substantial ways across different
languages/countries depending on the cultures, beliefs/religions, popularity of
social media, types of platforms, freedom of speech and the power of people
versus governments. We observe that politics is at the root of most of the
collected misinformation across all three languages in this dataset. We further
observe the different impact of government restrictions on platforms and
platform restrictions on content in Iran, China, and the US and their impact on
a key question of our age: how do we control misinformation without silencing
the voices we need to hold governments accountable?","['Golshan Madraki', 'Isabella Grasso', 'Jacqueline Otala', 'Yu Liu', 'Jeanna Matthews']",5,0.68990254
"The digital spread of misinformation is one of the leading threats to
democracy, public health, and the global economy. Popular strategies for
mitigating misinformation include crowdsourcing, machine learning, and media
literacy programs that require social media users to classify news in binary
terms as either true or false. However, research on peer influence suggests
that framing decisions in binary terms can amplify judgment errors and limit
social learning, whereas framing decisions in probabilistic terms can reliably
improve judgments. In this preregistered experiment, we compare online peer
networks that collaboratively evaluate the veracity of news by communicating
either binary or probabilistic judgments. Exchanging probabilistic estimates of
news veracity substantially improved individual and group judgments, with the
effect of eliminating polarization in news evaluation. By contrast, exchanging
binary classifications reduced social learning and entrenched polarization. The
benefits of probabilistic social learning are robust to participants'
education, gender, race, income, religion, and partisanship.","['Douglas Guilbeault', 'Samuel Woolley', 'Joshua Becker']",3,0.67317605
"The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.","['Qingyu Chen', 'Robert Leaman', 'Alexis Allot', 'Ling Luo', 'Chih-Hsuan Wei', 'Shankai Yan', 'Zhiyong Lu']",5,0.7344731
"Although many fact-checking systems have been developed in academia and
industry, fake news is still proliferating on social media. These systems
mostly focus on fact-checking but usually neglect online users who are the main
drivers of the spread of misinformation. How can we use fact-checked
information to improve users' consciousness of fake news to which they are
exposed? How can we stop users from spreading fake news? To tackle these
questions, we propose a novel framework to search for fact-checking articles,
which address the content of an original tweet (that may contain
misinformation) posted by online users. The search can directly warn fake news
posters and online users (e.g. the posters' followers) about misinformation,
discourage them from spreading fake news, and scale up verified content on
social media. Our framework uses both text and images to search for
fact-checking articles, and achieves promising results on real-world datasets.
Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.","['Nguyen Vo', 'Kyumin Lee']",4,0.8220475
"Recently, the term ""fake news"" has been broadly and extensively utilized for
disinformation, misinformation, hoaxes, propaganda, satire, rumors, click-bait,
and junk news. It has become a serious problem around the world. We present a
new system, FaNDS, that detects fake news efficiently. The system is based on
several concepts used in some previous works but in a different context. There
are two main concepts: an Inconsistency Graph and Energy Flow. The
Inconsistency Graph contains news items as nodes and inconsistent opinions
between them for edges. Energy Flow assigns each node an initial energy and
then some energy is propagated along the edges until the energy distribution on
all nodes converges. To illustrate FaNDS we use the original data from the Fake
News Challenge (FNC-1). First, the data has to be reconstructed in order to
generate the Inconsistency Graph. The graph contains various subgraphs with
well-defined shapes that represent different types of connections between the
news items. Then the Energy Flow method is applied. The nodes with high energy
are the candidates for being fake news. In our experiments, all these were
indeed fake news as we checked each using several reliable web sites. We
compared FaNDS to several other fake news detection methods and found it to be
more sensitive in discovering fake news items.","['Jiawei Xu', 'Vladimir Zadorozhny', 'Danchen Zhang', 'John Grant']",4,0.7094367
"The COVID-19 pandemic has impacted on every human activity and, because of
the urgency of finding the proper responses to such an unprecedented emergency,
it generated a diffused societal debate. The online version of this discussion
was not exempted by the presence of d/misinformation campaigns, but differently
from what already witnessed in other debates, the COVID-19 -- intentional or
not -- flow of false information put at severe risk the public health, reducing
the effectiveness of governments' countermeasures. In the present manuscript,
we study the effective impact of misinformation in the Italian societal debate
on Twitter during the pandemic, focusing on the various discursive communities.
In order to extract the discursive communities, we focus on verified users,
i.e. accounts whose identity is officially certified by Twitter. We thus infer
the various discursive communities based on how verified users are perceived by
standard ones: if two verified accounts are considered as similar by non
unverified ones, we link them in the network of certified accounts. We first
observe that, beside being a mostly scientific subject, the COVID-19 discussion
show a clear division in what results to be different political groups. At this
point, by using a commonly available fact-checking software (NewsGuard), we
assess the reputation of the pieces of news exchanged. We filter the network of
retweets (i.e. users re-broadcasting the same elementary piece of information,
or tweet) from random noise and check the presence of messages displaying an
url. The impact of misinformation posts reaches the 22.1% in the right and
center-right wing community and its contribution is even stronger in absolute
numbers, due to the activity of this group: 96% of all non reputable urls
shared by political groups come from this community.","['Guido Caldarelli', 'Rocco de Nicola', 'Marinella Petrocchi', 'Manuel Pratelli', 'Fabio Saracco']",3,0.79358816
"We analyze the relationship between partisanship, echo chambers, and
vulnerability to online misinformation by studying news sharing behavior on
Twitter. While our results confirm prior findings that online misinformation
sharing is strongly correlated with right-leaning partisanship, we also uncover
a similar, though weaker trend among left-leaning users. Because of the
correlation between a user's partisanship and their position within a partisan
echo chamber, these types of influence are confounded. To disentangle their
effects, we perform a regression analysis and find that vulnerability to
misinformation is most strongly influenced by partisanship for both left- and
right-leaning users.","['Dimitar Nikolov', 'Alessandro Flammini', 'Filippo Menczer']",3,0.70412517
"The integrity of democratic political discourse is at the core to guarantee
free and fair elections. With social media often dictating the tones and trends
of politics-related discussion, it is of paramount important to be able to
study online chatter, especially in the run up to important voting events, like
in the case of the upcoming November 3, 2020 U.S. Presidential Election.
Limited access to social media data is often the first barrier to impede,
hinder, or slow down progress, and ultimately our understanding of online
political discourse. To mitigate this issue and try to empower the
Computational Social Science research community, we decided to publicly release
a massive-scale, longitudinal dataset of U.S. politics- and election-related
tweets. This multilingual dataset that we have been collecting for over one
year encompasses hundreds of millions of tweets and tracks all salient U.S.
politics trends, actors, and events between 2019 and 2020. It predates and
spans the whole period of Republican and Democratic primaries, with real-time
tracking of all presidential contenders of both sides of the isle. After that,
it focuses on presidential and vice-presidential candidates. Our dataset
release is curated, documented and will be constantly updated on a
weekly-basis, until the November 3, 2020 election and beyond. We hope that the
academic community, computational journalists, and research practitioners alike
will all take advantage of our dataset to study relevant scientific and social
issues, including problems like misinformation, information manipulation,
interference, and distortion of online political discourse that have been
prevalent in the context of recent election events in the United States and
worldwide.
  Our dataset is available at:
https://github.com/echen102/us-pres-elections-2020","['Emily Chen', 'Ashok Deb', 'Emilio Ferrara']",10,0.673646
"The growth of misinformation technology necessitates the need to identify
fake videos. One approach to preventing the consumption of these fake videos is
provenance which allows the user to authenticate media content to its original
source. This research designs and investigates the use of provenance indicators
to help users identify fake videos. We first interview users regarding their
experiences with different misinformation modes (text, image, video) to guide
the design of indicators within users' existing perspectives. Then, we conduct
a participatory design study to develop and design fake video indicators.
Finally, we evaluate participant-designed indicators via both expert
evaluations and quantitative surveys with a large group of end-users. Our
results provide concrete design guidelines for the emerging issue of fake
videos. Our findings also raise concerns regarding users' tendency to
overgeneralize from misinformation warning messages, suggesting the need for
further research on warning design in the ongoing fight against misinformation.","['Imani N. Sherman', 'Elissa M. Redmiles', 'Jack W. Stokes']",11,0.64631486
"In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.",['Gautam Kishore Shahi'],13,0.5801319
"Given a combinatorial optimization problem taking an input, can we learn a
strategy to solve it from the examples of input-solution pairs without knowing
its objective function? In this paper, we consider such a setting and study the
misinformation prevention problem. Given the examples of attacker-protector
pairs, our goal is to learn a strategy to compute protectors against future
attackers, without the need of knowing the underlying diffusion model. To this
end, we design a structured prediction framework, where the main idea is to
parameterize the scoring function using random features constructed through
distance functions on randomly sampled subgraphs, which leads to a kernelized
scoring function with weights learnable via the large margin method. Evidenced
by experiments, our method can produce near-optimal protectors without using
any information of the diffusion model, and it outperforms other possible
graph-based and learning-based methods by an evident margin.",['Guangmo Tong'],2,0.6965578
"The authors of fake news often use facts from verified news sources and mix
them with misinformation to create confusion and provoke unrest among the
readers. The spread of fake news can thereby have serious implications on our
society. They can sway political elections, push down the stock price or crush
reputations of corporations or public figures. Several websites have taken on
the mission of checking rumors and allegations, but are often not fast enough
to check the content of all the news being disseminated. Especially social
media websites have offered an easy platform for the fast propagation of
information. Towards limiting fake news from being propagated among social
media users, the task of this year's PAN 2020 challenge lays the focus on the
fake news spreaders. The aim of the task is to determine whether it is possible
to discriminate authors that have shared fake news in the past from those that
have never done it. In this notebook, we describe our profiling system for the
fake news detection task on Twitter. For this, we conduct different feature
extraction techniques and learning experiments from a multilingual perspective,
namely English and Spanish. Our final submitted systems use character n-grams
as features in combination with a linear SVM for English and Logistic
Regression for the Spanish language. Our submitted models achieve an overall
accuracy of 73% and 79% on the English and Spanish official test set,
respectively. Our experiments show that it is difficult to differentiate
solidly fake news spreaders on Twitter from users who share credible
information leaving room for further investigations. Our model ranked 3rd out
of 72 competitors.","['Inna Vogel', 'Meghana Meghana']",4,0.8429736
"False information spread via the internet and social media influences public
opinion and user activity, while generative models enable fake content to be
generated faster and more cheaply than had previously been possible. In the not
so distant future, identifying fake content generated by deep learning models
will play a key role in protecting users from misinformation. To this end, a
dataset containing human and computer-generated headlines was created and a
user study indicated that humans were only able to identify the fake headlines
in 47.8% of the cases. However, the most accurate automatic approach,
transformers, achieved an overall accuracy of 85.7%, indicating that content
generated from language models can be filtered out accurately.","['Antonis Maronikolakis', 'Hinrich Schutze', 'Mark Stevenson']",4,0.66650295
"In late 2019, the gravest pandemic in a century began spreading across the
world. A state of uncertainty related to what has become known as SARS-CoV-2
has since fueled conspiracy narratives on social media about the origin,
transmission and medical treatment of and vaccination against the resulting
disease, COVID-19. Using social media intelligence to monitor and understand
the proliferation of conspiracy narratives is one way to analyze the
distribution of misinformation on the pandemic. We analyzed more than 9.5M
German language tweets about COVID-19. The results show that only about 0.6% of
all those tweets deal with conspiracy theory narratives. We also found that the
political orientation of users correlates with the volume of content users
contribute to the dissemination of conspiracy narratives, implying that
partisan communicators have a higher motivation to take part in conspiratorial
discussions on Twitter. Finally, we showed that contrary to other studies,
automated accounts do not significantly influence the spread of misinformation
in the German speaking Twitter sphere. They only represent about 1.31% of all
conspiracy-related activities in our database.","['Morteza Shahrezaye', 'Miriam Meckel', 'L√©a Steinacker', 'Viktor Suter']",5,0.74448824
"Search and recommendation systems are ubiquitous and irreplaceable tools in
our daily lives. Despite their critical role in selecting and ranking the most
relevant information, they typically do not consider the veracity of
information presented to the user. In this paper, we introduce an audit
methodology to investigate the extent of misinformation presented in search
results and recommendations on online marketplaces. We investigate the factors
and personalization attributes that influence the amount of misinformation in
searches and recommendations. Recently, several media reports criticized Amazon
for hosting and recommending items that promote misinformation on topics such
as vaccines. Motivated by those reports, we apply our algorithmic auditing
methodology on Amazon to verify those claims. Our audit study investigates (a)
factors that might influence the search algorithms of Amazon and (b)
personalization attributes that contribute to amplifying the amount of
misinformation recommended to users in their search results and
recommendations. Our audit study collected ~526k search results and ~182k
homepage recommendations, with ~8.5k unique items. Each item is annotated for
its stance on vaccines' misinformation (pro, neutral, or anti). Our study
reveals that (1) the selection and ranking by the default Featured search
algorithm of search results that have misinformation stances are positively
correlated with the stance of search queries and customers' evaluation of items
(ratings and reviews), (2) misinformation stances of search results are neither
affected by users' activities nor by interacting (browsing, wish-listing,
shopping) with items that have a misinformation stance, and (3) a filter bubble
built-in users' homepages have a misinformation stance positively correlated
with the misinformation stance of items that a user interacts with.","['Eslam Hussein', 'Hoda Eldardiry']",0,0.62308437
"The suicide of Indian actor Sushant Singh Rajput in the midst of the COVID-19
lockdown triggered a media frenzy of prime time coverage that lasted several
months and became a political hot button issue. Using data from Twitter,
YouTube, and an archive of debunked misinformation stories, we found two
important patterns. First, that retweet rates on Twitter clearly suggest that
commentators benefited from talking about the case, which got higher engagement
than other topics. Second, that politicians, in particular, were instrumental
in changing the course of the discourse by referring to the case as 'murder',
rather than 'suicide'. In conclusion, we consider the effects of Rajput's
outsider status as a small-town implant in the film industry within the broader
narrative of systemic injustice, as well as the gendered aspects of mob justice
that have taken aim at his former partner in the months since.","['Syeda Zainab Akbar', 'Ankur Sharma', 'Himani Negi', 'Anmol Panda', 'Joyojeet Pal']",10,0.6374615
"Online social networks provide a platform for sharing information and free
expression. However, these networks are also used for malicious purposes, such
as distributing misinformation and hate speech, selling illegal drugs, and
coordinating sex trafficking or child exploitation. This paper surveys the
state of the art in keeping online platforms and their users safe from such
harm, also known as the problem of preserving integrity. This survey comes from
the perspective of having to combat a broad spectrum of integrity violations at
Facebook. We highlight the techniques that have been proven useful in practice
and that deserve additional attention from the academic community. Instead of
discussing the many individual violation types, we identify key aspects of the
social-media eco-system, each of which is common to a wide variety violation
types. Furthermore, each of these components represents an area for research
and development, and the innovations that are found can be applied widely.","['Alon Halevy', 'Cristian Canton Ferrer', 'Hao Ma', 'Umut Ozertem', 'Patrick Pantel', 'Marzieh Saeidi', 'Fabrizio Silvestri', 'Ves Stoyanov']",0,0.6893617
"The rise of the internet and computational power in recent years allowed for
the exponential growth of misinformation phenomena. An issue that was a
non-issue a decade ago, became a challenge for societal cohesion. The emergence
of this new threat has led many stakeholders, especially in Europe, to act in
order to tackle this phenomenon. This paper provides in its first part a
literature review on misinformation in Europe, and in its second part a
webometrics analysis on the identified key stakeholders. In the results we
discuss who those stakeholders are, what actions do they perform to limit
misinformation and whether those actions have an impact.","['Emmanouil Koulas', 'Marios Anthopoulos', 'Sotiria Grammenou', 'Christos Kaimakamis', 'Konstantinos Kousaris', 'Fotini-Rafailia Panavou', 'Orestis Piskioulis', 'Syed Iftikhar H. Shah', 'Vasilios Peristeras']",0,0.6973426
"From the moment the first COVID-19 vaccines are rolled out, there will need
to be a large fraction of the global population ready in line. It is therefore
crucial to start managing the growing global hesitancy to any such COVID-19
vaccine. The current approach of trying to convince the ""no""s cannot work
quickly enough, nor can the current policy of trying to find, remove and/or
rebut all the individual pieces of COVID and vaccine misinformation. Instead,
we show how this can be done in a simpler way by moving away from chasing
misinformation content and focusing instead on managing the ""yes--no--not-sure""
hesitancy ecosystem.","['N. F. Johnson', 'N. Velasquez', 'R. Leahy', 'N. Johnson Restrepo', 'O. Jha', 'Y. Lupu']",12,0.65259284
"Memes are graphics and text overlapped so that together they present concepts
that become dubious if one of them is absent. It is spread mostly on social
media platforms, in the form of jokes, sarcasm, motivating, etc. After the
success of BERT in Natural Language Processing (NLP), researchers inclined to
Visual-Linguistic (VL) multimodal problems like memes classification, image
captioning, Visual Question Answering (VQA), and many more. Unfortunately, many
memes get uploaded each day on social media platforms that need automatic
censoring to curb misinformation and hate. Recently, this issue has attracted
the attention of researchers and practitioners. State-of-the-art methods that
performed significantly on other VL dataset, tends to fail on memes
classification. In this context, this work aims to conduct a comprehensive
study on memes classification, generally on the VL multimodal problems and
cutting edge solutions. We propose a generalized framework for VL problems. We
cover the early and next-generation works on VL problems. Finally, we identify
and articulate several open research issues and challenges. This is the first
study that presents the generalized view of the advanced classification
techniques concerning memes classification to the best of our knowledge. We
believe this study presents a clear road-map for the Machine Learning (ML)
research community to implement and enhance memes classification techniques.","['Tariq Habib Afridi', 'Aftab Alam', 'Muhammad Numan Khan', 'Jawad Khan', 'Young-Koo Lee']",6,0.5579599
"The ongoing, fluid nature of the COVID-19 pandemic requires individuals to
regularly seek information about best health practices, local community
spreading, and public health guidelines. In the absence of a unified response
to the pandemic in the United States and clear, consistent directives from
federal and local officials, people have used social media to collectively
crowdsource COVID-19 elites, a small set of trusted COVID-19 information
sources. We take a census of COVID-19 crowdsourced elites in the United States
who have received sustained attention on Twitter during the pandemic. Using a
mixed methods approach with a panel of Twitter users linked to public U.S.
voter registration records, we find that journalists, media outlets, and
political accounts have been consistently amplified around COVID-19, while
epidemiologists, public health officials, and medical professionals make up
only a small portion of all COVID-19 elites on Twitter. We show that COVID-19
elites vary considerably across demographic groups, and that there are notable
racial, geographic, and political similarities and disparities between various
groups and the demographics of their elites. With this variation in mind, we
discuss the potential for using the disproportionate online voice of
crowdsourced COVID-19 elites to equitably promote timely public health
information and mitigate rampant misinformation.","['Ryan J. Gallagher', 'Larissa Doroshenko', 'Sarah Shugars', 'David Lazer', 'Brooke Foucault Welles']",5,0.75525737
"Narratives are fundamental to our perception of the world and are pervasive
in all activities that involve the representation of events in time. Yet,
modern online information systems do not incorporate narratives in their
representation of events occurring over time. This article aims to bridge this
gap, combining the theory of narrative representations with the data from
modern online systems. We make three key contributions: a theory-driven
computational representation of narratives, a novel extraction algorithm to
obtain these representations from data, and an evaluation of our approach. In
particular, given the effectiveness of visual metaphors, we employ a route map
metaphor to design a narrative map representation. The narrative map
representation illustrates the events and stories in the narrative as a series
of landmarks and routes on the map. Each element of our representation is
backed by a corresponding element from formal narrative theory, thus providing
a solid theoretical background to our method. Our approach extracts the
underlying graph structure of the narrative map using a novel optimization
technique focused on maximizing coherence while respecting structural and
coverage constraints. We showcase the effectiveness of our approach by
performing a user evaluation to assess the quality of the representation,
metaphor, and visualization. Evaluation results indicate that the Narrative Map
representation is a powerful method to communicate complex narratives to
individuals. Our findings have implications for intelligence analysts,
computational journalists, and misinformation researchers.","['Brian Keith', 'Tanushree Mitra']",2,0.56831074
"While misinformation and disinformation have been thriving in social media
for years, with the emergence of the COVID-19 pandemic, the political and the
health misinformation merged, thus elevating the problem to a whole new level
and giving rise to the first global infodemic. The fight against this infodemic
has many aspects, with fact-checking and debunking false and misleading claims
being among the most important ones. Unfortunately, manual fact-checking is
time-consuming and automatic fact-checking is resource-intense, which means
that we need to pre-filter the input social media posts and to throw out those
that do not appear to be check-worthy. With this in mind, here we propose a
model for detecting check-worthy tweets about COVID-19, which combines deep
contextualized text representations with modeling the social context of the
tweet. We further describe a number of additional experiments and comparisons,
which we believe should be useful for future research as they provide some
indication about what techniques are effective for the task. Our official
submission to the English version of CLEF-2020 CheckThat! Task 1, system
Team_Alex, was ranked second with a MAP score of 0.8034, which is almost tied
with the wining system, lagging behind by just 0.003 MAP points absolute.","['Alex Nikolov', 'Giovanni Da San Martino', 'Ivan Koychev', 'Preslav Nakov']",5,0.6881124
"In recent years, misinformation on the Web has become increasingly rampant.
The research community has responded by proposing systems and challenges, which
are beginning to be useful for (various subtasks of) detecting misinformation.
However, most proposed systems are based on deep learning techniques which are
fine-tuned to specific domains, are difficult to interpret and produce results
which are not machine readable. This limits their applicability and adoption as
they can only be used by a select expert audience in very specific settings. In
this paper we propose an architecture based on a core concept of Credibility
Reviews (CRs) that can be used to build networks of distributed bots that
collaborate for misinformation detection. The CRs serve as building blocks to
compose graphs of (i) web content, (ii) existing credibility signals
--fact-checked claims and reputation reviews of websites--, and (iii)
automatically computed reviews. We implement this architecture on top of
lightweight extensions to Schema.org and services providing generic NLP tasks
for semantic similarity and stance detection. Evaluations on existing datasets
of social-media posts, fake news and political speeches demonstrates several
advantages over existing systems: extensibility, domain-independence,
composability, explainability and transparency via provenance. Furthermore, we
obtain competitive results without requiring finetuning and establish a new
state of the art on the Clef'18 CheckThat! Factuality task.","['Ronald Denaux', 'Jose Manuel Gomez-Perez']",1,0.74558645
"The growing trend of sharing news/contents, through social media platforms
and the World Wide Web has been seen to impact our perception of the truth,
altering our views about politics, economics, relationships, needs and wants.
This is because of the growing spread of misinformation and disinformation
intentionally or unintentionally by individuals and organizations. This trend
has grave political, social, ethical, and privacy implications for society due
to 1) the rapid developments in the field of Machine Learning (ML) and Deep
Learning (DL) algorithms in creating realistic-looking yet fake digital content
(such as text, images, and videos), 2) the ability to customize the content
feeds and to create a polarized so-called ""filter-bubbles"" leveraging the
availability of the big-data. Therefore, there is an ethical need to combat the
flow of fake content. This paper attempts to resolve some of the aspects of
this combat by presenting a high-level overview of TRUSTD, a blockchain and
collective signature-based ecosystem to help content creators in getting their
content backed by the community, and to help users judge on the credibility and
correctness of these contents.","['Zakwan Jaroucheh', 'Mohamad Alissa', 'William J Buchanan']",0,0.6646168
"Misinformation about critical issues such as climate change and vaccine
safety is oftentimes amplified on online social and search platforms. The
crowdsourcing of content credibility assessment by laypeople has been proposed
as one strategy to combat misinformation by attempting to replicate the
assessments of experts at scale. In this work, we investigate news credibility
assessments by crowds versus experts to understand when and how ratings between
them differ. We gather a dataset of over 4,000 credibility assessments taken
from 2 crowd groups---journalism students and Upwork workers---as well as 2
expert groups---journalists and scientists---on a varied set of 50 news
articles related to climate science, a topic with widespread disconnect between
public opinion and expert consensus. Examining the ratings, we find differences
in performance due to the makeup of the crowd, such as rater demographics and
political leaning, as well as the scope of the tasks that the crowd is assigned
to rate, such as the genre of the article and partisanship of the publication.
Finally, we find differences between expert assessments due to differing expert
criteria that journalism versus science experts use---differences that may
contribute to crowd discrepancies, but that also suggest a way to reduce the
gap by designing crowd tasks tailored to specific expert criteria. From these
findings, we outline future research directions to better design crowd
processes that are tailored to specific crowds and types of content.","['Md Momen Bhuiyan', 'Amy X. Zhang', 'Connie Moon Sehat', 'Tanushree Mitra']",3,0.6796372
"Progress in generative modelling, especially generative adversarial networks,
have made it possible to efficiently synthesize and alter media at scale.
Malicious individuals now rely on these machine-generated media, or deepfakes,
to manipulate social discourse. In order to ensure media authenticity, existing
research is focused on deepfake detection. Yet, the adversarial nature of
frameworks used for generative modeling suggests that progress towards
detecting deepfakes will enable more realistic deepfake generation. Therefore,
it comes at no surprise that developers of generative models are under the
scrutiny of stakeholders dealing with misinformation campaigns. At the same
time, generative models have a lot of positive applications. As such, there is
a clear need to develop tools that ensure the transparent use of generative
modeling, while minimizing the harm caused by malicious applications.
  Our technique optimizes over the source of entropy of each generative model
to probabilistically attribute a deepfake to one of the models. We evaluate our
method on the seminal example of face synthesis, demonstrating that our
approach achieves 97.62% attribution accuracy, and is less sensitive to
perturbations and adversarial examples. We discuss the ethical implications of
our work, identify where our technique can be used, and highlight that a more
meaningful legislative framework is required for a more transparent and ethical
use of generative modeling. Finally, we argue that model developers should be
capable of claiming plausible deniability and propose a second framework to do
so -- this allows a model developer to produce evidence that they did not
produce media that they are being accused of having produced.","['Baiwu Zhang', 'Jin Peng Zhou', 'Ilia Shumailov', 'Nicolas Papernot']",7,0.76867485
"Many governments have managed to control their COVID-19 outbreak with a
simple message: keep the effective '$R$ number' $R<1$ to prevent widespread
contagion and flatten the curve. This raises the question whether a similar
policy could control dangerous online 'infodemics' of information,
misinformation and disinformation. Here we show, using multi-platform data from
the COVID-19 infodemic, that its online spreading instead encompasses a
different dynamical regime where communities and users within and across
independent platforms, sporadically form temporary active links on similar
timescales to the viral spreading. This allows material that might have died
out, to evolve and even mutate. This has enabled niche networks that were
already successfully spreading hate and anti-vaccination material, to rapidly
become global super-spreaders of narratives featuring fake COVID-19 treatments,
anti-Asian sentiment and conspiracy theories. We derive new tools that
incorporate these coupled social-viral dynamics, including an online $R$, to
help prevent infodemic spreading at all scales: from spreading across platforms
(e.g. Facebook, 4Chan) to spreading within a given subpopulation, or community,
or topic. By accounting for similar social and viral timescales, the same
mathematical theory also offers a quantitative description of other
unconventional infection profiles such as rumors spreading in financial markets
and colds spreading in schools.","['N. F. Johnson', 'N. Velasquez', 'O. K. Jha', 'H. Niyazi', 'R. Leahy', 'N. Johnson Restrepo', 'R. Sear', 'P. Manrique', 'Y. Lupu', 'P. Devkota', 'S. Wuchty']",5,0.7112577
"Recently, news consumption using online news portals has increased
exponentially due to several reasons, such as low cost and easy accessibility.
However, such online platforms inadvertently also become the cause of spreading
false information across the web. They are being misused quite frequently as a
medium to disseminate misinformation and hoaxes. Such malpractices call for a
robust automatic fake news detection system that can keep us at bay from such
misinformation and hoaxes. We propose a robust yet simple fake news detection
system, leveraging the tools for paraphrasing, grammar-checking, and
word-embedding. In this paper, we try to the potential of these tools in
jointly unearthing the authenticity of a news article. Notably, we leverage
Spinbot (for paraphrasing), Grammarly (for grammar-checking), and GloVe (for
word-embedding) tools for this purpose. Using these tools, we were able to
extract novel features that could yield state-of-the-art results on the Fake
News AMT dataset and comparable results on Celebrity datasets when combined
with some of the essential features. More importantly, the proposed method is
found to be more robust empirically than the existing ones, as revealed in our
cross-domain analysis and multi-domain analysis.","['Akansha Gautam', 'Koteswar Rao Jerripothula']",4,0.83521503
"Ten years ago, DARPA launched the 'Network Challenge', more commonly known as
the 'DARPA Red Balloon Challenge'. Ten red weather balloons were fixed at
unknown locations in the US. An open challenge was launched to locate all ten,
the first to do so would be declared the winner receiving a cash prize. A team
from MIT Media Lab was able to locate them all within 9 hours using social
media and a novel reward scheme that rewarded viral recruitment. This
achievement was rightly seen as proof of the remarkable ability of social
media, then relatively nascent, to solve real world problems such as
large-scale spatial search. Upon reflection, however, the challenge was also
remarkable as it succeeded despite many efforts to provide false information on
the location of the balloons. At the time the false reports were filtered based
on manual inspection of visual proof and comparing the IP addresses of those
reporting with the purported coordinates of the balloons. In the ten years
since, misinformation on social media has grown in prevalence and
sophistication to be one of the defining social issues of our time. Seen
differently we can cast the misinformation observed in the Red Balloon
Challenge, and unexpected adverse effects in other social mobilisation
challenges subsequently, not as bugs but as essential features. We further
investigate the role of the increasing levels of political polarisation in
modulating social mobilisation. We confirm that polarisation not only impedes
the overall success of mobilisation, but also leads to a low reachability to
oppositely polarised states, significantly hampering recruitment. We find that
diversifying geographic pathways of social influence are key to circumvent
barriers of political mobilisation and can boost the success of new open
challenges.","['Alex Rutherford', 'Manuel Cebrian', 'Inho Hong', 'Iyad Rahwan']",5,0.56113297
"Misinformation is an ever increasing problem that is difficult to solve for
the research community and has a negative impact on the society at large. Very
recently, the problem has been addressed with a crowdsourcing-based approach to
scale up labeling efforts: to assess the truthfulness of a statement, instead
of relying on a few experts, a crowd of (non-expert) judges is exploited. We
follow the same approach to study whether crowdsourcing is an effective and
reliable method to assess statements truthfulness during a pandemic. We
specifically target statements related to the COVID-19 health emergency, that
is still ongoing at the time of the study and has arguably caused an increase
of the amount of misinformation that is spreading online (a phenomenon for
which the term ""infodemic"" has been used). By doing so, we are able to address
(mis)information that is both related to a sensitive and personal issue like
health and very recent as compared to when the judgment is done: two issues
that have not been analyzed in related work. In our experiment, crowd workers
are asked to assess the truthfulness of statements, as well as to provide
evidence for the assessments as a URL and a text justification. Besides showing
that the crowd is able to accurately judge the truthfulness of the statements,
we also report results on many different aspects, including: agreement among
workers, the effect of different aggregation functions, of scales
transformations, and of workers background / bias. We also analyze workers
behavior, in terms of queries submitted, URLs found / selected, text
justifications, and other behavioral data like clicks and mouse actions
collected by means of an ad hoc logger.","['Kevin Roitero', 'Michael Soprano', 'Beatrice Portelli', 'Damiano Spina', 'Vincenzo Della Mea', 'Giuseppe Serra', 'Stefano Mizzaro', 'Gianluca Demartini']",0,0.7563458
"Health misinformation has been found to be prevalent on social media,
particularly in new public health crises in which there is limited scientific
information. However, social media can also play a role in limiting and
refuting health misinformation. Using as a case study US President Donald
Trump's controversial comments about the promise and power of UV light- and
disinfectant-based treatments, this data memo examines how these comments were
discussed and responded to on Twitter. We find that these comments fell into
established politically partisan narratives and dominated discussion of both
politics and COVID in the days following. Contestation of the comments was much
more prevalent than support. Supporters attacked media coverage in line with
existing Trump narratives. Contesters responded with humour and shared
mainstream media coverage condemning the comments. These practices would have
strengthened the original misinformation through repetition and done little to
construct a successful refutation for those who might have believed them. This
research adds much-needed knowledge to our understanding of the information
environment surrounding COVID and demonstrates that, despite calls for the
depoliticization of health information in this public health crisis, this is
largely being approached as a political issue along divisive, polarised,
partisan lines.","['Gillian Bolsover', 'Janet Tokitsu Tizon']",5,0.67681456
"Bots are user accounts in social media which are controlled by computer
programs. Similar to many other things, they are used for both good and evil
purposes. One nefarious use-case for them is to spread misinformation or biased
data in the networks. There are many pieces of research being performed based
on social media data and their results validity is extremely threatened by the
harmful data bots spread. Consequently, effective methods and tools are
required for detecting bots and then removing misleading data spread by the
bots. In the present research, a method for detecting Instagram bots is
proposed. There is no data set including samples of Instagram bots and genuine
accounts, thus the current research has begun with gathering such a data set
with respect to generality concerns such that it includes 1,000 data points in
each group. The main approach is supervised machine learning and classic models
are preferred compared to deep neural networks. The final model is evaluated
using multiple methods starting with 10-fold cross-validation. After that,
confidence in classification studies and is followed by feature importance
analysis and feature behavior against the target probability computed by the
model. In the end, an experiment is designed to measure the models
effectiveness in an operational environment. Finally, It is strongly concluded
that the model performs very well in all evaluation experiments.","['Muhammad Bazm', 'Masoud Asadpour']",13,0.7956822
"The COVID-19 pandemic has reshaped the demand for goods and services
worldwide. The combination of a public health emergency, economic distress, and
misinformation-driven panic have pushed customers and vendors towards the
shadow economy. In particular, dark web marketplaces (DWMs), commercial
websites accessible via free software, have gained significant popularity.
Here, we analyse 851,199 listings extracted from 30 DWMs between January 1,
2020 and November 16, 2020. We identify 788 listings directly related to
COVID-19 products and monitor the temporal evolution of product categories
including Personal Protective Equipment (PPE), medicines (e.g.,
hydroxyclorochine), and medical frauds. Finally, we compare trends in their
temporal evolution with variations in public attention, as measured by Twitter
posts and Wikipedia page visits. We reveal how the online shadow economy has
evolved during the COVID-19 pandemic and highlight the importance of a
continuous monitoring of DWMs, especially now that real vaccines are available
and in short supply. We anticipate our analysis will be of interest both to
researchers and public agencies focused on the protection of public health.","['Alberto Bracci', 'Matthieu Nadini', 'Maxwell Aliapoulios', 'Damon McCoy', 'Ian Gray', 'Alexander Teytelboym', 'Angela Gallo', 'Andrea Baronchelli']",5,0.64294195
"Social media platforms have been exploited to conduct election interference
in recent years. In particular, the Russian-backed Internet Research Agency
(IRA) has been identified as a key source of misinformation spread on Twitter
prior to the 2016 U.S. presidential election. The goal of this research is to
understand whether general Twitter users changed their behavior in the year
following first contact from an IRA account. We compare the before and after
behavior of contacted users to determine whether there were differences in
their mean tweet count, the sentiment of their tweets, and the frequency and
sentiment of tweets mentioning @realDonaldTrump or @HillaryClinton. Our results
indicate that users overall exhibited statistically significant changes in
behavior across most of these metrics, and that those users that engaged with
the IRA generally showed greater changes in behavior.","['Upasana Dutta', 'Rhett Hanscom', 'Jason Shuo Zhang', 'Richard Han', 'Tamara Lehman', 'Qin Lv', 'Shivakant Mishra']",10,0.7707511
"From conspiracy theories to fake cures and fake treatments, COVID-19 has
become a hot-bed for the spread of misinformation online. It is more important
than ever to identify methods to debunk and correct false information online.
In this paper, we present a methodology and analyses to characterize the two
competing COVID-19 misinformation communities online: (i) misinformed users or
users who are actively posting misinformation, and (ii) informed users or users
who are actively spreading true information, or calling out misinformation. The
goals of this study are two-fold: (i) collecting a diverse set of annotated
COVID-19 Twitter dataset that can be used by the research community to conduct
meaningful analysis; and (ii) characterizing the two target communities in
terms of their network structure, linguistic patterns, and their membership in
other communities. Our analyses show that COVID-19 misinformed communities are
denser, and more organized than informed communities, with a possibility of a
high volume of the misinformation being part of disinformation campaigns. Our
analyses also suggest that a large majority of misinformed users may be
anti-vaxxers. Finally, our sociolinguistic analyses suggest that COVID-19
informed users tend to use more narratives than misinformed users.","['Shahan Ali Memon', 'Kathleen M. Carley']",5,0.7648885
"The COVID-19 pandemic has been the subject of a vast amount of
misinformation, particularly in digital information environments, and major
social media platforms recently publicized some of the countermeasures they are
adopting. This presents an opportunity to examine the nature of the
misinformation and disinformation being produced, and the theoretical and
technological paradigm used to counter it. I argue that this approach is based
on a conception of misinformation as epistemic pollution that can only justify
a limited and potentially inadequate response , and that some of the measures
undertaken in practice outrun this. In fact, social networks manage ecological
and architectural conditions that influence discourse on their platforms in
ways that should motivate reconsideration of the justifications that ground
epistemic interventions to combat misinformation, and the types of intervention
that they warrant. The editorial role of platforms should not be framed solely
as the management of epistemic pollution, but instead as managing the epistemic
environment in which narratives and social epistemic processes take place.
There is an element of inevitable epistemic paternalism involved in this, and
exploration of the independent constraints on its justifiability can help
determine proper limits of its exercise in practice.",['Andrew Buzzell'],0,0.630357
"The rapid dissemination of health misinformation poses an increasing risk to
public health. To best understand the way of combating health misinformation,
it is important to acknowledge how the fundamental characteristics of
misinformation differ from domain to domain. This paper presents a pathway
towards domain-specific characterization of misinformation so that we can
address the concealed behavior of health misinformation compared to others and
take proper initiative accordingly for combating it. With this aim, we have
mentioned several possible approaches to identify discriminating features of
medical misinformation from other types of misinformation. Thereafter, we
briefly propose a research plan followed by possible challenges to meet up. The
findings of the proposed research idea will provide new directions to the
misinformation research community.","['Fariha Afsana', 'Muhammad Ashad Kabir', 'Naeemul Hassan', 'Manoranjan Paul']",0,0.6882255
"There has been a growing interest within CSCW community in understanding the
characteristics of misinformation propagated through computational media, and
the devising techniques to address the associated challenges. However, most
work in this area has been concentrated on the cases in the western world
leaving a major portion of this problem unaddressed that is situated in the
Global South. This paper aims to broaden the scope of this discourse by
focusing on this problem in the context of Bangladesh, a country in the Global
South. The spread of misinformation on Facebook in Bangladesh, a country with a
population over 163 million, has resulted in chaos, hate attacks, and killings.
By interviewing journalists, fact-checkers, in addition to surveying the
general public, we analyzed the current state of verifying misinformation in
Bangladesh. Our findings show that most people in the `news audience' want the
news media to verify the authenticity of online information that they see
online. However, the newspaper journalists say that fact-checking online
information is not a part of their job, and it is also beyond their capacity
given the amount of information being published online everyday. We further
find that the voluntary fact-checkers in Bangladesh are not equipped with
sufficient infrastructural support to fill in this gap. We show how our
findings are connected to some of the core concerns of CSCW community around
social media, collaboration, infrastructural politics, and information
inequality. From our analysis, we also suggest several pathways to increase the
impact of fact-checking efforts through collaboration, technology design, and
infrastructure development.","['Md Mahfuzul Haque', 'Mohammad Yousuf', 'Ahmed Shatil Alam', 'Pratyasha Saha', 'Syed Ishtiaque Ahmed', 'Naeemul Hassan']",5,0.64922345
"Combating fake news and misinformation propagation is a challenging task in
the post-truth era. News feed and search algorithms could potentially lead to
unintentional large-scale propagation of false and fabricated information with
users being exposed to algorithmically selected false content. Our research
investigates the effects of an Explainable AI assistant embedded in news review
platforms for combating the propagation of fake news. We design a news
reviewing and sharing interface, create a dataset of news stories, and train
four interpretable fake news detection algorithms to study the effects of
algorithmic transparency on end-users. We present evaluation results and
analysis from multiple controlled crowdsourced studies. For a deeper
understanding of Explainable AI systems, we discuss interactions between user
engagement, mental model, trust, and performance measures in the process of
explaining. The study results indicate that explanations helped participants to
build appropriate mental models of the intelligent assistants in different
conditions and adjust their trust accordingly for model limitations.","['Sina Mohseni', 'Fan Yang', 'Shiva Pentyala', 'Mengnan Du', 'Yi Liu', 'Nic Lupfer', 'Xia Hu', 'Shuiwang Ji', 'Eric Ragan']",4,0.801039
"A primary indicator of success in the fight against COVID-19 is avoiding
stress on critical care infrastructure and services (CCIS). However, CCIS will
likely remain stressed until sustained herd immunity is built. There are also
secondary considerations for success: mitigating economic damage; curbing the
spread of misinformation, improving morale, and preserving a sense of control;
building global trust for diplomacy, trade and travel; and restoring
reliability and normalcy to day-to-day life, among others. We envision
technology plays a pivotal role. Here, we focus on the effective use of readily
available technology to improve the primary and secondary success criteria for
the fight against SARS-CoV-2. In a multifaceted technology approach, we start
with effective technology use for remote patient monitoring (RPM) of COVID-19
with the following objectives:
  1. Deploy readily available technology for continuous real-time remote
monitoring of patient vitals with the help of biosensors on a large scale.
  2. Effective and safe remote large-scale communitywide care of low-severity
cases as a buffer against surges in COVID-19 hospitalizations to reduce strain
on critical care services and emergency hospitals.
  3. Improve the patient, their family, and their community's sense of control
and morale.
  4. Propose a clear technology and medical definition of remote patient
monitoring for COVID-19 to address an urgent technology need; address
obfuscated, narrow, and erroneous information and provide examples; and urge
publishers to be clear and complete in their disclosures.
  5. Leverage the cloud-based distributed cognitive RPM platform for community
leaders and decision makers to enable planning and resource management,
pandemic research, damage prevention and containment, and receiving feedback on
strategies and executions.","['Ashlesha Nesarikar', 'Waqas Haque', 'Suchith Vuppala', 'Abhijit Nesarikar']",9,0.590871
"Research on infodemics, i.e., the rapid spread of (mis)information related to
a hazardous event, such as the COVID-19 pandemic, requires the integration of a
multiplicity of scientific disciplines. The dynamics emerging from infodemics
have the potential to generate complex behavioral patterns. In order to react
appropriately, it is of ultimate importance for the fields of Business and
Economics to understand the dynamics emerging from it. In the short run,
dynamics might lead to an adaptation in household spending or to a shift in
buying behavior towards online providers. In the long run, changes in
investments, consumer behavior, and markets are to be expected. We argue that
the dynamics emerge from complex interactions among multiple factors, such as
information and misinformation accessible for individuals and the formation and
revision of beliefs. (Mis)information accessible to individuals is, amongst
others, affected by algorithms specifically designed to provide personalized
information, while automated fact-checking algorithms can help reduce the
amount of circulating misinformation. The formation and revision of individual
(and probably false) beliefs and individual fact-checking and interpretation of
information are heavily affected by linguistic patterns inherent to information
during pandemics and infodemics and further factors, such as affect, intuition
and motives. We argue that, in order to get a deep(er) understanding of the
dynamics emerging from infodemics, the fields of Business and Economics should
integrate the perspectives of Computer Science and Information Systems,
(Computational) Linguistics, and Cognitive Science into the wider context of
economic systems (e.g., organizations, markets or industries) and propose a way
to do so.","['Stephan Leitner', 'Bartosz Gula', 'Dietmar Jannach', 'Ulrike Krieg-Holz', 'Friederike Wall']",0,0.7482636
"In this work we carry out an exploratory analysis of online conversations on
the Italian Facebook during the recent COVID-19 pandemic. We analyze the
circulation of controversial topics associated with the origin of the virus,
which involve popular targets of misinformation, such as migrants and 5G
technology. We collected over 1.5 M posts in Italian language and related to
COVID-19, shared by nearly 80k public pages and groups for a period of four
months since January 2020. Overall, we find that potentially harmful content
shared by unreliable sources is substantially negligible compared to
traditional news websites, and that discussions over controversial topics has a
limited engagement w.r.t to the pandemic in general. Besides, we highlight a
""small-worldness"" effect in the URL sharing diffusion network, indicating that
users navigating through a limited set of pages could reach almost the entire
pool of shared content related to the pandemic, thus being easily exposed to
harmful propaganda as well as to verified information on the virus.","['Alessandro Celestini', 'Marco Di Giovanni', 'Stefano Guarino', 'Francesco Pierri']",5,0.6997532
"One of the defining moments of the year 2020 is the outbreak of Coronavirus
Disease (Covid-19), a deadly virus affecting the body's respiratory system to
the point of needing a breathing aid via ventilators. As of June 21, 2020 there
are 12,929,306 confirmed cases and 569,738 confirmed deaths across 216
countries, areas or territories. The scale of spread and impact of the pandemic
left many nations grappling with preventive and curative approaches. The
infamous lockdown measure introduced to mitigate the virus spread has altered
many aspects of our social routines in which demand for online-based services
skyrocketed. As the virus propagate, so does misinformation and fake news
around it via online social media, which seems to favour virality over
veracity. With a majority of the populace confined to their homes for a long
period, vulnerability to the toxic impact of online misinformation is high. A
case in point is the various myths and disinformation associated with the
Covid-19, which, if left unchecked, could lead to a catastrophic outcome and
hamper the fight against the virus.
  While the scientific community is actively engaged in identifying the virus
treatment, there is a growing interest in combating the associated harmful
infodemic. To this end, researchers have been curating and documenting various
datasets about Covid-19. In line with existing studies, we provide an expansive
collection of curated datasets to support the fight against the pandemic,
especially concerning misinformation. The collection consists of 3 categories
of Twitter data, information about standard practices from credible sources and
a chronicle of global situation reports. We describe how to retrieve the
hydrated version of the data and proffer some research problems that could be
addressed using the data.","['Isa Inuwa-Dutse', 'Ioannis Korkontzelos']",5,0.7987701
"On February 2, 2020, the World Health Organization declared a COVID-19 social
media ""infodemic"", with special attention to misinformation -- frequently
understood as false claims. To understand the infodemic's scope and scale, we
analyzed over 500 million posts from Twitter and Facebook about COVID-19 and
other health topics, between March 8 and May 1, 2020. Following prior work, we
assumed URL source credibility is a proxy for false content, but we also tested
this assumption. Contrary to expectations, we found that messages about
COVID-19 were more likely to contain links to more credible sources.
Additionally, messages linking to government sources, and to news with
intermediate credibility, were shared more often, on average, than links to
non-credible sources. These results suggest that more ambiguous forms of
misinformation about COVID-19 may be more likely to be disseminated through
credible sources when compared to other health topics. Furthermore, the
assumption that credibility is an adequate proxy for false content may
overestimate the prevalence of false content online: less than 25% of posts
linking to the least credible sources contained false content. Our results
emphasize the importance of distinguishing between explicit falsehoods and more
ambiguous forms of misinformation due to the search for meaning in an
environment of scientific uncertainty.","['David A. Broniatowski', 'Daniel Kerchner', 'Fouzia Farooq', 'Xiaolei Huang', 'Amelia M. Jamison', 'Mark Dredze', 'Sandra Crouse Quinn']",5,0.68436
"Photorealistic image generation has reached a new level of quality due to the
breakthroughs of generative adversarial networks (GANs). Yet, the dark side of
such deepfakes, the malicious use of generated media, raises concerns about
visual misinformation. While existing research work on deepfake detection
demonstrates high accuracy, it is subject to advances in generation techniques
and adversarial iterations on detection countermeasure techniques. Thus, we
seek a proactive and sustainable solution on deepfake detection, that is
agnostic to the evolution of generative models, by introducing artificial
fingerprints into the models.
  Our approach is simple and effective. We first embed artificial fingerprints
into training data, then validate a surprising discovery on the transferability
of such fingerprints from training data to generative models, which in turn
appears in the generated deepfakes. Experiments show that our fingerprinting
solution (1) holds for a variety of cutting-edge generative models, (2) leads
to a negligible side effect on generation quality, (3) stays robust against
image-level and model-level perturbations, (4) stays hard to be detected by
adversaries, and (5) converts deepfake detection and attribution into trivial
tasks and outperforms the recent state-of-the-art baselines. Our solution
closes the responsibility loop between publishing pre-trained generative model
inventions and their possible misuses, which makes it independent of the
current arms race. Code and models are available at
https://github.com/ningyu1991/ArtificialGANFingerprints .","['Ning Yu', 'Vladislav Skripniuk', 'Sahar Abdelnabi', 'Mario Fritz']",7,0.85531044
"Newsfeed algorithms frequently amplify misinformation and other low-quality
content. How can social media platforms more effectively promote reliable
information? Existing approaches are difficult to scale and vulnerable to
manipulation. In this paper, we propose using the political diversity of a
website's audience as a quality signal. Using news source reliability ratings
from domain experts and web browsing data from a diverse sample of 6,890 U.S.
citizens, we first show that websites with more extreme and less politically
diverse audiences have lower journalistic standards. We then incorporate
audience diversity into a standard collaborative filtering framework and show
that our improved algorithm increases the trustworthiness of websites suggested
to users -- especially those who most frequently consume misinformation --
while keeping recommendations relevant. These findings suggest that partisan
audience diversity is a valuable signal of higher journalistic standards that
should be incorporated into algorithmic ranking decisions.","['Saumya Bhadani', 'Shun Yamaya', 'Alessandro Flammini', 'Filippo Menczer', 'Giovanni Luca Ciampaglia', 'Brendan Nyhan']",4,0.7039332
"Propaganda campaigns aim at influencing people's mindset with the purpose of
advancing a specific agenda. They exploit the anonymity of the Internet, the
micro-profiling ability of social networks, and the ease of automatically
creating and managing coordinated networks of accounts, to reach millions of
social network users with persuasive messages, specifically targeted to topics
each individual user is sensitive to, and ultimately influencing the outcome on
a targeted issue. In this survey, we review the state of the art on
computational propaganda detection from the perspective of Natural Language
Processing and Network Analysis, arguing about the need for combined efforts
between these communities. We further discuss current challenges and future
research directions.","['Giovanni Da San Martino', 'Stefano Cresci', 'Alberto Barron-Cedeno', 'Seunghak Yu', 'Roberto Di Pietro', 'Preslav Nakov']",0,0.6747303
"With the outbreak of the COVID-19 pandemic, people turned to social media to
read and to share timely information including statistics, warnings, advice,
and inspirational stories. Unfortunately, alongside all this useful
information, there was also a new blending of medical and political
misinformation and disinformation, which gave rise to the first global
infodemic. While fighting this infodemic is typically thought of in terms of
factuality, the problem is much broader as malicious content includes not only
fake news, rumors, and conspiracy theories, but also promotion of fake cures,
panic, racism, xenophobia, and mistrust in the authorities, among others. This
is a complex problem that needs a holistic approach combining the perspectives
of journalists, fact-checkers, policymakers, government entities, social media
platforms, and society as a whole. Taking them into account we define an
annotation schema and detailed annotation instructions, which reflect these
perspectives. We performed initial annotations using this schema, and our
initial experiments demonstrated sizable improvements over the baselines. Now,
we issue a call to arms to the research community and beyond to join the fight
by supporting our crowdsourcing annotation efforts.","['Firoj Alam', 'Fahim Dalvi', 'Shaden Shaar', 'Nadir Durrani', 'Hamdy Mubarak', 'Alex Nikolov', 'Giovanni Da San Martino', 'Ahmed Abdelali', 'Hassan Sajjad', 'Kareem Darwish', 'Preslav Nakov']",5,0.74980104
"Social media platforms have been extensively used during natural disasters.
However, most prior work has lacked focus on studying their usage during
disasters in the Global South, where Internet access and social media
utilization differs from developing countries. In this paper, we study how
social media was used in the aftermath of the 7.1-magnitude earthquake that hit
Mexico on September 19 of 2017 (known as the #19S earthquake). We conduct an
analysis of how participants utilized social media platforms in the #19S
aftermath. Our research extends investigations of crisis informatics by: 1)
examining how participants used different social media platforms in the
aftermath of a natural disaster in a Global South country; 2) uncovering how
individuals developed their own processes to verify news reports using an
on-the-ground citizen approach; 3) revealing how people developed their own
mechanisms to deal with outdated information. For this, we surveyed 356 people.
Additionally, we analyze one month of activity from: Facebook (12,606 posts),
Twitter (2,909,109 tweets), Slack (28,782 messages), and GitHub (2,602
commits). This work offers a multi-platform view on user behavior to coordinate
relief efforts, reduce the spread of misinformation and deal with obsolete
information which seems to have been essential to help in the coordination and
efficiency of relief efforts. Finally, based on our findings, we make
recommendations for technology design to improve the effectiveness of social
media use during crisis response efforts and mitigate the spread of
misinformation across social media platforms.","['Claudia Flores-Saviaga', 'Saiph Savage']",5,0.69137454
"How can the birth and evolution of ideas and communities in a network be
studied over time? We use a multimodal pipeline, consisting of network mapping,
topic modeling, bridging centrality, and divergence to analyze Twitter data
surrounding the COVID-19 pandemic. We use network mapping to detect accounts
creating content surrounding COVID-19, then Latent Dirichlet Allocation to
extract topics, and bridging centrality to identify topical and non-topical
bridges, before examining the distribution of each topic and bridge over time
and applying Jensen-Shannon divergence of topic distributions to show
communities that are converging in their topical narratives.","['Liz McQuillan', 'Erin McAweeney', 'Alicia Bargar', 'Alex Ruch']",2,0.66222346
"Query autocompletions help users of search engines to speed up their searches
by recommending completions of partially typed queries in a drop down box.
These recommended query autocompletions are usually based on large logs of
queries that were previously entered by the search engine's users. Therefore,
misinformation entered -- either accidentally or purposely to manipulate the
search engine -- might end up in the search engine's recommendations,
potentially harming organizations, individuals, and groups of people. This
paper proposes an alternative approach for generating query autocompletions by
extracting anchor texts from a large web crawl, without the need to use query
logs. Our evaluation shows that even though query log autocompletions perform
better for shorter queries, anchor text autocompletions outperform query log
autocompletions for queries of 2 words or more.",['Djoerd Hiemstra'],1,0.5092125
"The role of statisticians in society is to provide tools, techniques, and
guidance with regards to how much to trust data. This role is increasingly more
important with more data and more misinformation than ever before. The American
Statistical Association recently released two statements on p-values, and
provided four guiding principles. We evaluate their claims using these
principles and find that they failed to adhere to them. In this age of
distrust, we have an opportunity to be role models of trustworthiness, and
responsibility to take it.",['Joshua T. Vogelstein'],0,0.5116644
"Social and technical trends have significantly changed methods for evaluating
and disseminating computing research. Traditional venues for reviewing and
publishing, such as conferences and journals, worked effectively in the past.
Recently, trends have created new opportunities but also put new pressures on
the process of review and dissemination. For example, many conferences have
seen large increases in the number of submissions. Likewise, dissemination of
research ideas has become dramatically through publication venues such as
arXiv.org and social media networks. While these trends predate COVID-19, the
pandemic could accelerate longer term changes. Based on interviews with leading
academics in computing research, our findings include: (1) Trends impacting
computing research are largely positive and have increased the participation,
scope, accessibility, and speed of the research process. (2) Challenges remain
in securing the integrity of the process, including addressing ways to scale
the review process, avoiding attempts to misinform or confuse the dissemination
of results, and ensuring fairness and broad participation in the process
itself. Based on these findings, we recommend: (1) Regularly polling members of
the computing research community, including program and general conference
chairs, journal editors, authors, reviewers, etc., to identify specific
challenges they face to better understand these issues. (2) An influential
body, such as the Computing Research Association regularly issues a ""State of
the Computing Research Enterprise"" report to update the community on trends,
both positive and negative, impacting the computing research enterprise. (3) A
deeper investigation, specifically to better understand the influence that
social media and preprint archives have on computing research, is conducted.","['Benjamin Zorn', 'Tom Conte', 'Keith Marzullo', 'Suresh Venkatasubramanian']",0,0.70249474
"Due to its impact, COVID-19 has been stressing the academy to search for
curing, mitigating, or controlling it. However, when it comes to controlling,
there are still few studies focused on under-reporting estimates. It is
believed that under-reporting is a relevant factor in determining the actual
mortality rate and, if not considered, can cause significant misinformation.
Therefore, the objective of this work is to estimate the under-reporting of
cases and deaths of COVID-19 in Brazilian states using data from the Infogripe
on notification of Severe Acute Respiratory Infection (SARI). The methodology
is based on the concepts of inertia and the use of event detection techniques
to study the time series of hospitalized SARI cases. The estimate of real cases
of the disease, called novelty, is calculated by comparing the difference in
SARI cases in 2020 (after COVID-19) with the total expected cases in recent
years (2016 to 2019) derived from a seasonal exponential moving average. The
results show that under-reporting rates vary significantly between states and
that there are no general patterns for states in the same region in Brazil.
  The published version of this paper is made available at
https://doi.org/10.1007/s00354-021-00125-3.
  Please cite as: B. Paix\~ao, L. Baroni, M. Pedroso, R. Salles, L. Escobar, C.
de Sousa, R. de Freitas Saldanha, J. Soares, R. Coutinho, et al., 2021,
Estimation of COVID-19 Under-Reporting in the Brazilian States Through SARI,
New Generation Computing","['Balthazar Paix√£o', 'Lais Baroni', 'Rebecca Salles', 'Luciana Escobar', 'Carlos de Sousa', 'Marcel Pedroso', 'Raphael Saldanha', 'Rafaelli Coutinho', 'Fabio Porto', 'Eduardo Ogasawara']",5,0.5610541
"Due to the convenience of access-on-demand to information and business
solutions, mobile apps have become an important asset in the digital world. In
the context of the Covid-19 pandemic, app developers have joined the response
effort in various ways by releasing apps that target different user bases
(e.g., all citizens or journalists), offer different services (e.g., location
tracking or diagnostic-aid), provide generic or specialized information, etc.
While many apps have raised some concerns by spreading misinformation or even
malware, the literature does not yet provide a clear landscape of the different
apps that were developed. In this study, we focus on the Android ecosystem and
investigate Covid-related Android apps. In a best-effort scenario, we attempt
to systematically identify all relevant apps and study their characteristics
with the objective to provide a First taxonomy of Covid-related apps,
broadening the relevance beyond the implementation of contact tracing. Overall,
our study yields a number of empirical insights that contribute to enlarge the
knowledge on Covid-related apps: (1) Developer communities contributed rapidly
to the Covid-19, with dedicated apps released as early as January 2020; (2)
Covid-related apps deliver digital tools to users (e.g., health diaries), serve
to broadcast information to users (e.g., spread statistics), and collect data
from users (e.g., for tracing); (3) Covid-related apps are less complex than
standard apps; (4) they generally do not seem to leak sensitive data; (5) in
the majority of cases, Covid-related apps are released by entities with past
experience on the market, mostly official government entities or public health
organizations.","['Jordan Samhi', 'Kevin Allix', 'Tegawend√© F. Bissyand√©', 'Jacques Klein']",3,0.5221479
"It is a widely accepted fact that state-sponsored Twitter accounts operated
during the 2016 US presidential election, spreading millions of tweets with
misinformation and inflammatory political content. Whether these social media
campaigns of the so-called ""troll"" accounts were able to manipulate public
opinion is still in question. Here, we quantify the influence of troll accounts
on Twitter by analyzing 152.5 million tweets (by 9.9 million users) from that
period. The data contain original tweets from 822 troll accounts identified as
such by Twitter itself. We construct and analyse a very large interaction graph
of 9.3 million nodes and 169.9 million edges using graph analysis techniques,
along with a game-theoretic centrality measure. Then, we quantify the influence
of all Twitter accounts on the overall information exchange as is defined by
the retweet cascades. We provide a global influence ranking of all Twitter
accounts and we find that one troll account appears in the top-100 and four in
the top-1000. This combined with other findings presented in this paper
constitute evidence that the driving force of virality and influence in the
network came from regular users - users who have not been classified as trolls
by Twitter. On the other hand, we find that on average, troll accounts were
tens of times more influential than regular users were. Moreover, 23% and 22%
of regular accounts in the top-100 and top-1000 respectively, have now been
suspended by Twitter. This raises questions about their authenticity and
practices during the 2016 US presidential election.","['Nikos Salamanos', 'Michael J. Jensen', 'Costas Iordanou', 'Michael Sirivianos']",10,0.775635
"The global COVID-19 pandemic has led to the online proliferation of health-,
political-, and conspiratorial-based misinformation. Understanding the reach
and belief in this misinformation is vital to managing this crisis, as well as
future crises. The results from our global survey finds a troubling reach of
and belief in COVID-related misinformation, as well as a correlation with those
that primarily consume news from social media, and, in the United States, a
strong correlation with political leaning.","['Sophie Nightingale', 'Hany Farid']",5,0.7296272
"Malicious actors create inauthentic social media accounts controlled in part
by algorithms, known as social bots, to disseminate misinformation and agitate
online discussion. While researchers have developed sophisticated methods to
detect abuse, novel bots with diverse behaviors evade detection. We show that
different types of bots are characterized by different behavioral features. As
a result, supervised learning techniques suffer severe performance
deterioration when attempting to detect behaviors not observed in the training
data. Moreover, tuning these models to recognize novel bots requires retraining
with a significant amount of new annotations, which are expensive to obtain. To
address these issues, we propose a new supervised learning method that trains
classifiers specialized for each class of bots and combines their decisions
through the maximum rule. The ensemble of specialized classifiers (ESC) can
better generalize, leading to an average improvement of 56\% in F1 score for
unseen accounts across datasets. Furthermore, novel bot behaviors are learned
with fewer labeled examples during retraining. We deployed ESC in the newest
version of Botometer, a popular tool to detect social bots in the wild, with a
cross-validation AUC of 0.99.","['Mohsen Sayyadiharikandeh', 'Onur Varol', 'Kai-Cheng Yang', 'Alessandro Flammini', 'Filippo Menczer']",13,0.7871933
"Information diffusion prediction is a fundamental task for understanding the
information propagation process. It has wide applications in such as
misinformation spreading prediction and malicious account detection. Previous
works either concentrate on utilizing the context of a single diffusion
sequence or using the social network among users for information diffusion
prediction. However, the diffusion paths of different messages naturally
constitute a dynamic diffusion graph. For one thing, previous works cannot
jointly utilize both the social network and diffusion graph for prediction,
which is insufficient to model the complexity of the diffusion process and
results in unsatisfactory prediction performance. For another, they cannot
learn users' dynamic preferences. Intuitively, users' preferences are changing
as time goes on and users' personal preference determines whether the user will
repost the information. Thus, it is beneficial to consider users' dynamic
preferences in information diffusion prediction.
  In this paper, we propose a novel dynamic heterogeneous graph convolutional
network (DyHGCN) to jointly learn the structural characteristics of the social
graph and dynamic diffusion graph. Then, we encode the temporal information
into the heterogeneous graph to learn the users' dynamic preferences. Finally,
we apply multi-head attention to capture the context-dependency of the current
diffusion path to facilitate the information diffusion prediction task.
Experimental results show that DyHGCN significantly outperforms the
state-of-the-art models on three public datasets, which shows the effectiveness
of the proposed model.","['Chunyuan Yuan', 'Jiacheng Li', 'Wei Zhou', 'Yijun Lu', 'Xiaodan Zhang', 'Songlin Hu']",2,0.70676625
"Debunking misinformation is an important and time-critical task as there
could be adverse consequences when misinformation is not quashed promptly.
However, the usual supervised approach to debunking via misinformation
classification requires human-annotated data and is not suited to the fast
time-frame of newly emerging events such as the COVID-19 outbreak. In this
paper, we postulate that misinformation itself has higher perplexity compared
to truthful statements, and propose to leverage the perplexity to debunk false
claims in an unsupervised manner. First, we extract reliable evidence from
scientific and news sources according to sentence similarity to the claims.
Second, we prime a language model with the extracted evidence and finally
evaluate the correctness of given claims based on the perplexity scores at
debunking time. We construct two new COVID-19-related test sets, one is
scientific, and another is political in content, and empirically verify that
our system performs favorably compared to existing systems. We are releasing
these datasets publicly to encourage more research in debunking misinformation
on COVID-19 and other topics.","['Nayeon Lee', 'Yejin Bang', 'Andrea Madotto', 'Pascale Fung']",1,0.7152631
"Public health practitioners and policy makers grapple with the challenge of
devising effective message-based interventions for debunking public health
misinformation in cyber communities. ""Framing"" and ""personalization"" of the
message is one of the key features for devising a persuasive messaging
strategy. For an effective health communication, it is imperative to focus on
""preference-based framing"" where the preferences of the target sub-community
are taken into consideration. To achieve that, it is important to understand
and hence characterize the target sub-communities in terms of their social
interactions. In the context of health-related misinformation, vaccination
remains to be the most prevalent topic of discord. Hence, in this paper, we
conduct a sociolinguistic analysis of the two competing vaccination communities
on Twitter: ""pro-vaxxers"" or individuals who believe in the effectiveness of
vaccinations, and ""anti-vaxxers"" or individuals who are opposed to
vaccinations. Our data analysis show significant linguistic variation between
the two communities in terms of their usage of linguistic intensifiers,
pronouns, and uncertainty words. Our network-level analysis show significant
differences between the two communities in terms of their network density,
echo-chamberness, and the EI index. We hypothesize that these sociolinguistic
differences can be used as proxies to characterize and understand these
communities to devise better message interventions.","['Shahan Ali Memon', 'Aman Tyagi', 'David R. Mortensen', 'Kathleen M. Carley']",12,0.71179384
"As the novel coronavirus spread globally, a growing public panic was
expressed over the internet. We examine the public discussion concerning
COVID-19 on Twitter. We use a dataset of 67 million tweets from 12 million
users collected between January 29, 2020 and March 4, 2020. We categorize users
based on their home countries, social identities, and political orientation. We
find that news media, government officials, and individual news reporters
posted a majority of influential tweets, while the most influential ones are
still written by regular users. Tweets mentioning ""fake news"" URLs and
disinformation story-lines are also more likely to be spread by regular users.
Unlike real news and normal tweets, tweets containing URLs pointing to ""fake
news"" sites are most likely to be retweeted within the source country and so
are less likely to spread internationally.","['Binxuan Huang', 'Kathleen M. Carley']",10,0.77434766
"WhatsApp was alleged to be widely used to spread misinformation and
propaganda during elections in Brazil and India. Due to the private encrypted
nature of the messages on WhatsApp, it is hard to track the dissemination of
misinformation at scale. In this work, using public WhatsApp data, we observe
that misinformation has been largely shared on WhatsApp public groups even
after they were already fact-checked by popular fact-checking agencies. This
represents a significant portion of misinformation spread in both Brazil and
India in the groups analyzed. We posit that such misinformation content could
be prevented if WhatsApp had a means to flag already fact-checked content. To
this end, we propose an architecture that could be implemented by WhatsApp to
counter such misinformation. Our proposal respects the current end-to-end
encryption architecture on WhatsApp, thus protecting users' privacy while
providing an approach to detect the misinformation that benefits from
fact-checking efforts.","['Julio C. S. Reis', 'Philipe de Freitas Melo', 'Kiran Garimella', 'Fabr√≠cio Benevenuto']",10,0.5624448
"Most of the information operations involve users who may foster polarization
and distrust toward science and mainstream journalism, without these users
being conscious of their role. Gab is well known to be an extremist-friendly
platform that performs little control on the posted content. Thus it represents
an ideal benchmark for studying phenomena potentially related to polarization
such as misinformation spreading. The combination of these factors may lead to
hate as well as to episodes of harm in the real world. In this work we provide
a characterization of the interaction patterns within Gab around the COVID-19
topic. To assess the spreading of different content type, we analyze
consumption patterns based on both interaction type and source reliability.
Overall we find that there are no strong statistical differences in the social
response to questionable and reliable content, both following a power law
distribution. However, questionable and reliable sources display structural and
topical differences in the use of hashtags. The commenting behaviour of users
in terms of both lifetime and sentiment reveals that questionable and reliable
posts are perceived in the same manner. We can conclude that despite evident
differences between questionable and reliable posts Gab users do not perform
such a differentiation thus treating them as a whole. Our results provide
insights toward the understanding of coordinated inauthentic behavior and on
the early-warning of information operation.","['Gabriele Etta', 'Alessandro Galeazzi', 'Matteo Cinelli', 'Mauro Conti', 'Walter Quattrociocchi']",3,0.7496409
"Social media has become an important communication channel during high impact
events, such as the COVID-19 pandemic. As misinformation in social media can
rapidly spread, creating social unrest, curtailing the spread of misinformation
during such events is a significant data challenge. While recent solutions that
are based on machine learning have shown promise for the detection of
misinformation, most widely used methods include approaches that rely on either
handcrafted features that cannot be optimal for all scenarios, or those that
are based on deep learning where the interpretation of the prediction results
is not directly accessible. In this work, we propose a data-driven solution
that is based on the ICA model, such that knowledge discovery and detection of
misinformation are achieved jointly. To demonstrate the effectiveness of our
method and compare its performance with deep learning methods, we developed a
labeled COVID-19 Twitter dataset based on socio-linguistic criteria.","['Zois Boukouvalas', 'Christine Mallinson', 'Evan Crothers', 'Nathalie Japkowicz', 'Aritran Piplai', 'Sudip Mittal', 'Anupam Joshi', 'T√ºlay Adalƒ±']",0,0.7335844
"With the emergence and rapid proliferation of social media platforms and
social networking sites, recent years have witnessed a surge of misinformation
spreading in our daily life. Drawing on a large-scale dataset which covers more
than 1.4M posts and 18M comments, we investigate the propagation of two
distinct narratives--(i) conspiracy information, whose claims are generally
unsubstantiated and thus referred as misinformation to some extent, and (ii)
scientific information, whose origins are generally readily identifiable and
verifiable--in an online social media platform. We find that conspiracy
cascades tend to propagate in a multigenerational branching process while
science cascades are more likely to grow in a breadth-first manner.
Specifically, conspiracy information triggers larger cascades, involves more
users and generations, persists longer, is more viral and bursty than science
information. Content analysis reveals that conspiracy cascades contain more
negative words and emotional words which convey anger, fear, disgust, surprise
and trust. We also find that conspiracy cascades are more concerned with
political and controversial topics. After applying machine learning models, we
achieve an AUC score of nearly 90% in discriminating conspiracy from science
narratives.
  We find that conspiracy cascades are more likely to be controlled by a
broader set of users than science cascades, imposing new challenges on the
management of misinformation. Although political affinity is thought to affect
the consumption of misinformation, there is very little evidence that political
orientation of the information source plays a role during the propagation of
conspiracy information. Our study provides complementing evidence to current
misinformation research and has practical policy implications to stem the
propagation and mitigate the influence of misinformation online.","['Yafei Zhang', 'Lin Wang', 'Jonathan J. H. Zhu', 'Xiaofan Wang']",3,0.78085196
"While the COVID-19 pandemic continues its global devastation, numerous
accompanying challenges emerge. One important challenge we face is to
efficiently and effectively use recently gathered data and find computational
tools to combat the COVID-19 infodemic, a typical information overloading
problem. Novel coronavirus presents many questions without ready answers; its
uncertainty and our eagerness in search of solutions offer a fertile
environment for infodemic. It is thus necessary to combat the infodemic and
make a concerted effort to confront COVID-19 and mitigate its negative impact
in all walks of life when saving lives and maintaining normal orders during
trying times. In this position paper of combating the COVID-19 infodemic, we
illustrate its need by providing real-world examples of rampant conspiracy
theories, misinformation, and various types of scams that take advantage of
human kindness, fear, and ignorance. We present three key challenges in this
fight against the COVID-19 infodemic where researchers and practitioners
instinctively want to contribute and help. We demonstrate that these three
challenges can and will be effectively addressed by collective wisdom,
crowdsourcing, and collaborative research.","['Kaize Ding', 'Kai Shu', 'Yichuan Li', 'Amrita Bhattacharjee', 'Huan Liu']",5,0.6560315
"Misinformation such as fake news has drawn a lot of attention in recent
years. It has serious consequences on society, politics and economy. This has
lead to a rise of manually fact-checking websites such as Snopes and
Politifact. However, the scale of misinformation limits their ability for
verification. In this demonstration, we propose BRENDA a browser extension
which can be used to automate the entire process of credibility assessments of
false claims. Behind the scenes BRENDA uses a tested deep neural network
architecture to automatically identify fact check worthy claims and classifies
as well as presents the result along with evidence to the user. Since BRENDA is
a browser extension, it facilities fast automated fact checking for the end
user without having to leave the Webpage.","['Bjarte Botnevik', 'Eirik Sakariassen', 'Vinay Setty']",4,0.66031754
"As the COVID-19 virus quickly spreads around the world, unfortunately,
misinformation related to COVID-19 also gets created and spreads like wild
fire. Such misinformation has caused confusion among people, disruptions in
society, and even deadly consequences in health problems. To be able to
understand, detect, and mitigate such COVID-19 misinformation, therefore, has
not only deep intellectual values but also huge societal impacts. To help
researchers combat COVID-19 health misinformation, therefore, we present CoAID
(Covid-19 heAlthcare mIsinformation Dataset), with diverse COVID-19 healthcare
misinformation, including fake news on websites and social platforms, along
with users' social engagement about such news. CoAID includes 4,251 news,
296,000 related user engagements, 926 social platform posts about COVID-19, and
ground truth labels. The dataset is available at:
https://github.com/cuilimeng/CoAID.","['Limeng Cui', 'Dongwon Lee']",5,0.752351
"COVID-19 resulted in an infodemic, which could erode public trust, impede
virus containment, and outlive the pandemic itself. The evolving and fragmented
media landscape is a key driver of the spread of misinformation. Using
misinformation identified by the fact-checking platform by Tencent and posts on
Weibo, our results showed that the evolution of misinformation follows an
issue-attention cycle, pertaining to topics such as city lockdown, cures, and
preventions, and school reopening. Sources of authority weigh in on these
topics, but their influence is complicated by peoples' pre-existing beliefs and
cultural practices. Finally, social media has a complicated relationship with
established or legacy media systems. Sometimes they reinforce each other, but
in general, social media may have a topic cycle of its own making. Our findings
shed light on the distinct characteristics of misinformation during the
COVID-19 and offer insights into combating misinformation in China and across
the world at large.","['Yan Leng', 'Yujia Zhai', 'Shaojing Sun', 'Yifei Wu', 'Jordan Selzer', 'Sharon Strover', 'Julia Fensel', 'Alex Pentland', 'Ying Ding']",5,0.8169411
"WhatsApp is a key medium for the spread of news and rumors, often shared as
images. We study a large collection of politically-oriented WhatsApp groups in
India, focusing on the period leading up to the 2019 Indian national elections.
By labeling samples of random and popular images, we find that around 13% of
shared images are known misinformation and most fall into three types of
images. Machine learning methods can be used to predict whether a viral image
is misinformation, but are brittle to shifts in content over time.","['Kiran Garimella', 'Dean Eckles']",10,0.5289106
"Social media platforms attempting to curb abuse and misinformation have been
accused of political bias. We deploy neutral social bots who start following
different news sources on Twitter, and track them to probe distinct biases
emerging from platform mechanisms versus user interactions. We find no strong
or consistent evidence of political bias in the news feed. Despite this, the
news and information to which U.S. Twitter users are exposed depend strongly on
the political leaning of their early connections. The interactions of
conservative accounts are skewed toward the right, whereas liberal accounts are
exposed to moderate content shifting their experience toward the political
center. Partisan accounts, especially conservative ones, tend to receive more
followers and follow more automated accounts. Conservative accounts also find
themselves in denser communities and are exposed to more low-credibility
content.","['Wen Chen', 'Diogo Pacheco', 'Kai-Cheng Yang', 'Filippo Menczer']",10,0.77363497
"In this work we propose an improvement of the $x$-space algorithm developed
for solving a class of min--max bilevel optimization problems (Tang Y., Richard
J.P.P., Smith J.C. (2016), A class of algorithms for mixed-integer bilevel
min--max optimization. Journal of Global Optimization, 66(2), 225--262). In
this setting, the leader of the upper level problem aims at restricting the
follower's decisions by minimizing an objective function, which the follower
intends to maximize in the lower level problem by making decisions still
available to her. The $x$-space algorithm solves upper and lower bound problems
consecutively until convergence, and requires the dualization of an
approximation of the follower's problem in formulating the lower bound problem.
We first reformulate the lower bound problem using the properties of an optimal
solution to the original formulation, which makes the dualization step
unnecessary. The reformulation makes possible the integration of a greedy
covering heuristic into the solution scheme, which results in a considerable
increase in the efficiency. The new algorithm referred to as the improved
$x$-space algorithm is implemented and applied to a recent min--max bilevel
optimization problem that arises in the context of reducing the misinformation
spread in social networks. It is also assessed on the benchmark instances of
two other bilevel problems: zero-one knapsack problem with interdiction and
maximum clique problem with interdiction. Numerical results indicate that the
performance of the new algorithm is superior to that of the original algorithm,
and also compares favorably with a recent algorithm developed for mixed-integer
bilevel linear programs.","['K√ºbra Tanƒ±nmƒ±≈ü', 'Necati Aras', 'ƒ∞. Kuban Altƒ±nel']",2,0.5956173
"Social Networks (SNs) have been gradually applied by utility companies as an
addition to smart grid and are proved to be helpful in smoothing load curves
and reducing energy usage. However, SNs also bring in new threats to smart
grid: misinformation in SNs may cause smart grid users to alter their demand,
resulting in transmission line overloading and in turn leading to catastrophic
impact to the grid. In this paper, we discuss the interdependency in the social
network coupled smart grid and focus on its vulnerability. That is, how much
can the smart grid be damaged when misinformation related to it diffuses in
SNs? To analytically study the problem, we propose the Misinformation Attack
Problem in Social-Smart Grid (MAPSS) that identifies the top critical nodes in
the SN, such that the smart grid can be greatly damaged when misinformation
propagates from those nodes. This problem is challenging as we have to
incorporate the complexity of the two networks concurrently. Nevertheless, we
propose a technique that can explicitly take into account information diffusion
in SN, power flow balance and cascading failure in smart grid integratedly when
evaluating node criticality, based on which we propose various strategies in
selecting the most critical nodes. Also, we introduce controlled load shedding
as a protection strategy to reduce the impact of cascading failure. The
effectiveness of our algorithms are demonstrated by experiments on IEEE bus
test cases as well as the Pegase data set.","['Tianyi Pan', 'Subhankar Mishra', 'Lan N. Nguyen', 'Gunhee Lee', 'Jungmin Kang', 'Jungtaek Seo', 'My T. Thai']",2,0.6783739
"Information is key during a crisis such as the one produced by the current
COVID-19 pandemic as it greatly shapes people opinion, behavior and their
psychology. Infodemic of misinformation is an important secondary crisis
associated to the pandemic. Infodemics can amplify the real negative
consequences of the pandemic in different dimensions: social, economic and even
sanitary. For instance, infodemics can lead to hatred between population groups
that fragment the society influencing its response or result in negative habits
that help the pandemic propagate. On the contrary, reliable and trustful
information along with messages of hope and solidarity can be used to control
the pandemic, build safety nets and help promote resilience. We propose the
foundation of a framework to characterize leaders in Twitter based on the
analysis of the social graph derived from the activity in this social network.
Centrality metrics are used to characterize the topology of the network and the
nodes as potential leaders. These metrics are compared with the user popularity
metrics managed by Twitter. We then assess the resulting topology of clusters
of leaders visually. We propose this tool to be the basis for a system to
detect and empower users with a positive influence in the collective behavior
of the network and the propagation of information.","['David Pastor-Escuredo', 'Carlota Tarazona']",5,0.70519227
"Truthfulness judgments are a fundamental step in the process of fighting
misinformation, as they are crucial to train and evaluate classifiers that
automatically distinguish true and false statements. Usually such judgments are
made by experts, like journalists for political statements or medical doctors
for medical statements. In this paper, we follow a different approach and rely
on (non-expert) crowd workers. This of course leads to the following research
question: Can crowdsourcing be reliably used to assess the truthfulness of
information and to create large-scale labeled collections for information
credibility systems? To address this issue, we present the results of an
extensive study based on crowdsourcing: we collect thousands of truthfulness
assessments over two datasets, and we compare expert judgments with crowd
judgments, expressed on scales with various granularity levels. We also measure
the political bias and the cognitive background of the workers, and quantify
their effect on the reliability of the data provided by the crowd.","['Kevin Roitero', 'Michael Soprano', 'Shaoyang Fan', 'Damiano Spina', 'Stefano Mizzaro', 'Gianluca Demartini']",0,0.6847575
"The year 2020 is experiencing a global health and economic crisis due to the
COVID-19 pandemic. Countries across the world are using digital technologies to
fight this global crisis. These digital technologies, in one way or another,
strongly rely on the availability of wireless communication technologies. In
this paper, we present the role of wireless communications in the COVID-19
pandemic from different perspectives. First, we show how these technologies are
helping to combat this pandemic, including monitoring of the virus spread,
enabling healthcare automation, and allowing virtual education and
conferencing. Also, we show the importance of digital inclusiveness in the
pandemic and possible solutions to connect the unconnected. Next, we discuss
the challenges faced by wireless technologies, including privacy, security, and
misinformation. Then, we present the importance of wireless communication
technologies in the survival of the global economy, such as automation of
industries and supply chain, e-commerce, and supporting occupations that are at
risk. Finally, we reveal that how the technologies developed during the
pandemic can be helpful in the post-pandemic era.","['Nasir Saeed', 'Ahmed Bader', 'Tareq Y. Al-Naffouri', 'Mohamed-Slim Alouini']",9,0.556612
"During the COVID-19 pandemic, social media has become a home ground for
misinformation. To tackle this infodemic, scientific oversight, as well as a
better understanding by practitioners in crisis management, is needed. We have
conducted an exploratory study into the propagation, authors and content of
misinformation on Twitter around the topic of COVID-19 in order to gain early
insights. We have collected all tweets mentioned in the verdicts of
fact-checked claims related to COVID-19 by over 92 professional fact-checking
organisations between January and mid-July 2020 and share this corpus with the
community. This resulted in 1 500 tweets relating to 1 274 false and 276
partially false claims, respectively. Exploratory analysis of author accounts
revealed that the verified twitter handle(including Organisation/celebrity) are
also involved in either creating (new tweets) or spreading (retweet) the
misinformation. Additionally, we found that false claims propagate faster than
partially false claims. Compare to a background corpus of COVID-19 tweets,
tweets with misinformation are more often concerned with discrediting other
information on social media. Authors use less tentative language and appear to
be more driven by concerns of potential harm to others. Our results enable us
to suggest gaps in the current scientific coverage of the topic as well as
propose actions for authorities and social media users to counter
misinformation.","['Gautam Kishore Shahi', 'Anne Dirkson', 'Tim A. Majchrzak']",5,0.74573416
"COVID-19 infodemic has been spreading faster than the pandemic itself. The
misinformation riding upon the infodemic wave poses a major threat to people's
health and governance systems. Since social media is the largest source of
information, managing the infodemic not only requires mitigating of
misinformation but also an early understanding of psychological patterns
resulting from it. During the COVID-19 crisis, Twitter alone has seen a sharp
45% increase in the usage of its curated events page, and a 30% increase in its
direct messaging usage, since March 6th 2020. In this study, we analyze the
psychometric impact and coupling of the COVID-19 infodemic with the official
bulletins related to COVID-19 at the national and state level in India. We look
at these two sources with a psycho-linguistic lens of emotions and quantified
the extent and coupling between the two. We modified path, a deep skip-gram
based open-sourced lexicon builder for effective capture of health-related
emotions. We were then able to capture the time-evolution of health-related
emotions in social media and official bulletins. An analysis of lead-lag
relationships between the time series of extracted emotions from official
bulletins and social media using Granger's causality showed that state
bulletins were leading the social media for some emotions such as Medical
Emergency. Further insights that are potentially relevant for the policymaker
and the communicators actively engaged in mitigating misinformation are also
discussed. Our paper also introduces CoronaIndiaDataset2, the first social
media based COVID-19 dataset at national and state levels from India with over
5.6 million national and 2.6 million state-level tweets. Finally, we present
our findings as COVibes, an interactive web application capturing psychometric
insights captured upon the CoronaIndiaDataset, both at a national and state
level.","['Baani Leen Kaur Jolly', 'Palash Aggrawal', 'Amogh Gulati', 'Amarjit Singh Sethi', 'Ponnurangam Kumaraguru', 'Tavpritesh Sethi']",5,0.8183393
"News feeds in virtually all social media platforms include engagement
metrics, such as the number of times each post is liked and shared. We find
that exposure to these social engagement signals increases the vulnerability of
users to misinformation. This finding has important implications for the design
of social media interactions in the misinformation age. To reduce the spread of
misinformation, we call for technology platforms to rethink the display of
social engagement metrics. Further research is needed to investigate whether
and how engagement metrics can be presented without amplifying the spread of
low-credibility information.","['Mihai Avram', 'Nicholas Micallef', 'Sameer Patil', 'Filippo Menczer']",3,0.7211882
"Distinguishing between misinformation and real information is one of the most
challenging problems in today's interconnected world. The vast majority of the
state-of-the-art in detecting misinformation is fully supervised, requiring a
large number of high-quality human annotations. However, the availability of
such annotations cannot be taken for granted, since it is very costly,
time-consuming, and challenging to do so in a way that keeps up with the
proliferation of misinformation. In this work, we are interested in exploring
scenarios where the number of annotations is limited. In such scenarios, we
investigate how tapping on a diverse number of resources that characterize a
news article, henceforth referred to as ""aspects"" can compensate for the lack
of labels. In particular, our contributions in this paper are twofold: 1) We
propose the use of three different aspects: article content, context of social
sharing behaviors, and host website/domain features, and 2) We introduce a
principled tensor based embedding framework that combines all those aspects
effectively. We propose HiJoD a 2-level decomposition pipeline which not only
outperforms state-of-the-art methods with F1-scores of 74% and 81% on Twitter
and Politifact datasets respectively but also is an order of magnitude faster
than similar ensemble approaches.","['Sara Abdali', 'Neil Shah', 'Evangelos E. Papalexakis']",0,0.6534962
"Recently, messaging applications, such as WhatsApp, have been reportedly
abused by misinformation campaigns, especially in Brazil and India. A notable
form of abuse in WhatsApp relies on several manipulated images and memes
containing all kinds of fake stories. In this work, we performed an extensive
data collection from a large set of WhatsApp publicly accessible groups and
fact-checking agency websites. This paper opens a novel dataset to the research
community containing fact-checked fake images shared through WhatsApp for two
distinct scenarios known for the spread of fake news on the platform: the 2018
Brazilian elections and the 2019 Indian elections.","['Julio C. S. Reis', 'Philipe de Freitas Melo', 'Kiran Garimella', 'Jussara M. Almeida', 'Dean Eckles', 'Fabr√≠cio Benevenuto']",10,0.5788145
"We describe Mega-COV, a billion-scale dataset from Twitter for studying
COVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as
back as 2007), multilingual (comes in 100+ languages), and has a significant
number of location-tagged tweets (~169M tweets). We release tweet IDs from the
dataset. We also develop and release two powerful models, one for identifying
whether or not a tweet is related to the pandemic (best F1=97%) and another for
detecting misinformation about COVID-19 (best F1=92%). A human annotation study
reveals the utility of our models on a subset of Mega-COV. Our data and models
can be useful for studying a wide host of phenomena related to the pandemic.
Mega-COV and our models are publicly available.","['Muhammad Abdul-Mageed', 'AbdelRahim Elmadany', 'El Moatez Billah Nagoudi', 'Dinesh Pabbi', 'Kunal Verma', 'Rannie Lin']",5,0.6274712
"With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Addressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus confirming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.","['Firoj Alam', 'Shaden Shaar', 'Fahim Dalvi', 'Hassan Sajjad', 'Alex Nikolov', 'Hamdy Mubarak', 'Giovanni Da San Martino', 'Ahmed Abdelali', 'Nadir Durrani', 'Kareem Darwish', 'Abdulaziz Al-Homaid', 'Wajdi Zaghouani', 'Tommaso Caselli', 'Gijs Danoe', 'Friso Stolk', 'Britt Bruntink', 'Preslav Nakov']",5,0.7772747
"The world is facing the challenge of climate crisis. Despite the consensus in
scientific community about anthropogenic global warming, the web is flooded
with articles spreading climate misinformation. These articles are carefully
constructed by climate change counter movement (cccm) organizations to
influence the narrative around climate change. We revisit the literature on
climate misinformation in social sciences and repackage it to introduce in the
community of NLP. Despite considerable work in detection of fake news, there is
no misinformation dataset available that is specific to the domain.of climate
change. We try to bridge this gap by scraping and releasing articles with known
climate change misinformation.","['Shraey Bhatia', 'Jey Han Lau', 'Timothy Baldwin']",3,0.5476496
"As the novel coronavirus spreads across the world, concerns regarding the
spreading of misinformation about it are also growing. Here we estimate the
prevalence of links to low-credibility information on Twitter during the
outbreak, and the role of bots in spreading these links. We find that the
combined volume of tweets linking to low-credibility information is comparable
to the volume of New York Times articles and CDC links. Content analysis
reveals a politicization of the pandemic. The majority of this content spreads
via retweets. Social bots are involved in both posting and amplifying
low-credibility information, although the majority of volume is generated by
likely humans. Some of these accounts appear to amplify low-credibility sources
in a coordinated fashion.","['Kai-Cheng Yang', 'Christopher Torres-Lugo', 'Filippo Menczer']",5,0.65438753
"For years, many studies employed sentiment analysis to understand the
reasoning behind people's choices and feelings, their communication styles, and
the communities which they belong to. We argue that gaining more in-depth
insight into moral dimensions coupled with sentiment analysis can potentially
provide superior results. Understanding moral foundations can yield powerful
results in terms of perceiving the intended meaning of the text data, as the
concept of morality provides additional information on the unobservable
characteristics of information processing and non-conscious cognitive
processes. Therefore, we studied latent moral loadings of Syrian White
Helmets-related tweets of Twitter users from April 1st, 2018 to April 30th,
2019. For the operationalization and quantification of moral rhetoric in
tweets, we use Extended Moral Foundations Dictionary in which five
psychological dimensions (Harm/Care, Fairness/Reciprocity, In-group/Loyalty,
Authority/Respect and Purity/Sanctity) are considered. We show that people tend
to share more tweets involving the virtue moral rhetoric than the tweets
involving the vice rhetoric. We observe that the pattern of the moral rhetoric
of tweets among these five dimensions are very similar during different time
periods, while the strength of the five dimension is time-variant. Even though
there is no significant difference between the use of Fairness/Reciprocity,
In-group/Loyalty or Purity/Sanctity rhetoric, the less use of Harm/Care
rhetoric is significant and remarkable. Besides, the strength of the moral
rhetoric and the polarization in morality across people are mostly observed in
tweets involving Harm/Care rhetoric despite the number of tweets involving the
Harm/Care dimension is low.","['Ece √áiƒüdem Mutlu', 'Toktam Oghaz', 'Ege T√ºt√ºnc√ºler', 'Jasser Jasser', 'Ivan Garibay']",10,0.56500566
"The increased focus on misinformation has spurred development of data and
systems for detecting the veracity of a claim as well as retrieving
authoritative evidence. The Fact Extraction and VERification (FEVER) dataset
provides such a resource for evaluating end-to-end fact-checking, requiring
retrieval of evidence from Wikipedia to validate a veracity prediction. We show
that current systems for FEVER are vulnerable to three categories of realistic
challenges for fact-checking -- multiple propositions, temporal reasoning, and
ambiguity and lexical variation -- and introduce a resource with these types of
claims. Then we present a system designed to be resilient to these ""attacks""
using multiple pointer networks for document selection and jointly modeling a
sequence of evidence sentences and veracity relation predictions. We find that
in handling these attacks we obtain state-of-the-art results on FEVER, largely
due to improved evidence retrieval.","['Christopher Hidey', 'Tuhin Chakrabarty', 'Tariq Alhindi', 'Siddharth Varia', 'Kriste Krstovski', 'Mona Diab', 'Smaranda Muresan']",1,0.8298571
"The novel coronavirus (COVID-19) pandemic outbreak is drastically shaping and
reshaping many aspects of our life, with a huge impact on our social life. In
this era of lockdown policies in most of the major cities around the world, we
see a huge increase in people and professional engagement in social media.
Social media is playing an important role in news propagation as well as
keeping people in contact. At the same time, this source is both a blessing and
a curse as the coronavirus infodemic has become a major concern, and is already
a topic that needs special attention and further research. In this paper, we
provide a multilingual coronavirus (COVID-19) Instagram dataset that we have
been continuously collected since March 30, 2020. We are making our dataset
available to the research community at Github. We believe that this
contribution will help the community to better understand the dynamics behind
this phenomenon in Instagram, as one of the major social media. This dataset
could also help study the propagation of misinformation related to this
outbreak.","['Koosha Zarei', 'Reza Farahbakhsh', 'Noel Crespi', 'Gareth Tyson']",5,0.7533121
"Generative deep learning algorithms have progressed to a point where it is
difficult to tell the difference between what is real and what is fake. In
2018, it was discovered how easy it is to use this technology for unethical and
malicious applications, such as the spread of misinformation, impersonation of
political leaders, and the defamation of innocent individuals. Since then,
these `deepfakes' have advanced significantly.
  In this paper, we explore the creation and detection of deepfakes and provide
an in-depth view of how these architectures work. The purpose of this survey is
to provide the reader with a deeper understanding of (1) how deepfakes are
created and detected, (2) the current trends and advancements in this domain,
(3) the shortcomings of the current defense solutions, and (4) the areas which
require further research and attention.","['Yisroel Mirsky', 'Wenke Lee']",11,0.77490973
"The World Health Organization have emphasised that misinformation - spreading
rapidly through social media - poses a serious threat to the COVID-19 response.
Drawing from theories of health perception and cognitive load, we develop and
test a research model hypothesizing why people share unverified COVID-19
information through social media. Our findings suggest a person's trust in
online information and perceived information overload are strong predictors of
unverified information sharing. Furthermore, these factors, along with a
person's perceived COVID-19 severity and vulnerability influence cyberchondria.
Females were significantly more likely to suffer from cyberchondria, however,
males were more likely to share news without fact checking their source. Our
findings suggest that to mitigate the spread of COVID-19 misinformation and
cyberchondria, measures should be taken to enhance a healthy skepticism of
health news while simultaneously guarding against information overload.","['Samuli Laato', 'A. K. M. Najmul Islam', 'Muhammad Nazrul Islam', 'Eoin Whelan']",5,0.78260124
"The massive amount of misinformation spreading on the Internet on a daily
basis has enormous negative impacts on societies. Therefore, we need automated
systems helping fact-checkers in the combat against misinformation. In this
paper, we propose a model prioritizing the claims based on their
check-worthiness. We use BERT model with additional features including
domain-specific controversial topics, word embeddings, and others. In our
experiments, we show that our proposed model outperforms all state-of-the-art
models in both test collections of CLEF Check That! Lab in 2018 and 2019. We
also conduct a qualitative analysis to shed light-detecting check-worthy
claims. We suggest requesting rationales behind judgments are needed to
understand subjective nature of the task and problematic labels.","['Yavuz Selim Kartal', 'Busra Guvenen', 'Mucahid Kutlu']",1,0.73233926
"With the spiraling pandemic of the Coronavirus Disease 2019 (COVID-19), it
has becoming inherently important to disseminate accurate and timely
information about the disease. Due to the ubiquity of Internet connectivity and
smart devices, social sensing is emerging as a dynamic AI-driven sensing
paradigm to extract real-time observations from online users. In this paper, we
propose CovidSens, a vision of social sensing based risk alert systems to
spontaneously obtain and analyze social data to infer COVID-19 propagation.
CovidSens can actively help to keep the general public informed about the
COVID-19 spread and identify risk-prone areas. The CovidSens concept is
motivated by three observations: 1) people actively share their experience of
COVID-19 via online social media, 2) official warning channels and news
agencies are relatively slower than people reporting on social media, and 3)
online users are frequently equipped with powerful mobile devices that can
perform data processing and analytics. We envision unprecedented opportunities
to leverage posts generated by ordinary people to build real-time sensing and
analytic system for gathering and circulating COVID-19 propagation data.
Specifically, the vision of CovidSens attempts to answer the questions: How to
distill reliable information on COVID-19 with prevailing rumors and
misinformation? How to inform the general public about the state of the spread
timely and effectively? How to leverage the computational power on edge devices
to construct fully integrated edge-based social sensing platforms? In this
vision paper, we discuss the roles of CovidSens and identify potential
challenges in developing reliable social sensing based risk alert systems. We
envision that approaches originating from multiple disciplines can be effective
in addressing the challenges. Finally, we outline a few research directions for
future work in CovidSens.","['Md Tahmid Rashid', 'Dong Wang']",5,0.7237737
"The 2019 coronavirus disease (COVID-19), emerged late December 2019 in China,
is now rapidly spreading across the globe. At the time of writing this paper,
the number of global confirmed cases has passed two millions and half with over
180,000 fatalities. Many countries have enforced strict social distancing
policies to contain the spread of the virus. This have changed the daily life
of tens of millions of people, and urged people to turn their discussions
online, e.g., via online social media sites like Twitter. In this work, we
describe the first Arabic tweets dataset on COVID-19 that we have been
collecting since January 1st, 2020. The dataset would help researchers and
policy makers in studying different societal issues related to the pandemic.
Many other tasks related to behavioral change, information sharing,
misinformation and rumors spreading can also be analyzed.","['Sarah Alqurashi', 'Ahmad Alhindi', 'Eisa Alanazi']",5,0.746232
"Satirical news detection is an important yet challenging task to prevent
spread of misinformation. Many feature based and end-to-end neural nets based
satirical news detection systems have been proposed and delivered promising
results. Existing approaches explore comprehensive word features from satirical
news articles, but lack semantic metrics using word vectors for tweet form
satirical news. Moreover, the vagueness of satire and news parody determines
that a news tweet can hardly be classified with a binary decision, that is,
satirical or legitimate. To address these issues, we collect satirical and
legitimate news tweets, and propose a semantic feature based approach. Features
are extracted by exploring inconsistencies in phrases, entities, and between
main and relative clauses. We apply game-theoretic rough set model to detect
satirical news, in which probabilistic thresholds are derived by game
equilibrium and repetition learning mechanism. Experimental results on the
collected dataset show the robustness and improvement of the proposed approach
compared with Pawlak rough set model and SVM.","['Yue Zhou', 'Yan Zhang', 'JingTao Yao']",4,0.7107043
"As the COVID-19 pandemic continues its march around the world, an
unprecedented amount of open data is being generated for genetics and
epidemiological research. The unparalleled rate at which many research groups
around the world are releasing data and publications on the ongoing pandemic is
allowing other scientists to learn from local experiences and data generated in
the front lines of the COVID-19 pandemic. However, there is a need to integrate
additional data sources that map and measure the role of social dynamics of
such a unique world-wide event into biomedical, biological, and epidemiological
analyses. For this purpose, we present a large-scale curated dataset of over
152 million tweets, growing daily, related to COVID-19 chatter generated from
January 1st to April 4th at the time of writing. This open dataset will allow
researchers to conduct a number of research projects relating to the emotional
and mental responses to social distancing measures, the identification of
sources of misinformation, and the stratified measurement of sentiment towards
the pandemic in near real time.","['Juan M. Banda', 'Ramya Tekumalla', 'Guanyu Wang', 'Jingyuan Yu', 'Tuo Liu', 'Yuning Ding', 'Katya Artemova', 'Elena Tutubalina', 'Gerardo Chowell']",5,0.74174786
"The information ecosystem today is overwhelmed by an unprecedented quantity
of data on versatile topics are with varied quality. However, the quality of
information disseminated in the field of medicine has been questioned as the
negative health consequences of health misinformation can be life-threatening.
There is currently no generic automated tool for evaluating the quality of
online health information spanned over a broad range. To address this gap, in
this paper, we applied a data mining approach to automatically assess the
quality of online health articles based on 10 quality criteria. We have
prepared a labeled dataset with 53012 features and applied different feature
selection methods to identify the best feature subset with which our trained
classifier achieved an accuracy of 84%-90% varied over 10 criteria. Our
semantic analysis of features shows the underpinning associations between the
selected features & assessment criteria and further rationalize our assessment
approach. Our findings will help in identifying high-quality health articles
and thus aiding users in shaping their opinion to make the right choice while
picking health-related help from online.","['Fariha Afsana', 'Muhammad Ashad Kabir', 'Naeemul Hassan', 'Manoranjan Paul']",1,0.5154361
"Modern online media, such as Twitter, Instagram, and YouTube, enable anyone
to become an information producer and to offer online content for potentially
global consumption. By increasing the amount of globally accessible real-time
information, today's ubiquitous producers contribute to a world, where an
individual consumes vanishingly smaller fractions of all produced content. In
general, consumers preferentially select information that closely matches their
individual views and values. The bias inherent in such selection is further
magnified by today's information curation services that maximize user
engagement (and thus service revenue) by filtering new content in accordance
with observed consumer preferences. Consequently, individuals get exposed to
increasingly narrower bands of the ideology spectrum. Societies get fragmented
into increasingly ideologically isolated enclaves. These enclaves (or
echo-chambers) then become vulnerable to misinformation spread, which in turn
further magnifies polarization and bias. We call this dynamic the paradox of
information access; a growing ideological fragmentation in the age of sharing.
This article describes the technical, economic, and socio-cognitive
contributors to this paradox, and explores research directions towards its
mitigation.","['Tarek Abdelzaher', 'Heng Ji', 'Jinyang Li', 'Chaoqi Yang', 'John Dellaverson', 'Lixia Zhang', 'Chao Xu', 'Boleslaw K. Szymanski']",3,0.63020134
"Social media has greatly enabled people to participate in online activities
at an unprecedented rate. However, this unrestricted access also exacerbates
the spread of misinformation and fake news online which might cause confusion
and chaos unless being detected early for its mitigation. Given the rapidly
evolving nature of news events and the limited amount of annotated data,
state-of-the-art systems on fake news detection face challenges due to the lack
of large numbers of annotated training instances that are hard to come by for
early detection. In this work, we exploit multiple weak signals from different
sources given by user and content engagements (referred to as weak social
supervision), and their complementary utilities to detect fake news. We jointly
leverage the limited amount of clean data along with weak signals from social
engagements to train deep neural networks in a meta-learning framework to
estimate the quality of different weak instances. Experiments on realworld
datasets demonstrate that the proposed framework outperforms state-of-the-art
baselines for early detection of fake news without using any user engagements
at prediction time.","['Kai Shu', 'Guoqing Zheng', 'Yichuan Li', 'Subhabrata Mukherjee', 'Ahmed Hassan Awadallah', 'Scott Ruston', 'Huan Liu']",4,0.7719391
"The paper develops a stochastic model of drift in human beliefs that shows
that today's sheer volume of accessible information, combined with consumers'
confirmation bias and natural preference to more outlying content, necessarily
lead to increased polarization. The model explains the paradox of growing
ideological fragmentation in the age of increased sharing. As social media,
search engines, and other real-time information sharing outlets purport to
facilitate access to information, a need for content filtering arises due to
the ensuing information overload. In general, consumers select information that
matches their individual views and values. The bias inherent in such selection
is echoed by today's information curation services that maximize user
engagement by filtering new content in accordance with observed consumer
preferences. Consequently, individuals get exposed to increasingly narrower
bands of the ideology spectrum, thus fragmenting society into increasingly
ideologically isolated enclaves. We call this dynamic the paradox of
information access. The model also suggests the disproportionate damage
attainable with a small infusion of well-positioned misinformation. The paper
describes the modeling methodology, and evaluates modeling results for
different population sizes and parameter settings.","['Chao Xu', 'Jinyang Li', 'Tarek Abdelzaher', 'Heng Ji', 'Boleslaw K. Szymanski', 'John Dellaverson']",0,0.67992413
"Critical thinking and skepticism are fundamental mechanisms that one may use
to prevent the spreading of rumors, fake-news and misinformation. We consider a
simple model in which agents without previous contact with the rumor, being
skeptically oriented, may convince spreaders to stop their activity or, once
exposed to the rumor, decide not to propagate it as a consequence, for example,
of fact-checking. We extend a previous, mean-field analysis of the combined
effect of these two mechanisms, active and passive skepticism, to include
spatial correlations. This can be done either analytically, through the pair
approximation, or simulating an agent-based version on diverse networks. Our
results show that while in mean-field there is no coexistence between spreaders
and susceptibles (although, depending on the parameters, there may be
bistability depending on the initial conditions), when spatial correlations are
included, because of the protective effect of the isolation provided by removed
agents, coexistence is possible.","['Marco Antonio Amaral', 'W. G. Dantas', 'Jeferson J. Arenzon']",2,0.6525372
"During the summer of 2019-20, while Australia suffered unprecedented
bushfires across the country, false narratives regarding arson and limited
backburning spread quickly on Twitter, particularly using the hashtag
#ArsonEmergency. Misinformation and bot- and troll-like behaviour were detected
and reported by social media researchers and the news soon reached mainstream
media. This paper examines the communication and behaviour of two polarised
online communities before and after news of the misinformation became public
knowledge. Specifically, the Supporter community actively engaged with others
to spread the hashtag, using a variety of news sources pushing the arson
narrative, while the Opposer community engaged less, retweeted more, and
focused its use of URLs to link to mainstream sources, debunking the narratives
and exposing the anomalous behaviour. This influenced the content of the
broader discussion. Bot analysis revealed the active accounts were
predominantly human, but behavioural and content analysis suggests Supporters
engaged in trolling, though both communities used aggressive language.","['Derek Weber', 'Mehwish Nasim', 'Lucia Falzon', 'Lewis Mitchell']",10,0.7709955
"We show that malicious COVID-19 content, including hate speech,
disinformation, and misinformation, exploits the multiverse of online hate to
spread quickly beyond the control of any individual social media platform.
Machine learning topic analysis shows quantitatively how online hate
communities are weaponizing COVID-19, with topics evolving rapidly and content
becoming increasingly coherent. Our mathematical analysis provides a
generalized form of the public health R0 predicting the tipping point for
multiverse-wide viral spreading, which suggests new policy options to mitigate
the global spread of malicious COVID-19 content without relying on future
coordination between all online platforms.","['N. Vel√°squez', 'R. Leahy', 'N. Johnson Restrepo', 'Y. Lupu', 'R. Sear', 'N. Gabriel', 'O. Jha', 'B. Goldberg', 'N. F. Johnson']",5,0.66259414
"Since December 2019, COVID-19 has been spreading rapidly across the world.
Not surprisingly, conversation about COVID-19 is also increasing. This article
is a first look at the amount of conversation taking place on social media,
specifically Twitter, with respect to COVID-19, the themes of discussion, where
the discussion is emerging from, myths shared about the virus, and how much of
it is connected to other high and low quality information on the Internet
through shared URL links. Our preliminary findings suggest that a meaningful
spatio-temporal relationship exists between information flow and new cases of
COVID-19, and while discussions about myths and links to poor quality
information exist, their presence is less dominant than other crisis specific
themes. This research is a first step toward understanding social media
conversation about COVID-19.","['Lisa Singh', 'Shweta Bansal', 'Leticia Bode', 'Ceren Budak', 'Guangqing Chi', 'Kornraphop Kawintiranon', 'Colton Padden', 'Rebecca Vanarsdall', 'Emily Vraga', 'Yanchen Wang']",5,0.7216373
"We study the perception of COVID-2019 epidemic in Polish society using
quantitative analysis of its digital footprints on the Internet (on Twitter,
Google, YouTube, Wikipedia and electronic media represented by Event Registry)
from January 2020 to 12.03.2020 (before and after official introduction to
Poland on 04.03.2020). To this end we utilize data mining, social network
analysis, natural language processing techniques. Each examined internet
platform was analyzed for representativeness and composition of the target
group. We identified three temporal major cluster of the interest before
disease introduction on the topic COVID-2019: China- and Italy-related peaks on
all platforms, as well as a peak on social media related to the recent special
law on combating COVID-2019. Besides, there was a peak in interest on the day
of officially confirmed introduction as well as an exponential increase of
interest when the Polish government declared war against disease with a massive
mitigation program. From sociolingistic perspective, we found that concepts and
issues of threat, fear and prevention prevailed before introduction. After
introduction, practical concepts about disease and epidemic dominate. We have
found out that Twitter reflected the structural division of the Polish
political sphere. We were able to identify clear communities of governing
party, mainstream oppostition and protestant group and potential sources of
misinformation. We have also detected bluring boundaries between comminities
after disease introduction.","['Andrzej Jarynowski', 'Monika Wojta-Kempa', 'Vitaly Belik']",5,0.7161937
"Twitter has become one of the most sought after places to discuss a wide
variety of topics, including medically relevant issues such as cancer. This
helps spread awareness regarding the various causes, cures and prevention
methods of cancer. However, no proper analysis has been performed, which
discusses the validity of such claims. In this work, we aim to tackle the
misinformation spread in such platforms. We collect and present a dataset
regarding tweets which talk specifically about cancer and propose an
attention-based deep learning model for automated detection of misinformation
along with its spread. We then do a comparative analysis of the linguistic
variation in the text corresponding to misinformation and truth. This analysis
helps us gather relevant insights on various social aspects related to
misinformed tweets.","['Rakesh Bal', 'Sayan Sinha', 'Swastika Dutta', 'Rishabh Joshi', 'Sayan Ghosh', 'Ritam Dutt']",5,0.7105847
"The ongoing Coronavirus (COVID-19) pandemic highlights the
inter-connectedness of our present-day globalized world. With social distancing
policies in place, virtual communication has become an important source of
(mis)information. As increasing number of people rely on social media platforms
for news, identifying misinformation and uncovering the nature of online
discourse around COVID-19 has emerged as a critical task. To this end, we
collected streaming data related to COVID-19 using the Twitter API, starting
March 1, 2020. We identified unreliable and misleading contents based on
fact-checking sources, and examined the narratives promoted in misinformation
tweets, along with the distribution of engagements with these tweets. In
addition, we provide examples of the spreading patterns of prominent
misinformation tweets. The analysis is presented and updated on a publically
accessible dashboard (https://usc-melady.github.io/COVID-19-Tweet-Analysis) to
track the nature of online discourse and misinformation about COVID-19 on
Twitter from March 1 - June 5, 2020. The dashboard provides a daily list of
identified misinformation tweets, along with topics, sentiments, and emerging
trends in the COVID-19 Twitter discourse. The dashboard is provided to improve
visibility into the nature and quality of information shared online, and
provide real-time access to insights and information extracted from the
dataset.","['Karishma Sharma', 'Sungyong Seo', 'Chuizheng Meng', 'Sirisha Rambhatla', 'Yan Liu']",5,0.78036404
"In digital environments where substantial amounts of information are shared
online, news headlines play essential roles in the selection and diffusion of
news articles. Some news articles attract audience attention by showing
exaggerated or misleading headlines. This study addresses the \textit{headline
incongruity} problem, in which a news headline makes claims that are either
unrelated or opposite to the contents of the corresponding article. We present
\textit{BaitWatcher}, which is a lightweight web interface that guides readers
in estimating the likelihood of incongruence in news articles before clicking
on the headlines. BaitWatcher utilizes a hierarchical recurrent encoder that
efficiently learns complex textual representations of a news headline and its
associated body text. For training the model, we construct a million scale
dataset of news articles, which we also release for broader research use. Based
on the results of a focus group interview, we discuss the importance of
developing an interpretable AI agent for the design of a better interface for
mitigating the effects of online misinformation.","['Kunwoo Park', 'Taegyun Kim', 'Seunghyun Yoon', 'Meeyoung Cha', 'Kyomin Jung']",4,0.7884922
"The objective of this work is to explore popular discourse about the COVID-19
pandemic and policies implemented to manage it. Using Natural Language
Processing, Text Mining, and Network Analysis to analyze corpus of tweets that
relate to the COVID-19 pandemic, we identify common responses to the pandemic
and how these responses differ across time. Moreover, insights as to how
information and misinformation were transmitted via Twitter, starting at the
early stages of this pandemic, are presented. Finally, this work introduces a
dataset of tweets collected from all over the world, in multiple languages,
dating back to January 22nd, when the total cases of reported COVID-19 were
below 600 worldwide. The insights presented in this work could help inform
decision makers in the face of future pandemics, and the dataset introduced can
be used to acquire valuable knowledge to help mitigate the COVID-19 pandemic.","['Christian E. Lopez', 'Malolan Vasu', 'Caleb Gallemore']",5,0.73899823
"In this paper, we present an updated version of the NELA-GT-2018 dataset
(N{\o}rregaard, Horne, and Adal{\i} 2019), entitled NELA-GT-2019. NELA-GT-2019
contains 1.12M news articles from 260 sources collected between January 1st
2019 and December 31st 2019. Just as with NELA-GT-2018, these sources come from
a wide range of mainstream news sources and alternative news sources. Included
with the dataset are source-level ground truth labels from 7 different
assessment sites covering multiple dimensions of veracity. The NELA-GT-2019
dataset can be found at: https://doi.org/10.7910/DVN/O7FWPO","['Maur√≠cio Gruppi', 'Benjamin D. Horne', 'Sibel Adalƒ±']",4,0.40855205
"Recent years have seen a marked increase in the spread of misinformation, a
phenomenon which has been accelerated and amplified by social media such as
Facebook and Twitter. While some actors spread misinformation to push a
specific agenda, it has also been widely documented that others aim to simply
disrupt the network by increasing disagreement and polarization across the
network and thereby destabilizing society. Popular social networks are also
vulnerable to large-scale attacks. Motivated by this reality, we introduce a
simple model of network disruption where an adversary can take over a limited
number of user profiles in a social network with the aim of maximizing
disagreement and/or polarization in the network.
  We investigate this model both theoretically and empirically. We show that
the adversary will always change the opinion of a taken-over profile to an
extreme in order to maximize disruption. We also prove that an adversary can
increase disagreement / polarization at most linearly in the number of user
profiles it takes over. Furthermore, we present a detailed empirical study of
several natural algorithms for the adversary on both synthetic networks and
real world (Reddit and Twitter) data sets. These show that even simple,
unsophisticated heuristics, such as targeting centrists, can disrupt a network
effectively, causing a large increase in disagreement / polarization. Studying
the problem of network disruption through the lens of an adversary thus
highlights the seriousness of the problem.","['Mayee F. Chen', 'Miklos Z. Racz']",2,0.68370014
"Nowadays fake news are heavily discussed in public and political debates.
Even though the phenomenon of intended false information is rather old,
misinformation reaches a new level with the rise of the internet and
participatory platforms. Due to Facebook and Co., purposeful false information
- often called fake news - can be easily spread by everyone. Because of a high
data volatility and variety in content types (text, images,...) debunking of
fake news is a complex challenge. This is especially true for automated
approaches, which are prone to fail validating the veracity of the information.
This work focuses on an a gamified approach to strengthen the resilience of
consumers towards fake news. The game FakeYou motivates its players to
critically analyze headlines regarding their trustworthiness. Further, the game
follows a ""learning by doing strategy"": by generating own fake headlines, users
should experience the concepts of convincing fake headline formulations. We
introduce the game itself, as well as the underlying technical infrastructure.
A first evaluation study shows, that users tend to use specific stylistic
devices to generate fake news. Further, the results indicate, that creating
good fakes and identifying correct headlines are challenging and hard to learn.","['Lena Clever', 'Dennis Assenmacher', 'Kilian M√ºller', 'Moritz Vinzent Seiler', 'Dennis M. Riehle', 'Mike Preuss', 'Christian Grimme']",4,0.82127297
"At the time of this writing, the novel coronavirus (COVID-19) pandemic
outbreak has already put tremendous strain on many countries' citizens,
resources and economies around the world. Social distancing measures, travel
bans, self-quarantines, and business closures are changing the very fabric of
societies worldwide. With people forced out of public spaces, much conversation
about these phenomena now occurs online, e.g., on social media platforms like
Twitter. In this paper, we describe a multilingual coronavirus (COVID-19)
Twitter dataset that we have been continuously collecting since January 22,
2020. We are making our dataset available to the research community
(https://github.com/echen102/COVID-19-TweetIDs). It is our hope that our
contribution will enable the study of online conversation dynamics in the
context of a planetary-scale epidemic outbreak of unprecedented proportions and
implications. This dataset could also help track scientific coronavirus
misinformation and unverified rumors, or enable the understanding of fear and
panic -- and undoubtedly more. Ultimately, this dataset may contribute towards
enabling informed solutions and prescribing targeted policy interventions to
fight this global crisis.","['Emily Chen', 'Kristina Lerman', 'Emilio Ferrara']",5,0.7710521
"Background: The COVID-19 pandemic has uncovered the potential of digital
misinformation in shaping the health of nations. The deluge of unverified
information that spreads faster than the epidemic itself is an unprecedented
phenomenon that has put millions of lives in danger. Mitigating this Infodemic
requires strong health messaging systems that are engaging, vernacular,
scalable, effective and continuously learn the new patterns of misinformation.
  Objective: We created WashKaro, a multi-pronged intervention for mitigating
misinformation through conversational AI, machine translation and natural
language processing. WashKaro provides the right information matched against
WHO guidelines through AI, and delivers it in the right format in local
languages.
  Methods: We theorize (i) an NLP based AI engine that could continuously
incorporate user feedback to improve relevance of information, (ii) bite sized
audio in the local language to improve penetrance in a country with skewed
gender literacy ratios, and (iii) conversational but interactive AI engagement
with users towards an increased health awareness in the community. Results: A
total of 5026 people who downloaded the app during the study window, among
those 1545 were active users. Our study shows that 3.4 times more females
engaged with the App in Hindi as compared to males, the relevance of
AI-filtered news content doubled within 45 days of continuous machine learning,
and the prudence of integrated AI chatbot Satya increased thus proving the
usefulness of an mHealth platform to mitigate health misinformation.
  Conclusion: We conclude that a multi-pronged machine learning application
delivering vernacular bite-sized audios and conversational AI is an effective
approach to mitigate health misinformation.","['Rohan Pandey', 'Vaibhav Gautam', 'Ridam Pal', 'Harsh Bandhey', 'Lovedeep Singh Dhingra', 'Himanshu Sharma', 'Chirag Jain', 'Kanav Bhagat', 'Arushi', 'Lajjaben Patel', 'Mudit Agarwal', 'Samprati Agrawal', 'Rishabh Jalan', 'Akshat Wadhwa', 'Ayush Garg', 'Vihaan Misra', 'Yashwin Agrawal', 'Bhavika Rana', 'Ponnurangam Kumaraguru', 'Tavpritesh Sethi']",9,0.6886735
"Polarization is a growing, global problem. As such, many social media based
solutions have been proposed in order to reduce it. In this study, we propose a
new solution that recommends topics to celebrities to encourage them to join a
polarized debate and increase exposure to contrarian content - bursting the
filter bubble. Using a state-of-the art model that quantifies the degree of
polarization, this paper makes a first attempt to empirically answer the
question: Can celebrities burst filter bubbles? We use a case study to analyze
how people react when celebrities are involved in a controversial topic and
conclude with a list possible research directions.","['Tuƒürulcan Elmas', 'Kristina Hardi', 'Rebekah Overdorf', 'Karl Aberer']",3,0.5077942
"Fake news is a growing problem in the last years, especially during
elections. It's hard work to identify what is true and what is false among all
the user generated content that circulates every day. Technology can help with
that work and optimize the fact-checking process. In this work, we address the
challenge of finding similar content in order to be able to suggest to a
fact-checker articles that could have been verified before and thus avoid that
the same information is verified more than once. This is especially important
in collaborative approaches to fact-checking where members of large teams will
not know what content others have already fact-checked.","['Caio Almeida', 'D√©bora Santos']",4,0.62849104
"In this paper, we present a resource allocation mechanism for the problem of
incentivizing filtering among a finite number of strategic social media
platforms. We consider the presence of a strategic government and private
knowledge of how misinformation affects the users of the social media
platforms. Our proposed mechanism incentivizes social media platforms to filter
misleading information efficiently, and thus indirectly prevents the spread of
fake news. In particular, we design an economically inspired mechanism that
strongly implements all generalized Nash equilibria for efficient filtering of
misleading information in the induced game. We show that our mechanism is
individually rational, budget balanced, while it has at least one equilibrium.
Finally, we show that for quasi-concave utilities and constraints, our
mechanism admits a generalized Nash equilibrium and implements a Pareto
efficient solution.","['Aditya Dave', 'Ioannis Vasileios Chremos', 'Andreas A. Malikopoulos']",2,0.59461915
"The increasing popularity of social media promotes the proliferation of fake
news, which has caused significant negative societal effects. Therefore, fake
news detection on social media has recently become an emerging research area of
great concern. With the development of multimedia technology, fake news
attempts to utilize multimedia content with images or videos to attract and
mislead consumers for rapid dissemination, which makes visual content an
important part of fake news. Despite the importance of visual content, our
understanding of the role of visual content in fake news detection is still
limited. This chapter presents a comprehensive review of the visual content in
fake news, including the basic concepts, effective visual features,
representative detection methods and challenging issues of multimedia fake news
detection. This chapter can help readers to understand the role of visual
content in fake news detection, and effectively utilize visual content to
assist in detecting multimedia fake news.","['Juan Cao', 'Peng Qi', 'Qiang Sheng', 'Tianyun Yang', 'Junbo Guo', 'Jintao Li']",4,0.7529831
"We address the diffusion of information about the COVID-19 with a massive
data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze
engagement and interest in the COVID-19 topic and provide a differential
assessment on the evolution of the discourse on a global scale for each
platform and their users. We fit information spreading with epidemic models
characterizing the basic reproduction numbers $R_0$ for each social media
platform. Moreover, we characterize information spreading from questionable
sources, finding different volumes of misinformation in each platform. However,
information from both reliable and questionable sources do not present
different spreading patterns. Finally, we provide platform-dependent numerical
estimates of rumors' amplification.","['Matteo Cinelli', 'Walter Quattrociocchi', 'Alessandro Galeazzi', 'Carlo Michele Valensise', 'Emanuele Brugnoli', 'Ana Lucia Schmidt', 'Paola Zola', 'Fabiana Zollo', 'Antonio Scala']",5,0.6471074
"An identity denotes the role an individual or a group plays in highly
differentiated contemporary societies. In this paper, our goal is to classify
Twitter users based on their role identities. We first collect a coarse-grained
public figure dataset automatically, then manually label a more fine-grained
identity dataset. We propose a hierarchical self-attention neural network for
Twitter user role identity classification. Our experiments demonstrate that the
proposed model significantly outperforms multiple baselines. We further propose
a transfer learning scheme that improves our model's performance by a large
margin. Such transfer learning also greatly reduces the need for a large amount
of human labeled data.","['Binxuan Huang', 'Kathleen M. Carley']",2,0.59267545
"In the age of social media, disasters and epidemics usher not only a
devastation and affliction in the physical world, but also prompt a deluge of
information, opinions, prognoses and advice to billions of internet users. The
coronavirus epidemic of 2019-2020, or COVID-19, is no exception, with the World
Health Organization warning of a possible ""infodemic"" of fake news. In this
study, we examine the alternative narratives around the coronavirus outbreak
through advertisements promoted on Facebook, the largest social media platform
in the US. Using the new Facebook Ads Library, we discover advertisers from
public health and non-profit sectors, alongside those from news media,
politics, and business, incorporating coronavirus into their messaging and
agenda. We find the virus used in political attacks, donation solicitations,
business promotion, stock market advice, and animal rights campaigning. Among
these, we find several instances of possible misinformation, ranging from
bioweapons conspiracy theories to unverifiable claims by politicians. As we
make the dataset available to the community, we hope the advertising domain
will become an important part of quality control for public health
communication and public discourse in general.","['Yelena Mejova', 'Kyriaki Kalimeri']",5,0.696548
"Community-aware centrality is an emerging research area in network science
concerned with the importance of nodes in relation to community structure.
Measures are a function of a network's structure and a given partition.
Previous approaches extend classical centrality measures to account for
community structure with little connection to community detection theory. In
contrast, we propose cluster-quality vitality measures, i.e., modularity
vitality, a community-aware measure which is well-grounded in both centrality
and community detection theory. Modularity vitality quantifies positive and
negative contributions to community structure, which indicate a node's role as
a community bridge or hub. We derive a computationally efficient method of
calculating modularity vitality for all nodes in O(M + NC) time, where C is the
number of communities. We systematically fragment networks by removing central
nodes, and find that modularity vitality consistently outperforms existing
community-aware centrality measures. Modularity vitality is over 8 times more
effective than the next-best method on a million-node infrastructure network.
This result does not generalize to social media communication networks, which
exhibit extreme robustness to all community-aware centrality attacks. This
robustness suggests that user-based interventions to mitigate misinformation
diffusion will be ineffective. Finally, we demonstrate that modularity vitality
provides a new approach to community-deception.","['Thomas Magelinski', 'Mihovil Bartulovic', 'Kathleen M. Carley']",2,0.6508966
"As online social networks continue to be commonly used for the dissemination
of information to the public, understanding the phenomena that govern
information diffusion is crucial for many security and safety-related
applications, such as maximizing information spread and misinformation
containment during crises and natural disasters. In this study, we hypothesize
that the features that contribute to information diffusion in online social
networks are significantly influenced by the type of event being studied. We
classify Twitter events as either informative or trending and then explore the
node-to-node influence dynamics associated with information spread. We build a
model based on Bayesian Logistic Regression for learning and prediction and
Random Forests for feature selection. Experimental results from real-world data
sets show that the proposed model outperforms state-of-the-art diffusion
prediction models, achieving 93% accuracy in informative events and 86% in
trending events. We observed that the models for informative and trending
events differ significantly, both in the diffusion process and in the user
features that govern the diffusion. Our findings show that followers play an
important role in the diffusion process and it is possible to use the diffusion
and OSN behavior of users for predicting the trending character of a message
without having to count the number of reactions.","['Abiola Osho', 'Colin Goodman', 'George Amariucai']",2,0.67947215
"With the increasing use of online social networks as a source of news and
information, the propensity for a rumor to disseminate widely and quickly poses
a great concern, especially in disaster situations where users do not have
enough time to fact-check posts before making the informed decision to react to
a post that appears to be credible. In this study, we explore the propagation
pattern of rumors on Twitter by exploring the dynamics of microscopic-level
misinformation spread, based on the latent message and user interaction
attributes. We perform supervised learning for feature selection and
prediction. Experimental results with real-world data sets give the models'
prediction accuracy at about 90\% for the diffusion of both True and False
topics. Our findings confirm that rumor cascades run deeper and that rumor
masked as news, and messages that incite fear, will diffuse faster than other
messages. We show that the models for True and False message propagation differ
significantly, both in the prediction parameters and in the message features
that govern the diffusion. Finally, we show that the diffusion pattern is an
important metric in identifying the credibility of a tweet.","['Abiola Osho', 'Caden Waters', 'George Amariucai']",4,0.7436082
"Machine learning based language models have recently made significant
progress, which introduces a danger to spread misinformation. To combat this
potential danger, several methods have been proposed for detecting text written
by these language models. This paper presents two classes of black-box attacks
on these detectors, one which randomly replaces characters with homoglyphs, and
the other a simple scheme to purposefully misspell words. The homoglyph and
misspelling attacks decrease a popular neural text detector's recall on neural
text from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that
the attacks are transferable to other neural text detectors.","['Max Wolff', 'Stuart Wolff']",8,0.62266976
"Since its inception, Facebook has become an integral part of the online
social community. People rely on Facebook to make connections with others and
build communities. As a result, it is paramount to protect the integrity of
such a rapidly growing network in a fast and scalable manner. In this paper, we
present our efforts to protect various social media entities at Facebook from
people who try to abuse our platform. We present a novel Temporal Interaction
EmbeddingS (TIES) model that is designed to capture rogue social interactions
and flag them for further suitable actions. TIES is a supervised, deep
learning, production ready model at Facebook-scale networks. Prior works on
integrity problems are mostly focused on capturing either only static or
certain dynamic features of social entities. In contrast, TIES can capture both
these variant behaviors in a unified model owing to the recent strides made in
the domains of graph embedding and deep sequential pattern learning. To show
the real-world impact of TIES, we present a few applications especially for
preventing spread of misinformation, fake account detection, and reducing ads
payment risks in order to enhance the platform's integrity.","['Nima Noorshams', 'Saurabh Verma', 'Aude Hofleitner']",3,0.6263472
"Artificial neural networks are well-known to be susceptible to catastrophic
forgetting when continually learning from sequences of tasks. Various continual
(or ""incremental"") learning approaches have been proposed to avoid catastrophic
forgetting, but they are typically adversary agnostic, i.e., they do not
consider the possibility of a malicious attack. In this effort, we explore the
vulnerability of Elastic Weight Consolidation (EWC), a popular continual
learning algorithm for avoiding catastrophic forgetting. We show that an
intelligent adversary can bypass the EWC's defenses, and instead cause gradual
and deliberate forgetting by introducing small amounts of misinformation to the
model during training. We demonstrate such an adversary's ability to assume
control of the model via injection of ""backdoor"" attack samples on both
permuted and split benchmark variants of the MNIST dataset. Importantly, once
the model has learned the adversarial misinformation, the adversary can then
control the amount of forgetting of any task. Equivalently, the malicious actor
can create a ""false memory"" about any task by inserting carefully-designed
backdoor samples to any fraction of the test instances of that task. Perhaps
most damaging, we show this vulnerability to be very acute; neural network
memory can be easily compromised with the addition of backdoor samples into as
little as 1% of the training data of even a single task.","['Muhammad Umer', 'Glenn Dawson', 'Robi Polikar']",2,0.6882276
"Numerous real-world systems, for instance, the communication platforms and
transportation systems, can be abstracted into complex networks. Containing
spreading dynamics (e.g., epidemic transmission and misinformation propagation)
in networked systems is a hot topic in multiple fronts. Most of the previous
strategies are based on the immunization of nodes. However, sometimes, these
node--based strategies can be impractical. For instance, in the train
transportation networks, it is dramatic to isolating train stations for flu
prevention. On the contrary, temporarily suspending some connections between
stations is more acceptable. Thus, we pay attention to the edge-based
containing strategy. In this study, we develop a theoretical framework to find
the optimal edge for containing the spreading of the
susceptible-infected-susceptible model on complex networks. In specific, by
performing a perturbation method to the discrete-Markovian-chain equations of
the SIS model, we derive a formula that approximately provides the decremental
outbreak size after the deactivation of a certain edge in the network. Then, we
determine the optimal edge by simply choosing the one with the largest
decremental outbreak size. Note that our proposed theoretical framework
incorporates the information of both network structure and spreading dynamics.
Finally, we test the performance of our method by extensive numerical
simulations. Results demonstrate that our strategy always outperforms other
strategies based only on structural properties (degree or edge betweenness
centrality). The theoretical framework in this study can be extended to other
spreading models and offers inspirations for further investigations on
edge-based immunization strategies.","['Jiajun Xian', 'Dan Yang', 'Liming Pan', 'Wei Wang']",2,0.82630634
"Misinformation spread presents a technological and social threat to society.
With the advance of AI-based language models, automatically generated texts
have become difficult to identify and easy to create at scale. We present ""The
Rumour Mill"", a playful art piece, designed as a commentary on the spread of
rumours and automatically-generated misinformation. The mill is a tabletop
interactive machine, which invites a user to experience the process of creating
believable text by interacting with different tangible controls on the mill.
The user manipulates visible parameters to adjust the genre and type of an
automatically generated text rumour. The Rumour Mill is a physical
demonstration of the state of current technology and its ability to generate
and manipulate natural language text, and of the act of starting and spreading
rumours.","['Nanna Inie', 'Jeanette Falk Olesen', 'Leon Derczynski']",8,0.5431948
"Some consider large-scale language models that can generate long and coherent
pieces of text as dangerous, since they may be used in misinformation
campaigns. Here we formulate large-scale language model output detection as a
hypothesis testing problem to classify text as genuine or generated. We show
that error exponents for particular language models are bounded in terms of
their perplexity, a standard measure of language generation performance. Under
the assumption that human language is stationary and ergodic, the formulation
is extended from considering specific language models to considering maximum
likelihood language models, among the class of k-order Markov approximations;
error probabilities are characterized. Some discussion of incorporating
semantic side information is also given.","['Lav R. Varshney', 'Nitish Shirish Keskar', 'Richard Socher']",8,0.7126736
"Recent work in the domain of misinformation detection has leveraged rich
signals in the text and user identities associated with content on social
media. But text can be strategically manipulated and accounts reopened under
different aliases, suggesting that these approaches are inherently brittle. In
this work, we investigate an alternative modality that is naturally robust: the
pattern in which information propagates. Can the veracity of an unverified
rumor spreading online be discerned solely on the basis of its pattern of
diffusion through the social network?
  Using graph kernels to extract complex topological information from Twitter
cascade structures, we train accurate predictive models that are blind to
language, user identities, and time, demonstrating for the first time that such
""sanitized"" diffusion patterns are highly informative of veracity. Our results
indicate that, with proper aggregation, the collective sharing pattern of the
crowd may reveal powerful signals of rumor truth or falsehood, even in the
early stages of propagation.","['Nir Rosenfeld', 'Aron Szanto', 'David C. Parkes']",2,0.7093893
"Region visual features enhance the generative capability of the machines
based on features, however they lack proper interaction attentional perceptions
and thus ends up with biased or uncorrelated sentences or pieces of
misinformation. In this work, we propose Attribute Interaction-Tensor Product
Representation (aiTPR) which is a convenient way of gathering more information
through orthogonal combination and learning the interactions as physical
entities (tensors) and improving the captions. Compared to previous works,
where features are added up to undefined feature spaces, TPR helps in
maintaining sanity in combinations and orthogonality helps in defining familiar
spaces. We have introduced a new concept layer that defines the objects and
also their interactions that can play a crucial role in determination of
different descriptions. The interaction portions have contributed heavily for
better caption quality and has out-performed different previous works on this
domain and MSCOCO dataset. We introduced, for the first time, the notion of
combining regional image features and abstracted interaction likelihood
embedding for image captioning.",['Chiranjib Sur'],7,0.5500783
"In the Social Web scenario, large amounts of User-Generated Content (UGC) are
diffused through social media often without almost any form of traditional
trusted intermediaries. Therefore, the risk of running into misinformation is
not negligible. For this reason, assessing and mining the credibility of online
information constitutes nowadays a fundamental research issue. Credibility,
also referred as believability, is a quality perceived by individuals, who are
not always able to discern, with their own cognitive capacities, genuine
information from fake one. Hence, in the last years, several approaches have
been proposed to automatically assess credibility in social media. Many of them
are based on data-driven models, i.e., they employ machine learning techniques
to identify misinformation, but recently also model-driven approaches are
emerging, as well as graph-based approaches focusing on credibility
propagation, and knowledge-based ones exploiting Semantic Web technologies.
Three of the main contexts in which the assessment of information credibility
has been investigated concern: (i) the detection of opinion spam in review
sites, (ii) the detection of fake news in microblogging, and (iii) the
credibility assessment of online health-related information. In this article,
the main issues connected to the evaluation of information credibility in the
Social Web, which are shared by the above-mentioned contexts, are discussed. A
concise survey of the approaches and methodologies that have been proposed in
recent years to address these issues is also presented.","['Gabriella Pasi', 'Marco Viviani']",0,0.77588904
"Over the past few years, we have observed different media outlets' attempts
to shift public opinion by framing information to support a narrative that
facilitate their goals. Malicious users referred to as ""pathogenic social
media"" (PSM) accounts are more likely to amplify this phenomena by spreading
misinformation to viral proportions. Understanding the spread of misinformation
from account-level perspective is thus a pressing problem. In this work, we aim
to present a feature-driven approach to detect PSM accounts in social media.
Inspired by the literature, we set out to assess PSMs from three broad
perspectives: (1) user-related information (e.g., user activity, profile
characteristics), (2) source-related information (i.e., information linked via
URLs shared by users) and (3) content-related information (e.g., tweets
characteristics). For the user-related information, we investigate malicious
signals using causality analysis (i.e., if user is frequently a cause of viral
cascades) and profile characteristics (e.g., number of followers, etc.). For
the source-related information, we explore various malicious properties linked
to URLs (e.g., URL address, content of the associated website, etc.). Finally,
for the content-related information, we examine attributes (e.g., number of
hashtags, suspicious hashtags, etc.) from tweets posted by users. Experiments
on real-world Twitter data from different countries demonstrate the
effectiveness of the proposed approach in identifying PSM users.","['Hamidreza Alvari', 'Ghazaleh Beigi', 'Soumajyoti Sarkar', 'Scott W. Ruston', 'Steven R. Corman', 'Hasan Davulcu', 'Paulo Shakarian']",3,0.70922345
"As robots become more prevalent, the importance of the field of human-robot
interaction (HRI) grows accordingly. As such, we should endeavor to employ the
best statistical practices. Likert scales are commonly used metrics in HRI to
measure perceptions and attitudes. Due to misinformation or honest mistakes,
most HRI researchers do not adopt best practices when analyzing Likert data. We
conduct a review of psychometric literature to determine the current standard
for Likert scale design and analysis. Next, we conduct a survey of four years
of the International Conference on Human-Robot Interaction (2016 through 2019)
and report on incorrect statistical practices and design of Likert scales.
During these years, only 3 of the 110 papers applied proper statistical testing
to correctly-designed Likert scales. Our analysis suggests there are areas for
meaningful improvement in the design and testing of Likert scales. Lastly, we
provide recommendations to improve the accuracy of conclusions drawn from
Likert data.","['Mariah L. Schrum', 'Michael Johnson', 'Muyleng Ghuy', 'Matthew C. Gombolay']",8,0.52655506
"In recent years, disinformation including fake news, has became a global
phenomenon due to its explosive growth, particularly on social media. The wide
spread of disinformation and fake news can cause detrimental societal effects.
Despite the recent progress in detecting disinformation and fake news, it is
still non-trivial due to its complexity, diversity, multi-modality, and costs
of fact-checking or annotation. The goal of this chapter is to pave the way for
appreciating the challenges and advancements via: (1) introducing the types of
information disorder on social media and examine their differences and
connections; (2) describing important and emerging tasks to combat
disinformation for characterization, detection and attribution; and (3)
discussing a weak supervision approach to detect disinformation with limited
labeled data. We then provide an overview of the chapters in this book that
represent the recent advancements in three related parts: (1) user engagements
in the dissemination of information disorder; (2) techniques on detecting and
mitigating disinformation; and (3) trending issues such as ethics, blockchain,
clickbaits, etc. We hope this book to be a convenient entry point for
researchers, practitioners, and students to understand the problems and
challenges, learn state-of-the-art solutions for their specific needs, and
quickly identify new research problems in their domains.","['Kai Shu', 'Suhang Wang', 'Dongwon Lee', 'Huan Liu']",0,0.82645905
"Background. In Italy, in recent years, vaccination coverage for key
immunizations as MMR has been declining to worryingly low levels. In 2017, the
Italian Gov't expanded the number of mandatory immunizations introducing
penalties to unvaccinated children's families. During the 2018 general
elections campaign, immunization policy entered the political debate with the
Gov't in charge blaming oppositions for fuelling vaccine scepticism. A new
Gov't established in 2018 temporarily relaxed penalties. Objectives and
Methods. Using a sentiment analysis on tweets posted in Italian during 2018, we
aimed to: (i) characterize the temporal flow of vaccines communication on
Twitter (ii) evaluate the polarity of vaccination opinions and usefulness of
Twitter data to estimate vaccination parameters, and (iii) investigate whether
the contrasting announcements at the highest political level might have
originated disorientation amongst the Italian public. Results. Vaccine-relevant
tweeters interactions peaked in response to main political events. Out of
retained tweets, 70.0% resulted favourable to vaccination, 16.5% unfavourable,
and 13.6% undecided, respectively. The smoothed time series of polarity
proportions exhibit frequent large changes in the favourable proportion,
enhanced by an up and down trend synchronized with the switch between gov't
suggesting evidence of disorientation among the public. Conclusion. The
reported evidence of disorientation documents that critical immunization
topics, should never be used for political consensus. This is especially true
given the increasing role of online social media as information source, which
might yield to social pressures eventually harmful for vaccine uptake, and is
worsened by the lack of institutional presence on Twitter, calling for efforts
to contrast misinformation and the ensuing spread of hesitancy.","['Samantha Ajovalasit', 'Veronica Dorgali', 'Angelo Mazza', ""Alberto D'Onofrio"", 'Piero Manfredi']",12,0.82775617
"Patients increasingly turn to search engines and online content before, or in
place of, talking with a health professional. Low quality health information,
which is common on the internet, presents risks to the patient in the form of
misinformation and a possibly poorer relationship with their physician. To
address this, the DISCERN criteria (developed at University of Oxford) are used
to evaluate the quality of online health information. However, patients are
unlikely to take the time to apply these criteria to the health websites they
visit. We built an automated implementation of the DISCERN instrument (Brief
version) using machine learning models. We compared the performance of a
traditional model (Random Forest) with that of a hierarchical encoder
attention-based neural network (HEA) model using two language embeddings, BERT
and BioBERT. The HEA BERT and BioBERT models achieved average F1-macro scores
across all criteria of 0.75 and 0.74, respectively, outperforming the Random
Forest model (average F1-macro = 0.69). Overall, the neural network based
models achieved 81% and 86% average accuracy at 100% and 80% coverage,
respectively, compared to 94% manual rating accuracy. The attention mechanism
implemented in the HEA architectures not only provided 'model explainability'
by identifying reasonable supporting sentences for the documents fulfilling the
Brief DISCERN criteria, but also boosted F1 performance by 0.05 compared to the
same architecture without an attention mechanism. Our research suggests that it
is feasible to automate online health information quality assessment, which is
an important step towards empowering patients to become informed partners in
the healthcare process.","['Laura Kinkead', 'Ahmed Allam', 'Michael Krauthammer']",1,0.5490572
"We present some initial results from a case study in social media data
harvesting and visualization utilizing the tools and analytical features of
NodeXL applied to a degree asymmetric vertex graph set. We consider twitter
graphs harvested for topics related to suicidal ideation, suicide attempts,
self-harm and bullycide. While the twitter-sphere only captures a small and age
biased sample of communications it is a readily available public database for a
wealth of rich topics yielding a large sample set. All these topics gave rise
to highly asymmetric vertex degree graphs and all shared the same general
topological features. We find a strong preference for in degree vertex
information transfer with a 4:25 out degree to in degree vertex ratio with a
power law distribution. Overall there is a low global clustering coefficient
average of 0.038 and a graph clustering density of 0.00034 for
Clauset-Newman-Moore grouping with a maximum geodesic distance of 6.
Eigenvector centrality does not give any large central impact vertices and
betweenness centrality shows many bridging vertices indicating a sparse
community structure. Parts of speech sentiment scores show a strong asymmetry
of predominant negative scores for almost all word and word pairs with salience
greater than one. We used an Hoaxy analysis to check for deliberate
misinformation on these topics by a Twitter-Bot.","['Keith Andrew', 'Eric Steinfelds', 'Karla M. Andrew', 'Kay Opalenik']",2,0.6229774
"With the increasing growth of social media, people have started relying
heavily on the information shared therein to form opinions and make decisions.
While such a reliance is motivation for a variety of parties to promote
information, it also makes people vulnerable to exploitation by slander,
misinformation, terroristic and predatorial advances. In this work, we aim to
understand and detect such attempts at persuasion. Existing works on detecting
persuasion in text make use of lexical features for detecting persuasive
tactics, without taking advantage of the possible structures inherent in the
tactics used. We formulate the task as a multi-class classification problem and
propose an unsupervised, domain-independent machine learning framework for
detecting the type of persuasion used in text, which exploits the inherent
sentence structure present in the different persuasion tactics. Our work shows
promising results as compared to existing work.","['Rahul Radhakrishnan Iyer', 'Katia Sycara']",0,0.70238113
"The reported work points at developing a practical approach for power
transmission planners to secure power networks from potential deliberate
attacks. We study the interaction between a system planner (defender) and a
rational attacker who threatens the operation of the power grid. In addition to
the commonly used hardening strategy for protecting the network, a new sort of
resource is introduced under the deception concept. Feint and deception are
acknowledged as effective tools for misleading the attacker in strategic
planning. To this end, the defender deception is mathematically formulated by
releasing misinformation about his plan in the shared cognition-based model. To
reduce the risk of damage in case of deception failure, preemptive-goal
programming is utilized to prioritize the hardening strategy for the vital
components. Furthermore, the value of posturing is introduced which is the
benefits that the deception brings to the system. The problems are formulated
as tri-level mixed-integer linear programming and solved by the
constraint-and-column generation method. Comprehensive simulation studies
performed on WSCC 9-bus and IEEE 118-bus systems indicate how the defender will
save significant cost from protecting his network with posturing rather than
hardening and the proposed approach is a promising development to ensure the
secure operation of power networks.","['Hamzeh Davarikia', 'Masoud Barati', 'Mustafa Al-Assad', 'Yupo Chan']",2,0.6794135
"Allowing machines to choose whether to kill humans would be devastating for
world peace and security. But how do we equip machines with the ability to
learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying
machine learning to human texts can extract deontological ethical reasoning
about ""right"" and ""wrong"" conduct by calculating a moral bias score on a
sentence level using sentence embeddings. The machine learned that it is
objectionable to kill living beings, but it is fine to kill time; It is
essential to eat, yet one might not eat dirt; it is important to spread
information, yet one should not spread misinformation. However, the evaluated
moral bias was restricted to simple actions -- one verb -- and a ranking of
actions with surrounding context. Recently BERT ---and variants such as RoBERTa
and SBERT--- has set a new state-of-the-art performance for a wide range of NLP
tasks. But has BERT also a better moral compass? In this paper, we discuss and
show that this is indeed the case. Thus, recent improvements of language
representations also improve the representation of the underlying ethical and
moral values of the machine. We argue that through an advanced semantic
representation of text, BERT allows one to get better insights of moral and
ethical values implicitly represented in text. This enables the Moral Choice
Machine (MCM) to extract more accurate imprints of moral choices and ethical
values.","['Patrick Schramowski', 'Cigdem Turan', 'Sophie Jentzsch', 'Constantin Rothkopf', 'Kristian Kersting']",9,0.52056694
"Online social networks provide a convenient platform for the spread of
rumors, which could lead to serious aftermaths such as economic losses and
public panic. The classical rumor blocking problem aims to launch a set of
nodes as a positive cascade to compete with misinformation in order to limit
the spread of rumors. However, most of the related researches were based on
one-dimensional diffusion model. In reality, there are more than one feature
associated with an object. The user's impression on this object is determined
not just by one feature but by his/her overall evaluation on all of these
features. Thus, the influence spread of this object can be decomposed into the
spread of multiple features. Based on that, we propose a Multi-Feature
diffusion model (MF-model) in this paper, and a novel problem, Multi-Feature
Rumor Blocking (MFRB), is formulated on a multi-layer network structure
according to this model. To solve MFRB, we design a creative sampling method,
called Multi-Sampling, which can be applied to a multi-layer network structure.
Inspired by martingale analysis, the Revised-IMM algorithm is proposed, and
returns a satisfactory approximate solution to MFRB. Finally, we evaluate our
proposed algorithm by conducting experiments on real datasets, and show the
effectiveness and accuracy of the Revised-IMM algorithm and significantly
outperforms other baseline algorithms.","['Jianxiong Guo', 'Tiantian Chen', 'Weili Wu']",2,0.81476796
"Thanks to the fast progress in synthetic media generation, creating realistic
false images has become very easy. Such images can be used to wrap ""rich"" fake
news with enhanced credibility, spawning a new wave of high-impact, high-risk
misinformation campaigns. Therefore, there is a fast-growing interest in
reliable detectors of manipulated media. The most powerful detectors, to date,
rely on the subtle traces left by any device on all images acquired by it. In
particular, due to proprietary in-camera processes, like demosaicing or
compression, each camera model leaves trademark traces that can be exploited
for forensic analyses. The absence or distortion of such traces in the target
image is a strong hint of manipulation. In this paper, we challenge such
detectors to gain better insight into their vulnerabilities. This is an
important study in order to build better forgery detectors able to face
malicious attacks. Our proposal consists of a GAN-based approach that injects
camera traces into synthetic images. Given a GAN-generated image, we insert the
traces of a specific camera model into it and deceive state-of-the-art
detectors into believing the image was acquired by that model. Likewise, we
deceive independent detectors of synthetic GAN images into believing the image
is real. Experiments prove the effectiveness of the proposed method in a wide
array of conditions. Moreover, no prior information on the attacked detectors
is needed, but only sample images from the target camera.","['Davide Cozzolino', 'Justus Thies', 'Andreas R√∂ssler', 'Matthias Nie√üner', 'Luisa Verdoliva']",7,0.7934294
"With the prevalence of misinformation online, researchers have focused on
developing various machine learning algorithms to detect fake news. However,
users' perception of machine learning outcomes and related behaviors have been
widely ignored. Hence, this paper proposed to bridge this gap by studying how
to pass the detection results of machine learning to the users, and aid their
decisions in handling misinformation. An online experiment was conducted, to
evaluate the effect of the proposed machine learning warning sign against a
control condition. We examined participants' detection and sharing of news. The
data showed that warning sign's effects on participants' trust toward the fake
news were not significant. However, we found that people's uncertainty about
the authenticity of the news dropped with the presence of the machine learning
warning sign. We also found that social media experience had effects on users'
trust toward the fake news, and age and social media experience had effects on
users' sharing decision. Therefore, the results indicate that there are many
factors worth studying that affect people's trust in the news. Moreover, the
warning sign in communicating machine learning detection results is different
from ordinary warnings and needs more detailed research and design. These
findings hold important implications for the design of machine learning
warnings.",['Limeng Cui'],0,0.7154694
"With the pervasiveness of online media data as a source of information
verifying the validity of this information is becoming even more important yet
quite challenging. Rumors spread a large quantity of misinformation on
microblogs. In this study we address two common issues within the context of
microblog social media. First we detect rumors as a type of misinformation
propagation and next we go beyond detection to perform the task of rumor
classification. WE explore the problem using a standard data set. We devise
novel features and study their impact on the task. We experiment with various
levels of preprocessing as a precursor of the classification as well as
grouping of features. We achieve and f-measure of over 0.82 in RDC task in
mixed rumors data set and 84 percent in a single rumor data set using a
two-step classification approach.","['Sardar Hamidian', 'Mona T Diab']",4,0.6665405
"In this modern era, communication has become faster and easier. This means
fallacious information can spread as fast as reality. Considering the damage
that fake news kindles on the psychology of people and the fact that such news
proliferates faster than truth, we need to study the phenomenon that helps
spread fake news. An unbiased data set that depends on reality for rating news
is necessary to construct predictive models for its classification. This paper
describes the methodology to create such a data set. We collect our data from
snopes.com which is a fact-checking organization. Furthermore, we intend to
create this data set not only for classification of the news but also to find
patterns that reason the intent behind misinformation. We also formally define
an Internet Claim, its credibility, and the sentiment behind such a claim. We
try to realize the relationship between the sentiment of a claim with its
credibility. This relationship pours light on the bigger picture behind the
propagation of misinformation. We pave the way for further research based on
the methodology described in this paper to create the data set and usage of
predictive modeling along with research-based on psychology/mentality of people
to understand why fake news spreads much faster than reality.","['Amey Parundekar', 'Susan Elias', 'Ashwin Ashok']",4,0.7538326
"As the decade turns, we reflect on nearly thirty years of successful
manipulation of the world's public equity markets. This reflection highlights a
few of the key enabling ingredients and lessons learned along the way. A
quantitative understanding of market impact and its decay, which we cover
briefly, lets you move long-term market prices to your advantage at acceptable
cost. Hiding your footprints turns out to be less important than moving prices
in the direction most people want them to move. Widespread (if misplaced) trust
of market prices -- buttressed by overestimates of the cost of manipulation and
underestimates of the benefits to certain market participants -- makes price
manipulation a particularly valuable and profitable tool. Of the many recent
stories heralding the dawn of the present golden age of misinformation, the
manipulation leading to the remarkable increase in the market capitalization of
the world's publicly traded companies over the past three decades is among the
best.",['Bruce Knuteson'],0,0.47391763
"Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which
allows a data-limited adversary with no knowledge of the training dataset to
clone the functionality of a target model, just by using black-box query
access. Such attacks are typically carried out by querying the target model
using inputs that are synthetically generated or sampled from a surrogate
dataset to construct a labeled dataset. The adversary can use this labeled
dataset to train a clone model, which achieves a classification accuracy
comparable to that of the target model. We propose ""Adaptive Misinformation"" to
defend against such model stealing attacks. We identify that all existing model
stealing attacks invariably query the target model with Out-Of-Distribution
(OOD) inputs. By selectively sending incorrect predictions for OOD queries, our
defense substantially degrades the accuracy of the attacker's clone model (by
up to 40%), while minimally impacting the accuracy (<0.5%) for benign users.
Compared to existing defenses, our defense has a significantly better security
vs accuracy trade-off and incurs minimal computational overhead.","['Sanjay Kariyappa', 'Moinuddin K Qureshi']",1,0.6109135
"Research in combating misinformation reports many negative results: facts may
not change minds, especially if they come from sources that are not trusted.
Individuals can disregard and justify lies told by trusted sources. This
problem is made even worse by social recommendation algorithms which help
amplify conspiracy theories and information confirming one's own biases due to
companies' efforts to optimize for clicks and watch time over individuals' own
values and public good. As a result, more nuanced voices and facts are drowned
out by a continuous erosion of trust in better information sources. Most
misinformation mitigation techniques assume that discrediting, filtering, or
demoting low veracity information will help news consumers make better
information decisions. However, these negative results indicate that some news
consumers, particularly extreme or conspiracy news consumers will not be
helped.
  We argue that, given this background, technology solutions to combating
misinformation should not simply seek facts or discredit bad news sources, but
instead use more subtle nudges towards better information consumption. Repeated
exposure to such nudges can help promote trust in better information sources
and also improve societal outcomes in the long run. In this article, we will
talk about technological solutions that can help us in developing such an
approach, and introduce one such model called Trust Nudging.","['Benjamin D. Horne', 'Maur√≠cio Gruppi', 'Sibel Adalƒ±']",4,0.6776501
"Fake news can significantly misinform people who often rely on online sources
and social media for their information. Current research on fake news detection
has mostly focused on analyzing fake news content and how it propagates on a
network of users. In this paper, we emphasize the detection of fake news by
assessing its credibility. By analyzing public fake news data, we show that
information on news sources (and authors) can be a strong indicator of
credibility. Our findings suggest that an author's history of association with
fake news, and the number of authors of a news article, can play a significant
role in detecting fake news. Our approach can help improve traditional fake
news detection methods, wherein content features are often used to detect fake
news.","['Niraj Sitaula', 'Chilukuri K. Mohan', 'Jennifer Grygiel', 'Xinyi Zhou', 'Reza Zafarani']",4,0.8261107
"Recent events have led to a burgeoning awareness on the misuse of social
media sites to affect political events, sway public opinion, and confuse the
voters. Such serious, hostile mass manipulation has motivated a large body of
works on bots/troll detection and fake news detection, which mostly focus on
classifying at the user level based on the content generated by the users. In
this study, we jointly analyze the connections among the users, as well as the
content generated by them to Spot Coordinated Groups (SCG), sets of users that
are likely to be organized towards impacting the general discourse. Given their
tiny size (relative to the whole data), detecting these groups is
computationally hard. Our proposed method detects these tiny-clusters
effectively and efficiently. We deploy our SCG method to summarize and explain
the coordinated groups on Twitter around the 2019 Canadian Federal Elections,
by analyzing over 60 thousand user accounts with 3.4 million followership
connections, and 1.3 million unique hashtags in the content of their tweets.
The users in the detected coordinated groups are over 4x more likely to get
suspended, whereas the hashtags which characterize their creed are linked to
misinformation campaigns.","['Junhao Wang', 'Sacha Levy', 'Ren Wang', 'Aayushi Kulshrestha', 'Reihaneh Rabbany']",10,0.75201726
"Fake news is a type of pervasive propaganda that spreads misinformation
online, taking advantage of social media's extensive reach to manipulate public
perception. Over the past three years, fake news has become a focal discussion
point in the media due to its impact on the 2016 U.S. presidential election.
Fake news can have severe real-world implications: in 2016, a man walked into a
pizzeria carrying a rifle because he read that Hillary Clinton was harboring
children as sex slaves. This project presents a high accuracy (87%) machine
learning classifier that determines the validity of news based on the word
distributions and specific linguistic and stylistic differences in the first
few sentences of an article. This can help readers identify the validity of an
article by looking for specific features in the opening lines aiding them in
making informed decisions. Using a dataset of 2,107 articles from 30 different
websites, this project establishes an understanding of the variations between
fake and credible news by examining the model, dataset, and features. This
classifier appears to use the differences in word distribution, levels of tone
authenticity, and frequency of adverbs, adjectives, and nouns. The
differentiation in the features of these articles can be used to improve future
classifiers. This classifier can also be further applied directly to browsers
as a Google Chrome extension or as a filter for social media outlets or news
websites to reduce the spread of misinformation.",['Qi Jia Sun'],4,0.80147696
"In fighting against fake news, many fact-checking systems comprised of
human-based fact-checking sites (e.g., snopes.com and politifact.com) and
automatic detection systems have been developed in recent years. However,
online users still keep sharing fake news even when it has been debunked. It
means that early fake news detection may be insufficient and we need another
complementary approach to mitigate the spread of misinformation. In this paper,
we introduce a novel application of text generation for combating fake news. In
particular, we (1) leverage online users named \emph{fact-checkers}, who cite
fact-checking sites as credible evidences to fact-check information in public
discourse; (2) analyze linguistic characteristics of fact-checking tweets; and
(3) propose and build a deep learning framework to generate responses with
fact-checking intention to increase the fact-checkers' engagement in
fact-checking activities. Our analysis reveals that the fact-checkers tend to
refute misinformation and use formal language (e.g. few swear words and
Internet slangs). Our framework successfully generates relevant responses, and
outperforms competing models by achieving up to 30\% improvements. Our
qualitative study also confirms that the superiority of our generated responses
compared with responses generated from the existing models.","['Nguyen Vo', 'Kyumin Lee']",4,0.8014841
"We present preliminary results on the online war surrounding distrust of
expertise in medical science -- specifically, the issue of vaccinations. While
distrust and misinformation in politics can damage democratic elections, in the
medical context it may also endanger lives through missed vaccinations and DIY
cancer cures. We find that this online health war has evolved into a highly
efficient network insurgency with direct inter-crowd links across countries,
continents and cultures. The online anti-vax crowds (referred to as Red) now
appear better positioned to groom new recruits (Green) than those supporting
established expertise (Blue). We also present preliminary results from a
mathematically-grounded, crowd-based analysis of the war's evolution, which
offers an explanation for how Red seems to be turning the tide on Blue.","['N. F. Johnson', 'N. Velasquez', 'N. Johnson Restrepo', 'R. Leahy', 'N. Gabriel', 'S. Wuchty', 'D. Broniatowski']",12,0.695269
"The blurry line between nefarious fake news and protected-speech satire has
been a notorious struggle for social media platforms. Further to the efforts of
reducing exposure to misinformation on social media, purveyors of fake news
have begun to masquerade as satire sites to avoid being demoted. In this work,
we address the challenge of automatically classifying fake news versus satire.
Previous work have studied whether fake news and satire can be distinguished
based on language differences. Contrary to fake news, satire stories are
usually humorous and carry some political or social message. We hypothesize
that these nuances could be identified using semantic and linguistic cues.
Consequently, we train a machine learning method using semantic representation,
with a state-of-the-art contextual language model, and with linguistic features
based on textual coherence metrics. Empirical evaluation attests to the merits
of our approach compared to the language-based baseline and sheds light on the
nuances between fake news and satire. As avenues for future work, we consider
studying additional linguistic features related to the humor aspect, and
enriching the data with current news events, to help identify a political or
social message.","['Or Levi', 'Pedram Hosseini', 'Mona Diab', 'David A. Broniatowski']",4,0.7165141
"Contagion, broadly construed, refers to anything that can spread infectiously
from peer to peer. Examples include communicable diseases, rumors,
misinformation, ideas, innovations, bank failures, and electrical blackouts.
Sometimes, as in the 1918 Spanish flu epidemic, a contagion mutates at some
point as it spreads through a network. Here, using a simple
susceptible-infected (SI) model of contagion, we explore the downstream impact
of a single mutation event. Assuming that this mutation occurs at a random node
in the contact network, we calculate the distribution of the number of
""descendants,"" $d$, downstream from the initial ""Patient Zero"" mutant. We find
that the tail of the distribution decays as $d^{-2}$ for complete graphs,
random graphs, small-world networks, networks with block-like structure, and
other infinite-dimensional networks. This prediction agrees with the observed
statistics of memes propagating and mutating on Facebook, and is expected to
hold for other effectively infinite-dimensional networks, such as the global
human contact network. In a wider context, our approach suggests a possible
starting point for a mesoscopic theory of contagion. Such a theory would focus
on the paths traced by a spreading contagion, thereby furnishing an
intermediate level of description between that of individual nodes and the
total infected population. We anticipate that contagion pathways will hold
valuable lessons, given their role as the conduits through which single
mutations, innovations, or failures can sweep through a network as a whole.","['Jonas S. Juul', 'Steven H. Strogatz']",2,0.71874917
"It is a widely accepted fact that state-sponsored Twitter accounts operated
during the 2016 US presidential election spreading millions of tweets with
misinformation and inflammatory political content. Whether these social media
campaigns of the so-called ""troll"" accounts were able to manipulate public
opinion is still in question. Here we aim to quantify the influence of troll
accounts and the impact they had on Twitter by analyzing 152.5 million tweets
from 9.9 million users, including 822 troll accounts. The data collected during
the US election campaign, contain original troll tweets before they were
deleted by Twitter. From these data, we constructed a very large interaction
graph; a directed graph of 9.3 million nodes and 169.9 million edges. Recently,
Twitter released datasets on the misinformation campaigns of 8,275
state-sponsored accounts linked to Russia, Iran and Venezuela as part of the
investigation on the foreign interference in the 2016 US election. These data
serve as ground-truth identifier of troll users in our dataset. Using graph
analysis techniques we qualify the diffusion cascades of web and media context
that have been shared by the troll accounts. We present strong evidence that
authentic users were the source of the viral cascades. Although the trolls were
participating in the viral cascades, they did not have a leading role in them
and only four troll accounts were truly influential.","['Nikos Salamanos', 'Michael J. Jensen', 'Xinlei He', 'Yang Chen', 'Michael Sirivianos']",10,0.7929747
"The prevalence of social media has made information sharing possible across
the globe. The downside, unfortunately, is the wide spread of misinformation.
Methods applied in most previous rumor classifiers give an equal weight, or
attention, to words in the microblog, and do not take the context beyond
microblog contents into account; therefore, the accuracy becomes plateaued. In
this research, we propose an ensemble neural architecture to detect rumor on
Twitter. The architecture incorporates word attention and context from the
author to enhance the classification performance. In particular, the word-level
attention mechanism enables the architecture to put more emphasis on important
words when constructing the text representation. To derive further context,
microblog posts composed by individual authors are exploited since they can
reflect style and characteristics in spreading information, which are
significant cues to help classify whether the shared content is rumor or
legitimate news. The experiment on the real-world Twitter dataset collected
from two well-known rumor tracking websites demonstrates promising results.","['Sansiri Tarnpradab', 'Kien A. Hua']",3,0.6724051
"WhatsApp is the most popular messaging app in the world. The closed nature of
the app, in addition to the ease of transferring multimedia and sharing
information to large-scale groups make WhatsApp unique among other platforms,
where an anonymous encrypted messages can become viral, reaching multiple users
in a short period of time. The personal feeling and immediacy of messages
directly delivered to the user's phone on WhatsApp was extensively abused to
spread unfounded rumors and create misinformation campaigns during recent
elections in Brazil and India. WhatsApp has been deploying measures to mitigate
this problem, such as reducing the limit for forwarding a message to at most
five users at once. Despite the welcomed effort to counter the problem, there
is no evidence so far on the real effectiveness of such restrictions. In this
work, we propose a methodology to evaluate the effectiveness of such measures
on the spreading of misinformation circulating on WhatsApp. We use an
epidemiological model and real data gathered from WhatsApp in Brazil, India and
Indonesia to assess the impact of limiting virality features in this kind of
network. Our results suggest that the current efforts deployed by WhatsApp can
offer significant delays on the information spread, but they are ineffective in
blocking the propagation of misinformation campaigns through public groups when
the content has a high viral nature.","['Philipe de Freitas Melo', 'Carolina Coimbra Vieira', 'Kiran Garimella', 'Pedro O. S. Vaz de Melo', 'Fabr√≠cio Benevenuto']",10,0.5775368
"In recent years, generative adversarial networks (GANs) and its variants have
achieved unprecedented success in image synthesis. They are widely adopted in
synthesizing facial images which brings potential security concerns to humans
as the fakes spread and fuel the misinformation. However, robust detectors of
these AI-synthesized fake faces are still in their infancy and are not ready to
fully tackle this emerging challenge. In this work, we propose a novel
approach, named FakeSpotter, based on monitoring neuron behaviors to spot
AI-synthesized fake faces. The studies on neuron coverage and interactions have
successfully shown that they can be served as testing criteria for deep
learning systems, especially under the settings of being exposed to adversarial
attacks. Here, we conjecture that monitoring neuron behavior can also serve as
an asset in detecting fake faces since layer-by-layer neuron activation
patterns may capture more subtle features that are important for the fake
detector. Experimental results on detecting four types of fake faces
synthesized with the state-of-the-art GANs and evading four perturbation
attacks show the effectiveness and robustness of our approach.","['Run Wang', 'Felix Juefei-Xu', 'Lei Ma', 'Xiaofei Xie', 'Yihao Huang', 'Jian Wang', 'Yang Liu']",7,0.76085675
"Our main contribution in this work is novel results of multilingual models
that go beyond typical applications of rumor or misinformation detection in
English social news content to identify fine-grained classes of digital
deception across multiple languages (e.g. Russian, Spanish, etc.). In addition,
we present models for multimodal deception detection from images and text and
discuss the limitations of image only and text only models. Finally, we
elaborate on the ongoing work on measuring deceptive content (in particular
disinformation) spread across social platforms.","['Maria Glenski', 'Ellyn Ayton', 'Josh Mendoza', 'Svitlana Volkova']",8,0.71232855
"The massive spread of misinformation in social networks has become a global
risk, implicitly influencing public opinion and threatening social/political
development. Misinformation detection (MID) has thus become a surging research
topic in recent years. As a promising and rapid developing research field, we
find that many efforts have been paid to new research problems and approaches
of MID. Therefore, it is necessary to give a comprehensive review of the new
research trends of MID. We first give a brief review of the literature history
of MID, based on which we present several new research challenges and
techniques of it, including early detection, detection by multimodal data
fusion, and explanatory detection. We further investigate the extraction and
usage of various crowd intelligence in MID, which paves a promising way to
tackle MID challenges. Finally, we give our own views on the open issues and
future research directions of MID, such as model adaptivity/generality to new
events, embracing of novel machine learning models, explanatory detection
models, and so on.","['Bin Guo', 'Yasan Ding', 'Lina Yao', 'Yunji Liang', 'Zhiwen Yu']",0,0.69472563
"Recent years have witnessed a significant increase in the online sharing of
medical information, with videos representing a large fraction of such online
sources. Previous studies have however shown that more than half of the
health-related videos on platforms such as YouTube contain misleading
information and biases. Hence, it is crucial to build computational tools that
can help evaluate the quality of these videos so that users can obtain accurate
information to help inform their decisions. In this study, we focus on the
automatic detection of misinformation in YouTube videos. We select prostate
cancer videos as our entry point to tackle this problem. The contribution of
this paper is twofold. First, we introduce a new dataset consisting of 250
videos related to prostate cancer manually annotated for misinformation.
Second, we explore the use of linguistic, acoustic, and user engagement
features for the development of classification models to identify
misinformation. Using a series of ablation experiments, we show that we can
build automatic models with accuracies of up to 74%, corresponding to a 76.5%
precision and 73.2% recall for misinformative instances.","['Rui Hou', 'Ver√≥nica P√©rez-Rosas', 'Stacy Loeb', 'Rada Mihalcea']",0,0.5996283
"The numerous expanding online social networks offer fast channels for
misinformation spreading, which could have a serious impact on socioeconomic
systems. Researchers across multiple areas have paid attention to this issue
with a view of addressing it. However, no systematical theoretical study has
been performed to date on observing misinformation spreading on correlated
multiplex networks. In this study, we propose a multiplex network-based
misinformation spreading model, considering the fact that each individual can
obtain misinformation from multiple platforms. Subsequently, we develop a
heterogeneous edge-base compartmental theory to comprehend the spreading
dynamics of our proposed model. In addition, we establish an analytical method
based on stability analysis to obtain the misinformation outbreak threshold. On
the basis of these theories, we finally analyze the influence of different
dynamical and structural parameters on the misinformation spreading dynamics.
Results show that the misinformation outbreak size $R(\infty)$ grows
continuously with the effective transmission probability $\beta$ once $\beta$
exceeds a certain value, that is, the outbreak threshold $\beta_c$. A large
average degrees, strong degree heterogeneity, or positive inter-layer
correlation will reduce $\beta_c$, accelerating the outbreak of misinformation.
Besides, increasing the degree heterogeneity or a more positive inter-layer
correlation will both enlarge (reduce) $R(\infty)$ for small (large) values of
$\beta$. Our systematic theoretical analysis results agree well with the
numerical simulation results. Our proposed model and accurate theoretical
analysis will serve as a useful framework to understand and predict the
spreading dynamics of misinformation on multiplex networks, and thereby pave
the way to address this serious issue.","['Jiajun Xian', 'Dan Yang', 'Liming Pan', 'Wei Wang', 'Zhen Wang']",2,0.7881396
"Recent developments in neural language models (LMs) have raised concerns
about their potential misuse for automatically spreading misinformation. In
light of these concerns, several studies have proposed to detect
machine-generated fake news by capturing their stylistic differences from
human-written text. These approaches, broadly termed stylometry, have found
success in source attribution and misinformation detection in human-written
texts. However, in this work, we show that stylometry is limited against
machine-generated misinformation. While humans speak differently when trying to
deceive, LMs generate stylistically consistent text, regardless of underlying
motive. Thus, though stylometry can successfully prevent impersonation by
identifying text provenance, it fails to distinguish legitimate LM applications
from those that introduce false information. We create two benchmarks
demonstrating the stylistic similarity between malicious and legitimate uses of
LMs, employed in auto-completion and editing-assistance settings. Our findings
highlight the need for non-stylometry approaches in detecting machine-generated
misinformation, and open up the discussion on the desired evaluation
benchmarks.","['Tal Schuster', 'Roei Schuster', 'Darsh J Shah', 'Regina Barzilay']",1,0.7091844
"Competitive information diffusion on large-scale social networks reveals
fundamental characteristics of rumor contagions and has profound influence on
public opinion formation. There has been growing interest in exploring
dynamical mechanisms of the competing evolutions recently. Nevertheless, the
impacts of population homophily, which determines powerful collective human
behaviors, remains unclear. In this paper, we incorporate homophily effects
into a modified competitive ignorant-spreader-ignorant (SIS) rumor diffusion
model with generalized population preference. Using microscopic Markov chain
approach, we first derive the phase diagram of competing diffusion results and
examine how competitive information spreads and evolves on social networks. We
then explore the detailed effects of homophily, which is modeled by a rewiring
mechanism. Results show that homophily promotes the formation of divided ""echo
chambers"" and protects the disadvantaged information from extinction, which
further changes or even reverses the evolutionary advantage, i.e., the
difference of final proportions of the competitive information. We highlight
the conclusion that the reversals may happen only when the initially
disadvantaged information has stronger transmission ability, owning diffusion
advantage over the other one. Our framework provides profound insight into
competing dynamics with population homophily, which may pave ways for further
controlling misinformation and guiding public belief systems. Moreover, the
reversing condition sheds light on designing effective competing strategies in
many real scenarios.","['Longzhao Liu', 'Xin Wang', 'Yi Zheng', 'Wenyi Fang', 'Shaoting Tang', 'Zhiming Zheng']",2,0.6316837
"The buzz over the so-called ""fake news"" has created concerns about a
degenerated media environment and led to the need for technological solutions.
As the detection of fake news is increasingly considered a technological
problem, it has attracted considerable research. Most of these studies
primarily focus on utilizing information extracted from textual news content.
In contrast, we focus on detecting fake news solely based on structural
information of social networks. We suggest that the underlying network
connections of users that share fake news are discriminative enough to support
the detection of fake news. Thereupon, we model each post as a network of
friendship interactions and represent a collection of posts as a
multidimensional tensor. Taking into account the available labeled data, we
propose a tensor factorization method which associates the class labels of data
samples with their latent representations. Specifically, we combine a
classification error term with the standard factorization in a unified
optimization process. Results on real-world datasets demonstrate that our
proposed method is competitive against state-of-the-art methods by implementing
an arguably simpler approach.","['Frosso Papanastasiou', 'Georgios Katsimpras', 'Georgios Paliouras']",4,0.8415731
"While the purpose of most fake news is misinformation and political
propaganda, our team sees it as a new type of myth that is created by people in
the age of internet identities and artificial intelligence. Seeking insights on
the fear and desire hidden underneath these modified or generated stories, we
use machine learning methods to generate fake articles and present them in the
form of an online news blog. This paper aims to share the details of our
pipeline and the techniques used for full generation of fake news, from dataset
collection to presentation as a media art project on the internet.","['V√≠t R≈Ø≈æiƒçka', 'Eunsu Kang', 'David Gordon', 'Ankita Patel', 'Jacqui Fashimpaur', 'Manzil Zaheer']",4,0.78535414
"We study the problem of automatic fact-checking, paying special attention to
the impact of contextual and discourse information. We address two related
tasks: (i) detecting check-worthy claims, and (ii) fact-checking claims. We
develop supervised systems based on neural networks, kernel-based support
vector machines, and combinations thereof, which make use of rich input
representations in terms of discourse cues and contextual features. For the
check-worthiness estimation task, we focus on political debates, and we model
the target claim in the context of the full intervention of a participant and
the previous and the following turns in the debate, taking into account
contextual meta information. For the fact-checking task, we focus on answer
verification in a community forum, and we model the veracity of the answer with
respect to the entire question--answer thread in which it occurs as well as
with respect to other related posts from the entire forum. We develop annotated
datasets for both tasks and we run extensive experimental evaluation,
confirming that both types of information ---but especially contextual
features--- play an important role.","['Pepa Atanasova', 'Preslav Nakov', 'Llu√≠s M√†rquez', 'Alberto Barr√≥n-Cede√±o', 'Georgi Karadzhov', 'Tsvetomila Mihaylova', 'Mitra Mohtarami', 'James Glass']",1,0.721912
"Arabic Twitter space is crawling with bots that fuel political feuds, spread
misinformation, and proliferate sectarian rhetoric. While efforts have long
existed to analyze and detect English bots, Arabic bot detection and
characterization remains largely understudied. In this work, we contribute new
insights into the role of bots in spreading religious hatred on Arabic Twitter
and introduce a novel regression model that can accurately identify Arabic
language bots. Our assessment shows that existing tools that are highly
accurate in detecting English bots don't perform as well on Arabic bots. We
identify the possible reasons for this poor performance, perform a thorough
analysis of linguistic, content, behavioral and network features, and report on
the most informative features that distinguish Arabic bots from humans as well
as the differences between Arabic and English bots. Our results mark an
important step toward understanding the behavior of malicious bots on Arabic
Twitter and pave the way for a more effective Arabic bot detection tools.","['Nuha Albadi', 'Maram Kurdi', 'Shivakant Mishra']",13,0.7401885
"In two-player zero-sum stochastic games, where two competing players make
decisions under uncertainty, a pair of optimal strategies is traditionally
described by Nash equilibrium and computed under the assumption that the
players have perfect information about the stochastic transition model of the
environment. However, implementing such strategies may make the players
vulnerable to unforeseen changes in the environment. In this paper, we
introduce entropy-regularized stochastic games where each player aims to
maximize the causal entropy of its strategy in addition to its expected payoff.
The regularization term balances each player's rationality with its belief
about the level of misinformation about the transition model. We consider both
entropy-regularized $N$-stage and entropy-regularized discounted stochastic
games, and establish the existence of a value in both games. Moreover, we prove
the sufficiency of Markovian and stationary mixed strategies to attain the
value, respectively, in $N$-stage and discounted games. Finally, we present
algorithms, which are based on convex optimization problems, to compute the
optimal strategies. In a numerical example, we demonstrate the proposed method
on a motion planning scenario and illustrate the effect of the regularization
term on the expected payoff.","['Yagiz Savas', 'Mohamadreza Ahmadi', 'Takashi Tanaka', 'Ufuk Topcu']",2,0.5956576
"This White Paper is a call to action for astronomers to respond to climate
change with a large structural transition within our profession. Many
astronomers are deeply concerned about climate change and act upon it in their
personal and professional lives, and many organizations within astronomy have
incorporated incremental changes. We need a collective impact model to better
network and grow our efforts so that we can achieve results that are on the
scale appropriate to address climate change at the necessary level indicated by
scientific research; e.g., becoming carbon neutral by 2050. We need to
implement strategies within two primary drivers of our field: (1) Education and
Outreach, and (2) Research Practices and Infrastructure. (1) In the classroom
and through public talks, astronomers reach a large audience. Astronomy is
closely connected to the science of climate change, and it is arguably the most
important topic we include in our curriculum. Due to misinformation and
disinformation, climate change communication is different than for other areas
of science. We therefore need to expand our communication and implement
effective strategies, for which there is now a considerable body of research.
(2) On a per-person basis astronomers have an outsized carbon impact. There are
numerous ways we can reduce our footprint; e.g., in the design and operation of
telescope facilities and in the optimization and reduction of travel.
Fortunately, many of these solutions are win-win scenarios, e.g., increasing
the online presence of conferences will reduce the carbon footprint while
increasing participation, especially for astronomers working with fewer
financial resources. Astronomers have an obligation to act on climate change in
every way possible, and we need to do it now. In this White Paper, we outline a
plan for collective impact using a Networked Improvement Community (NIC)
approach.","['Kathryn Williamson', 'Travis A. Rector', 'James Lowenthal']",0,0.4753406
"Beliefs are not facts, but they are factive - they feel like facts. This
property is what can make misinformation dangerous. Being able to deliberately
navigate through a landscape of often conflicting factive statements is
difficult when there is no way to show the relationships between them without
incorporating the information in linear, narrative forms. In this paper, we
present a mechanism to produce maps of belief places, where populations agree
on salient features of fictional environments, and belief spaces, where
subgroups have related but distinct perspectives. Using a model developed using
agent-based simulation, we show that by observing the repeated behaviors of
human participants in the same social context, it is possible to build maps
that show the shared narrative environment overlaid with traces that show
unique, individual or subgroup perspectives. Our contribution is a
proof-of-concept system, based on the affordances of fantasy tabletop
role-playing games, which support multiple groups interacting with the same
dungeon in a controlled, online environment. The techniques used in this
process are mathematically straightforward, and should be generalizable to
auto-generating larger-scale maps of belief spaces from other corpora, such as
discussions on social media.","['Philip Feldman', 'Aaron Dant', 'Wayne Lutters']",3,0.64838934
"We study a setting where a group of agents, each receiving partially
informative private signals, seek to collaboratively learn the true underlying
state of the world (from a finite set of hypotheses) that generates their joint
observation profiles. To solve this problem, we propose a distributed learning
rule that differs fundamentally from existing approaches, in that it does not
employ any form of ""belief-averaging"". Instead, agents update their beliefs
based on a min-rule. Under standard assumptions on the observation model and
the network structure, we establish that each agent learns the truth
asymptotically almost surely. As our main contribution, we prove that with
probability 1, each false hypothesis is ruled out by every agent exponentially
fast at a network-independent rate that is strictly larger than existing rates.
We then develop a computationally-efficient variant of our learning rule that
is provably resilient to agents who do not behave as expected (as represented
by a Byzantine adversary model) and deliberately try to spread misinformation.","['Aritra Mitra', 'John A. Richards', 'Shreyas Sundaram']",2,0.7284471
"We conduct a preliminary analysis of comments on political YouTube content
containing misinformation in comparison to comments on trustworthy or
apolitical videos, labelling the bias and factual ratings of our channels
according to Media Bias Fact Check where applicable. One of our most
interesting discoveries is that especially-polarized or misinformative
political channels (Left-Bias, Right-Bias, PragerU, Conspiracy-Pseudoscience,
and Questionable Source) generate 7.5x more comments per view and 10.42x more
replies per view than apolitical or Pro-Science channels; in particular,
Conspiracy-Pseudoscience and Questionable Sources generate 8.3x more comments
per view and 11.0x more replies per view than apolitical and Pro-Science
channels. We also compared average thread lengths, average comment lengths, and
profanity rates across channels, and present simple machine learning
classifiers for predicting the bias category of a video based on these
statistics.","['Aarash Heydari', 'Janny Zhang', 'Shaan Appel', 'Xinyi Wu', 'Gireeja Ranade']",3,0.575233
"Echo chambers and opinion polarization recently quantified in several
sociopolitical contexts and across different social media, raise concerns on
their potential impact on the spread of misinformation and on openness of
debates. Despite increasing efforts, the dynamics leading to the emergence of
these phenomena stay unclear. We propose a model that introduces the dynamics
of radicalization, as a reinforcing mechanism driving the evolution to extreme
opinions from moderate initial conditions. Inspired by empirical findings on
social interaction dynamics, we consider agents characterized by heterogeneous
activities and homophily. We show that the transition between a global
consensus and emerging radicalized states is mostly governed by social
influence and by the controversialness of the topic discussed. Compared with
empirical data of polarized debates on Twitter, the model qualitatively
reproduces the observed relation between users' engagement and opinions, as
well as opinion segregation in the interaction network. Our findings shed light
on the mechanisms that may lie at the core of the emergence of echo chambers
and polarization in social media.","['Fabian Baumann', 'Philipp Lorenz-Spreen', 'Igor M. Sokolov', 'Michele Starnini']",3,0.6243056
"Identifying misinformation is increasingly being recognized as an important
computational task with high potential social impact. Misinformation and fake
contents are injected into almost every domain of news including politics,
health, science, business, etc., among which, the fakeness in health domain
pose serious adverse effects to scare or harm the society. Misinformation
contains scientific claims or content from social media exaggerated with strong
emotion content to attract eyeballs. In this paper, we consider the utility of
the affective character of news articles for fake news identification in the
health domain and present evidence that emotion cognizant representations are
significantly more suited for the task. We outline a technique to leverage
emotion intensity lexicons to develop emotionized text representations, and
evaluate the utility of such a representation for identifying fake news
relating to health in various supervised and unsupervised scenarios. The
consistent and significant empirical gains that we observe over a range of
technique types and parameter settings establish the utility of the emotional
information in news articles, an often overlooked aspect, for the task of
misinformation identification in the health domain.","['Anoop K', 'Deepak P', 'Lajish V L']",4,0.7050768
"Ranking models are typically designed to provide rankings that optimize some
measure of immediate utility to the users. As a result, they have been unable
to anticipate an increasing number of undesirable long-term consequences of
their proposed rankings, from fueling the spread of misinformation and
increasing polarization to degrading social discourse. Can we design ranking
models that understand the consequences of their proposed rankings and, more
importantly, are able to avoid the undesirable ones? In this paper, we first
introduce a joint representation of rankings and user dynamics using Markov
decision processes. Then, we show that this representation greatly simplifies
the construction of consequential ranking models that trade off the immediate
utility and the long-term welfare. In particular, we can obtain optimal
consequential rankings just by applying weighted sampling on the rankings
provided by models that maximize measures of immediate utility. However, in
practice, such a strategy may be inefficient and impractical, specially in high
dimensional scenarios. To overcome this, we introduce an efficient
gradient-based algorithm to learn parameterized consequential ranking models
that effectively approximate optimal ones. We showcase our methodology using
synthetic and real data gathered from Reddit and show that ranking models
derived using our methodology provide ranks that may mitigate the spread of
misinformation and improve the civility of online discussions.","['Behzad Tabibian', 'Vicen√ß G√≥mez', 'Abir De', 'Bernhard Sch√∂lkopf', 'Manuel Gomez Rodriguez']",2,0.62907064
"Over the past few years, we have been witnessing the rise of misinformation
on the Web. People fall victims of fake news during their daily lives and
assist their further propagation knowingly and inadvertently. There have been
many initiatives that are trying to mitigate the damage caused by fake news,
focusing on signals from either domain flag-lists, online social networks or
artificial intelligence. In this work, we present Check-It, a system that
combines, in an intelligent way, a variety of signals into a pipeline for fake
news identification. Check-It is developed as a web browser plugin with the
objective of efficient and timely fake news detection, respecting the user's
privacy. Experimental results show that Check-It is able to outperform the
state-of-the-art methods. On a dataset, consisting of 9 millions of articles
labeled as fake and real, Check-It obtains classification accuracies that
exceed 99%.","['Demetris Paschalides', 'Alexandros Kornilakis', 'Chrysovalantis Christodoulou', 'Rafael Andreou', 'George Pallis', 'Marios D. Dikaiakos', 'Evangelos Markatos']",4,0.7960105
"Fake news and misinformation have been increasingly used to manipulate
popular opinion and influence political processes. To better understand fake
news, how they are propagated, and how to counter their effect, it is necessary
to first identify them. Recently, approaches have been proposed to
automatically classify articles as fake based on their content. An important
challenge for these approaches comes from the dynamic nature of news: as new
political events are covered, topics and discourse constantly change and thus,
a classifier trained using content from articles published at a given time is
likely to become ineffective in the future. To address this challenge, we
propose a topic-agnostic (TAG) classification strategy that uses linguistic and
web-markup features to identify fake news pages. We report experimental results
using multiple data sets which show that our approach attains high accuracy in
the identification of fake news, even as topics evolve over time.","['Sonia Castelo', 'Thais Almeida', 'Anas Elghafari', 'A√©cio Santos', 'Kien Pham', 'Eduardo Nakamura', 'Juliana Freire']",4,0.8597878
"The spread of misinformation through synthetically generated yet realistic
images and videos has become a significant problem, calling for robust
manipulation detection methods. Despite the predominant effort of detecting
face manipulation in still images, less attention has been paid to the
identification of tampered faces in videos by taking advantage of the temporal
information present in the stream. Recurrent convolutional models are a class
of deep learning models which have proven effective at exploiting the temporal
information from image streams across domains. We thereby distill the best
strategy for combining variations in these models along with domain specific
face preprocessing techniques through extensive experimentation to obtain
state-of-the-art performance on publicly available video-based facial
manipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face
and FaceSwap tampered faces in video streams. Evaluation is performed on the
recently introduced FaceForensics++ dataset, improving the previous
state-of-the-art by up to 4.55% in accuracy.","['Ekraam Sabir', 'Jiaxin Cheng', 'Ayush Jaiswal', 'Wael AbdAlmageed', 'Iacopo Masi', 'Prem Natarajan']",7,0.8358511
"Many researchers from a variety of fields including computer science, network
science and mathematics have focused on how to contain the outbreaks of
Internet misinformation that threaten social systems and undermine societal
health. Most research on this topic treats the connections among individuals as
static, but these connections change in time, and thus social networks are also
temporal networks. Currently there is no theoretical approach to the problem of
containing misinformation outbreaks in temporal networks. We thus propose a
misinformation spreading model for temporal networks and describe it using a
new theoretical approach. We propose a heuristic-containing (HC) strategy based
on optimizing final outbreak size that outperforms simplified strategies such
as those that are random-containing (RC) and targeted-containing (TC). We
verify the effectiveness of our HC strategy on both artificial and real-world
networks by performing extensive numerical simulations and theoretical
analyses. We find that the HC strategy greatly increases the outbreak threshold
and decreases the final outbreak threshold.","['Wei Wang', 'Yuanhui Ma', 'Tao Wu', 'Yang Dai', 'Xingshu Chen', 'Lidia A. Braunstein']",2,0.7469057
"The rise of ubiquitous deepfakes, misinformation, disinformation, propaganda
and post-truth, often referred to as fake news, raises concerns over the role
of Internet and social media in modern democratic societies. Due to its rapid
and widespread diffusion, digital deception has not only an individual or
societal cost (e.g., to hamper the integrity of elections), but it can lead to
significant economic losses (e.g., to affect stock market performance) or to
risks to national security. Blockchain and other Distributed Ledger
Technologies (DLTs) guarantee the provenance, authenticity and traceability of
data by providing a transparent, immutable and verifiable record of
transactions while creating a peer-to-peer secure platform for storing and
exchanging information. This overview aims to explore the potential of DLTs and
blockchain to combat digital deception, reviewing initiatives that are
currently under development and identifying their main current challenges.
Moreover, some recommendations are enumerated to guide future researchers on
issues that will have to be tackled to face fake news, disinformation and
deepfakes, as an integral part of strengthening the resilience against
cyber-threats on today's online media.","['Paula Fraga-Lamas', 'Tiago M. Fern√°ndez-Caram√©s']",0,0.68490326
"Fact checking is an essential task in journalism; its importance has been
highlighted due to recently increased concerns and efforts in combating
misinformation. In this paper, we present an automated fact-checking platform
which given a claim, it retrieves relevant textual evidence from a document
collection, predicts whether each piece of evidence supports or refutes the
claim, and returns a final verdict. We describe the architecture of the system
and the user interface, focusing on the choices made to improve its
user-friendliness and transparency. We conduct a user study of the
fact-checking platform in a journalistic setting: we integrated it with a
collection of news articles and provide an evaluation of the platform using
feedback from journalists in their workflow. We found that the predictions of
our platform were correct 58\% of the time, and 59\% of the returned evidence
was relevant.","['Sebasti√£o Miranda', 'David Nogueira', 'Afonso Mendes', 'Andreas Vlachos', 'Andrew Secker', 'Rebecca Garrett', 'Jeff Mitchel', 'Zita Marinho']",1,0.68652606
"In this paper, we present a dataset of 713k articles collected between
02/2018-11/2018. These articles are collected directly from 194 news and media
outlets including mainstream, hyper-partisan, and conspiracy sources. We
incorporate ground truth ratings of the sources from 8 different assessment
sites covering multiple dimensions of veracity, including reliability, bias,
transparency, adherence to journalistic standards, and consumer trust. The
NELA-GT-2018 dataset can be found at https://doi.org/10.7910/DVN/ULHLCB.","['Jeppe Norregaard', 'Benjamin D. Horne', 'Sibel Adali']",4,0.5753783
"Recent studies, targeting Facebook, showed the tendency of users to interact
with information adhering to their preferred narrative and to ignore dissenting
information. Primarily driven by confirmation bias, users tend to join
polarized clusters where they cooperate to reinforce a like-minded system of
beliefs, thus facilitating fake news and misinformation cascades. To gain a
deeper understanding of these phenomena, in this work we analyze the lexicons
used by the communities of users emerging on Facebook around verified and
unverified contents. We show how the lexical approach provides important
insights about the kind of information processed by the two communities of
users and about their overall sentiment. Furthermore, by focusing on comment
threads, we observe a strong positive correlation between the lexical
convergence of co-commenters and their number of interactions, which in turns
suggests that such a trend could be a proxy for the emergence of collective
identities and polarization in opinion dynamics.","['Emanuele Brugnoli', 'Matteo Cinelli', 'Fabiana Zollo', 'Walter Quattrociocchi', 'Antonio Scala']",3,0.80805194
"Automatic fact-checking systems detect misinformation, such as fake news, by
(i) selecting check-worthy sentences for fact-checking, (ii) gathering related
information to the sentences, and (iii) inferring the factuality of the
sentences. Most prior research on (i) uses hand-crafted features to select
check-worthy sentences, and does not explicitly account for the recent finding
that the top weighted terms in both check-worthy and non-check-worthy sentences
are actually overlapping [15]. Motivated by this, we present a neural
check-worthiness sentence ranking model that represents each word in a sentence
by \textit{both} its embedding (aiming to capture its semantics) and its
syntactic dependencies (aiming to capture its role in modifying the semantics
of other terms in the sentence). Our model is an end-to-end trainable neural
network for check-worthiness ranking, which is trained on large amounts of
unlabelled data through weak supervision. Thorough experimental evaluation
against state of the art baselines, with and without weak supervision, shows
our model to be superior at all times (+13% in MAP and +28% at various
Precision cut-offs from the best baseline with statistical significance).
Empirical analysis of the use of weak supervision, word embedding pretraining
on domain-specific data, and the use of syntactic dependencies of our model
reveals that check-worthy sentences contain notably more identical syntactic
dependencies than non-check-worthy sentences.","['Casper Hansen', 'Christian Hansen', 'Stephen Alstrup', 'Jakob Grue Simonsen', 'Christina Lioma']",1,0.7195397
"Background:
  Tools used to appraise the credibility of health information are
time-consuming to apply and require context-specific expertise, limiting their
use for quickly identifying and mitigating the spread of misinformation as it
emerges. Our aim was to estimate the proportion of vaccination-related posts on
Twitter are likely to be misinformation, and how unevenly exposure to
misinformation was distributed among Twitter users.
  Methods:
  Sampling from 144,878 vaccination-related web pages shared on Twitter between
January 2017 and March 2018, we used a seven-point checklist adapted from two
validated tools to appraise the credibility of a small subset of 474. These
were used to train several classifiers (random forest, support vector machines,
and a recurrent neural network with transfer learning), using the text from a
web page to predict whether the information satisfies each of the seven
criteria.
  Results:
  Applying the best performing classifier to the 144,878 web pages, we found
that 14.4% of relevant posts to text-based communications were linked to
webpages of low credibility and made up 9.2% of all potential
vaccination-related exposures. However, the 100 most popular links to
misinformation were potentially seen by between 2 million and 80 million
Twitter users, and for a substantial sub-population of Twitter users engaging
with vaccination-related information, links to misinformation appear to
dominate the vaccination-related information to which they were exposed.
  Conclusions:
  We proposed a new method for automatically appraising the credibility of
webpages based on a combination of validated checklist tools. The results
suggest that an automatic credibility appraisal tool can be used to find
populations at higher risk of exposure to misinformation or applied proactively
to add friction to the sharing of low credibility vaccination information.","['Zubair Shah', 'Didi Surian', 'Amalie Dyda', 'Enrico Coiera', 'Kenneth D. Mandl', 'Adam G. Dunn']",12,0.7499998
"The prevalence of new technologies and social media has amplified the effects
of misinformation on our societies. Thus, it is necessary to create
computational tools to mitigate their effects effectively. This study aims to
provide a critical overview of computational approaches concerned with
combating misinformation. To this aim, I offer an overview of scholarly
definitions of misinformation. I adopt a framework for studying misinformation
that suggests paying attention to the source, content, and consumers as the
three main elements involved in the process of misinformation and I provide an
overview of literature from disciplines of psychology, media studies, and
cognitive sciences that deal with each of these elements. Using the framework,
I overview the existing computational methods that deal with 1) misinformation
detection and fact-checking using Content 2) Identifying untrustworthy Sources
and social bots, and 3) Consumer-facing tools and methods aiming to make humans
resilient to misinformation. I find that the vast majority of works in computer
science and information technology is concerned with the crucial tasks of
detection and verification of content and sources of misinformation. Moreover,
I find that computational research focusing on Consumers of Misinformation in
Human-Computer Interaction (HCI) and related fields are very sparse and often
do not deal with the subtleties of this process. The majority of existing
interfaces and systems are less concerned with the usability of the tools
rather than the robustness and accuracy of the detection methods. Using this
survey, I call for an interdisciplinary approach towards human-misinformation
interaction that focuses on building methods and tools that robustly deal with
such complex psychological/social phenomena.",['Alireza Karduni'],0,0.82528675
"The prevalence of misinformation on online social media has tangible
empirical connections to increasing political polarization and partisan
antipathy in the United States. Ranking algorithms for social recommendation
often encode broad assumptions about network structure (like homophily) and
group cognition (like, social action is largely imitative). Assumptions like
these can be na\""ive and exclusionary in the era of fake news and ideological
uniformity towards the political poles. We examine these assumptions with aid
from the user-centric framework of trustworthiness in social recommendation.
The constituent dimensions of trustworthiness (diversity, transparency,
explainability, disruption) highlight new opportunities for discouraging
dogmatization and building decision-aware, transparent news recommender
systems.","['Taha Hassan', 'D. Scott McCrickard']",3,0.57597363
"Recent years have witnessed a surge of manipulation of public opinion and
political events by malicious social media actors. These users are referred to
as ""Pathogenic Social Media (PSM)"" accounts. PSMs are key users in spreading
misinformation in social media to viral proportions. These accounts can be
either controlled by real users or automated bots. Identification of PSMs is
thus of utmost importance for social media authorities. The burden usually
falls to automatic approaches that can identify these accounts and protect
social media reputation. However, lack of sufficient labeled examples for
devising and training sophisticated approaches to combat these accounts is
still one of the foremost challenges facing social media firms. In contrast,
unlabeled data is abundant and cheap to obtain thanks to massive user-generated
data. In this paper, we propose a semi-supervised causal inference PSM
detection framework, SemiPsm, to compensate for the lack of labeled data. In
particular, the proposed method leverages unlabeled data in the form of
manifold regularization and only relies on cascade information. This is in
contrast to the existing approaches that use exhaustive feature engineering
(e.g., profile information, network structure, etc.). Evidence from empirical
experiments on a real-world ISIS-related dataset from Twitter suggests
promising results of utilizing unlabeled instances for detecting PSMs.","['Hamidreza Alvari', 'Elham Shaabani', 'Soumajyoti Sarkar', 'Ghazaleh Beigi', 'Paulo Shakarian']",13,0.6764927
"This paper combines data-driven and model-driven methods for real-time
misinformation detection. Our algorithm, named QuickStop, is an optimal
stopping algorithm based on a probabilistic information spreading model
obtained from labeled data. The algorithm consists of an offline machine
learning algorithm for learning the probabilistic information spreading model
and an online optimal stopping algorithm to detect misinformation. The online
detection algorithm has both low computational and memory complexities. Our
numerical evaluations with a real-world dataset show that QuickStop outperforms
existing misinformation detection algorithms in terms of both accuracy and
detection time (number of observations needed for detection). Our evaluations
with synthetic data further show that QuickStop is robust to (offline) learning
errors.","['Honghao Wei', 'Xiaohan Kang', 'Weina Wang', 'Lei Ying']",2,0.65504587
"This is the old version of this project. Please find the new version at
1906.12233.",['Xin Liu'],12,0.08120075
"Image repurposing is a commonly used method for spreading misinformation on
social media and online forums, which involves publishing untampered images
with modified metadata to create rumors and further propaganda. While manual
verification is possible, given vast amounts of verified knowledge available on
the internet, the increasing prevalence and ease of this form of semantic
manipulation call for the development of robust automatic ways of assessing the
semantic integrity of multimedia data. In this paper, we present a novel method
for image repurposing detection that is based on the real-world adversarial
interplay between a bad actor who repurposes images with counterfeit metadata
and a watchdog who verifies the semantic consistency between images and their
accompanying metadata, where both players have access to a reference dataset of
verified content, which they can use to achieve their goals. The proposed
method exhibits state-of-the-art performance on location-identity,
subject-identity and painting-artist verification, showing its efficacy across
a diverse set of scenarios.","['Ayush Jaiswal', 'Yue Wu', 'Wael AbdAlmageed', 'Iacopo Masi', 'Premkumar Natarajan']",7,0.7620418
"Information diffusion is usually modeled as a process in which immutable
pieces of information propagate over a network. In reality, however, messages
are not immutable, but may be morphed with every step, potentially entailing
large cumulative distortions. This process may lead to misinformation even in
the absence of malevolent actors, and understanding it is crucial for modeling
and improving online information systems. Here, we perform a controlled,
crowdsourced experiment in which we simulate the propagation of information
from medical research papers. Starting from the original abstracts, crowd
workers iteratively shorten previously produced summaries to increasingly
smaller lengths. We also collect control summaries where the original abstract
is compressed directly to the final target length. Comparing cascades to
controls allows us to separate the effect of the length constraint from that of
accumulated distortion. Via careful manual coding, we annotate lexical and
semantic units in the medical abstracts and track them along cascades. We find
that iterative summarization has a negative impact due to the accumulation of
error, but that high-quality intermediate summaries result in less distorted
messages than in the control case. Different types of information behave
differently; in particular, the conclusion of a medical abstract (i.e., its key
message) is distorted most. Finally, we compare abstractive with extractive
summaries, finding that the latter are less prone to semantic distortion.
Overall, this work is a first step in studying information cascades without the
assumption that disseminated content is immutable, with implications on our
understanding of the role of word-of-mouth effects on the misreporting of
science.","['Manoel Horta Ribeiro', 'Kristina Gligoriƒá', 'Robert West']",2,0.6535409
"Ranking algorithms play a crucial role in online platforms ranging from
search engines to recommender systems. In this paper, we identify a surprising
consequence of popularity-based rankings: the fewer the items reporting a given
signal, the higher the share of the overall traffic they collectively attract.
This few-get-richer effect emerges in settings where there are few distinct
classes of items (e.g., left-leaning news sources versus right-leaning news
sources), and items are ranked based on their popularity. We demonstrate
analytically that the few-get-richer effect emerges when people tend to click
on top-ranked items and have heterogeneous preferences for the classes of
items. Using simulations, we analyze how the strength of the effect changes
with assumptions about the setting and human behavior. We also test our
predictions experimentally in an online experiment with human participants. Our
findings have important implications to understand the spread of
misinformation.","['Fabrizio Germano', 'Vicen√ß G√≥mez', 'Ga√´l Le Mens']",4,0.64570916
"This paper presents the construction of a Knowledge Graph about relations
between agents in a political system. It discusses the main modeling
challenges, with emphasis on the issue of trust and provenance. Implementation
decisions are also presented","['Daniel Schwabe', 'Carlos Laufer', 'Antonio Busson']",1,0.41775066
"In recent years, the phenomenon of online misinformation and junk news
circulating on social media has come to constitute an important and widespread
problem affecting public life online across the globe, particularly around
important political events such as elections. At the same time, there have been
calls for more transparency around misinformation on social media platforms, as
many of the most popular social media platforms function as ""walled gardens,""
where it is impossible for researchers and the public to readily examine the
scale and nature of misinformation activity as it unfolds on the platforms. In
order to help address this, we present the Junk News Aggregator, a publicly
available interactive web tool, which allows anyone to examine, in near
real-time, all of the public content posted to Facebook by important junk news
sources in the US. It allows the public to gain access to and examine the
latest articles posted on Facebook (the most popular social media platform in
the US and one where content is not readily accessible at scale from the open
Web), as well as organise them by time, news publisher, and keywords of
interest, and sort them based on all eight engagement metrics available on
Facebook. Therefore, the Aggregator allows the public to gain insights on the
volume, content, key themes, and types and volumes of engagement received by
content posted by junk news publishers, in near real-time, hence opening up and
offering transparency in these activities as they unfold, at scale across the
top most popular junk news publishers. In this way, the Aggregator can help
increase transparency around the nature, volume, and engagement with junk news
on social media, and serve as a media literacy tool for the public.","['Dimitra Liotsiou', 'Bence Kollanyi', 'Philip N. Howard']",3,0.65238667
"The proliferation of fake news on social media has opened up new directions
of research for timely identification and containment of fake news, and
mitigation of its widespread impact on public opinion. While much of the
earlier research was focused on identification of fake news based on its
contents or by exploiting users' engagements with the news on social media,
there has been a rising interest in proactive intervention strategies to
counter the spread of misinformation and its impact on society. In this survey,
we describe the modern-day problem of fake news and, in particular, highlight
the technical challenges associated with it. We discuss existing methods and
techniques applicable to both identification and mitigation, with a focus on
the significant advances in each method and their advantages and limitations.
In addition, research has often been limited by the quality of existing
datasets and their specific application contexts. To alleviate this problem, we
comprehensively compile and summarize characteristic features of available
datasets. Furthermore, we outline new directions of research to facilitate
future development of effective and interdisciplinary solutions.","['Karishma Sharma', 'Feng Qian', 'He Jiang', 'Natali Ruchansky', 'Ming Zhang', 'Yan Liu']",0,0.79824114
"Online misinformation has been considered as one of the top global risks as
it may cause serious consequences such as economic damages and public panic.
The misinformation prevention problem aims at generating a positive cascade
with appropriate seed nodes in order to compete against the misinformation. In
this paper, we study the misinformation prevention problem under the prominent
independent cascade model. Due to the #P-hardness in computing influence, the
core problem is to design effective sampling methods to estimate the function
value. The main contribution of this paper is a novel sampling method.
Different from the classic reverse sampling technique which treats all nodes
equally and samples the node uniformly, the proposed method proceeds with a
hybrid sampling process which is able to attach high weights to the users who
are prone to be affected by the misinformation. Consequently, the new sampling
method is more powerful in generating effective samples used for computing seed
nodes for the positive cascade. Based on the new hybrid sample technique, we
design an algorithm offering a $(1-1/e-\epsilon)$-approximation. We
experimentally evaluate the proposed method on extensive datasets and show that
it significantly outperforms the state-of-the-art solutions.","['Gunagmo Tong', 'Ding-Zhu Du']",2,0.75765365
"Echo chambers in online social networks, in which users prefer to interact
only with ideologically-aligned peers, are believed to facilitate
misinformation spreading and contribute to radicalize political discourse. In
this paper, we gauge the effects of echo chambers in information spreading
phenomena over political communication networks. Mining 12 million Twitter
messages, we reconstruct a network in which users interchange opinions related
to the impeachment of the former Brazilian President Dilma Rousseff. We define
a continuous {political position} parameter, independent of the network's
structure, that allows to quantify the presence of echo chambers in the
strongly connected component of the network, reflected in two well-separated
communities of similar sizes with opposite views of the impeachment process. By
means of simple spreading models, we show that the capability of users in
propagating the content they produce, measured by the associated spreadability,
strongly depends on their attitude. Users expressing pro-impeachment sentiments
are capable to transmit information, on average, to a larger audience than
users expressing anti-impeachment sentiments. Furthermore, the users'
spreadability is correlated to the diversity, in terms of political position,
of the audience reached. Our method can be exploited to identify the presence
of echo chambers and their effects across different contexts and shed light
upon the mechanisms allowing to break echo chambers.","['Wesley Cota', 'Silvio C. Ferreira', 'Romualdo Pastor-Satorras', 'Michele Starnini']",3,0.73487777
"Recently, online social networks have become major battlegrounds for
political campaigns, viral marketing, and the dissemination of news. As a
consequence, ''bad actors'' are increasingly exploiting these platforms,
becoming a key challenge for their administrators, businesses and the society
in general. The spread of fake news is a classical example of the abuse of
social networks by these actors. While some have advocated for stricter
policies to control the spread of misinformation in social networks, this often
happens in detriment of their democratic and organic structure. In this paper
we study how to limit the influence of a target set of users in a network via
the removal of a few edges. The idea is to control the diffusion processes
while minimizing the amount of disturbance in the network structure.
  We formulate the influence limitation problem in a data-driven fashion, by
taking into account past propagation traces. Moreover, we consider two types of
constraints over the set of edge removals, a budget constraint and also a, more
general, set of matroid constraints. These problems lead to interesting
challenges in terms of algorithm design. For instance, we are able to show that
influence limitation is APX-hard and propose deterministic and probabilistic
approximation algorithms for the budgeted and matroid version of the problem,
respectively. Our experiments show that the proposed solutions outperform the
baselines by up to 40%.","['Sourav Medya', 'Arlei Silva', 'Ambuj Singh']",2,0.7141003
"Analysing how people react to rumours associated with news in social media is
an important task to prevent the spreading of misinformation, which is nowadays
widely recognized as a dangerous tendency. In social media conversations, users
show different stances and attitudes towards rumourous stories. Some users take
a definite stance, supporting or denying the rumour at issue, while others just
comment it, or ask for additional evidence related to the veracity of the
rumour. On this line, a new shared task has been proposed at SemEval-2017 (Task
8, SubTask A), which is focused on rumour stance classification in English
tweets. The goal is predicting user stance towards emerging rumours in Twitter,
in terms of supporting, denying, querying, or commenting the original rumour,
looking at the conversation threads originated by the rumour. This paper
describes a new approach to this task, where the use of conversation-based and
affective-based features, covering different facets of affect, has been
explored. Our classification model outperforms the best-performing systems for
stance classification at SemEval-2017 Task 8, showing the effectiveness of the
feature set proposed.","['Endang Wahyu Pamungkas', 'Valerio Basile', 'Viviana Patti']",3,0.6744909
"Information is a critical dimension in warfare. Inaccurate information such
as misinformation or disinformation further complicates military operations. In
this paper, we examine the value of misinformation and disinformation to a
military leader who through investment in people, programs and technology is
able to affect the accuracy of information communicated between other actors.
We model the problem as a partially observable stochastic game with three
agents, a leader and two followers. We determine the value to the leader of
misinformation or disinformation being communicated between two (i) adversarial
followers and (ii) allied followers. We demonstrate that only under certain
conditions, the prevalent intuition that the leader would benefit from less
(more) accurate communication between adversarial (allied) followers is valid.
We analyzed why the intuition may fail and show a holistic paradigm taking into
account both the reward structures and policies of agents is necessary in order
to correctly determine the value of misinformation and disinformation. Our
research identifies efficient targeted investments to affect the accuracy of
information communicated between followers to the leader's advantage.","['Yanling Chang', 'Matthew F. Keblis', 'Ran Li', 'Eleftherios Iakovou', 'Chelsea C. White III']",0,0.69767565
"Network immunization is an extensively recognized issue in several domains
like virtual network security, public health and social media, to deal with the
problem of node inoculation so as to minimize the transmission through the
links existed in these networks. We aim to identify top ranked nodes to
immunize networks, leading to control the outbreak of epidemics or
misinformation. We consider group based centrality and define a heuristic
objective criteria to establish the target of key nodes finding in network
which if immunized result in essential network vulnerability. We propose a
group based game theoretic payoff division approach, by employing Shapley value
to assign the surplus acquired by participating nodes in different groups
through the positional power and functional influence over other nodes. We tag
these key nodes as Shapley Value based Information Delimiters (SVID).
Experiments on empirical data sets and model networks establish the efficacy of
our proposed approach and acknowledge performance of node inoculation to
delimit contagion outbreak.","['Chandni Saxena', 'M. N. Doja', 'Tanvir Ahmad']",2,0.72028077
"The online social networks facilitate naturally for the users to share
information. On these platforms, each user shares information based on his or
her interests. The particular information being shared by a user may be
legitimate or fake. Sometimes a misinformation, propagated by users and group
can create chaos or in some cases, might leads to cases of riots. Nowadays the
third party like ALT news and Cobrapost check the information authenticity, but
it takes too much time to validate the news. Therefore, a robust and new system
is required to check the information authenticity within the network, to stop
the propagation of misinformation. In this paper, we propose a blockchain based
framework for sharing the information securely at the peer level. In the
blockchain model, a chain is created by combining blocks of information. Each
node of network propagates the information based on its credibility to its peer
nodes. The credibility of a node will vary according to the respective
information. Trust is calculated between sender and receiver in two ways:(i)
Local trust used for sharing information at the peer level and (ii) global
trust is used for a credibility check of each user in the network. We evaluate
our framework using real dataset derived from Facebook. Our approach achieves
an accuracy of 83% which shows the effectiveness of our proposed framework.","['Md Arquam', 'Anurag Singh', 'Rajesh Sharma']",4,0.65999913
"Technology is increasingly used -- unintentionally (misinformation) or
intentionally (disinformation) -- to spread false information at scale, with
potentially broad-reaching societal effects. For example, technology enables
increasingly realistic false images and videos, and hyper-personal targeting
means different people may see different versions of reality. This report is
the culmination of a PhD-level special topics course
(https://courses.cs.washington.edu/courses/cse599b/18au/) in Computer Science &
Engineering at the University of Washington's Paul G. Allen School in the fall
of 2018. The goals of this course were to study (1) how technologies and
today's technical platforms enable and support the creation and spread of such
mis- and disinformation, as well as (2) how technical approaches could be used
to mitigate these issues. In this report, we summarize the space of
technology-enabled mis- and disinformation based on our investigations, and
then surface our lessons and recommendations for technologists, researchers,
platform designers, policymakers, and users.","['John Akers', 'Gagan Bansal', 'Gabriel Cadamuro', 'Christine Chen', 'Quanze Chen', 'Lucy Lin', 'Phoebe Mulcaire', 'Rajalakshmi Nandakumar', 'Matthew Rockett', 'Lucy Simko', 'John Toman', 'Tongshuang Wu', 'Eric Zeng', 'Bill Zorn', 'Franziska Roesner']",0,0.64519
"As of today, abuse is a pressing issue to participants and administrators of
Online Social Networks (OSN). Abuse in Twitter can spawn from arguments
generated for influencing outcomes of a political election, the use of bots to
automatically spread misinformation, and generally speaking, activities that
deny, disrupt, degrade or deceive other participants and, or the network. Given
the difficulty in finding and accessing a large enough sample of abuse ground
truth from the Twitter platform, we built and deployed a custom crawler that we
use to judiciously collect a new dataset from the Twitter platform with the aim
of characterizing the nature of abusive users, a.k.a abusive birds, in the
wild. We provide a comprehensive set of features based on users' attributes, as
well as social-graph metadata. The former includes metadata about the account
itself, while the latter is computed from the social graph among the sender and
the receiver of each message. Attribute-based features are useful to
characterize user's accounts in OSN, while graph-based features can reveal the
dynamics of information dissemination across the network. In particular, we
derive the Jaccard index as a key feature to reveal the benign or malicious
nature of directed messages in Twitter. To the best of our knowledge, we are
the first to propose such a similarity metric to characterize abuse in Twitter.","['Alvaro Garcia-Recuero', 'Aneta Morawin', 'Gareth Tyson']",13,0.72631025
"Terms like 'misinformation', 'fake news', and 'echo chambers' permeate
current discussions on the state of the Internet. We believe a lack of
technological support to evaluate, contest, and reason about information
online---as opposed to merely disseminating it---lies at the root of these
problems. Several argument technologies support such functionality, but have
seen limited use outside of niche communities. Most research systems
overemphasize argument analysis and structure, standing in stark contrast with
the informal dialectical nature of everyday argumentation. Conversely,
non-academic systems overlook important implications for design which can be
derived from theory. In this paper, we present the design of a system aiming to
strike a balance between structured argumentation and ease of use. Socratrees
is a website for collaborative argumentative discussion targeting layman users,
but includes sophisticated community guidelines and novel features inspired by
informal logic. During an exploratory study, we evaluate the usefulness of our
imposed structure on argumentation and investigate how users perceive it.
Contributing to arguments remains a complex task, but most users learned to do
so effectively with minimal guidance and all recognized that the structure of
Socratrees may improve online discussion and results in a clearer overview of
arguments.",['Steven Jeuris'],0,0.6319664
"As people rely on social media as their primary sources of news, the spread
of misinformation has become a significant concern. In this large-scale study
of news in social media we analyze eleven million posts and investigate
propagation behavior of users that directly interact with news accounts
identified as spreading trusted versus malicious content. Unlike previous work,
which looks at specific rumors, topics, or events, we consider all content
propagated by various news sources. Moreover, we analyze and contrast
population versus sub-population behaviour (by demographics) when spreading
misinformation, and distinguish between two types of propagation, i.e., direct
retweets and mentions. Our evaluation examines how evenly, how many, how
quickly, and which users propagate content from various types of news sources
on Twitter.
  Our analysis has identified several key differences in propagation behavior
from trusted versus suspicious news sources. These include high inequity in the
diffusion rate based on the source of disinformation, with a small group of
highly active users responsible for the majority of disinformation spread
overall and within each demographic. Analysis by demographics showed that users
with lower annual income and education share more from disinformation sources
compared to their counterparts. News content is shared significantly more
quickly from trusted, conspiracy, and disinformation sources compared to
clickbait and propaganda. Older users propagate news from trusted sources more
quickly than younger users, but they share from suspicious sources after longer
delays. Finally, users who interact with clickbait and conspiracy sources are
likely to share from propaganda accounts, but not the other way around.","['Maria Glenski', 'Tim Weninger', 'Svitlana Volkova']",3,0.7798046
"Nowadays, thanks to Web 2.0 technologies, people have the possibility to
generate and spread contents on different social media in a very easy way. In
this context, the evaluation of the quality of the information that is
available online is becoming more and more a crucial issue. In fact, a constant
flow of contents is generated every day by often unknown sources, which are not
certified by traditional authoritative entities. This requires the development
of appropriate methodologies that can evaluate in a systematic way these
contents, based on `objective' aspects connected with them. This would help
individuals, who nowadays tend to increasingly form their opinions based on
what they read online and on social media, to come into contact with
information that is actually useful and verified. Wikipedia is nowadays one of
the biggest online resources on which users rely as a source of information.
The amount of collaboratively generated content that is sent to the online
encyclopedia every day can let to the possible creation of low-quality articles
(and, consequently, misinformation) if not properly monitored and revised. For
this reason, in this paper, the problem of automatically assessing the quality
of Wikipedia articles is considered. In particular, the focus is on the
analysis of hand-crafted features that can be employed by supervised machine
learning techniques to perform the classification of Wikipedia articles on
qualitative bases. With respect to prior literature, a wider set of
characteristics connected to Wikipedia articles are taken into account and
illustrated in detail. Evaluations are performed by considering a labeled
dataset provided in a prior work, and different supervised machine learning
algorithms, which produced encouraging results with respect to the considered
features.","['Elias Bassani', 'Marco Viviani']",0,0.6902443
"Nowadays, artificial intelligence algorithms are used for targeted and
personalized content distribution in the large scale as part of the intense
competition for attention in the digital media environment. Unfortunately,
targeted information dissemination may result in intellectual isolation and
discrimination. Further, as demonstrated in recent political events in the US
and EU, malicious bots and social media users can create and propagate targeted
`fake news' content in different forms for political gains. From the other
direction, fake news detection algorithms attempt to combat such problems by
identifying misinformation and fraudulent user profiles. This paper reviews
common news feed algorithms as well as methods for fake news detection, and we
discuss how news feed algorithms could be misused to promote falsified content,
affect news diversity, or impact credibility. We review how news feed
algorithms and recommender engines can enable confirmation bias to isolate
users to certain news sources and affecting the perception of reality. As a
potential solution for increasing user awareness of how content is selected or
sorted, we argue for the use of interpretable and explainable news feed
algorithms. We discuss how improved user awareness and system transparency
could mitigate unwanted outcomes of echo chambers and bubble filters in social
media.","['Sina Mohseni', 'Eric Ragan']",4,0.8014799
"Detecting manipulated images has become a significant emerging challenge. The
advent of image sharing platforms and the easy availability of advanced photo
editing software have resulted in a large quantities of manipulated images
being shared on the internet. While the intent behind such manipulations varies
widely, concerns on the spread of fake news and misinformation is growing.
Current state of the art methods for detecting these manipulated images suffers
from the lack of training data due to the laborious labeling process. We
address this problem in this paper, for which we introduce a manipulated image
generation process that creates true positives using currently available
datasets. Drawing from traditional work on image blending, we propose a novel
generator for creating such examples. In addition, we also propose to further
create examples that force the algorithm to focus on boundary artifacts during
training. Strong experimental results validate our proposal.","['Peng Zhou', 'Bor-Chun Chen', 'Xintong Han', 'Mahyar Najibi', 'Abhinav Shrivastava', 'Ser Nam Lim', 'Larry S. Davis']",7,0.78088975
"The increasing concern with misinformation has stimulated research efforts on
automatic fact checking. The recently-released FEVER dataset introduced a
benchmark fact-verification task in which a system is asked to verify a claim
using evidential sentences from Wikipedia documents. In this paper, we present
a connected system consisting of three homogeneous neural semantic matching
models that conduct document retrieval, sentence selection, and claim
verification jointly for fact extraction and verification. For evidence
retrieval (document retrieval and sentence selection), unlike traditional
vector space IR models in which queries and sources are matched in some
pre-designed term vector space, we develop neural models to perform deep
semantic matching from raw textual input, assuming no intermediate term
representation and no access to structured external knowledge bases. We also
show that Pageview frequency can also help improve the performance of evidence
retrieval results, that later can be matched by using our neural semantic
matching network. For claim verification, unlike previous approaches that
simply feed upstream retrieved evidence and the claim to a natural language
inference (NLI) model, we further enhance the NLI model by providing it with
internal semantic relatedness scores (hence integrating it with the evidence
retrieval modules) and ontological WordNet features. Experiments on the FEVER
dataset indicate that (1) our neural semantic matching method outperforms
popular TF-IDF and encoder models, by significant margins on all evidence
retrieval metrics, (2) the additional relatedness score and WordNet features
improve the NLI model via better semantic awareness, and (3) by formalizing all
three subtasks as a similar semantic matching problem and improving on all
three stages, the complete model is able to achieve the state-of-the-art
results on the FEVER test set.","['Yixin Nie', 'Haonan Chen', 'Mohit Bansal']",1,0.79130864
"A significant number of images shared on social media platforms such as
Facebook and Instagram contain text in various forms. It's increasingly
becoming commonplace for bad actors to share misinformation, hate speech or
other kinds of harmful content as text overlaid on images on such platforms. A
scene-text understanding system should hence be able to handle text in various
orientations that the adversary might use. Moreover, such a system can be
incorporated into screen readers used to aid the visually impaired. In this
work, we extend the scene-text extraction system at Facebook, Rosetta, to
efficiently handle text in various orientations. Specifically, we incorporate
the Rotation Region Proposal Networks (RRPN) in our text extraction pipeline
and offer practical suggestions for building and deploying a model for
detecting and recognizing text in arbitrary orientations efficiently.
Experimental results show a significant improvement on detecting rotated text.","['Jing Huang', 'Viswanath Sivakumar', 'Mher Mnatsakanyan', 'Guan Pang']",8,0.568004
"Fake news, rumor, incorrect information, and misinformation detection are
nowadays crucial issues as these might have serious consequences for our social
fabrics. The rate of such information is increasing rapidly due to the
availability of enormous web information sources including social media feeds,
news blogs, online newspapers etc.
  In this paper, we develop various deep learning models for detecting fake
news and classifying them into the pre-defined fine-grained categories.
  At first, we develop models based on Convolutional Neural Network (CNN) and
Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations
obtained from these two models are fed into a Multi-layer Perceptron Model
(MLP) for the final classification. Our experiments on a benchmark dataset show
promising results with an overall accuracy of 44.87\%, which outperforms the
current state of the art.","['Arjun Roy', 'Kingshuk Basak', 'Asif Ekbal', 'Pushpak Bhattacharyya']",4,0.7448052
"Fake news and misinformation spread in developing countries as fast as they
do in developed countries with increasing penetration of the internet and
social media. However, fighting misinformation is more difficult in developing
countries where resources and necessary technologies are scarce. This study
provides an understanding of the challenges various fact-checking initiatives
face in three South Asian countries--Bangladesh, India, and Nepal. In-depth
interviews were conducted with senior editors of six fact-checking initiatives.
Challenges identified include lack of resources, technologies, and political
pressure. An analysis of Facebook pages of these initiatives shows increasing
user engagement with their posts.","['Md Mahfuzul Haque', 'Mohammad Yousuf', 'Zahedur Arman', 'Md Main Uddin Rony', 'Ahmed Shatil Alam', 'Kazi Mehedi Hasan', 'Md Khadimul Islam', 'Naeemul Hassan']",0,0.6180054
"Fake news are nowadays an issue of pressing concern, given their recent rise
as a potential threat to high-quality journalism and well-informed public
discourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage
the development of machine learning-based classification systems for stance
detection (i.e., for identifying whether a particular news article agrees,
disagrees, discusses, or is unrelated to a particular news headline), thus
helping in the detection and analysis of possible instances of fake news. This
article presents a new approach to tackle this stance detection problem, based
on the combination of string similarity features with a deep neural
architecture that leverages ideas previously advanced in the context of
learning efficient text representations, document classification, and natural
language inference. Specifically, we use bi-directional Recurrent Neural
Networks, together with max-pooling over the temporal/sequential dimension and
neural attention, for representing (i) the headline, (ii) the first two
sentences of the news article, and (iii) the entire news article. These
representations are then combined/compared, complemented with similarity
features inspired on other FNC-1 approaches, and passed to a final layer that
predicts the stance of the article towards the headline. We also explore the
use of external sources of information, specifically large datasets of sentence
pairs originally proposed for training and evaluating natural language
inference methods, in order to pre-train specific components of the neural
network architecture (e.g., the RNNs used for encoding sentences). The obtained
results attest to the effectiveness of the proposed ideas and show that our
model, particularly when considering pre-training and the combination of neural
representations together with similarity features, slightly outperforms the
previous state-of-the-art.","['Lu√≠s Borges', 'Bruno Martins', 'P√°vel Calado']",4,0.72823554
"Ranking algorithms are the information gatekeepers of the Internet era. We
develop a stylized model to study the effects of ranking algorithms on opinion
dynamics. We consider a search engine that uses an algorithm based on
popularity and on personalization. We find that popularity-based rankings
generate an advantage of the fewer effect: fewer websites reporting a given
signal attract relatively more traffic overall. This highlights a novel,
ranking-driven channel that explains the diffusion of misinformation, as
websites reporting incorrect information may attract an amplified amount of
traffic precisely because they are few. Furthermore, when individuals provide
sufficiently positive feedback to the ranking algorithm, popularity-based
rankings tend to aggregate information while personalization acts in the
opposite direction.","['Fabrizio Germano', 'Francesco Sobbrio']",3,0.54167086
"The widespread online misinformation could cause public panic and serious
economic damages. The misinformation containment problem aims at limiting the
spread of misinformation in online social networks by launching competing
campaigns. Motivated by realistic scenarios, we present the first analysis of
the misinformation containment problem for the case when an arbitrary number of
cascades are allowed. This paper makes four contributions. First, we provide a
formal model for multi-cascade diffusion and introduce an important concept
called as cascade priority. Second, we show that the misinformation containment
problem cannot be approximated within a factor of
$\Omega(2^{\log^{1-\epsilon}n^4})$ in polynomial time unless $NP \subseteq
DTIME(n^{\polylog{n}})$. Third, we introduce several types of cascade priority
that are frequently seen in real social networks. Finally, we design novel
algorithms for solving the misinformation containment problem. The
effectiveness of the proposed algorithm is supported by encouraging
experimental results.","['Guangmo Tong', 'Weili Wu', 'Ding-Zhu Du']",2,0.735448
"Misinformation such as fake news is one of the big challenges of our society.
Research on automated fact-checking has proposed methods based on supervised
learning, but these approaches do not consider external evidence apart from
labeled training instances. Recent approaches counter this deficit by
considering external sources related to a claim. However, these methods require
substantial feature modeling and rich lexicons. This paper overcomes these
limitations of prior work with an end-to-end model for evidence-aware
credibility assessment of arbitrary textual claims, without any human
intervention. It presents a neural network model that judiciously aggregates
signals from external evidence articles, the language of these articles and the
trustworthiness of their sources. It also derives informative features for
generating user-comprehensible explanations that makes the neural network
predictions transparent to the end-user. Experiments with four datasets and
ablation studies show the strength of our method.","['Kashyap Popat', 'Subhabrata Mukherjee', 'Andrew Yates', 'Gerhard Weikum']",1,0.77360946
"We measure trends in the diffusion of misinformation on Facebook and Twitter
between January 2015 and July 2018. We focus on stories from 570 sites that
have been identified as producers of false stories. Interactions with these
sites on both Facebook and Twitter rose steadily through the end of 2016.
Interactions then fell sharply on Facebook while they continued to rise on
Twitter, with the ratio of Facebook engagements to Twitter shares falling by
approximately 60 percent. We see no similar pattern for other news, business,
or culture sites, where interactions have been relatively stable over time and
have followed similar trends on the two platforms both before and after the
election.","['Hunt Allcott', 'Matthew Gentzkow', 'Chuan Yu']",10,0.720736
"The integrity of democratic elections depends on voters' access to accurate
information. However, modern media environments, which are dominated by social
media, provide malicious actors with unprecedented ability to manipulate
elections via misinformation, such as fake news. We study a zero-sum game
between an attacker, who attempts to subvert an election by propagating a fake
new story or other misinformation over a set of advertising channels, and a
defender who attempts to limit the attacker's impact. Computing an equilibrium
in this game is challenging as even the pure strategy sets of players are
exponential. Nevertheless, we give provable polynomial-time approximation
algorithms for computing the defender's minimax optimal strategy across a range
of settings, encompassing different population structures as well as models of
the information available to each player. Experimental results confirm that our
algorithms provide near-optimal defender strategies and showcase variations in
the difficulty of defending elections depending on the resources and knowledge
available to the defender.","['Bryan Wilder', 'Yevgeniy Vorobeychik']",4,0.6081626
"Social media's unfettered access has made it an important venue for health
discussion and a resource for patients and their loved ones. However, the
quality of the information available, as well as the motivations of its
posters, has been questioned. This work examines the individuals on social
media that are posting questionable health-related information, and in
particular promoting cancer treatments which have been shown to be ineffective
(making it a kind of misinformation, willful or not). Using a multi-stage user
selection process, we study 4,212 Twitter users who have posted about one of
139 such ""treatments"", and compare them to a baseline of users generally
interested in cancer. Considering features capturing user attributes, writing
style, and sentiment, we build a classifier which is able to identify users
prone to propagate such misinformation at an accuracy of over 90%, providing a
potential tool for public health officials to identify such individuals for
preventive intervention.","['Amira Ghenai', 'Yelena Mejova']",5,0.62870044
"Online social networks provide users with unprecedented opportunities to
engage with diverse opinions. At the same time, they enable confirmation bias
on large scales by empowering individuals to self-select narratives they want
to be exposed to. A precise understanding of such tradeoffs is still largely
missing. We introduce a social learning model where most participants in a
network update their beliefs unbiasedly based on new information, while a
minority of participants reject information that is incongruent with their
preexisting beliefs. This simple mechanism generates permanent opinion
polarization and cascade dynamics, and accounts for the aforementioned tradeoff
between confirmation bias and social connectivity through analytic results. We
investigate the model's predictions empirically using US county-level data on
the impact of Internet access on the formation of beliefs about global warming.
We conclude by discussing policy implications of our model, highlighting the
downsides of debunking and suggesting alternative strategies to contrast
misinformation.","['Orowa Sikder', 'Robert E. Smith', 'Pierpaolo Vivo', 'Giacomo Livan']",3,0.78264695
"We propose a novel approach towards adversarial attacks on neural networks
(NN), focusing on tampering the data used for training instead of generating
attacks on trained models. Our network-agnostic method creates a backdoor
during training which can be exploited at test time to force a neural network
to exhibit abnormal behaviour. We demonstrate on two widely used datasets
(CIFAR-10 and SVHN) that a universal modification of just one pixel per image
for all the images of a class in the training set is enough to corrupt the
training procedure of several state-of-the-art deep neural networks causing the
networks to misclassify any images to which the modification is applied. Our
aim is to bring to the attention of the machine learning community, the
possibility that even learning-based methods that are personally trained on
public datasets can be subject to attacks by a skillful adversary.","['Michele Alberti', 'Vinaychandran Pondenkandath', 'Marcel W√ºrsch', 'Manuel Bouillon', 'Mathias Seuret', 'Rolf Ingold', 'Marcus Liwicki']",7,0.71504164
"WhatsApp is, as of 2018, a significant component of the global information
and communication infrastructure, especially in developing countries. However,
probably due to its strong end-to-end encryption, WhatsApp became an attractive
place for the dissemination of misinformation, extremism and other forms of
undesirable behavior. In this paper, we investigate the public perception of
WhatsApp through the lens of media. We analyze two large datasets of news and
show the kind of content that is being associated with WhatsApp in different
regions of the world and over time. Our analyses include the examination of
named entities, general vocabulary, and topics addressed in news articles that
mention WhatsApp, as well as the polarity of these texts. Among other results,
we demonstrate that the vocabulary and topics around the term ""whatsapp"" in the
media have been changing over the years and in 2018 concentrate on matters
related to misinformation, politics and criminal scams. More generally, our
findings are useful to understand the impact that tools like WhatsApp play in
the contemporary society and how they are seen by the communities themselves.","['Josemar Alves Caetano', 'Gabriel Magno', 'Evandro Cunha', 'Wagner Meira Jr.', 'Humberto T. Marques-Neto', 'Virgilio Almeida']",3,0.6123427
"Practical solutions to bootstrap security in today's information and
communication systems critically depend on centralized services for
authentication as well as key and trust management. This is particularly true
for mobile users. Identity providers such as Google or Facebook have active
user bases of two billion each, and the subscriber number of mobile operators
exceeds five billion unique users as of early 2018. If these centralized
services go completely `dark' due to natural or man made disasters, large scale
blackouts, or country-wide censorship, the users are left without practical
solutions to bootstrap security on their mobile devices. Existing distributed
solutions, for instance, the so-called web-of-trust are not sufficiently
lightweight. Furthermore, they support neither cross-application on mobile
devices nor strong protection of key material using hardware security modules.
We propose Sea of Lights(SoL), a practical lightweight scheme for bootstrapping
device-to-device security wirelessly, thus, enabling secure distributed
self-organized networks. It is tailored to operate `in the dark' and provides
strong protection of key material as well as an intuitive means to build a
lightweight web-of-trust. SoL is particularly well suited for local or urban
operation in scenarios such as the coordination of emergency response, where it
helps containing/limiting the spreading of misinformation. As a proof of
concept, we implement SoL in the Android platform and hence test its
feasibility on real mobile devices. We further evaluate its key performance
aspects using simulation.","['Flor √Ålvarez', 'Max Kolhagen', 'Matthias Hollick']",2,0.46069828
"We present Verifi2, a visual analytic system to support the investigation of
misinformation on social media. On the one hand, social media platforms empower
individuals and organizations by democratizing the sharing of information. On
the other hand, even well-informed and experienced social media users are
vulnerable to misinformation. To address the issue, various models and studies
have emerged from multiple disciplines to detect and understand the effects of
misinformation. However, there is still a lack of intuitive and accessible
tools that help social media users distinguish misinformation from verified
news. In this paper, we present Verifi2, a visual analytic system that uses
state-of-the-art computational methods to highlight salient features from text,
social network, and images. By exploring news on a source level through
multiple coordinated views in Verifi2, users can interact with the complex
dimensions that characterize misinformation and contrast how real and
suspicious news outlets differ on these dimensions. To evaluate Verifi2, we
conduct interviews with experts in digital media, journalism, education,
psychology, and computing who study misinformation. Our interviews show
promising potential for Verifi2 to serve as an educational tool on
misinformation. Furthermore, our interview results highlight the complexity of
the problem of combating misinformation and call for more work from the
visualization community.","['Alireza Karduni', 'Isaac Cho', 'Ryan Wesslen', 'Sashank Santhanam', 'Svitlana Volkova', 'Dustin Arendt', 'Samira Shaikh', 'Wenwen Dou']",0,0.8541831
"In this article, we quantitatively analyze how the term ""fake news"" is being
shaped in news media in recent years. We study the perception and the
conceptualization of this term in the traditional media using eight years of
data collected from news outlets based in 20 countries. Our results not only
corroborate previous indications of a high increase in the usage of the
expression ""fake news"", but also show contextual changes around this expression
after the United States presidential election of 2016. Among other results, we
found changes in the related vocabulary, in the mentioned entities, in the
surrounding topics and in the contextual polarity around the term ""fake news"",
suggesting that this expression underwent a change in perception and
conceptualization after 2016. These outcomes expand the understandings on the
usage of the term ""fake news"", helping to comprehend and more accurately
characterize this relevant social phenomenon linked to misinformation and
manipulation.","['Evandro Cunha', 'Gabriel Magno', 'Josemar Caetano', 'Douglas Teixeira', 'Virgilio Almeida']",4,0.7009809
"Society's reliance on social media as a primary source of news has spawned a
renewed focus on the spread of misinformation. In this work, we identify the
differences in how social media accounts identified as bots react to news
sources of varying credibility, regardless of the veracity of the content those
sources have shared. We analyze bot and human responses annotated using a
fine-grained model that labels responses as being an answer, appreciation,
agreement, disagreement, an elaboration, humor, or a negative reaction. We
present key findings of our analysis into the prevalence of bots, the variety
and speed of bot and human reactions, and the disparity in authorship of
reaction tweets between these two sub-populations. We observe that bots are
responsible for 9-15% of the reactions to sources of any given type but
comprise only 7-10% of accounts responsible for reaction-tweets; trusted news
sources have the highest proportion of humans who reacted; bots respond with
significantly shorter delays than humans when posting answer-reactions in
response to sources identified as propaganda. Finally, we report significantly
different inequality levels in reaction rates for accounts identified as bots
vs not.","['Maria Glenski', 'Tim Weninger', 'Svitlana Volkova']",13,0.72688043
"In this work, we consider misinformation propagating through a social network
and study the problem of its prevention. In this problem, a ""bad"" campaign
starts propagating from a set of seed nodes in the network and we use the
notion of a limiting (or ""good"") campaign to counteract the effect of
misinformation. The goal is to identify a set of $k$ users that need to be
convinced to adopt the limiting campaign so as to minimize the number of people
that adopt the ""bad"" campaign at the end of both propagation processes.
  This work presents \emph{RPS} (Reverse Prevention Sampling), an algorithm
that provides a scalable solution to the misinformation mitigation problem. Our
theoretical analysis shows that \emph{RPS} runs in $O((k + l)(n + m)(\frac{1}{1
- \gamma}) \log n / \epsilon^2 )$ expected time and returns a $(1 - 1/e -
\epsilon)$-approximate solution with at least $1 - n^{-l}$ probability (where
$\gamma$ is a typically small network parameter and $l$ is a confidence
parameter). The time complexity of \emph{RPS} substantially improves upon the
previously best-known algorithms that run in time $\Omega(m n k \cdot
POLY(\epsilon^{-1}))$. We experimentally evaluate \emph{RPS} on large datasets
and show that it outperforms the state-of-the-art solution by several orders of
magnitude in terms of running time. This demonstrates that misinformation
mitigation can be made practical while still offering strong theoretical
guarantees.","['Michael Simpson', 'Venkatesh Srinivasan', 'Alex Thomo']",2,0.6178098
"The recently increased focus on misinformation has stimulated research in
fact checking, the task of assessing the truthfulness of a claim. Research in
automating this task has been conducted in a variety of disciplines including
natural language processing, machine learning, knowledge representation,
databases, and journalism. While there has been substantial progress, relevant
papers and articles have been published in research communities that are often
unaware of each other and use inconsistent terminology, thus impeding
understanding and further progress. In this paper we survey automated fact
checking research stemming from natural language processing and related
disciplines, unifying the task formulations and methodologies across papers and
authors. Furthermore, we highlight the use of evidence as an important
distinguishing factor among them cutting across task formulations and methods.
We conclude with proposing avenues for future NLP research on automated fact
checking.","['James Thorne', 'Andreas Vlachos']",1,0.7008862
"A large body of research work and efforts have been focused on detecting fake
news and building online fact-check systems in order to debunk fake news as
soon as possible. Despite the existence of these systems, fake news is still
wildly shared by online users. It indicates that these systems may not be fully
utilized. After detecting fake news, what is the next step to stop people from
sharing it? How can we improve the utilization of these fact-check systems? To
fill this gap, in this paper, we (i) collect and analyze online users called
guardians, who correct misinformation and fake news in online discussions by
referring fact-checking URLs; and (ii) propose a novel fact-checking URL
recommendation model to encourage the guardians to engage more in fact-checking
activities. We found that the guardians usually took less than one day to reply
to claims in online conversations and took another day to spread verified
information to hundreds of millions of followers. Our proposed recommendation
model outperformed four state-of-the-art models by 11%~33%. Our source code and
dataset are available at https://github.com/nguyenvo09/CombatingFakeNews.","['Nguyen Vo', 'Kyumin Lee']",4,0.76808894
"Cognitive biases have been shown to lead to faulty decision-making. Recent
research has demonstrated that the effect of cognitive biases, anchoring bias
in particular, transfers to information visualization and visual analytics.
However, it is still unclear how users of visual interfaces can be anchored and
the impact of anchoring on user performance and decision-making process. To
investigate, we performed two rounds of between-subjects, in-laboratory
experiments with 94 participants to analyze the effect of visual anchors and
strategy cues in decision-making with a visual analytic system that employs
coordinated multiple view design. The decision-making task is identifying
misinformation from Twitter news accounts. Participants were randomly assigned
one of three treatment groups (including control) in which participant training
processes were modified. Our findings reveal that strategy cues and visual
anchors (scenario videos) can significantly affect user activity, speed,
confidence, and, under certain circumstances, accuracy. We discuss the
implications of our experiment results on training users how to use a newly
developed visual interface. We call for more careful consideration into how
visualization designers and researchers train users to avoid unintentionally
anchoring users and thus affecting the end result.","['Ryan Wesslen', 'Sashank Santhanam', 'Alireza Karduni', 'Isaac Cho', 'Samira Shaikh', 'Wenwen Dou']",0,0.6443912
"""Fake news"" is a recent phenomenon, but misinformation and propaganda are
not. Our new communication technologies make it easy for us to be exposed to
high volumes of true, false, irrelevant, and unprovable information. Future AI
is expected to amplify the problem even more. At the same time, our brains are
reaching their limits in handling information. How should we respond to
propaganda? Technology can help, but relying on it alone will not suffice in
the long term. We also need ethical policies, laws, regulations, and trusted
authorities, including fact-checkers. However, we will not solve the problem
without the active engagement of the educated citizen. Epistemological
education, recognition of self biases and protection of our channels of
communication and trusted networks are all needed to overcome the problem and
continue our progress as democratic societies.",['Panagiotis Metaxas'],4,0.6316118
"Commenting platforms, such as Disqus, have emerged as a major online
communication platform with millions of users and posts. Their popularity has
also attracted parasitic and malicious behav- iors, such as trolling and
spamming. There has been relatively little research on modeling and
safeguarding these platforms. As our key contribution, we develop a systematic
approach to detect malicious users on commenting platforms focusing on having:
(a) interpretable, and (b) fine-grained classification of malice. Our work has
two key novelties: (a) we propose two classifications methods, with one
following a two stage approach, which first maps observ- able features to
behaviors and then maps these behaviors to user roles, and (b) we use a
comprehensive set of 73 features that span four dimensions of information. We
use 7 million comments during a 9 month period, and we show that our
classification methods can distinguish between benign, and malicious roles
(spammers, trollers, and fanatics) with a 0.904 AUC. Our work is a solid step
to- wards ensuring that commenting platforms are a safe and pleasant medium for
the exchange of ideas.","['Tai Ching Li', 'Joobin Gharibshah', 'Evangelos E. Papalexakis', 'Michalis Faloutsos']",13,0.56006515
"What are the key-features that enable an information diffusion model to
explain the inherent dynamic, and often competitive, nature of real-world
propagation phenomena? In this paper we aim to answer this question by
proposing a novel class of diffusion models, inspired by the classic Linear
Threshold model, and built around the following aspects: trust/distrust in the
user relationships, which is leveraged to model different effects of social
influence on the decisions taken by an individual; changes in adopting one or
alternative information items; hesitation towards adopting an information item
over time; latency in the propagation; time horizon for the unfolding of the
diffusion process; and multiple cascades of information that might occur
competitively. To the best of our knowledge, the above aspects have never been
unified into the same LT-based diffusion model. We also define different
strategies for the selection of the initial influencers to simulate
non-competitive and competitive diffusion scenarios, particularly related to
the problem of limitation of misinformation spread. Results on publicly
available networks have shown the meaningfulness and uniqueness of our models.","['Antonio Cali√≤', 'Andrea Tagarelli']",2,0.74039704
"The advent of WWW changed the way we can produce and access information.
Recent studies showed that users tend to select information that is consistent
with their system of beliefs, forming polarized groups of like-minded people
around shared narratives where dissenting information is ignored. In this
environment, users cooperate to frame and reinforce their shared narrative
making any attempt at debunking inefficient. Such a configuration occurs even
in the consumption of news online, and considering that 63% of users access
news directly form social media, one hypothesis is that more polarization
allows for further spreading of misinformation. Along this path, we focus on
the polarization of users around news outlets on Facebook in different European
countries (Italy, France, Spain and Germany). First, we compare the pages'
posting behavior and the users' interacting patterns across countries and
observe different posting, liking and commenting rates. Second, we explore the
tendency of users to interact with different pages (i.e., selective exposure)
and the emergence of polarized communities generated around specific pages.
Then, we introduce a new metric -- i.e., polarization rank -- to measure
polarization of communities for each country. We find that Italy is the most
polarized country, followed by France, Germany and lastly Spain. Finally, we
present a variation of the Bounded Confidence Model to simulate the emergence
of these communities by considering the users' engagement and trust on the
news. Our findings suggest that trust in information broadcaster plays a
pivotal role against polarization of users online.","['Ana Luc√≠a Schmidt', 'Fabiana Zollo', 'Antonio Scala', 'Walter Quattrociocchi']",3,0.73542535
"In the last years, the study of rumor spreading on social networks produced a
lot of interest among the scientific community, expecially due to the role of
social networks in the last political events. The goal of this work is to
reproduce real-like diffusions of information and misinformation in a
scale-free network using a multi-agent-based model. The data concerning the
virtual spreading are easily obtainable, in particular the diffusion of
information during the announcement for the discovery of the Higgs Boson on
Twitter was recorded and investigated in detail. We made some assumptions on
the micro behavior of our agents and registered the effects in a statistical
analysis replying the real data diffusion. Then, we studied an hypotetical
response to a misinformation diffusion adding debunking agents and trying to
model a critic response from the agents using real data from a hoax regarding
the Occupy Wall Street movement. After tuning our model to reproduce these
results, we measured some network properties and proved the emergence of
substantially separated structures like echochambers, independently from the
network size scale, i.e. with one hundred, one thousand and ten thousand
agents.","['Mattia Mazzoli', 'Tullio Re', 'Roberto Bertilone', 'Marco Maggiora', 'Jacopo Pellegrino']",3,0.6613765
"A key challenge for Bitcoin cryptocurrency holders, such as startups using
ICOs to raise funding, is managing their FX risk. Specifically, a misinformed
decision to convert Bitcoin to fiat currency could, by itself, cost USD
millions.
  In contrast to financial exchanges, Blockchain based crypto-currencies expose
the entire transaction history to the public. By processing all transactions,
we model the network with a high fidelity graph so that it is possible to
characterize how the flow of information in the network evolves over time. We
demonstrate how this data representation permits a new form of microstructure
modeling - with the emphasis on the topological network structures to study the
role of users, entities and their interactions in formation and dynamics of
crypto-currency investment risk. In particular, we identify certain sub-graphs
('chainlets') that exhibit predictive influence on Bitcoin price and
volatility, and characterize the types of chainlets that signify extreme
losses.","['Cuneyt Akcora', 'Matthew Dixon', 'Yulia Gel', 'Murat Kantarcioglu']",2,0.54865944
"Fake news may be intentionally created to promote economic, political and
social interests, and can lead to negative impacts on humans beliefs and
decisions. Hence, detection of fake news is an emerging problem that has become
extremely prevalent during the last few years. Most existing works on this
topic focus on manual feature extraction and supervised classification models
leveraging a large number of labeled (fake or real) articles. In contrast, we
focus on content-based detection of fake news articles, while assuming that we
have a small amount of labels, made available by manual fact-checkers or
automated sources. We argue this is a more realistic setting in the presence of
massive amounts of content, most of which cannot be easily factchecked. To that
end, we represent collections of news articles as multi-dimensional tensors,
leverage tensor decomposition to derive concise article embeddings that capture
spatial/contextual information about each news article, and use those
embeddings to create an article-by-article graph on which we propagate limited
labels. Results on three real-world datasets show that our method performs on
par or better than existing models that are fully supervised, in that we
achieve better detection accuracy using fewer labels. In particular, our
proposed method achieves 75.43% of accuracy using only 30% of labels of a
public dataset while an SVM-based classifier achieved 67.43%. Furthermore, our
method achieves 70.92% of accuracy in a large dataset using only 2% of labels.","['Gisel Bastidas Guacho', 'Sara Abdali', 'Neil Shah', 'Evangelos E. Papalexakis']",4,0.76391196
"Deception boosts security for systems and components by denial, deceit,
misinformation, camouflage and obfuscation. In this work an extensive overview
of the deception technology environment is presented. Taxonomies, theoretical
backgrounds, psychological aspects as well as concepts, implementations, legal
aspects and ethics are discussed and compared.","['Daniel Fraunholz', 'Simon Duque Anton', 'Christoph Lipps', 'Daniel Reti', 'Daniel Krohmer', 'Frederic Pohl', 'Matthias Tammen', 'Hans Dieter Schotten']",0,0.45879138
"Currency trading (Forex) is the largest world market in terms of volume. We
analyze trading and tweeting about the EUR-USD currency pair over a period of
three years. First, a large number of tweets were manually labeled, and a
Twitter stance classification model is constructed. The model then classifies
all the tweets by the trading stance signal: buy, hold, or sell (EUR vs. USD).
The Twitter stance is compared to the actual currency rates by applying the
event study methodology, well-known in financial economics. It turns out that
there are large differences in Twitter stance distribution and potential
trading returns between the four groups of Twitter users: trading robots,
spammers, trading companies, and individual traders. Additionally, we observe
attempts of reputation manipulation by post festum removal of tweets with poor
predictions, and deleting/reposting of identical tweets to increase the
visibility without tainting one's Twitter timeline.","['Igor Mozetiƒç', 'Peter Gabrov≈°ek', 'Petra Kralj Novak']",10,0.5971996
"The fake news epidemic makes it imperative to develop a diagnostic framework
that is both parsimonious and valid to guide present and future efforts in fake
news detection. This paper represents one of the very first attempts to fill a
void in the research on this topic. The LeSiE (Lexical Structure, Simplicity,
Emotion) framework we created and validated allows lay people to identify
potential fake news without the use of calculators or complex statistics by
looking out for three simple cues.","['Murphy Choy', 'Mark Chong']",4,0.72325003
"The complexity and diversity of today's media landscape provides many
challenges for researchers studying news producers. These producers use many
different strategies to get their message believed by readers through the
writing styles they employ, by repetition across different media sources with
or without attribution, as well as other mechanisms that are yet to be studied
deeply. To better facilitate systematic studies in this area, we present a
large political news data set, containing over 136K news articles, from 92 news
sources, collected over 7 months of 2017. These news sources are carefully
chosen to include well-established and mainstream sources, maliciously fake
sources, satire sources, and hyper-partisan political blogs. In addition to
each article we compute 130 content-based and social media engagement features
drawn from a wide range of literature on political bias, persuasion, and
misinformation. With the release of the data set, we also provide the source
code for feature computation. In this paper, we discuss the first release of
the data set and demonstrate 4 use cases of the data and features: news
characterization, engagement characterization, news attribution and content
copying, and discovering news narratives.","['Benjamin D. Horne', 'William Dron', 'Sara Khedr', 'Sibel Adali']",4,0.7523466
"Until recently, social media was seen to promote democratic discourse on
social and political issues. However, this powerful communication platform has
come under scrutiny for allowing hostile actors to exploit online discussions
in an attempt to manipulate public opinion. A case in point is the ongoing U.S.
Congress' investigation of Russian interference in the 2016 U.S. election
campaign, with Russia accused of using trolls (malicious accounts created to
manipulate) and bots to spread misinformation and politically biased
information. In this study, we explore the effects of this manipulation
campaign, taking a closer look at users who re-shared the posts produced on
Twitter by the Russian troll accounts publicly disclosed by U.S. Congress
investigation. We collected a dataset with over 43 million election-related
posts shared on Twitter between September 16 and October 21, 2016, by about 5.7
million distinct users. This dataset included accounts associated with the
identified Russian trolls. We use label propagation to infer the ideology of
all users based on the news sources they shared. This method enables us to
classify a large number of users as liberal or conservative with precision and
recall above 90%. Conservatives retweeted Russian trolls about 31 times more
often than liberals and produced 36x more tweets. Additionally, most retweets
of troll content originated from two Southern states: Tennessee and Texas.
Using state-of-the-art bot detection techniques, we estimated that about 4.9%
and 6.2% of liberal and conservative users respectively were bots. Text
analysis on the content shared by trolls reveals that they had a mostly
conservative, pro-Trump agenda. Although an ideologically broad swath of
Twitter users was exposed to Russian Trolls in the period leading up to the
2016 U.S. Presidential election, it was mainly conservatives who helped amplify
their message.","['Adam Badawy', 'Emilio Ferrara', 'Kristina Lerman']",10,0.80658513
"US voters shared large volumes of polarizing political news and information
in the form of links to content from Russian, WikiLeaks and junk news sources.
Was this low quality political information distributed evenly around the
country, or concentrated in swing states and particular parts of the country?
In this data memo we apply a tested dictionary of sources about political news
and information being shared over Twitter over a ten day period around the 2016
Presidential Election. Using self-reported location information, we place a
third of users by state and create a simple index for the distribution of
polarizing content around the country. We find that (1) nationally, Twitter
users got more misinformation, polarizing and conspiratorial content than
professionally produced news. (2) Users in some states, however, shared more
polarizing political news and information than users in other states. (3)
Average levels of misinformation were higher in swing states than in
uncontested states, even when weighted for the relative size of the user
population in each state. We conclude with some observations about the impact
of strategically disseminated polarizing information on public life.","['Philip N. Howard', 'Bence Kollanyi', 'Samantha Bradshaw', 'Lisa-Maria Neudert']",10,0.7263951
"Social media provides political news and information for both active duty
military personnel and veterans. We analyze the subgroups of Twitter and
Facebook users who spend time consuming junk news from websites that target US
military personnel and veterans with conspiracy theories, misinformation, and
other forms of junk news about military affairs and national security issues.
(1) Over Twitter we find that there are significant and persistent interactions
between current and former military personnel and a broad network of extremist,
Russia-focused, and international conspiracy subgroups. (2) Over Facebook, we
find significant and persistent interactions between public pages for military
and veterans and subgroups dedicated to political conspiracy, and both sides of
the political spectrum. (3) Over Facebook, the users who are most interested in
conspiracy theories and the political right seem to be distributing the most
junk news, whereas users who are either in the military or are veterans are
among the most sophisticated news consumers, and share very little junk news
through the network.","['John D. Gallacher', 'Vlad Barash', 'Philip N. Howard', 'John Kelly']",10,0.63930935
"Users polarization and confirmation bias play a key role in misinformation
spreading on online social media. Our aim is to use this information to
determine in advance potential targets for hoaxes and fake news. In this paper,
we introduce a general framework for promptly identifying polarizing content on
social media and, thus, ""predicting"" future fake news topics. We validate the
performances of the proposed methodology on a massive Italian Facebook dataset,
showing that we are able to identify topics that are susceptible to
misinformation with 77% accuracy. Moreover, such information may be embedded as
a new feature in an additional classifier able to recognize fake news with 91%
accuracy. The novelty of our approach consists in taking into account a series
of characteristics related to users behavior on online social media, making a
first, important step towards the smoothing of polarization and the mitigation
of misinformation phenomena.","['Michela Del Vicario', 'Walter Quattrociocchi', 'Antonio Scala', 'Fabiana Zollo']",4,0.77978134
"This paper examines how an event from one random variable provides pointwise
mutual information about an event from another variable via probability mass
exclusions. We start by introducing probability mass diagrams, which provide a
visual representation of how a prior distribution is transformed to a posterior
distribution through exclusions. With the aid of these diagrams, we identify
two distinct types of probability mass exclusions---namely informative and
misinformative exclusions. Then, motivated by Fano's derivation of the
pointwise mutual information, we propose four postulates which aim to decompose
the pointwise mutual information into two separate informational components: a
non-negative term associated with the informative exclusion and a non-positive
term associated with the misinformative exclusions. This yields a novel
derivation of a familiar decomposition of the pointwise mutual information into
entropic components. We conclude by discussing the relevance of considering
information in terms of probability mass exclusions to the ongoing effort to
decompose multivariate information.","['Conor Finn', 'Joseph T Lizier']",2,0.622061
"In this paper, the problem of misinformation propagation is studied for an
Internet of Battlefield Things (IoBT) system in which an attacker seeks to
inject false information in the IoBT nodes in order to compromise the IoBT
operations. In the considered model, each IoBT node seeks to counter the
misinformation attack by finding the optimal probability of accepting a given
information that minimizes its cost at each time instant. The cost is expressed
in terms of the quality of information received as well as the infection cost.
The problem is formulated as a mean-field game with multiclass agents which is
suitable to model a massive heterogeneous IoBT system. For this game, the
mean-field equilibrium is characterized, and an algorithm based on the forward
backward sweep method is proposed to find the mean-field equilibrium. Then, the
finite IoBT case is considered, and the conditions of convergence of the Nash
equilibria in the finite case to the mean-field equilibrium are presented.
Numerical results show that the proposed scheme can achieve a 1.2-fold increase
in the quality of information (QoI) compared to a baseline scheme in which the
IoBT nodes are always transmitting. The results also show that the proposed
scheme can reduce the proportion of infected nodes by 99% compared to the
baseline.","['Nof Abuzainab', 'Walid Saad']",2,0.7146411
"Massive amounts of fake news and conspiratorial content have spread over
social media before and after the 2016 US Presidential Elections despite
intense fact-checking efforts. How do the spread of misinformation and
fact-checking compete? What are the structural and dynamic characteristics of
the core of the misinformation diffusion network, and who are its main
purveyors? How to reduce the overall amount of misinformation? To explore these
questions we built Hoaxy, an open platform that enables large-scale, systematic
studies of how misinformation and fact-checking spread and compete on Twitter.
Hoaxy filters public tweets that include links to unverified claims or
fact-checking articles. We perform k-core decomposition on a diffusion network
obtained from two million retweets produced by several hundred thousand
accounts over the six months before the election. As we move from the periphery
to the core of the network, fact-checking nearly disappears, while social bots
proliferate. The number of users in the main core reaches equilibrium around
the time of the election, with limited churn and increasingly dense
connections. We conclude by quantifying how effectively the network can be
disrupted by penalizing the most central nodes. These findings provide a first
look at the anatomy of a massive online misinformation diffusion network.","['Chengcheng Shao', 'Pik-Mai Hui', 'Lei Wang', 'Xinwen Jiang', 'Alessandro Flammini', 'Filippo Menczer', 'Giovanni Luca Ciampaglia']",10,0.76980317
"Wikidata is a free and open knowledge base from the Wikimedia Foundation,
that not only acts as a central storage of structured data for other projects
of the organization, but also for a growing array of information systems,
including search engines. Like Wikipedia, Wikidata's content can be created and
edited by anyone; which is the main source of its strength, but also allows for
malicious users to vandalize it, risking the spreading of misinformation
through all the systems that rely on it as a source of structured facts. Our
task at the WSDM Cup 2017 was to come up with a fast and reliable prediction
system that narrows down suspicious edits for human revision. Elaborating on
previous works by Heindorf et al. we were able to outperform all other
contestants, while incorporating new interesting features, unifying the
programming language used to only Python and refactoring the feature extractor
into a simpler and more compact code base.","['Rafael Crescenzi', 'Marcelo Fernandez', 'Federico A. Garcia Calabria', 'Pablo Albani', 'Diego Tauziet', 'Adriana Baravalle', ""Andr√©s Sebasti√°n D'Ambrosio""]",1,0.59585273
"As political polarization in the United States continues to rise, the
question of whether polarized individuals can fruitfully cooperate becomes
pressing. Although diversity of individual perspectives typically leads to
superior team performance on complex tasks, strong political perspectives have
been associated with conflict, misinformation and a reluctance to engage with
people and perspectives beyond one's echo chamber. It is unclear whether
self-selected teams of politically diverse individuals will create higher or
lower quality outcomes. In this paper, we explore the effect of team political
composition on performance through analysis of millions of edits to Wikipedia's
Political, Social Issues, and Science articles. We measure editors' political
alignments by their contributions to conservative versus liberal articles. A
survey of editors validates that those who primarily edit liberal articles
identify more strongly with the Democratic party and those who edit
conservative ones with the Republican party. Our analysis then reveals that
polarized teams---those consisting of a balanced set of politically diverse
editors---create articles of higher quality than politically homogeneous teams.
The effect appears most strongly in Wikipedia's Political articles, but is also
observed in Social Issues and even Science articles. Analysis of article ""talk
pages"" reveals that politically polarized teams engage in longer, more
constructive, competitive, and substantively focused but linguistically diverse
debates than political moderates. More intense use of Wikipedia policies by
politically diverse teams suggests institutional design principles to help
unleash the power of politically polarized teams.","['Feng Shi', 'Misha Teplitskiy', 'Eamon Duede', 'James Evans']",3,0.53489685
"Online social networking sites are experimenting with the following
crowd-powered procedure to reduce the spread of fake news and misinformation:
whenever a user is exposed to a story through her feed, she can flag the story
as misinformation and, if the story receives enough flags, it is sent to a
trusted third party for fact checking. If this party identifies the story as
misinformation, it is marked as disputed. However, given the uncertain number
of exposures, the high cost of fact checking, and the trade-off between flags
and exposures, the above mentioned procedure requires careful reasoning and
smart algorithms which, to the best of our knowledge, do not exist to date.
  In this paper, we first introduce a flexible representation of the above
procedure using the framework of marked temporal point processes. Then, we
develop a scalable online algorithm, Curb, to select which stories to send for
fact checking and when to do so to efficiently reduce the spread of
misinformation with provable guarantees. In doing so, we need to solve a novel
stochastic optimal control problem for stochastic differential equations with
jumps, which is of independent interest. Experiments on two real-world datasets
gathered from Twitter and Weibo show that our algorithm may be able to
effectively reduce the spread of fake news and misinformation.","['Jooyeon Kim', 'Behzad Tabibian', 'Alice Oh', 'Bernhard Schoelkopf', 'Manuel Gomez-Rodriguez']",4,0.7361484
"Our work considers leveraging crowd signals for detecting fake news and is
motivated by tools recently introduced by Facebook that enable users to flag
fake news. By aggregating users' flags, our goal is to select a small subset of
news every day, send them to an expert (e.g., via a third-party fact-checking
organization), and stop the spread of news identified as fake by an expert. The
main objective of our work is to minimize the spread of misinformation by
stopping the propagation of fake news in the network. It is especially
challenging to achieve this objective as it requires detecting fake news with
high-confidence as quickly as possible. We show that in order to leverage
users' flags efficiently, it is crucial to learn about users' flagging
accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian
inference for detecting fake news and jointly learns about users' flagging
accuracy over time. Our algorithm employs posterior sampling to actively trade
off exploitation (selecting news that maximize the objective value at a given
epoch) and exploration (selecting news that maximize the value of information
towards learning about users' flagging accuracy). We demonstrate the
effectiveness of our approach via extensive experiments and show the power of
leveraging community signals for fake news detection.","['Sebastian Tschiatschek', 'Adish Singla', 'Manuel Gomez Rodriguez', 'Arpit Merchant', 'Andreas Krause']",4,0.7993572
"Election control considers the problem of an adversary who attempts to tamper
with a voting process, in order to either ensure that their favored candidate
wins (constructive control) or another candidate loses (destructive control).
As online social networks have become significant sources of information for
potential voters, a new tool in an attacker's arsenal is to effect control by
harnessing social influence, for example, by spreading fake news and other
forms of misinformation through online social media.
  We consider the computational problem of election control via social
influence, studying the conditions under which finding good adversarial
strategies is computationally feasible. We consider two objectives for the
adversary in both the constructive and destructive control settings:
probability and margin of victory (POV and MOV, respectively). We present
several strong negative results, showing, for example, that the problem of
maximizing POV is inapproximable for any constant factor. On the other hand, we
present approximation algorithms which provide somewhat weaker approximation
guarantees, such as bicriteria approximations for the POV objective and
constant-factor approximations for MOV. Finally, we present mixed integer
programming formulations for these problems. Experimental results show that our
approximation algorithms often find near-optimal control strategies, indicating
that election control through social influence is a salient threat to election
integrity.","['Bryan Wilder', 'Yevgeniy Vorobeychik']",2,0.6575191
"Misinformation and rumor can spread rapidly and widely through online social
networks and therefore rumor controlling has become a critical issue. It is
often assumed that there is a single authority whose goal is to minimize the
spread of rumor by generating a positive cascade. In this paper, we study a
more realistic scenario when there are multiple positive cascades generated by
different agents. For the multiple-cascade diffusion, we propose the P2P
independent cascade (PIC) model for private social communications. The main
part of this paper is an analysis of the rumor blocking effect (i.e. the number
of the users activated by rumor) when the agents non-cooperatively generate the
positive cascades. We show that the rumor blocking effect provided by the Nash
equilibrium will not be arbitrarily worse even if the positive cascades are
generated non-cooperatively. In addition, we give a discussion on how the
cascade priority and activation order affect the rumor blocking problem. We
experimentally examine the Nash equilibrium of the proposed games by
simulations done on real social network structures.","['Guangmo Amo Tong', 'Weili Wu', 'Ding-Zhu Du']",2,0.59458387
"Recent work have done a good job in modeling rumors and detecting them over
microblog streams. However, the performance of their automatic approaches are
not relatively high when looking early in the diffusion. A first intuition is
that, at early stage, most of the aggregated rumor features (e.g., propagation
features) are not mature and distinctive enough. The objective of rumor
debunking in microblogs, however, are to detect these misinformation as early
as possible. In this work, we leverage neural models in learning the hidden
representations of individual rumor-related tweets at the very beginning of a
rumor. Our extensive experiments show that the resulting signal improves our
classification performance over time, significantly within the first 10 hours.
To deepen the understanding of these low and high-level features in
contributing to the model performance over time, we conduct an extensive study
on a wide range of high impact rumor features for the 48 hours range. The end
model that engages these features are shown to be competitive, reaches over 90%
accuracy and out-performs strong baselines in our carefully cured dataset.",['Tu Nguyen'],4,0.52476084
"The emergence of ""Fake News"" and misinformation via online news and social
media has spurred an interest in computational tools to combat this phenomenon.
In this paper we present a new ""Related Fact Checks"" service, which can help a
reader critically evaluate an article and make a judgment on its veracity by
bringing up fact checks that are relevant to the article. We describe the core
technical problems that need to be solved in building a ""Related Fact Checks""
service, and present results from an evaluation of an implementation.",['Sreya Guha'],4,0.64042795
"Malicious crowdsourcing forums are gaining traction as sources of spreading
misinformation online, but are limited by the costs of hiring and managing
human workers. In this paper, we identify a new class of attacks that leverage
deep learning language models (Recurrent Neural Networks or RNNs) to automate
the generation of fake online reviews for products and services. Not only are
these attacks cheap and therefore more scalable, but they can control rate of
content output to eliminate the signature burstiness that makes crowdsourced
campaigns easy to detect.
  Using Yelp reviews as an example platform, we show how a two phased review
generation and customization attack can produce reviews that are
indistinguishable by state-of-the-art statistical detectors. We conduct a
survey-based user study to show these reviews not only evade human detection,
but also score high on ""usefulness"" metrics by users. Finally, we develop novel
automated defenses against these attacks, by leveraging the lossy
transformation introduced by the RNN training and generation cycle. We consider
countermeasures against our mechanisms, show that they produce unattractive
cost-benefit tradeoffs for attackers, and that they can be further curtailed by
simple constraints imposed by online service providers.","['Yuanshun Yao', 'Bimal Viswanath', 'Jenna Cryan', 'Haitao Zheng', 'Ben Y. Zhao']",13,0.64492404
"The volume and velocity of information that gets generated online limits
current journalistic practices to fact-check claims at the same rate.
Computational approaches for fact checking may be the key to help mitigate the
risks of massive misinformation spread. Such approaches can be designed to not
only be scalable and effective at assessing veracity of dubious claims, but
also to boost a human fact checker's productivity by surfacing relevant facts
and patterns to aid their analysis. To this end, we present a novel,
unsupervised network-flow based approach to determine the truthfulness of a
statement of fact expressed in the form of a (subject, predicate, object)
triple. We view a knowledge graph of background information about real-world
entities as a flow network, and knowledge as a fluid, abstract commodity. We
show that computational fact checking of such a triple then amounts to finding
a ""knowledge stream"" that emanates from the subject node and flows toward the
object node through paths connecting them. Evaluation on a range of real-world
and hand-crafted datasets of facts related to entertainment, business, sports,
geography and more reveals that this network-flow model can be very effective
in discerning true statements from false ones, outperforming existing
algorithms on many test cases. Moreover, the model is expressive in its ability
to automatically discover several useful path patterns and surface relevant
facts that may help a human fact checker corroborate or refute a claim.","['Prashant Shiralkar', 'Alessandro Flammini', 'Filippo Menczer', 'Giovanni Luca Ciampaglia']",1,0.77438694
"Online social media (OSM) has a enormous influence in today's world. Some
individuals view OSM as fertile ground for abuse and use it to disseminate
misinformation and political propaganda, slander competitors, and spread spam.
The crowdturfing industry employs large numbers of bots and human workers to
manipulate OSM and misrepresent public opinion. The detection of online
discussion topics manipulated by OSM \emph{abusers} is an emerging issue
attracting significant attention. In this paper, we propose an approach for
quantifying the authenticity of online discussions based on the similarity of
OSM accounts participating in the discussion to known abusers and legitimate
accounts. Our method uses several similarity functions for the analysis and
classification of OSM accounts. The proposed methods are demonstrated using
Twitter data collected for this study and previously published \emph{Arabic
honeypot dataset}. The former includes manually labeled accounts and abusers
who participated in crowdturfing platforms. Evaluation of the topic's
authenticity, derived from account similarity functions, shows that the
suggested approach is effective for discriminating between topics that were
strongly promoted by abusers and topics that attracted authentic public
interest.","['Aviad Elyashar', 'Jorge Bendahan', 'Rami Puzis']",3,0.6657258
"In this paper, we provide a systematic analysis of the Twitter discussion on
the 2016 Austrian presidential elections. In particular, we extracted and
analyzed a data-set consisting of 343645 Twitter messages related to the 2016
Austrian presidential elections. Our analysis combines methods from network
science, sentiment analysis, as well as bot detection. Among other things, we
found that: a) the winner of the election (Alexander Van der Bellen) was
considerably more popular and influential on Twitter than his opponent, b) the
Twitter followers of Van der Bellen substantially participated in the spread of
misinformation about him, c) there was a clear polarization in terms of the
sentiments spread by Twitter followers of the two presidential candidates, d)
the in-degree and out-degree distributions of the underlying communication
network are heavy-tailed, and e) compared to other recent events, such as the
2016 Brexit referendum or the 2016 US presidential elections, only a very small
number of bots participated in the Twitter discussion on the 2016 Austrian
presidential election.","['Ema Ku≈°en', 'Mark Strembeck']",10,0.69833416
"The massive spread of digital misinformation has been identified as a major
global risk and has been alleged to influence elections and threaten
democracies. Communication, cognitive, social, and computer scientists are
engaged in efforts to study the complex causes for the viral diffusion of
misinformation online and to develop solutions, while search and social media
platforms are beginning to deploy countermeasures. With few exceptions, these
efforts have been mainly informed by anecdotal evidence rather than systematic
data. Here we analyze 14 million messages spreading 400 thousand articles on
Twitter during and following the 2016 U.S. presidential campaign and election.
We find evidence that social bots played a disproportionate role in amplifying
low-credibility content. Accounts that actively spread articles from
low-credibility sources are significantly more likely to be bots. Automated
accounts are particularly active in amplifying content in the very early
spreading moments, before an article goes viral. Bots also target users with
many followers through replies and mentions. Humans are vulnerable to this
manipulation, retweeting bots who post links to low-credibility content.
Successful low-credibility sources are heavily supported by social bots. These
results suggest that curbing social bots may be an effective strategy for
mitigating the spread of online misinformation.","['Chengcheng Shao', 'Giovanni Luca Ciampaglia', 'Onur Varol', 'Kaicheng Yang', 'Alessandro Flammini', 'Filippo Menczer']",13,0.8066616
"In February 2016, World Health Organization declared the Zika outbreak a
Public Health Emergency of International Concern. With developing evidence it
can cause birth defects, and the Summer Olympics coming up in the worst
affected country, Brazil, the virus caught fire on social media. In this work,
use Zika as a case study in building a tool for tracking the misinformation
around health concerns on Twitter. We collect more than 13 million tweets --
spanning the initial reports in February 2016 and the Summer Olympics --
regarding the Zika outbreak and track rumors outlined by the World Health
Organization and Snopes fact checking website. The tool pipeline, which
incorporates health professionals, crowdsourcing, and machine learning, allows
us to capture health-related rumors around the world, as well as clarification
campaigns by reputable health organizations. In the case of Zika, we discover
an extremely bursty behavior of rumor-related topics, and show that, once the
questionable topic is detected, it is possible to identify rumor-bearing tweets
using automated techniques. Thus, we illustrate insights the proposed tools
provide into potentially harmful information on social media, allowing public
health researchers and practitioners to respond with a targeted and timely
action.","['Amira Ghenai', 'Yelena Mejova']",5,0.6405864
"Identifying public misinformation is a complicated and challenging task. An
important part of checking the veracity of a specific claim is to evaluate the
stance different news sources take towards the assertion. Automatic stance
evaluation, i.e. stance detection, would arguably facilitate the process of
fact checking. In this paper, we present our stance detection system which
claimed third place in Stage 1 of the Fake News Challenge. Despite our
straightforward approach, our system performs at a competitive level with the
complex ensembles of the top two winning teams. We therefore propose our system
as the 'simple but tough-to-beat baseline' for the Fake News Challenge stance
detection task.","['Benjamin Riedel', 'Isabelle Augenstein', 'Georgios P. Spithourakis', 'Sebastian Riedel']",4,0.60090846
"Social media are pervaded by unsubstantiated or untruthful rumors, that
contribute to the alarming phenomenon of misinformation. The widespread
presence of a heterogeneous mass of information sources may affect the
mechanisms behind the formation of public opinion. Such a scenario is a florid
environment for digital wildfires when combined with functional illiteracy,
information overload, and confirmation bias. In this essay, we focus on a
collection of works aiming at providing quantitative evidence about the
cognitive determinants behind misinformation and rumor spreading. We account
for users' behavior with respect to two distinct narratives: a) conspiracy and
b) scientific information sources. In particular, we analyze Facebook data on a
time span of five years in both the Italian and the US context, and measure
users' response to i) information consistent with one's narrative, ii) troll
contents, and iii) dissenting information e.g., debunking attempts. Our
findings suggest that users tend to a) join polarized communities sharing a
common narrative (echo chambers), b) acquire information confirming their
beliefs (confirmation bias) even if containing false claims, and c) ignore
dissenting information.","['Fabiana Zollo', 'Walter Quattrociocchi']",3,0.8196409
"With the rapid growth of social media, massive misinformation is also
spreading widely on social media, such as microblog, and bring negative effects
to human life. Nowadays, automatic misinformation identification has drawn
attention from academic and industrial communities. For an event on social
media usually consists of multiple microblogs, current methods are mainly based
on global statistical features. However, information on social media is full of
noisy and outliers, which should be alleviated. Moreover, most of microblogs
about an event have little contribution to the identification of
misinformation, where useful information can be easily overwhelmed by useless
information. Thus, it is important to mine significant microblogs for a
reliable misinformation identification method. In this paper, we propose an
Attention-based approach for Identification of Misinformation (AIM). Based on
the attention mechanism, AIM can select microblogs with largest attention
values for misinformation identification. The attention mechanism in AIM
contains two parts: content attention and dynamic attention. Content attention
is calculated based textual features of each microblog. Dynamic attention is
related to the time interval between the posting time of a microblog and the
beginning of the event. To evaluate AIM, we conduct a series of experiments on
the Weibo dataset and the Twitter dataset, and the experimental results show
that the proposed AIM model outperforms the state-of-the-art methods.","['Qiang Liu', 'Feng Yu', 'Shu Wu', 'Liang Wang']",3,0.64574265
"An important challenge in the process of tracking and detecting the
dissemination of misinformation is to understand the political gap between
people that engage with the so called ""fake news"". A possible factor
responsible for this gap is opinion polarization, which may prompt the general
public to classify content that they disagree or want to discredit as fake. In
this work, we study the relationship between political polarization and content
reported by Twitter users as related to ""fake news"". We investigate how
polarization may create distinct narratives on what misinformation actually is.
We perform our study based on two datasets collected from Twitter. The first
dataset contains tweets about US politics in general, from which we compute the
degree of polarization of each user towards the Republican and Democratic
Party. In the second dataset, we collect tweets and URLs that co-occurred with
""fake news"" related keywords and hashtags, such as #FakeNews and
#AlternativeFact, as well as reactions towards such tweets and URLs. We then
analyze the relationship between polarization and what is perceived as
misinformation, and whether users are designating information that they
disagree as fake. Our results show an increase in the polarization of users and
URLs associated with fake-news keywords and hashtags, when compared to
information not labeled as ""fake news"". We discuss the impact of our findings
on the challenges of tracking ""fake news"" in the ongoing battle against
misinformation.","['Manoel Horta Ribeiro', 'Pedro H. Calais', 'Virg√≠lio A. F. Almeida', 'Wagner Meira Jr']",10,0.76285696
"Many people use social media to seek information during disasters while
lacking access to traditional information sources. In this study, we analyze
Twitter data to understand information spreading activities of social media
users during hurricane Sandy. We create multiple subgraphs of Twitter users
based on activity levels and analyze network properties of the subgraphs. We
observe that user information sharing activity follows a power-law distribution
suggesting the existence of few highly active nodes in disseminating
information and many other nodes being less active. We also observe close
enough connected components and isolates at all levels of activity, and
networks become less transitive, but more assortative for larger subgraphs. We
also analyze the association between user activities and characteristics that
may influence user behavior to spread information during a crisis. Users become
more active in spreading information if they are centrally placed in the
network, less eccentric, and have higher degrees. Our analysis provides
insights on how to exploit user characteristics and network properties to
spread information or limit the spreading of misinformation during a crisis
event.","['Arif Mohaimin Sadri', 'Samiul Hasan', 'Satish V. Ukkusuri', 'Manuel Cebrian']",3,0.6637396
"Deriving relationships between images and tracing back their history of
modifications are at the core of Multimedia Phylogeny solutions, which aim to
combat misinformation through doctored visual media. Nonetheless, most recent
image phylogeny solutions cannot properly address cases of forged composite
images with multiple donors, an area known as multiple parenting phylogeny
(MPP). This paper presents a preliminary undirected graph construction solution
for MPP, without any strict assumptions. The algorithm is underpinned by robust
image representative keypoints and different geometric consistency checks among
matching regions in both images to provide regions of interest for direct
comparison. The paper introduces a novel technique to geometrically filter the
most promising matches as well as to aid in the shared region localization
task. The strength of the approach is corroborated by experiments with
real-world cases, with and without image distractors (unrelated cases).","['Aparna Bharati', 'Daniel Moreira', 'Allan Pinto', 'Joel Brogan', 'Kevin Bowyer', 'Patrick Flynn', 'Walter Scheirer', 'Anderson Rocha']",7,0.6613847
"Online health communities are a valuable source of information for patients
and physicians. However, such user-generated resources are often plagued by
inaccuracies and misinformation. In this work we propose a method for
automatically establishing the credibility of user-generated medical statements
and the trustworthiness of their authors by exploiting linguistic cues and
distant supervision from expert sources. To this end we introduce a
probabilistic graphical model that jointly learns user trustworthiness,
statement credibility, and language objectivity. We apply this methodology to
the task of extracting rare or unknown side-effects of medical drugs --- this
being one of the problems where large scale non-expert data has the potential
to complement expert medical knowledge. We show that our method can reliably
extract side-effects and filter out false statements, while identifying
trustworthy users that are likely to contribute valuable medical information.","['Subhabrata Mukherjee', 'Gerhard Weikum', 'Cristian Danescu-Niculescu-Mizil']",1,0.6342548
"It is generally accepted that, when moving in groups, animals process
information to coordinate their motion. Recent studies have begun to apply
rigorous methods based on Information Theory to quantify such distributed
computation. Following this perspective, we use transfer entropy to quantify
dynamic information flows locally in space and time across a school of fish
during directional changes around a circular tank, i.e. U-turns. This analysis
reveals peaks in information flows during collective U-turns and identifies two
different flows: an informative flow (positive transfer entropy) based on fish
that have already turned about fish that are turning, and a misinformative flow
(negative transfer entropy) based on fish that have not turned yet about fish
that are turning. We also reveal that the information flows are related to
relative position and alignment between fish, and identify spatial patterns of
information and misinformation cascades. This study offers several
methodological contributions and we expect further application of these
methodologies to reveal intricacies of self-organisation in other animal groups
and active matter in general.","['Emanuele Crosato', 'Li Jiang', 'Valentin Lecheval', 'Joseph T. Lizier', 'X. Rosalind Wang', 'Pierre Tichit', 'Guy Theraulaz', 'Mikhail Prokopenko']",2,0.5551394
"In recent years, the reliability of information on the Internet has emerged
as a crucial issue of modern society. Social network sites (SNSs) have
revolutionized the way in which information is spread by allowing users to
freely share content. As a consequence, SNSs are also increasingly used as
vectors for the diffusion of misinformation and hoaxes. The amount of
disseminated information and the rapidity of its diffusion make it practically
impossible to assess reliability in a timely manner, highlighting the need for
automatic hoax detection systems.
  As a contribution towards this objective, we show that Facebook posts can be
classified with high accuracy as hoaxes or non-hoaxes on the basis of the users
who ""liked"" them. We present two classification techniques, one based on
logistic regression, the other on a novel adaptation of boolean crowdsourcing
algorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users,
we obtain classification accuracies exceeding 99% even when the training set
contains less than 1% of the posts. We further show that our techniques are
robust: they work even when we restrict our attention to the users who like
both hoax and non-hoax posts. These results suggest that mapping the diffusion
pattern of information can be a useful component of automatic hoax detection
systems.","['Eugenio Tacchini', 'Gabriele Ballarin', 'Marco L. Della Vedova', 'Stefano Moret', 'Luca de Alfaro']",4,0.7166375
"Chatbots are one class of intelligent, conversational software agents
activated by natural language input (which can be in the form of text, voice,
or both). They provide conversational output in response, and if commanded, can
sometimes also execute tasks. Although chatbot technologies have existed since
the 1960s and have influenced user interface development in games since the
early 1980s, chatbots are now easier to train and implement. This is due to
plentiful open source code, widely available development platforms, and
implementation options via Software as a Service (SaaS). In addition to
enhancing customer experiences and supporting learning, chatbots can also be
used to engineer social harm - that is, to spread rumors and misinformation, or
attack people for posting their thoughts and opinions online. This paper
presents a literature review of quality issues and attributes as they relate to
the contemporary issue of chatbot development and implementation. Finally,
quality assessment approaches are reviewed, and a quality assessment method
based on these attributes and the Analytic Hierarchy Process (AHP) is proposed
and examined.","['Nicole M. Radziwill', 'Morgan C. Benton']",13,0.6271292
"We investigate the impact of misinformation about the contact structure on
the ability to predict disease outbreaks. We base our study on 31 empirical
temporal networks and tune the frequencies in errors in the node identities or
timestamps of contacts. We find that for both these spreading scenarios, the
maximal misprediction of both the outbreak size and time to extinction follows
an stretched exponential convergence as a function of the error frequency. We
furthermore determine the temporal-network structural factors influencing the
parameters of this convergence.","['Petter Holme', 'Luis E C Rocha']",2,0.6363661
"In 2010, a paper entitled ""From Obscurity to Prominence in Minutes: Political
Speech and Real-time search"" won the Best Paper Prize of the Web Science 2010
Conference. Among its findings were the discovery and documentation of what was
termed a ""Twitter-bomb"", an organized effort to spread misinformation about the
democratic candidate Martha Coakley through anonymous Twitter accounts. In this
paper, after summarizing the details of that event, we outline the recipe of
how social networks are used to spread misinformation. One of the most
important steps in such a recipe is the ""infiltration"" of a community of users
who are already engaged in conversations about a topic, to use them as organic
spreaders of misinformation in their extended subnetworks. Then, we take this
misinformation spreading recipe and indicate how it was successfully used to
spread fake news during the 2016 U.S. Presidential Election. The main
differences between the scenarios are the use of Facebook instead of Twitter,
and the respective motivations (in 2010: political influence; in 2016:
financial benefit through online advertising). After situating these events in
the broader context of exploiting the Web, we seize this opportunity to address
limitations of the reach of research findings and to start a conversation about
how communities of researchers can increase their impact on real-world societal
issues.","['Eni Mustafaraj', 'Panagiotis Takis Metaxas']",10,0.7424582
"The topic of fake news has drawn attention both from the public and the
academic communities. Such misinformation has the potential of affecting public
opinion, providing an opportunity for malicious parties to manipulate the
outcomes of public events such as elections. Because such high stakes are at
play, automatically detecting fake news is an important, yet challenging
problem that is not yet well understood. Nevertheless, there are three
generally agreed upon characteristics of fake news: the text of an article, the
user response it receives, and the source users promoting it. Existing work has
largely focused on tailoring solutions to one particular characteristic which
has limited their success and generality. In this work, we propose a model that
combines all three characteristics for a more accurate and automated
prediction. Specifically, we incorporate the behavior of both parties, users
and articles, and the group behavior of users who propagate fake news.
Motivated by the three characteristics, we propose a model called CSI which is
composed of three modules: Capture, Score, and Integrate. The first module is
based on the response and text; it uses a Recurrent Neural Network to capture
the temporal pattern of user activity on a given article. The second module
learns the source characteristic based on the behavior of users, and the two
are integrated with the third module to classify an article as fake or not.
Experimental analysis on real-world data demonstrates that CSI achieves higher
accuracy than existing models, and extracts meaningful latent representations
of both users and articles.","['Natali Ruchansky', 'Sungyong Seo', 'Yan Liu']",4,0.77665615
"The rising attention to the spreading of fake news and unsubstantiated rumors
on online social media and the pivotal role played by confirmation bias led
researchers to investigate different aspects of the phenomenon. Experimental
evidence showed that confirmatory information gets accepted even if containing
deliberately false claims while dissenting information is mainly ignored or
might even increase group polarization. It seems reasonable that, to address
misinformation problem properly, we have to understand the main determinants
behind content consumption and the emergence of narratives on online social
media. In this paper we address such a challenge by focusing on the discussion
around the Italian Constitutional Referendum by conducting a quantitative,
cross-platform analysis on both Facebook public pages and Twitter accounts. We
observe the spontaneous emergence of well-separated communities on both
platforms. Such a segregation is completely spontaneous, since no
categorization of contents was performed a priori. By exploring the dynamics
behind the discussion, we find that users tend to restrict their attention to a
specific set of Facebook pages/Twitter accounts. Finally, taking advantage of
automatic topic extraction and sentiment analysis techniques, we are able to
identify the most controversial topics inside and across both platforms. We
measure the distance between how a certain topic is presented in the
posts/tweets and the related emotional response of users. Our results provide
interesting insights for the understanding of the evolution of the core
narratives behind different echo chambers and for the early detection of
massive viral phenomena around false claims.","['Michela Del Vicario', 'Sabrina Gaito', 'Walter Quattrociocchi', 'Matteo Zignani', 'Fabiana Zollo']",3,0.8856925
"Obtaining meaningful quantitative descriptions of the statistical dependence
within multivariate systems is a difficult open problem. Recently, the Partial
Information Decomposition (PID) was proposed to decompose mutual information
(MI) about a target variable into components which are redundant, unique and
synergistic within different subsets of predictor variables. Here, we propose
to apply the elegant formalism of the PID to multivariate entropy, resulting in
a Partial Entropy Decomposition (PED). We implement the PED with an entropy
redundancy measure based on pointwise common surprisal; a natural definition
which is closely related to the definition of MI. We show how this approach can
reveal the dyadic vs triadic generative structure of multivariate systems that
are indistinguishable with classical Shannon measures. The entropy perspective
also shows that misinformation is synergistic entropy and hence that MI itself
includes both redundant and synergistic effects. We show the relationships
between the PED and MI in two predictors, and derive two alternative
information decompositions which we illustrate on several example systems. This
reveals that in entropy terms, univariate predictor MI is not a proper subset
of the joint MI, and we suggest this previously unrecognised fact explains in
part why obtaining a consistent PID has proven difficult. The PED also allows
separate quantification of mechanistic redundancy (related to the function of
the system) versus source redundancy (arising from dependencies between
inputs); an important distinction which no existing methods can address. The
new perspective provided by the PED helps to clarify some of the difficulties
encountered with the PID approach and the resulting decompositions provide
useful tools for practical data analysis across a wide range of application
areas.",['Robin A. A. Ince'],2,0.6779827
"The purpose of this study was to do a dataset distribution analysis, a
classification performance analysis, and a topical analysis concerning what
people are tweeting about four disease characteristics: symptoms, transmission,
prevention, and treatment. A combination of natural language processing and
machine learning techniques were used to determine what people are tweeting
about Zika. Specifically, a two-stage classifier system was built to find
relevant tweets on Zika, and then categorize these into the four disease
categories. Tweets in each disease category were then examined using latent
dirichlet allocation (LDA) to determine the five main tweet topics for each
disease characteristic. Results 1,234,605 tweets were collected. Tweets by
males and females were similar (28% and 23% respectively). The classifier
performed well on the training and test data for relevancy (F=0.87 and 0.99
respectively) and disease characteristics (F=0.79 and 0.90 respectively). Five
topics for each category were found and discussed with a focus on the symptoms
category. Through this process, we demonstrate how misinformation can be
discovered so that public health officials can respond to the tweets with
misinformation.","['Michele Miller', 'Dr. Tanvi Banerjee', 'RoopTeja Muppalla', 'Dr. William Romine', 'Dr. Amit Sheth']",5,0.5758707
"Given the huge impact that Online Social Networks (OSN) had in the way people
get informed and form their opinion, they became an attractive playground for
malicious entities that want to spread misinformation, and leverage their
effect. In fact, misinformation easily spreads on OSN and is a huge threat for
modern society, possibly influencing also the outcome of elections, or even
putting people's life at risk (e.g., spreading ""anti-vaccines"" misinformation).
Therefore, it is of paramount importance for our society to have some sort of
""validation"" on information spreading through OSN. The need for a wide-scale
validation would greatly benefit from automatic tools.
  In this paper, we show that it is difficult to carry out an automatic
classification of misinformation considering only structural properties of
content propagation cascades. We focus on structural properties, because they
would be inherently difficult to be manipulated, with the the aim of
circumventing classification systems. To support our claim, we carry out an
extensive evaluation on Facebook posts belonging to conspiracy theories (as
representative of misinformation), and scientific news (representative of
fact-checked content). Our findings show that conspiracy content actually
reverberates in a way which is hard to distinguish from the one scientific
content does: for the classification mechanisms we investigated, classification
F1-score never exceeds 0.65 during content propagation stages, and is still
less than 0.7 even after propagation is complete.","['Mauro Conti', 'Daniele Lain', 'Riccardo Lazzeretti', 'Giulio Lovisotto', 'Walter Quattrociocchi']",3,0.74719346
"Social media are massive marketplaces where ideas and news compete for our
attention. Previous studies have shown that quality is not a necessary
condition for online virality and that knowledge about peer choices can distort
the relationship between quality and popularity. However, these results do not
explain the viral spread of low-quality information, such as the digital
misinformation that threatens our democracy. We investigate quality
discrimination in a stylized model of online social network, where individual
agents prefer quality information, but have behavioral limitations in managing
a heavy flow of information. We measure the relationship between the quality of
an idea and its likelihood to become prevalent at the system level. We find
that both information overload and limited attention contribute to a
degradation in the market's discriminative power. A good tradeoff between
discriminative power and diversity of information is possible according to the
model. However, calibration with empirical data characterizing information load
and finite attention in real social media reveals a weak correlation between
quality and popularity of information. In these realistic conditions, the model
predicts that high-quality information has little advantage over low-quality
information.","['Xiaoyan Qiu', 'Diego F. M. Oliveira', 'Alireza Sahami Shirazi', 'Alessandro Flammini', 'Filippo Menczer']",0,0.65509176
"Social networks allow rapid spread of ideas and innovations while the
negative information can also propagate widely. When the cascades with
different opinions reaching the same user, the cascade arriving first is the
most likely to be taken by the user. Therefore, once misinformation or rumor is
detected, a natural containment method is to introduce a positive cascade
competing against the rumor. Given a budget $k$, the rumor blocking problem
asks for $k$ seed users to trigger the spread of the positive cascade such that
the number of the users who are not influenced by rumor can be maximized. The
prior works have shown that the rumor blocking problem can be approximated
within a factor of $(1-1/e-\delta)$ by a classic greedy algorithm combined with
Monte Carlo simulation with the running time of $O(\frac{k^3mn\ln
n}{\delta^2})$, where $n$ and $m$ are the number of users and edges,
respectively. Unfortunately, the Monte-Carlo-simulation-based methods are
extremely time consuming and the existing algorithms either trade performance
guarantees for practical efficiency or vice versa. In this paper, we present a
randomized algorithm which runs in $O(\frac{km\ln n}{\delta^2})$ expected time
and provides a $(1-1/e-\delta)$-approximation with a high probability. The
experimentally results on both the real-world and synthetic social networks
have shown that the proposed randomized rumor blocking algorithm is much more
efficient than the state-of-the-art method and it is able to find the seed
nodes which are effective in limiting the spread of rumor.","['Guangmo Tong', 'Weili Wu', 'Ling Guo', 'Deying Li', 'Cong Liu', 'Bin Liu', 'Ding-Zhu Du']",2,0.65477526
"While social networks can provide an ideal platform for up-to-date
information from individuals across the world, it has also proved to be a place
where rumours fester and accidental or deliberate misinformation often emerges.
In this article, we aim to support the task of making sense from social media
data, and specifically, seek to build an autonomous message-classifier that
filters relevant and trustworthy information from Twitter. For our work, we
collected about 100 million public tweets, including users' past tweets, from
which we identified 72 rumours (41 true, 31 false). We considered over 80
trustworthiness measures including the authors' profile and past behaviour, the
social network connections (graphs), and the content of tweets themselves. We
ran modern machine-learning classifiers over those measures to produce
trustworthiness scores at various time windows from the outbreak of the rumour.
Such time-windows were key as they allowed useful insight into the progression
of the rumours. From our findings, we identified that our model was
significantly more accurate than similar studies in the literature. We also
identified critical attributes of the data that give rise to the
trustworthiness scores assigned. Finally we developed a software demonstration
that provides a visual user interface to allow the user to examine the
analysis.","['Georgios Giasemidis', 'Colin Singleton', 'Ioannis Agrafiotis', 'Jason R. C. Nurse', 'Alan Pilgrim', 'Chris Willis', 'Danica Vukadinovic Greetham']",3,0.70522916
"Online Social Networks explode with activity whenever a crisis event takes
place. Most content generated as part of this activity is a mixture of text and
images, and is particularly useful for first responders to identify popular
topics of interest and gauge the pulse and sentiment of citizens. While
multiple researchers have used text to identify, analyze and measure themes and
public sentiment during such events, little work has explored visual themes
floating on networks in the form of images, and the sentiment inspired by them.
Given the potential of visual content for influencing users' thoughts and
emotions, we perform a large scale analysis to compare popular themes and
sentiment across images and textual content posted on Facebook during the
terror attacks that took place in Paris in 2015. Using state-of-the-art image
summarization techniques, we discovered multiple visual themes which were
popular in images, but were not identifiable through text. We uncovered
instances of misinformation and false flag (conspiracy) theories among popular
image themes, which were not prominent in user generated textual content, and
can be of particular inter- est to first responders. Our analysis also revealed
that while textual content posted after the attacks reflected negative
sentiment, images inspired positive sentiment. To the best of our knowledge,
this is the first large scale study of images posted on social networks during
a crisis event.","['Prateek Dewan', 'Varun Bharadhwaj', 'Aditi Mithal', 'Anshuman Suri', 'Ponnurangam Kumaraguru']",3,0.6097344
"Misinformation under the form of rumor, hoaxes, and conspiracy theories
spreads on social media at alarming rates. One hypothesis is that, since social
media are shaped by homophily, belief in misinformation may be more likely to
thrive on those social circles that are segregated from the rest of the
network. One possible antidote is fact checking which, in some cases, is known
to stop rumors from spreading further. However, fact checking may also backfire
and reinforce the belief in a hoax. Here we take into account the combination
of network segregation, finite memory and attention, and fact-checking efforts.
We consider a compartmental model of two interacting epidemic processes over a
network that is segregated between gullible and skeptic users. Extensive
simulation and mean-field analysis show that a more segregated network
facilitates the spread of a hoax only at low forgetting rates, but has no
effect when agents forget at faster rates. This finding may inform the
development of mitigation techniques and overall inform on the risks of
uncontrolled misinformation online.","['Marcella Tambuscio', 'Diego F. M. Oliveira', 'Giovanni Luca Ciampaglia', 'Giancarlo Ruffo']",3,0.70926964
"The massive diffusion of online social media allows for the rapid and
uncontrolled spreading of conspiracy theories, hoaxes, unsubstantiated claims,
and false news. Such an impressive amount of misinformation can influence
policy preferences and encourage behaviors strongly divergent from recommended
practices. In this paper, we study the statistical properties of viral
misinformation in online social media. By means of methods belonging to Extreme
Value Theory, we show that the number of extremely viral posts over time
follows a homogeneous Poisson process, and that the interarrival times between
such posts are independent and identically distributed, following an
exponential distribution. Moreover, we characterize the uncertainty around the
rate parameter of the Poisson process through Bayesian methods. Finally, we are
able to derive the predictive posterior probability distribution of the number
of posts exceeding a certain threshold of shares over a finite interval of
time.",['Alessandro Bessi'],2,0.71514666
"In online social networks, users tend to select information that adhere to
their system of beliefs and to form polarized groups of like minded people.
Polarization as well as its effects on online social interactions have been
extensively investigated. Still, the relation between group formation and
personality traits remains unclear. A better understanding of the cognitive and
psychological determinants of online social dynamics might help to design more
efficient communication strategies and to challenge the digital misinformation
threat. In this work, we focus on users commenting posts published by US
Facebook pages supporting scientific and conspiracy-like narratives, and we
classify the personality traits of those users according to their online
behavior. We show that different and conflicting communities are populated by
users showing similar psychological profiles, and that the dominant personality
model is the same in both scientific and conspiracy echo chambers. Moreover, we
observe that the permanence within echo chambers slightly shapes users'
psychological profiles. Our results suggest that the presence of specific
personality traits in individuals lead to their considerable involvement in
supporting narratives inside virtual echo chambers.",['Alessandro Bessi'],3,0.7131773
"Massive amounts of misinformation have been observed to spread in
uncontrolled fashion across social media. Examples include rumors, hoaxes, fake
news, and conspiracy theories. At the same time, several journalistic
organizations devote significant efforts to high-quality fact checking of
online claims. The resulting information cascades contain instances of both
accurate and inaccurate information, unfold over multiple time scales, and
often reach audiences of considerable size. All these factors pose challenges
for the study of the social dynamics of online news sharing. Here we introduce
Hoaxy, a platform for the collection, detection, and analysis of online
misinformation and its related fact-checking efforts. We discuss the design of
the platform and present a preliminary analysis of a sample of public tweets
containing both fake news and fact checking. We find that, in the aggregate,
the sharing of fact-checking content typically lags that of misinformation by
10--20 hours. Moreover, fake news are dominated by very active users, while
fact checking is a more grass-roots activity. With the increasing risks
connected to massive online misinformation, social news observatories have the
potential to help researchers, journalists, and the general public understand
the dynamics of real and fake news sharing.","['Chengcheng Shao', 'Giovanni Luca Ciampaglia', 'Alessandro Flammini', 'Filippo Menczer']",4,0.7983106
"While most online social media accounts are controlled by humans, these
platforms also host automated agents called social bots or sybil accounts.
Recent literature reported on cases of social bots imitating humans to
manipulate discussions, alter the popularity of users, pollute content and
spread misinformation, and even perform terrorist propaganda and recruitment
actions. Here we present BotOrNot, a publicly-available service that leverages
more than one thousand features to evaluate the extent to which a Twitter
account exhibits similarity to the known characteristics of social bots. Since
its release in May 2014, BotOrNot has served over one million requests via our
website and APIs.","['Clayton A. Davis', 'Onur Varol', 'Emilio Ferrara', 'Alessandro Flammini', 'Filippo Menczer']",13,0.76607746
"Our opinions, which things we like or dislike, depend on the opinions of
those around us. Nowadays, we are influenced by the opinions of online
strangers, expressed in comments and ratings on online platforms. Here, we
perform novel ""academic A/B testing"" experiments with over 2,500 participants
to measure the extent of that influence. In our experiments, the participants
watch and evaluate videos on mirror proxies of YouTube and Vimeo. We control
the comments and ratings that are shown underneath each of these videos. Our
study shows that from 5$\%$ up to 40$\%$ of subjects adopt the majority opinion
of strangers expressed in the comments. Using Bayes' theorem, we derive a
flexible and interpretable family of models of social influence, in which each
individual forms posterior opinions stochastically following a logit model. The
variants of our mixture model that maximize Akaike information criterion
represent two sub-populations, i.e., non-influenceable and influenceable
individuals. The prior opinions of the non-influenceable individuals are
strongly correlated with the external opinions and have low standard error,
whereas the prior opinions of influenceable individuals have high standard
error and become correlated with the external opinions due to social influence.
Our findings suggest that opinions are random variables updated via Bayes' rule
whose standard deviation is correlated with opinion influenceability. Based on
these findings, we discuss how to hinder opinion manipulation and
misinformation diffusion in the online realm.","['Przemyslaw A. Grabowicz', 'Francisco Romero-Ferrero', 'Theo Lins', 'Fabr√≠cio Benevenuto', 'Krishna P. Gummadi', 'Gonzalo G. de Polavieja']",3,0.6680558
"The wide availability of user-provided content in online social media
facilitates the aggregation of people around common interests, worldviews, and
narratives. Despite the enthusiastic rhetoric on the part of some that this
process generates ""collective intelligence"", the WWW also allows the rapid
dissemination of unsubstantiated conspiracy theories that often elicite rapid,
large, but naive social responses such as the recent case of Jade Helm 15 --
where a simple military exercise turned out to be perceived as the beginning of
the civil war in the US. We study how Facebook users consume information
related to two different kinds of narrative: scientific and conspiracy news. We
find that although consumers of scientific and conspiracy stories present
similar consumption patterns with respect to content, the sizes of the
spreading cascades differ. Homogeneity appears to be the primary driver for the
diffusion of contents, but each echo chamber has its own cascade dynamics. To
mimic these dynamics, we introduce a data-driven percolation model on signed
networks.","['Michela Del Vicario', 'Alessandro Bessi', 'Fabiana Zollo', 'Fabio Petroni', 'Antonio Scala', 'Guido Caldarelli', 'H. Eugene Stanley', 'Walter Quattrociocchi']",3,0.7790216
"Microblogging websites like Twitter have been shown to be immensely useful
for spreading information on a global scale within seconds. The detrimental
effect, however, of such platforms is that misinformation and rumors are also
as likely to spread on the network as credible, verified information. From a
public health standpoint, the spread of misinformation creates unnecessary
panic for the public. We recently witnessed several such scenarios during the
outbreak of Ebola in 2014 [14, 1]. In order to effectively counter the medical
misinformation in a timely manner, our goal here is to study the nature of such
misinformation and rumors in the United States during fall 2014 when a handful
of Ebola cases were confirmed in North America. It is a well known convention
on Twitter to use hashtags to give context to a Twitter message (a tweet). In
this study, we collected approximately 47M tweets from the Twitter streaming
API related to Ebola. Based on hashtags, we propose a method to classify the
tweets into two sets: credible and speculative. We analyze these two sets and
study how they differ in terms of a number of features extracted from the
Twitter API. In conclusion, we infer several interesting differences between
the two sets. We outline further potential directions to using this material
for monitoring and separating speculative tweets from credible ones, to enable
improved public health information.","['Janani Kalyanam', 'Sumithra Velupillai', 'Son Doan', 'Mike Conway', 'Gert Lanckriet']",5,0.6800736
"Social and political bots have a small but strategic role in Venezuelan
political conversations. These automated scripts generate content through
social media platforms and then interact with people. In this preliminary study
on the use of political bots in Venezuela, we analyze the tweeting, following
and retweeting patterns for the accounts of prominent Venezuelan politicians
and prominent Venezuelan bots. We find that bots generate a very small
proportion of all the traffic about political life in Venezuela. Bots are used
to retweet content from Venezuelan politicians but the effect is subtle in that
less than 10 percent of all retweets come from bot-related platforms.
Nonetheless, we find that the most active bots are those used by Venezuela's
radical opposition. Bots are pretending to be political leaders, government
agencies and political parties more than citizens. Finally, bots are promoting
innocuous political events more than attacking opponents or spreading
misinformation.","['Michelle Forelle', 'Phil Howard', 'Andr√©s Monroy-Hern√°ndez', 'Saiph Savage']",13,0.6517345
"According to the World Economic Forum, the diffusion of unsubstantiated
rumors on online social media is one of the main threats for our society.
  The disintermediated paradigm of content production and consumption on online
social media might foster the formation of homophile communities
(echo-chambers) around specific worldviews. Such a scenario has been shown to
be a vivid environment for the diffusion of false claims, in particular with
respect to conspiracy theories. Not rarely, viral phenomena trigger naive (and
funny) social responses -- e.g., the recent case of Jade Helm 15 where a simple
military exercise turned out to be perceived as the beginning of the civil war
in the US. In this work, we address the emotional dynamics of collective
debates around distinct kind of news -- i.e., science and conspiracy news --
and inside and across their respective polarized communities (science and
conspiracy news).
  Our findings show that comments on conspiracy posts tend to be more negative
than on science posts. However, the more the engagement of users, the more they
tend to negative commenting (both on science and conspiracy). Finally, zooming
in at the interaction among polarized communities, we find a general negative
pattern. As the number of comments increases -- i.e., the discussion becomes
longer -- the sentiment of the post is more and more negative.","['Fabiana Zollo', 'Petra Kralj Novak', 'Michela Del Vicario', 'Alessandro Bessi', 'Igor Mozetic', 'Antonio Scala', 'Guido Caldarelli', 'Walter Quattrociocchi']",3,0.7213505
"Social media enabled a direct path from producer to consumer of contents
changing the way users get informed, debate, and shape their worldviews. Such a
{\em disintermediation} weakened consensus on social relevant issues in favor
of rumors, mistrust, and fomented conspiracy thinking -- e.g., chem-trails
inducing global warming, the link between vaccines and autism, or the New World
Order conspiracy.
  In this work, we study through a thorough quantitative analysis how different
conspiracy topics are consumed in the Italian Facebook. By means of a
semi-automatic topic extraction strategy, we show that the most discussed
contents semantically refer to four specific categories: {\em environment},
{\em diet}, {\em health}, and {\em geopolitics}. We find similar patterns by
comparing users activity (likes and comments) on posts belonging to different
semantic categories. However, if we focus on the lifetime -- i.e., the distance
in time between the first and the last comment for each user -- we notice a
remarkable difference within narratives -- e.g., users polarized on geopolitics
are more persistent in commenting, whereas the less persistent are those
focused on diet related topics. Finally, we model users mobility across various
topics finding that the more a user is active, the more he is likely to join
all topics. Once inside a conspiracy narrative users tend to embrace the
overall corpus.","['Alessandro Bessi', 'Fabiana Zollo', 'Michela Del Vicario', 'Antonio Scala', 'Guido Caldarelli', 'Walter Quattrociocchi']",3,0.7940436
"The computer science research community has became increasingly interested in
the study of social media due to their pervasiveness in the everyday life of
millions of individuals. Methodological questions and technical challenges
abound as more and more data from social platforms become available for
analysis. This data deluge not only yields the unprecedented opportunity to
unravel questions about online individuals' behavior at scale, but also allows
to explore the potential perils that the massive adoption of social media
brings to our society. These communication channels provide plenty of
incentives (both economical and social) and opportunities for abuse. As social
media activity became increasingly intertwined with the events in the offline
world, individuals and organizations have found ways to exploit these platforms
to spread misinformation, to attack and smear others, or to deceive and
manipulate. During crises, social media have been effectively used for
emergency response, but fear-mongering actions have also triggered mass
hysteria and panic. Criminal gangs and terrorist organizations like ISIS adopt
social media for propaganda and recruitment. Synthetic activity and social bots
have been used to coordinate orchestrated astroturf campaigns, to manipulate
political elections and the stock market. The lack of effective content
verification systems on many of these platforms, including Twitter and
Facebook, rises concerns when younger users become exposed to cyber-bulling,
harassment, or hate speech, inducing risks like depression and suicide. This
article illustrates some of the recent advances facing these issues and
discusses what it remains to be done, including the challenges to address in
the future to make social media a more useful and accessible, safer and
healthier environment for all users.",['Emilio Ferrara'],3,0.6708462
"Social media have quickly become a prevalent channel to access information,
spread ideas, and influence opinions. However, it has been suggested that
social and algorithmic filtering may cause exposure to less diverse points of
view, and even foster polarization and misinformation. Here we explore and
validate this hypothesis quantitatively for the first time, at the collective
and individual levels, by mining three massive datasets of web traffic, search
logs, and Twitter posts. Our analysis shows that collectively, people access
information from a significantly narrower spectrum of sources through social
media and email, compared to search. The significance of this finding for
individual exposure is revealed by investigating the relationship between the
diversity of information sources experienced by users at the collective and
individual level. There is a strong correlation between collective and
individual diversity, supporting the notion that when we use social media we
find ourselves inside ""social bubbles"". Our results could lead to a deeper
understanding of how technology biases our exposure to new information.","['Dimitar Nikolov', 'Diego F. M. Oliveira', 'Alessandro Flammini', 'Filippo Menczer']",3,0.7612633
"Traditional fact checking by expert journalists cannot keep up with the
enormous volume of information that is now generated online. Computational fact
checking may significantly enhance our ability to evaluate the veracity of
dubious information. Here we show that the complexities of human fact checking
can be approximated quite well by finding the shortest path between concept
nodes under properly defined semantic proximity metrics on knowledge graphs.
Framed as a network problem this approach is feasible with efficient
computational techniques. We evaluate this approach by examining tens of
thousands of claims related to history, entertainment, geography, and
biographical information using a public knowledge graph extracted from
Wikipedia. Statements independently known to be true consistently receive
higher support via our method than do false ones. These findings represent a
significant step toward scalable computational fact-checking methods that may
one day mitigate the spread of harmful misinformation.","['Giovanni Luca Ciampaglia', 'Prashant Shiralkar', 'Luis M. Rocha', 'Johan Bollen', 'Filippo Menczer', 'Alessandro Flammini']",1,0.7121654
"This paper considers online reputation and polling systems where individuals
make recommendations based on their private observations and recommendations of
friends. Such interaction of individuals and their social influence is modelled
as social learning on a directed acyclic graph. Data incest (misinformation
propagation) occurs due to unintentional re-use of identical actions in the
for- mation of public belief in social learning; the information gathered by
each agent is mistakenly considered to be independent. This results in
overconfidence and bias in estimates of the state. Necessary and sufficient
conditions are given on the structure of information exchange graph to mitigate
data incest. Incest removal algorithms are presented. Experimental results on
human subjects are presented to illustrate the effect of social influence and
data incest on decision making. These experimental results indicate that social
learning protocols require careful design to handle and mitigate data incest.
The incest removal algorithms are illustrated in an expectation polling system
where participants in a poll respond with a summary of their friends' beliefs.
Finally, the principle of revealed preferences arising in micro-economics
theory is used to parse Twitter datasets to determine if social sensors are
utility maximizers and then determine their utility functions.","['Vikram Krishnamurthy', 'William Hoiles']",3,0.6451356
"Social media have become part of modern news reporting, used by journalists
to spread information and find sources, or as a news source by individuals. The
quest for prominence and recognition on social media sites like Twitter can
sometimes eclipse accuracy and lead to the spread of false information. As a
way to study and react to this trend, we introduce {\sc TwitterTrails}, an
interactive, web-based tool ({\tt twittertrails.com}) that allows users to
investigate the origin and propagation characteristics of a rumor and its
refutation, if any, on Twitter. Visualizations of burst activity, propagation
timeline, retweet and co-retweeted networks help its users trace the spread of
a story. Within minutes {\sc TwitterTrails} will collect relevant tweets and
automatically answer several important questions regarding a rumor: its
originator, burst characteristics, propagators and main actors according to the
audience. In addition, it will compute and report the rumor's level of
visibility and, as an example of the power of crowdsourcing, the audience's
skepticism towards it which correlates with the rumor's credibility. We
envision {\sc TwitterTrails} as valuable tool for individual use, but we
especially for amateur and professional journalists investigating recent and
breaking stories. Further, its expanding collection of investigated rumors can
be used to answer questions regarding the amount and success of misinformation
on Twitter.","['Samantha Finn', 'Panagiotis Takis Metaxas', 'Eni Mustafaraj']",10,0.71171224
"The spreading of unsubstantiated rumors on online social networks (OSN)
either unintentionally or intentionally (e.g., for political reasons or even
trolling) can have serious consequences such as in the recent case of rumors
about Ebola causing disruption to health-care workers. Here we show that
indicators aimed at quantifying information consumption patterns might provide
important insights about the virality of false claims. In particular, we
address the driving forces behind the popularity of contents by analyzing a
sample of 1.2M Facebook Italian users consuming different (and opposite) types
of information (science and conspiracy news). We show that users' engagement
across different contents correlates with the number of friends having similar
consumption patterns (homophily), indicating the area in the social network
where certain types of contents are more likely to spread. Then, we test
diffusion patterns on an external sample of $4,709$ intentional satirical false
claims showing that neither the presence of hubs (structural properties) nor
the most active users (influencers) are prevalent in viral phenomena. Instead,
we found out that in an environment where misinformation is pervasive, users'
aggregation around shared beliefs may make the usual exposure to conspiracy
stories (polarization) a determinant for the virality of false information.","['Aris Anagnostopoulos', 'Alessandro Bessi', 'Guido Caldarelli', 'Michela Del Vicario', 'Fabio Petroni', 'Antonio Scala', 'Fabiana Zollo', 'Walter Quattrociocchi']",3,0.77558327
"Despite the enthusiastic rhetoric about the so called \emph{collective
intelligence}, conspiracy theories -- e.g. global warming induced by chemtrails
or the link between vaccines and autism -- find on the Web a natural medium for
their dissemination. Users preferentially consume information according to
their system of beliefs and the strife within users of opposite narratives may
result in heated debates. In this work we provide a genuine example of
information consumption from a sample of 1.2 million of Facebook Italian users.
We show by means of a thorough quantitative analysis that information
supporting different worldviews -- i.e. scientific and conspiracist news -- are
consumed in a comparable way by their respective users. Moreover, we measure
the effect of the exposure to 4709 evidently false information (satirical
version of conspiracy theses) and to 4502 debunking memes (information aiming
at contrasting unsubstantiated rumors) of the most polarized users of
conspiracy claims. We find that either contrasting or teasing consumers of
conspiracy narratives increases their probability to interact again with
unsubstantiated rumors.","['Alessandro Bessi', 'Guido Caldarelli', 'Michela Del Vicario', 'Antonio Scala', 'Walter Quattrociocchi']",3,0.74009705
"This paper presents an experimental study to investigate the learning and
decision making behavior of individuals in a human society. Social learning is
used as the mathematical basis for modelling interaction of individuals that
aim to perform a perceptual task interactively. A psychology experiment was
conducted on a group of undergraduate students at the University of British
Columbia to examine whether the decision (action) of one individual affects the
decision of the subsequent individuals. The major experimental observation that
stands out here is that the participants of the experiment (agents) were
affected by decisions of their partners in a relatively large fraction (60%) of
trials. We fit a social learning model that mimics the interactions between
participants of the psychology experiment. Misinformation propagation (also
known as data incest) within the society under study is further investigated in
this paper.","['Maziyar Hamdi', 'Grayden Solman', 'Alan Kingstone', 'Vikram Krishnamurthy']",0,0.53769684
"The spread of rumors through social media and online social networks can not
only disrupt the daily lives of citizens but also result in loss of life and
property. A rumor spreads when individuals, who are unable decide the
authenticity of the information, mistake the rumor as genuine information and
pass it on to their acquaintances. We propose a solution where a set of
individuals (based on their degree) in the social network are trained and
provided resources to help them distinguish a rumor from genuine information.
By formulating an optimization problem we calculate the optimum set of
individuals, who must undergo training, and the quality of training that
minimizes the expected training cost and ensures an upper bound on the size of
the rumor outbreak. Our primary contribution is that although the optimization
problem turns out to be non convex, we show that the problem is equivalent to
solving a set of linear programs. This result also allows us to solve the
problem of minimizing the size of rumor outbreak for a given cost budget. The
optimum solution displays an interesting pattern which can be implemented as a
heuristic. These results can prove to be very useful for social planners and
law enforcement agencies for preventing dangerous rumors and misinformation
epidemics.","['Bhushan Kotnis', 'Joy Kuri']",2,0.59949374
"In this work we study, on a sample of 2.3 million individuals, how Facebook
users consumed different information at the edge of political discussion and
news during the last Italian electoral competition. Pages are categorized,
according to their topics and the communities of interests they pertain to, in
a) alternative information sources (diffusing topics that are neglected by
science and main stream media); b) online political activism; and c) main
stream media. We show that attention patterns are similar despite the different
qualitative nature of the information, meaning that unsubstantiated claims
(mainly conspiracy theories) reverberate for as long as other information.
Finally, we categorize users according to their interaction patterns among the
different topics and measure how a sample of this social ecosystem (1279 users)
responded to the injection of 2788 false information posts. Our analysis
reveals that users which are prominently interacting with alternative
information sources (i.e. more exposed to unsubstantiated claims) are more
prone to interact with false claims.","['Delia Mocanu', 'Luca Rossi', 'Qian Zhang', 'M√†rton Karsai', 'Walter Quattrociocchi']",3,0.827167
"The fact that finite direct sums of two or more mutually different spaces
from the family $\{\ell_{p} : 1\le p<\infty\}\cup c_{0}$ fail to have greedy
bases is stated in [Dilworth et al., Greedy bases for Besov spaces, Constr.
Approx. 34 (2011), no. 2, 281-296]. However, the concise proof that the authors
give of this fundamental result in greedy approximation relies on a fallacious
argument, namely the alleged uniqueness of unconditional basis up to
permutation of the spaces involved. The main goal of this note is to settle the
problem by providing a correct proof. For that we first show that all greedy
bases in an $\ell_{p}$ space have fundamental functions of the same order. As a
by-product of our work we obtain that every almost greedy basis of a Banach
space with unconditional basis and nontrivial type contains a greedy subbasis.","['Fernando Albiac', 'Jos√© L. Ansorena']",2,0.38136965
"The increasing pervasiveness of social media creates new opportunities to
study human social behavior, while challenging our capability to analyze their
massive data streams. One of the emerging tasks is to distinguish between
different kinds of activities, for example engineered misinformation campaigns
versus spontaneous communication. Such detection problems require a formal
definition of meme, or unit of information that can spread from person to
person through the social network. Once a meme is identified, supervised
learning methods can be applied to classify different types of communication.
The appropriate granularity of a meme, however, is hardly captured from
existing entities such as tags and keywords. Here we present a framework for
the novel task of detecting memes by clustering messages from large streams of
social data. We evaluate various similarity measures that leverage content,
metadata, network features, and their combinations. We also explore the idea of
pre-clustering on the basis of existing entities. A systematic evaluation is
carried out using a manually curated dataset as ground truth. Our analysis
shows that pre-clustering and a combination of heterogeneous features yield the
best trade-off between number of clusters and their quality, demonstrating that
a simple combination based on pairwise maximization of similarity is as
effective as a non-trivial optimization of parameters. Our approach is fully
automatic, unsupervised, and scalable for real-time detection of memes in
streaming data.","['Emilio Ferrara', 'Mohsen JafariAsbagh', 'Onur Varol', 'Vahed Qazvinian', 'Filippo Menczer', 'Alessandro Flammini']",2,0.77349114
"Viral spread on large graphs has many real-life applications such as malware
propagation in computer networks and rumor (or misinformation) spread in
Twitter-like online social networks. Although viral spread on large graphs has
been intensively analyzed on classical models such as
Susceptible-Infectious-Recovered, there still exits a deficit of effective
methods in practice to contain epidemic spread once it passes a critical
threshold. Against this backdrop, we explore methods of containing viral spread
in large networks with the focus on sparse random networks. The viral
containment strategy is to partition a large network into small components and
then to ensure the sanity of all messages delivered across different
components. With such a defense mechanism in place, an epidemic spread starting
from any node is limited to only those nodes belonging to the same component as
the initial infection node. We establish both lower and upper bounds on the
costs of inspecting inter-component messages. We further propose
heuristic-based approaches to partition large input graphs into small
components. Finally, we study the performance of our proposed algorithms under
different network topologies and different edge weight models.","['Milan Bradonjiƒá', 'Michael Molloy', 'Guanhua Yan']",2,0.77263415
"Managing large-scale transportation infrastructure projects is difficult due
to frequent misinformation about the costs which results in large cost overruns
that often threaten the overall project viability. This paper investigates the
explanations for cost overruns that are given in the literature. Overall, four
categories of explanations can be distinguished: technical, economic,
psychological, and political. Political explanations have been seen to be the
most dominant explanations for cost overruns. Agency theory is considered the
most interesting for political explanations and an eclectic theory is also
considered possible. Nonpolitical explanations are diverse in character,
therefore a range of different theories (including rational choice theory and
prospect theory), depending on the kind of explanation is considered more
appropriate than one all-embracing theory.","['Chantal C. Cantarelli', 'Bent Flybjerg', 'Eric J. E. Molin', 'Bert van Wee']",2,0.420464
"Consider a complete communication network on $n$ nodes, each of which is a
state machine. In synchronous 2-counting, the nodes receive a common clock
pulse and they have to agree on which pulses are ""odd"" and which are ""even"". We
require that the solution is self-stabilising (reaching the correct operation
from any initial state) and it tolerates $f$ Byzantine failures (nodes that
send arbitrary misinformation). Prior algorithms are expensive to implement in
hardware: they require a source of random bits or a large number of states.
  This work consists of two parts. In the first part, we use computational
techniques (often known as synthesis) to construct very compact deterministic
algorithms for the first non-trivial case of $f = 1$. While no algorithm exists
for $n < 4$, we show that as few as 3 states per node are sufficient for all
values $n \ge 4$. Moreover, the problem cannot be solved with only 2 states per
node for $n = 4$, but there is a 2-state solution for all values $n \ge 6$.
  In the second part, we develop and compare two different approaches for
synthesising synchronous counting algorithms. Both approaches are based on
casting the synthesis problem as a propositional satisfiability (SAT) problem
and employing modern SAT-solvers. The difference lies in how to solve the SAT
problem: either in a direct fashion, or incrementally within a counter-example
guided abstraction refinement loop. Empirical results suggest that the former
technique is more efficient if we want to synthesise time-optimal algorithms,
while the latter technique discovers non-optimal algorithms more quickly.","['Danny Dolev', 'Keijo Heljanko', 'Matti J√§rvisalo', 'Janne H. Korhonen', 'Christoph Lenzen', 'Joel Rybicki', 'Jukka Suomela', 'Siert Wieringa']",2,0.5184398
"This paper argues, first, that a major problem in the planning of large
infrastructure projects is the high level of misinformation about costs and
benefits that decision makers face in deciding whether to build, and the high
risks such misinformation generates. Second, it explores the causes of
misinformation and risk, mainly in the guise of optimism bias and strategic
misrepresentation. Finally, the paper presents a number of measures aimed at
improving planning and decision making for large infrastructure projects,
including changed incentive structures and better planning methods. Thus the
paper is organized as a simple triptych consisting in problems, causes, and
cures.",['Bent Flyvbjerg'],0,0.5007246
"Back cover text: Megaprojects and Risk provides the first detailed
examination of the phenomenon of megaprojects. It is a fascinating account of
how the promoters of multibillion-dollar megaprojects systematically and
self-servingly misinform parliaments, the public and the media in order to get
projects approved and built. It shows, in unusual depth, how the formula for
approval is an unhealthy cocktail of underestimated costs, overestimated
revenues, undervalued environmental impacts and overvalued economic development
effects. This results in projects that are extremely risky, but where the risk
is concealed from MPs, taxpayers and investors. The authors not only explore
the problems but also suggest practical solutions drawing on theory and hard,
scientific evidence from the several hundred projects in twenty nations that
illustrate the book. Accessibly written, it will be essential reading in its
field for students, scholars, planners, economists, auditors, politicians,
journalists and interested citizens.","['Bent Flyvbjerg', 'Nils Bruzelius', 'Werner Rothengatter']",0,0.4983353
"This report contains results of an experimental study of the distribution of
misinformation in online social networks (OSNs). We consider the classification
of the topologies of OSNs and analyze the parameters identified in order to
relate the topology of a real network with one of the classes. We propose an
algorithm for conducting a search for the percolation cluster in the social
graph.","['Konstantin Abramov', 'Yuri Monakhov']",2,0.5947804
"This paper presents the research of the influence of cognitive, behavioral,
representational factors on the susceptibility of the participants in social
networks to misinformation, as well as on the activity of the nodes in this
regard. The importance of this research consists of method of blocking the
propaganda. This is very important because when people involuntarily acquire
information some of them experience an undesired change in their social
attitude. Such phenomena typically lead towards the information warfare. A
model was developed during this research for calculating the level of
misinformation of the social network participant (network node) based on the
model of iterative learning process.","['Yuri Monakhov', 'Maria Medvednikova', 'Konstantin Abramov', 'Natalia Kostina', 'Roman Malyshev', 'Makarov Oleg', 'Irina Semenova']",0,0.7603117
"This article is a commentary on the verdict of the ""L'Aquila Six"", the group
of bureaucrats and scientists tried by an Italian court as a result of their
public statements in advance of the quake of 2009 Apr. 6 that left the city in
ruins and cause more than 300 deaths. It was not the worst such catastrophic
event in recent Italian history, but it was one of -- if not the -- worst
failures of risk assessment and preventive action. The six were found guilty
and condemned by a first level of the justice system to substantial prison
terms. The outcry provoked by the verdict in the world press and the
international scientific community has fueled the already fiery debate over
whether the six should have been tried at all. They have been presented as
martyrs to science being treated as scapegoats by a scientifically illiterate
justice system and inflamed local population for not being able to perform the
impossible (predict the event). Petitions of support have been drafted and
signed by thousands of working scientists and technical experts in many fields
excoriating the court and the country for such an outrage against the
scientific community, often accompanied by ominous warnings about the chilling
effect this will have on the availability of expert advice in times of need. My
purpose in this essay is to explain why this view of the events of the trial is
misguided, however well intentioned, and misinformed.",['Steven N. Shore'],3,0.39384866
"According to the media, in spring of this year the experiment CDF at Fermilab
has made most likely (""this result has a 99.7 percent chance of being correct"",
Discovery News) a great discovery (""the most significant in physics in half a
century"", NYT). However, since the very beginning, practically all particle
physics experts did not believe that was the case. This is the last of a quite
long series of fake claims based on trivial mistakes in the probabilistic
reasoning. The main purpose of this note is to invite everybody, but especially
journalists and general public, most times innocent victims of misinformation
of this kind, to mistrust claims not explicitly reported in terms of how much
we should believe something, under well stated conditions and assumptions. (A
last minute appendix has been added, with comments on the recent news
concerning the Higgs at LHC.)","[""G. D'Agostini""]",5,0.39198804
"Online social media are complementing and in some cases replacing
person-to-person social interaction and redefining the diffusion of
information. In particular, microblogs have become crucial grounds on which
public relations, marketing, and political battles are fought. We introduce an
extensible framework that will enable the real-time analysis of meme diffusion
in social media by mining, visualizing, mapping, classifying, and modeling
massive streams of public microblogging events. We describe a Web service that
leverages this framework to track political memes in Twitter and help detect
astroturfing, smear campaigns, and other misinformation in the context of U.S.
political elections. We present some cases of abusive behaviors uncovered by
our service. Finally, we discuss promising preliminary results on the detection
of suspicious memes via supervised learning based on features extracted from
the topology of the diffusion networks, sentiment analysis, and crowdsourced
annotations.","['Jacob Ratkiewicz', 'Michael Conover', 'Mark Meiss', 'Bruno Gon√ßalves', 'Snehal Patil', 'Alessandro Flammini', 'Filippo Menczer']",3,0.71222025
"We provide a model to investigate the tension between information aggregation
and spread of misinformation in large societies (conceptualized as networks of
agents communicating with each other). Each individual holds a belief
represented by a scalar. Individuals meet pairwise and exchange information,
which is modeled as both individuals adopting the average of their pre-meeting
beliefs. When all individuals engage in this type of information exchange, the
society will be able to effectively aggregate the initial information held by
all individuals. There is also the possibility of misinformation, however,
because some of the individuals are ""forceful,"" meaning that they influence the
beliefs of (some) of the other individuals they meet, but do not change their
own opinion. The paper characterizes how the presence of forceful agents
interferes with information aggregation. Under the assumption that even
forceful agents obtain some information (however infrequent) from some others
(and additional weak regularity conditions), we first show that beliefs in this
class of societies converge to a consensus among all individuals. This
consensus value is a random variable, however, and we characterize its
behavior. Our main results quantify the extent of misinformation in the society
by either providing bounds or exact results (in some special cases) on how far
the consensus value can be from the benchmark without forceful agents (where
there is efficient information aggregation). The worst outcomes obtain when
there are several forceful agents and forceful agents themselves update their
beliefs only on the basis of information they obtain from individuals most
likely to have received their own information previously.","['Daron Acemoglu', 'Asuman Ozdaglar', 'Ali ParandehGheibi']",3,0.6888996
"We consider a simple modal logic whose non-modal part has conjunction and
disjunction as connectives and whose modalities come in adjoint pairs, but are
not in general closure operators. Despite absence of negation and implication,
and of axioms corresponding to the characteristic axioms of (e.g.) T, S4 and
S5, such logics are useful, as shown in previous work by Baltag, Coecke and the
first author, for encoding and reasoning about information and misinformation
in multi-agent systems. For such a logic we present an algebraic semantics,
using lattices with agent-indexed families of adjoint pairs of operators, and a
cut-free sequent calculus. The calculus exploits operators on sequents, in the
style of ""nested"" or ""tree-sequent"" calculi; cut-admissibility is shown by
constructive syntactic methods. The applicability of the logic is illustrated
by reasoning about the muddy children puzzle, for which the calculus is
augmented with extra rules to express the facts of the muddy children scenario.","['Mehrnoosh Sadrzadeh', 'Roy Dyckhoff']",2,0.4848283
"We show in detail that the Hawking temperature calculated from the surface
gravity is in agreement with the result of exact semi-classical radiation
spectrum for higher dimensional linear dilaton black holes in various theories.
We extend the method derived first by Cl\'ement-Fabris-Marques for
4-dimensional linear dilaton black hole solutions to the higher dimensions in
theories such as Einstein-Maxwell-Dilaton, Einstein-Yang-Mills-Dilaton and
Einstein-Yang-Mills-Born-Infeld-Dilaton. Similar to the
Cl\'ement-Fabris-Marques results, it is proved that whenever an analytic
solution is available to the massless scalar wave equation in the background of
higher dimensional massive linear dilaton black holes, an exact computation of
the radiation spectrum leads to the Hawking temperature T_{H} in the high
frequency regime. The significance of the dimensionality on the value of T_{H}
is shown, explicitly. For a chosen dimension, we demonstrate how higher
dimensional linear dilaton black holes interpolate between the black hole
solutions with Yang-Mills and electromagnetic fields by altering the
Born-Infeld parameter in aspect of measurable quantity T_{H}. Finally, we
explain the reason of, why massless higher dimensional linear dilaton black
holes cannot radiate.","['S. Habib Mazharimousavi', 'I. Sakalli', 'M. Halilsoy']",2,0.34527504
"Even professional baseball players occasionally find it difficult to
gracefully approach seemingly routine pop-ups. This paper describes a set of
towering pop-ups with trajectories that exhibit cusps and loops near the apex.
For a normal fly ball, the horizontal velocity is continuously decreasing due
to drag caused by air resistance. But for pop-ups, the Magnus force (the force
due to the ball spinning in a moving airflow) is larger than the drag force. In
these cases the horizontal velocity decreases in the beginning, like a normal
fly ball, but after the apex, the Magnus force accelerates the horizontal
motion. We refer to this class of pop-ups as paradoxical because they appear to
misinform the typically robust optical control strategies used by fielders and
lead to systematic vacillation in running paths, especially when a trajectory
terminates near the fielder. In short, some of the dancing around when
infielders pursue pop-ups can be well explained as a combination of bizarre
trajectories and misguidance by the normally reliable optical control strategy,
rather than apparent fielder error. Former major league infielders confirm that
our model agrees with their experiences.","['Michael K. McBeath', 'Alan M. Nathan', 'A. Terry Bahill', 'David G. Baldwin']",2,0.23707813
"Interpretation of cosmological data to determine the number and values of
parameters describing the universe must not rely solely on statistics but
involve physical insight. When statistical techniques such as ""model selection""
or ""integrated survey optimization"" blindly apply Occam's Razor, this can lead
to painful results. We emphasize that the sensitivity to prior probabilities
and to the number of models compared can lead to ""prior selection"" rather than
robust model selection. A concrete example demonstrates that Information
Criteria can in fact misinform over a large region of parameter space.","['Eric V. Linder', 'Ramon Miquel']",1,0.5393391
"We model self-assembly of information in networks to investigate necessary
conditions for building a global perception of a system by local communication.
Our approach is to let agents chat in a model system to self-organize distant
communication-pathways. We demonstrate that simple local rules allow agents to
build a perception of the system, that is robust to dynamical changes and
mistakes. We find that messages are most effectively forwarded in the presence
of hubs, while transmission in hub-free networks is more robust against
misinformation and failures.","['M. Rosvall', 'K. Sneppen']",2,0.60235345
